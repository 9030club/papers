<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>faletto23a - Paper Analysis</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem;
            border-radius: 10px;
            margin-bottom: 2rem;
            text-align: center;
        }
        
        .header h1 {
            margin: 0;
            font-size: 2.5rem;
            font-weight: 300;
        }
        
        .header .subtitle {
            margin-top: 0.5rem;
            opacity: 0.9;
            font-size: 1.1rem;
        }
        
        .analysis-section {
            background: white;
            border-radius: 10px;
            padding: 2rem;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .analysis-section h2 {
            color: #333;
            border-bottom: 3px solid #667eea;
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }
        
        .question-block {
            margin-bottom: 2rem;
            padding: 1.5rem;
            border-left: 4px solid #667eea;
            background-color: #f8f9ff;
            border-radius: 0 8px 8px 0;
        }
        
        .question {
            font-weight: 600;
            color: #4a5568;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }
        
        .answer {
            color: #2d3748;
            line-height: 1.7;
            font-size: 1rem;
        }
        
        .paper-content {
            background: white;
            border-radius: 10px;
            padding: 2rem;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .paper-content h2 {
            color: #333;
            border-bottom: 3px solid #764ba2;
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }
        
        .paper-content pre {
            background-color: #f7fafc;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        
        .metadata {
            background: #e2e8f0;
            padding: 1rem;
            border-radius: 5px;
            margin-bottom: 1rem;
            font-size: 0.9rem;
            color: #4a5568;
        }
        
        .nav {
            position: sticky;
            top: 20px;
            background: white;
            padding: 1rem;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 2rem;
        }
        
        .nav a {
            color: #667eea;
            text-decoration: none;
            margin-right: 1rem;
            font-weight: 500;
        }
        
        .nav a:hover {
            text-decoration: underline;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .header h1 {
                font-size: 2rem;
            }
            
            .analysis-section, .paper-content {
                padding: 1rem;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>faletto23a</h1>
        <div class="subtitle">AI-Powered Research Paper Analysis</div>
    </div>
    
    <nav class="nav">
        <a href="#analysis">Analysis</a>
        <a href="#paper-content">Paper Content</a>
    </nav>
    
    <section id="analysis" class="analysis-section">
        <h2>Research Questions Analysis</h2>
        <div class="metadata">
            <strong>Analysis Model:</strong> gpt-4o | 
            <strong>Questions Analyzed:</strong> 10 | 
            <strong>PDF:</strong> faletto23a.pdf
        </div>
        
        <div class="question-block">
            <div class="question">Q1: What were they trying to do?</div>
            <div class="answer">The main goal of the paper is to improve the estimation of probabilities for rare events, which are often the result of a sequence of more common intermediate outcomes. The authors propose a method called PRESTO, which relaxes the proportional odds model by allowing different weights for transitions between categories while imposing an L1 penalty to encourage sparsity in these differences. This approach aims to leverage the more abundant data from intermediate events to enhance the estimation accuracy of rare event probabilities, overcoming the limitations of traditional logistic regression and the inflexibility of the proportional odds model.</div>
        </div>
        <div class="question-block">
            <div class="question">Q2: Why does it matter?</div>
            <div class="answer">The paper addresses the challenge of predicting rare events in situations with severe class imbalance by leveraging more common intermediate outcomes in sequential processes. This matters because accurately estimating probabilities of rare events is crucial in fields like online marketing and public policy, where decisions often depend on these estimates. The proposed PRESTO model improves upon traditional methods by relaxing the rigid assumptions of the proportional odds model, allowing for better estimation of rare event probabilities by borrowing strength from more abundant data. This approach is significant for practitioners who need reliable probability estimates for rare events to make informed decisions.</div>
        </div>
        <div class="question-block">
            <div class="question">Q3: What did they try?</div>
            <div class="answer">The authors proposed a method called PRESTO, which is a relaxation of the proportional odds model for ordinal regression. Instead of assuming that the decision boundaries between categories are governed by the same set of weights, PRESTO estimates separate weights for each transition between categories. The method imposes an L1 penalty on the differences between weights for the same feature in adjacent weight vectors, allowing for flexibility while leveraging more abundant data from intermediate events to improve the estimation of probabilities for rare events. This approach is designed to be more flexible than the proportional odds model and more effective than logistic regression in settings with class imbalance.</div>
        </div>
        <div class="question-block">
            <div class="question">Q4: Did it work?</div>
            <div class="answer">Yes, the proposed method, PRESTO, was effective in improving the estimation of probabilities for rare events. Theoretical results and experiments with both synthetic and real data demonstrated that PRESTO outperformed logistic regression and the proportional odds model, particularly in settings with sparse differences between decision boundary parameters. PRESTO was able to leverage more common intermediate events to enhance the estimation of rare event probabilities, even when the proportional odds assumption did not hold exactly.</div>
        </div>
        <div class="question-block">
            <div class="question">Q5: What did they compare it to?</div>
            <div class="answer">The paper compares the proposed PRESTO method to logistic regression and the proportional odds model. PRESTO is shown to outperform these methods in estimating probabilities of rare events, as logistic regression struggles with class imbalance and does not leverage information from more common classes, while the proportional odds model is too inflexible due to its assumptions. The experiments demonstrate that PRESTO provides better estimates, particularly in settings with sparse differences between adjacent decision boundary parameter vectors.</div>
        </div>
        <div class="question-block">
            <div class="question">Q6: What was it tested on?</div>
            <div class="answer">The method proposed in the paper, PRESTO, was tested on both synthetic and real data experiments. In the synthetic experiments, data was generated to simulate scenarios with sparse and dense differences between adjacent decision boundary parameter vectors, with varying levels of class rarity. The real data experiments involved a soup tasting dataset and a diabetes diagnosis dataset, where the model's performance in estimating rare event probabilities was compared against logistic regression and the proportional odds model. The evaluation setup involved measuring the mean squared error of estimated probabilities and using cross-validation to select model parameters.</div>
        </div>
        <div class="question-block">
            <div class="question">Q7: What's cool about it?</div>
            <div class="answer">The novel aspect of the paper is the introduction of PRESTO, a method that relaxes the proportional odds model to better estimate probabilities of rare events by leveraging more common intermediate outcomes. This approach is clever because it imposes an L1 penalty on the differences between weights for the same feature in adjacent categories, allowing the model to borrow strength from abundant data without the rigidity of the proportional odds assumption. The method's elegance lies in its ability to consistently estimate decision boundary weights under a sparsity assumption, outperforming traditional logistic regression and proportional odds models in both synthetic and real data experiments.</div>
        </div>
        <div class="question-block">
            <div class="question">Q8: What's sketchy about it?</div>
            <div class="answer">The paper presents some red flags and limitations. The proportional odds assumption may be too rigid for many real-world scenarios, as it assumes identical decision boundaries across classes, which might not hold true if features have varying influences at different stages. Additionally, the PRESTO model's reliance on sparsity assumptions for differences between adjacent decision boundaries could limit its applicability in settings where these differences are not sparse. Furthermore, the implementation of PRESTO using a modified ordinalNet package might not be the most efficient, potentially leading to slow convergence due to highly correlated features in the design matrix.</div>
        </div>
        <div class="question-block">
            <div class="question">Q9: Can anyone use this?</div>
            <div class="answer">The method proposed in the paper, PRESTO, is designed to improve the estimation of probabilities for rare events by leveraging more common intermediate events in a sequence. While it offers a practical solution for specific cases with class imbalance, its implementation involves complex statistical modeling and requires modifications to existing software, such as the R ordinalNet package. This suggests that while PRESTO is theoretically accessible to those with expertise in statistical modeling and programming, it may not be straightforward for general use without specialized knowledge and resources.</div>
        </div>
        <div class="question-block">
            <div class="question">Q10: What's still left to figure out?</div>
            <div class="answer">The paper identifies several areas for further exploration. It suggests that future work could involve exploring the use of \(\ell_1\) penalties for the coefficients themselves, not just the differences between them, to allow for simultaneous feature selection and model estimation. Additionally, the paper proposes investigating inference methods for PRESTO and examining its empirical performance on larger real-world datasets. It also raises the possibility of optimizing the penalization strategy by directly penalizing the decision boundary with the best balance of observed responses, rather than the first boundary, to potentially improve estimation accuracy.</div>
        </div>
    </section>
    
    <section id="paper-content" class="paper-content">
        <h2>Paper Content</h2>
        <pre># faletto23a

*Generated from PDF: faletto23a.pdf*

---

# Predicting Rare Events by Shrinking Towards Proportional Odds

## Gregory Faletto ${ }^{1}$ Jacob Bien ${ }^{1}$

#### Abstract

Training classifiers is difficult with severe class imbalance, but many rare events are the culmination of a sequence with much more common intermediate outcomes. For example, in online marketing a user first sees an ad, then may click on it, and finally may make a purchase; estimating the probability of purchases is difficult because of their rarity. We show both theoretically and through data experiments that the more abundant data in earlier steps may be leveraged to improve estimation of probabilities of rare events. We present PRESTO, a relaxation of the proportional odds model for ordinal regression. Instead of estimating weights for one separating hyperplane that is shifted by separate intercepts for each of the estimated Bayes decision boundaries between adjacent pairs of categorical responses, we estimate separate weights for each of these transitions. We impose an L1 penalty on the differences between weights for the same feature in adjacent weight vectors in order to shrink towards the proportional odds model. We prove that PRESTO consistently estimates the decision boundary weights under a sparsity assumption. Synthetic and real data experiments show that our method can estimate rare probabilities in this setting better than both logistic regression on the rare category, which fails to borrow strength from more abundant categories, and the proportional odds model, which is too inflexible.

## 1. Introduction

Estimating probabilities of rare events is known to be difficult due to class imbalance. However, sometimes these events are the culmination of a sequential process with intermediate outcomes. For example:

[^0]1. In online marketing, a customer is first served an ad, then may click on it, then may indicate interest in making a purchase (by "liking" the product, for example), and finally may make a purchase.
2. In health and medicine, many outcomes can be encoded as ordered categorical variables, like reported quality of life and disease progression (Norris et al., 2006).
3. Sales of high-price durable goods typically follow a sales funnel (Duncan \& Elkan, 2015). For example, when buying a car often a potential buyer first comes in to see a car, may take a test drive, and finally may buy the car.

In many of these cases, the intermediate events are much more common than the rare events. Though these intermediate events may not be of direct interest, if the features that contribute to the probability of advancing through earlier classes also contribute to the probability of advancing through later classes, then the more abundant intermediate events can be leveraged to improve estimation of the rare event probabilities.

The proportional odds model (McCullagh, 1980), also called the ordered logit model (Cameron \& Trivedi, 2005, Section 15.9.1), satisfies, for ordinal outcomes $k \in\{1, \ldots, K-1\}$,

$$
\log \left(\frac{\mathbb{P}(y \leq k \mid \boldsymbol{x})}{\mathbb{P}(y>k \mid \boldsymbol{x})}\right)=\alpha_{k}+\boldsymbol{\beta}^{\top} \boldsymbol{x}
$$

where $\boldsymbol{\beta} \in \mathbb{R}^{p}$ is a vector of weights and $\boldsymbol{x} \in \mathbb{R}^{p}$ is a vector of features. This implies that for all $k \in\{1, \ldots, K-1\}$

$$
p_{k}(\boldsymbol{x}):=\mathbb{P}(y \leq k \mid \boldsymbol{x})=F\left(\alpha_{k}+\boldsymbol{\beta}^{\top} \boldsymbol{x}\right)
$$

where $F(\cdot)$ is the logistic cumulative distribution function, $F(t)=\exp \{t\} /[1+\exp \{t\}]$. Notice that $\alpha_{k}+\boldsymbol{\beta}^{\top} \boldsymbol{x}$ is the Bayes decision boundary for the binary random variable $\mathbb{1}\{y \leq k\} \mid \boldsymbol{x}$. This problem could instead be cast as $K-1$ binary classification problems of the form (2) for adjacent classes:
$\log \left(\frac{\mathbb{P}(y \leq k \mid \boldsymbol{x})}{\mathbb{P}(y>k \mid \boldsymbol{x})}\right)=\alpha_{k}+\boldsymbol{\beta}_{k}^{\top} \boldsymbol{x}, \quad k \in\{1, \ldots, K-1\}$.
The condition that the weight vectors $\boldsymbol{\beta}_{k}$ of the separating hyperplanes in (3) are all equal, as in (1), has been called

[^0]:    ${ }^{1}$ Department of Data Sciences and Operations, University of Southern California, Los Angeles, CA, USA. Correspondence to: Gregory Faletto $<$ gregory.faletto@marshall.usc.edu $>$.

Proceedings of the $40^{\text {th }}$ International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).

the proportional odds assumption (McCullagh, 1980) or the parallel regression assumption (Greene, 2012, Section 18.3.2). One way to motivate this model is by supposing that the response is driven by a latent (unobserved) variable $U$,

$$
## U=\boldsymbol{\beta}^{\top} \boldsymbol{x}+\epsilon
$$

where $\epsilon$ has a standard logistic distribution and is independent of $\boldsymbol{x}$. Response $k$ is observed if and only if $-\alpha_{k} \leq U_{i}<-\alpha_{k-1}$ (where we define $\alpha_{0}:=-\infty$ and $\alpha_{K}:=\infty$ ). This model leads to (2). (See Section 3.3.2 of Agresti 2010 for a more detailed explanation.)
Because the proportional odds model assumes that the decision boundaries between adjacent classes are all governed by the same hyperplane defined by $\boldsymbol{\beta}$ (separated only by different intercepts $\alpha_{k}$ ), it assumes that the decision boundary between any two classes perfectly explains the decision boundary between any two other classes, other than an intercept term. If a rare event has much more common intermediate events before it, this model can therefore be very useful for better estimating the parameters of the model, and therefore better estimating the rare event probabilities. However, it could be that the proportional odds assumption is too rigid to be realistic, because observed features may have varying influence at different decision boundaries. For example:

1. In online marketing, users may click on an ad only to realize that the product is not what they were expecting, resulting in a particularly low probability of purchase.
2. For expensive goods like a home or car, potential buyers may express interest by going on a tour or taking a test drive purely out of curiosity; this may be distinct from their level of interest in actually making a purchase.
3. Students may place weights on different factors when deciding whether to apply to graduate school than they did when deciding whether to apply to an undergraduate program-they may have more appealing alternatives to additional schooling, they may face new financial or personal constraints because they are older, etc.

In each of these settings, if specific features vary in relevance for different decision boundaries while other features have about the same influence at every boundary, the proportional odds assumption may be too strong. Violations or relaxations of the proportional odds assumption along the lines of (3) have previously been considered by, for example, Brant (1990). Peterson \& Harrell Jr (1990) developed partial proportional odds models, which allow the proportional odds assumption to hold for some features but not others, an idea previously mentioned by Armstrong \& Sloan (1989).
(See Section 3.6.1 of Agresti 2010 for a textbook-level discussion). These relaxations have not been widely adopted because fitting separate weights for each outcome is too flexible unless $p(K-1) \ll n$ and all classes are reasonably common (and we discuss additional difficulties of this kind of model in Sections 3.1 and 4.1).

### 1.1. Our Contributions

In this paper we propose relaxing the proportional odds assumption as in (3), but controlling the amount of relaxation by placing $\ell_{1}$ penalties on the differences in weights corresponding to the same features in adjacent $\boldsymbol{\beta}_{k}$ vectors, in a way that is reminiscent of the fused lasso (Tibshirani et al., 2005). This model allows us to borrow strength from outcomes where data is much more abundant to improve rare probability estimates when outcomes are much more rare without making the strong assumption that the weights in these models are exactly equal. In particular, it allows for the proportional odds model to hold for some specific features in some adjacent pairs of decision boundaries, but not others.

We formalize the intuitive argument we outline above-that the proportional odds model allows for precise estimation of the $\boldsymbol{\beta}$ vector as long as at least one decision boundary is surrounded by reasonably well-balanced outcomes, and this allows for improved estimation of rare probabilities at the end of the sequence-through theoretical results in Section 2. Motivated by this argument but skeptical of the proportional odds assumption holding exactly, we propose PRESTO in Section 3 and prove that it consistently estimates $\boldsymbol{\beta}_{1}, \ldots, \boldsymbol{\beta}_{K-1}$ under a sparsity assumption in Section 3.1. In Section 4 we demonstrate through synthetic and real data experiments that PRESTO can outperform both logistic regression on the rare class and the proportional odds model, both in settings where the differences in adjacent $\boldsymbol{\beta}_{k}$ vectors are sparse, as PRESTO assumes, and in settings where these differences are not sparse. Before we move on from the introduction, we review related literature.

### 1.2. Related Work

The difficulty of classification with class imbalance has been well-known for decades. Johnson \& Khoshgoftaar (2019) provide a recent review focusing on deep learning methods for handling class imbalance, and they also provide references for many other ways of dealing with class imbalance. One particularly closely related work is Owen (2007), which explores how logistic regression handles a vanishingly rare class. A particularly popular approach, SMOTE (Chawla et al., 2002), has its own recent review paper (Fernández et al., 2018).

Tutz \& Gertheiss (2016) discuss the possibility of penalizing differences in weights between adjacent models, including

briefly proposing an $\ell_{1}$ penalty between weights in corresponding categories for proportional hazard models, though this is not the focus of their article and they only mention the idea very briefly without investigating it.

Wurm et al. (2021) propose a generalization of a proportional odds model (and implement it in the R package ordinalNet) that allows for the possibility that adjacent categories have equal (or very close) weights, but their method differs from ours. The most closely related model Wurm et al. propose is an over-parameterized semi-parallel model with both a matrix of separate parameters for each level, an approach reminiscent of Peterson \& Harrell Jr (1990). This results in more flexible, less structured models than our approach, which assumes similarity between adjacent $\boldsymbol{\beta}_{k}$ vectors. Further, Wurm et al. (2021) do not investigate the theoretical properties of their model, or the use of their model for improving estimates of rare event probabilities.

Ugba et al. (2021) and Pößnecker \& Tutz (2016) implement an $\ell_{2}$ rather than $\ell_{1}$ penalty between weights in models for adjacent decision boundaries. However, these works also focus on ordinal regression more generally, while we focus both theoretically and in simulations on leveraging common classes to improve estimated probabilities of rare events. Further, the $\ell_{1}$ penalty, which imposes sparse differences, allows the proportional odds assumption to hold for some features and decision boundaries and not others, while the $\ell_{2}$ ridge penalty used by Ugba et al. (2021) (and previously proposed by Tutz \& Gertheiss 2016, Section 4.2.2) relaxes the proportional odds assumption for all features but regularizes the relaxation. The $\ell_{2}$ group lasso penalty used by Pößnecker \& Tutz (2016) can impose the proportional odds assumption for a given feature either at all decision boundaries or none of them, making it less flexible than PRESTO.

Besides the fused and generalized (Tibshirani \& Taylor, 2011) lasso, our work relates more specifically to the generalized fused lasso (Höfling et al., 2010; Xin et al., 2014). Xin et al. (2014) in particular propose and analyze an algorithm to solve a class of optimization problems similar to the PRESTO optimization problem, (7), with $\ell_{1}$ fusion penalties. In contrast to the present work, Xin et al. (2014) focus almost entirely on the properties of their algorithm. Further, PRESTO lies outside their class of optimization problems because PRESTO directly penalizes only the coefficients in the first decision boundary, not all of the coefficients. This distinction is central to our proof strategy for Theorem 3.1; Xin et al. (2014) do not prove the consistency of their method. Viallon et al. (2013) provide theoretical results for the generalized fused lasso specifically in the cases of linear and logistic regression, though not for ordinal regression. Viallon et al. (2016) prove theoretical results for a
broader class of generalized linear models that still does not include the proportional odds model or a generalization like PRESTO. Lastly, Ekvall \& Bottai (2022) prove theoretical results for a class of models in which PRESTO can be expressed, and indeed we leverage their results in proving our own theory, though they do not directly consider fusion penalties.

## 2. Motivating Theory

We present the following theoretical results to motivate PRESTO. The thrust of our motivation is as follows:

1. Logistic regression does arbitrarily badly as class imbalance worsens (Theorem A. 1 in the appendix).
2. However, as one would expect, a logistic regression model's ability to estimate probabilities improves when the parameters $\boldsymbol{\beta}$ are known (Theorem 2.2).
3. The proportional odds model allows for precise estimation of $\boldsymbol{\beta}$ as long as two adjacent classes are reasonably common, even if the remaining classes are arbitrarily rare (Theorem 2.3).
4. Taking 2 and 3 together, our conclusion is that we can better estimate probabilities of rare events by using a method that leverages data from decision boundaries between abundant classes to better estimate decision boundaries near rare classes. (Both the proportional odds model and PRESTO leverage the data in this way.)

Before we present our results, we discuss the metrics we will use in our results and some of the assumptions we will make.

### 2.1. Preliminaries

Our goal is to characterize and compare the prediction error of estimated conditional probabilities of a rare class from both logistic regression and the proportional odds model. There are many settings where estimating rare probabilities accurately (as opposed to, for example, predicting class labels accurately) is important. For example, in online advertising, advertisers bid on the price to display an ad to a given user. Advertisers could bid optimally if they knew the true probability each user would click a given ad, so they'd like to estimate these probabilities as precisely as possible (He et al., 2014; Zhang et al., 2014). Another example is public policy, where scarce resources may be allocated based on estimated probability of bad outcomes (Von Wachter et al., 2019). To prioritize optimally, precisely estimated probabilities are needed, not just accurate labels.

A natural metric in an estimation setting is mean squared error, $\mathbb{E}\left[(\hat{\pi}(\boldsymbol{x})-\pi(\boldsymbol{x}))^{2}\right]$, where $\pi(\boldsymbol{x})$ is the actual probability of a rare event conditional on $\boldsymbol{x}$ and $\hat{\pi}(\boldsymbol{x})$ is an estimate. Further, we leverage asymptotic statistics and present results for large-sample estimators. We define the notions of asymptotic mean squared error we will use below:
Definition 2.1. Let $\hat{\theta}_{n}$ be a maximum likelihood estimator for a parameter $\theta \in \mathbb{R}$ from a sample size of $n$. Under regularity conditions, the sequence of random variables $\left\{\sqrt{n} \cdot\left(\hat{\theta}_{n}-\theta\right)\right\}$ converges in distribution to a Gaussian random variable. Then we define the asymptotic mean squared error of $\hat{\theta}_{n}$ to be (suppressing $n$ from the notation)

$$
\operatorname{Asym} . \operatorname{MSE}(\hat{\theta}):=\mathbb{E}\left[\left(\lim _{n \rightarrow \infty} \sqrt{n}\left[\hat{\theta}_{n}-\theta\right]\right)^{2}\right]
$$

Asymptotic metrics are commonly used to compare the performance of estimators. The asymptotic relative efficiency of two estimators is the ratio of their asymptotic variances,

$$
\operatorname{Asym} . \operatorname{Var}(\hat{\theta}):=\operatorname{Var}\left(\lim _{n \rightarrow \infty} \sqrt{n}\left[\hat{\theta}_{n}-\theta\right]\right)
$$

which is equal to $\operatorname{Asym} . \operatorname{MSE}\left(\hat{\theta}_{n}\right)$ for the (asymptotically unbiased) maximum likelihood estimators we consider. See Section 10.1.3 of Casella \& Berger (2021), Section 8.2 of van der Vaart (2000), or Section 4.4.5 of Greene (2012) for textbook-level discussions. The asymptotic MSE could also be used as an estimator of the MSE for large (but finite) $n$, under the heuristic reasoning that for large $n$,

$$
\begin{aligned}
\operatorname{MSE}(\hat{\theta}) & =\frac{1}{n} \mathbb{E}\left[(\sqrt{n} \cdot[\hat{\theta}-\theta])^{2}\right] \\
& \approx \frac{1}{n} \mathbb{E}\left[\left(\lim _{n \rightarrow \infty} \sqrt{n} \cdot[\hat{\theta}-\theta]\right)^{2}\right] \\
& =\frac{1}{n} \operatorname{Asym} . \operatorname{MSE}(\hat{\theta})
\end{aligned}
$$

See Section 4.4 of Greene (2012), Section 7.3 of Hansen (2022), or Section 3.5 of Wooldridge (2010) for more discussion of this kind of finite-sample estimation using asymptotic quantities.

We briefly present and discuss some of our assumptions.

- Assumption $X(\mathcal{A})$ : The random vectors $\boldsymbol{x}_{i} \in \mathbb{R}^{p}$ are independent and identically distributed (iid) for $i \in\{1, \ldots, n\}$, each with probability measure $d F(\boldsymbol{x})$ with measurable, bounded support $\mathcal{S} \subset \mathcal{A} \subseteq \mathbb{R}^{p}$, with $\operatorname{Cov}(\boldsymbol{X})$ positive definite.
- Assumption $Y(K)$ : The response $y_{i} \in\{1, \ldots, K\}$ is distributed conditionally on $\boldsymbol{x}_{i}$ as in the proportional odds model (1). (Note that if $K=2$, this is equivalent to the logistic regression model.) All
classes have positive probability for all $\boldsymbol{x}$ on the support of $\boldsymbol{x}_{i}$ (equivalently, the intercepts strictly differ: $\alpha_{1}<\ldots<\alpha_{K-1}$.)

Assumption $X(\mathcal{A})$ allows a very broad class of distributions, including both discrete and continuous random variables. Notice that the boundedness assumption within $X(\mathcal{A})$ implies that the matrix $\tilde{\boldsymbol{X}}:=\left(\mathbf{1}, \boldsymbol{X}\right)$ (where $\mathbf{1}$ is an $n$-vector of ones) has a finite maximum eigenvalue. When we will refer to it, we call it $\lambda_{\max }$ and write Assumption $X\left(\mathcal{A}, \lambda_{\max }\right)$.
From (2) we see that in the proportional odds model if the intercepts strictly differ $\left(\alpha_{1}<\ldots<\alpha_{K-1}\right)$ then for any $\boldsymbol{x}$ all of the classes have conditional probability strictly between 0 and 1 . That said, if the support of $\boldsymbol{X}$ is unbounded then all of the probabilities for individual classes can become arbitrarily close to 0 or 1 . Under Assumption $X(\mathcal{A})$, however, we can strictly bound quantities like $\sup _{\boldsymbol{x} \in \mathcal{S}}\left\{\pi_{k}(\boldsymbol{x})\right\}$ (where $\pi_{k}(\boldsymbol{x}):=p_{k}(\boldsymbol{x})-p_{k-1}(\boldsymbol{x})=\mathbb{P}(y=k \mid \boldsymbol{x})$ ) away from 0 or 1 .

Theorem 2.2 holds under Assumption $X\left([0, \infty)^{p}\right)$, though for any bounded $\mathcal{S} \subseteq \mathbb{R}^{p}$, there is some finite $a$ one could add to each coordinate to shift $\mathcal{S}$ to a subset of $[0, \infty)^{p}$; Theorem 2.2 would then apply to these translated features.

### 2.2. Theorem 2.2

Theorem 2.2 suggests a possible way to circumvent the problem of class imbalance. We compare the typical logistic regression intercept estimate $\hat{\alpha}$ to the quasi-estimated estimator $\hat{\alpha}_{q}$ obtained when one estimates only the intercept of the logistic regression model with a known $\boldsymbol{\beta}$. We also compare the resulting estimators of conditional probabilities for any $\boldsymbol{z} \in \mathbb{R}^{p}$ : the usual logistic regression estimator $\hat{\pi}(\boldsymbol{z})$ and $\hat{\pi}_{q}(\boldsymbol{z})$, the estimator when $\boldsymbol{\beta}$ is known. Theorem 2.2 proves the reasonable intuition that $\hat{\alpha}_{q}$ must be a better estimator than $\hat{\alpha}$, and likewise for $\hat{\pi}_{q}(\boldsymbol{z})$ and $\hat{\pi}(\boldsymbol{z})$.
Theorem 2.2. Assume $X\left([0, \infty)^{p}, \lambda_{\text {max }}\right)$ and $Y(2)$ hold. Let $\pi(\boldsymbol{x}):=\mathbb{P}(y=2 \mid \boldsymbol{x})$, and let $\pi_{\min }:=$ $\inf _{\boldsymbol{x} \in \mathcal{S}}\{\pi(\boldsymbol{x}) \wedge 1-\pi(\boldsymbol{x})\}$. Then

1 .

$$
\frac{\operatorname{Asym} . \operatorname{MSE}(\hat{\alpha})-\operatorname{Asym} . \operatorname{MSE}\left(\hat{\alpha}_{q}\right)}{\left[\operatorname{Asym} . \operatorname{MSE}\left(\hat{\alpha}_{q}\right)\right]^{2}} \geq \Delta
$$

where

$$
\Delta:=\frac{4 \pi_{\min }^{2}\left(1-\pi_{\min }\right)^{2}\|\mathbb{E}[\boldsymbol{X}]\|_{2}^{2}}{\lambda_{\max }}
$$

and
## 2. For any $\boldsymbol{z} \in \mathbb{R}^{p} \backslash\left\{\boldsymbol{z}^{*}\right\}$, where

$$
\boldsymbol{z}^{*}:=\frac{\mathbb{E}[\boldsymbol{X} \pi(\boldsymbol{X})[1-\pi(\boldsymbol{X})]]}{\mathbb{E}[\pi(\boldsymbol{X})[1-\pi(\boldsymbol{X})]]}
$$

it holds that

$$
\operatorname{Asym} \cdot \operatorname{MSE}\left(\hat{\pi}_{q}(\boldsymbol{z})\right)<\operatorname{Asym} \cdot \operatorname{MSE}(\hat{\pi}(\boldsymbol{z}))
$$

(For $\boldsymbol{z}^{*}$, the above holds with $\leq$ rather than $<$.)

Examining the first result, it is sensible that the lower bound for the gap between the asymptotic variances of the two estimators vanishes as $\pi_{\min }$ vanishes because if $\min \left\{\pi_{1}(\boldsymbol{x}), 1-\pi_{1}(\boldsymbol{x})\right\}$ becomes very small on the bounded support, then the imbalance between the two classes potentially becomes very large, and estimating the intercept becomes difficult regardless of whether or not $\boldsymbol{\beta}$ is known. As the class balance improves ( $\pi_{\min }$ becomes closer to its upper bound $1 / 2$ ), the guaranteed gap between $\operatorname{Asym} . \operatorname{MSE}(\hat{\alpha})$ and $\operatorname{Asym} . \operatorname{MSE}\left(\hat{\alpha}_{q}\right)$ becomes larger.

In addition to formally verifying intuition, Theorem 2.2 also quantifies the estimation gap between the rare intercept estimators in terms of noteworthy parameters and shows that the assumptions needed for this intuition to hold are minimal.

### 2.3. Theorem 2.3

Theorem 2.2 suggests that if only we could estimate $\boldsymbol{\beta}$ very well, we could improve our estimated probabilities even in the face of class imbalance. Theorem 2.3 suggests a way to leverage abundant data among other classes to do this.

In the proportional odds model (1), $\mathbb{R}^{p}$ is partitioned into regions with separating hyperplanes defined by $\boldsymbol{\beta}$, which we note are Bayes decision boundaries: for $\boldsymbol{x} \in \mathbb{R}^{p}$ such that $\alpha_{k}+\boldsymbol{\beta}^{\top} \boldsymbol{x}=0$, we have $p_{k}(\boldsymbol{x})=1 / 2$.

Consider the setting of ordered categorical data generated by the proportional odds model with categories 1 and 2 similarly common over the support of a bounded distribution of $\boldsymbol{x}_{i}$ and categories $3, \ldots, K$ all rare. In this setting, for many of the observed values of $\boldsymbol{x}_{i}$, the probabilities of being in class 1 or 2 will both be close to $1 / 2$. Intuitively it should be relatively easy to estimate $\boldsymbol{\beta}$ and $\alpha_{1}$, the parameters that define the Bayes decision boundary between classes 1 and 2, and therefore $p_{1}\left(\boldsymbol{x}_{i}\right)$. Theorem 2.2 suggests this should help us in estimating the rare class probabilities. In Theorem 2.3, we prove that even if class $K$ becomes arbitrarily rare, as long as the first two classes are reasonably well balanced, the proportional odds model still learns $\boldsymbol{\beta}$ quite well.

Theorem 2.3. Assume $X\left(\mathbb{R}^{p}\right)$ and $Y(3)$ hold. Assume for all $\boldsymbol{x} \in \mathcal{S}$ it holds that $\left|\pi_{k}(\boldsymbol{x})-1 / 2\right| \leq \Delta$ for $k \in\{1,2\}$ for some $\Delta \in(0,1 / 2)$ and let $M:=\sup _{\boldsymbol{x} \in \mathcal{S}}\|\boldsymbol{x}\|_{2}$ (notice that $X\left(\mathbb{R}^{p}\right)$ ensures that $M<\infty$ ). Suppose $\sup _{\boldsymbol{x} \in \mathcal{S}}\left\{\pi_{3}(\boldsymbol{x})\right\}=$
$\pi_{\text {rare }}$, where $\pi_{\text {rare }}$ is no greater than
$\min \left\{\frac{1}{2}\left(\frac{1}{2}-\Delta\right)\left(\frac{1}{2}+\Delta\right), \frac{\lambda_{\text {min }}\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)}{3 M^{2}(M+2)}\right\}$,
where $\lambda_{\text {min }}(\cdot)$ denotes the minimum eigenvalue of $\cdot$ and $I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}$ is a symmetric matrix composed of terms from the Fisher information matrix for the proportional odds model (see the definitions of these terms in Equations 8, 9, and 10 in the appendix). Then there exists $C<\infty$ not depending on $\pi_{\text {rare }}$ such that for any fixed $\boldsymbol{v} \in \mathbb{R}^{p}$,

$$
\frac{1}{\|\boldsymbol{v}\|_{2}^{2}} \operatorname{Asym} . \operatorname{MSE}\left(\boldsymbol{v}^{\top} \boldsymbol{\beta}^{\text {prop. odds }}\right) \leq C
$$

To give an example of applying this result, consider the choice $\boldsymbol{v}=(1,0, \ldots, 0)$. Then we have that $\operatorname{Asym} . \operatorname{MSE}\left(\hat{\beta}_{1}^{\text {prop. odds }}\right) \leq C$, so $\hat{\beta}_{1}^{\text {prop. odds }}$ (or any other estimated coefficient) has bounded asymptotic mean squared error. In contrast to logistic regression, the proportional odds model still learns $\boldsymbol{\beta}$ within a fixed precision even as $\pi_{\text {rare }}$ vanishes.
Remark 2.4. We briefly discuss the upper bound (5). For this bound to make sense, it must hold that the symmetric matrix $I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}$ is positive definite so that its minimum eigenvalue is strictly positive. The matrix $\boldsymbol{S}:=I_{\beta \beta}-$ $\frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}$ is the Schur complement of $I_{\alpha_{1} \alpha_{1}}=M_{1}$ in the submatrix

$$
\left(\begin{array}{ll}
## I_{\alpha_{1} \alpha_{1}} & I_{\beta \alpha_{1}}^{\top} \\
## I_{\beta \alpha_{1}} & I_{\beta \beta}
\end{array}\right)
$$

of the Fisher information matrix $I^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})$ for the proportional odds model (see Lemma D. 1 in the appendix). Note (6) is a principal submatrix of the positive definite $I^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})$, so is positive definite by Observation 7.1.2 in Horn \& Johnson (2012). From (8) we also know that $I_{\alpha_{1} \alpha_{1}}>0$, so $\boldsymbol{S}$ is positive definite by Theorem 1.12 in Zhang (2005). It seems plausible that

$$
I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}=\boldsymbol{S}-\frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}
$$

is also positive definite because $I_{\beta \beta}$ is the inverse of the asymptotic covariance matrix of $\hat{\boldsymbol{\beta}}^{\text {ideal }}$, the maximum likelihood estimator of $\boldsymbol{\beta}$ when $\alpha_{1}$ and $\alpha_{2}$ are known. We expect that $\operatorname{Cov}\left(\hat{\boldsymbol{\beta}}^{\text {ideal }}\right)$ would be small (and the eigenvalues of $I_{\beta \beta}$ would be large) in this setting because we can estimate $\boldsymbol{\beta}$ well due to the abundant observations in classes 1 and 2 (ensured if $\Delta$ is not too large), so we should be able to learn the decision boundary between these classes well. If the

eigenvalues of $I_{\beta \beta}$ are indeed large, it might be reasonable to expect $I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}$ to be positive definite. In Sections E. 1 and E. 2 in the appendix, we present more detailed analysis as well as the results of synthetic experiments that indicate that it is plausible both that $I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}$ is positive definite and that the upper bound (5) is reasonable.

## 3. Predicting Rare Events by Shrinking Towards proportional Odds (PRESTO)

Theorems 2.2 and 2.3 suggest a path to improve estimated probabilities for a rare event that is at the end of an ordered sequence: use the more common events that come before it to improve the estimation of the decision boundary affecting the rare class. In practice, however, the proportional odds model assumption is strong and unlikely to hold in many settings. PRESTO allows for this assumption to be relaxed; instead of assuming the $\boldsymbol{\beta}$ vectors governing the decision boundaries are identical, we assume they are in general different, but with differences that are (approximately) sparse.

One concrete model to motivate this is a relaxation of (4) along the lines of (3). Suppose that $U_{1}:=U$ as defined in (4) (with $\boldsymbol{\beta}_{1}=\boldsymbol{\beta}$ ), and it still holds that an observation is in class 1 if $U_{1} \geq-\alpha_{1}$. However, for $k \in\{2, \ldots, K-$ $1\}$, outcome $k$ is observed if and only if $-\alpha_{k} \leq U_{k}<$ $-\alpha_{k-1}+\boldsymbol{\psi}_{k}^{\top} \boldsymbol{x}$ for sparse vectors $\boldsymbol{\psi}_{2}, \ldots, \boldsymbol{\psi}_{K-1} \in \mathbb{R}^{p}$ satisfying $\boldsymbol{\psi}_{k}=\boldsymbol{\beta}_{k}-\boldsymbol{\beta}_{k-1}$, so $U_{k}=U_{k-1}+\boldsymbol{\psi}_{k}^{\top} \boldsymbol{x}$ for $k \in\{2, \ldots, K-1\}$. Note that this is within the scope of (3), but we assume a structure on the differing $\boldsymbol{\beta}_{k}$ vectors rather than allowing for arbitrary differences.

Assuming sparse differences in adjacent $\boldsymbol{\beta}_{k}$ vectors in this way suggests the following optimization problem for data $\boldsymbol{X}=\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{n}\right)^{\top}$ and $\boldsymbol{y}=\left(y_{1}, \ldots, y_{n}\right)$ :

$$
\begin{aligned}
\underset{\boldsymbol{\beta}, \boldsymbol{\alpha}}{\arg \min } & \left\{-\frac{1}{n} \sum_{i=1}^{n} \log \left[F\left(\alpha_{y_{i}}+\boldsymbol{\beta}_{y_{i}}^{\top} \boldsymbol{x}_{i}\right)\right.\right. \\
& \left.\left.-F\left(\alpha_{y_{i}-1}+\boldsymbol{\beta}_{y_{i}-1}^{\top} \boldsymbol{x}_{i}\right)\right]\right. \\
& \left.+\lambda_{n}\left(\sum_{j=1}^{p}\left|\beta_{j 1}\right|+\sum_{j=1}^{p} \sum_{k=2}^{K-1}\left|\beta_{j k}-\beta_{j, k-1}\right|\right)\right\}
\end{aligned}
$$

where we define $\alpha_{K}:=\infty, \alpha_{0}:=-\infty$ and $\boldsymbol{\beta}_{0}:=\mathbf{0}$. The penalties on the $\left|\beta_{j 1}\right|$ terms are sufficient to regularize all of the weights given the penalties on the difference terms starting from the $\boldsymbol{\beta}_{1}$ vector, improving parameter estimation. Like the proportional odds model and the generalized lasso (Tibshirani \& Taylor, 2011) optimization problem, (7) is strictly convex if and only if $\alpha_{y_{i}}+\boldsymbol{\beta}_{y_{i}}^{\top} \boldsymbol{x}_{i}>\alpha_{y_{i}-1}+\boldsymbol{\beta}_{y_{i}-1}^{\top} \boldsymbol{x}_{i}$ for all $i$ (Pratt, 1981).

This can be violated if the decision boundaries, which are not parallel, cross in the support of $\boldsymbol{X}$. In Section 4.1, we discuss the practical issues this presents when implementing relaxed proportional odds models like PRESTO, and in the next section, we prove PRESTO is consistent relying in part on an assumption that these decision boundaries do not cross in the support of $\boldsymbol{X}$. See Appendix H for details on how we estimate PRESTO in practice.

### 3.1. Theoretical Analysis

In this section, we present Theorem 3.1, which shows that PRESTO is a consistent estimator of $\boldsymbol{\beta}_{1}, \ldots, \boldsymbol{\beta}_{K-1}$ under suitable assumptions. Before stating Theorem 3.1, we present and briefly discuss some of the new assumptions we will make.

- Assumption $S(s, c)$ : The distribution of $y_{i} \mid$ $\boldsymbol{x}_{i}$ is distributed according to the PRESTO likelihood (7), where the true coefficients $\boldsymbol{\theta}_{*}=$ $\left(\boldsymbol{\beta}_{1}^{\top}, \boldsymbol{\psi}_{2}^{\top}, \ldots, \boldsymbol{\psi}_{K-1}^{\top}\right)^{\top} \in \mathbb{R}^{p(K-1)}$ are $s$-sparse (have $s$ nonzero entries for a fixed $s$ not increasing in $n$ or $p$ ). Further, $\left\|\boldsymbol{\theta}_{*}\right\|_{\infty} \leq c$ for a fixed $c$.
- Assumption $T(c)$ : For all small enough $\rho>0$, for all $\boldsymbol{\theta} \in \mathbb{R}^{p(K-1)}$ with $\left\|\boldsymbol{\theta}-\boldsymbol{\theta}_{*}\right\|_{1} \leq \rho$ it holds that none of the decision boundaries defined by $\boldsymbol{\theta}$ and the true $\alpha_{1}, \ldots, \alpha_{K-1}$ cross in $\mathcal{S}$. Also, $\max _{k \in\{1, \ldots, K-1\}}\left|\alpha_{k}\right| \leq c$.

The fixed sparsity assumption $S(s, c)$ is helpful theoretically and also because without it in higher dimensions it becomes increasingly difficult to have nonparallel decision boundaries that do not cross. The first part of Assumption $T(c)$ can be interpreted to mean that none of the decision boundaries cross "too closely" to $\mathcal{S}$. Other than these aspects, Assumptions $S(s, c)$ and $T(c)$ are mild.
Theorem 3.1. In a setting with fixed $K \geq 3$ and $p=$ $p_{n} \rightarrow \infty$ as $n \rightarrow \infty$ and satisfying $p_{n} \leq C_{1} n^{C_{2}}$ for some $C_{1}>0$ and $C_{2} \in(0,1)$, consider estimating PRESTO with penalty $\lambda_{n}=C_{3} \log \left(p_{n}[K-1]\right) / n$ for some $C_{3}>$ 0. Suppose Assumption $X\left(\mathbb{R}^{p_{n}}\right)$ holds and there is some $C_{4}<\infty$ such that $\sup _{\boldsymbol{x} \in \mathcal{S}}\|\boldsymbol{x}\|_{\infty} \leq C_{4}$ and Assumptions $S\left(s, C_{4}\right)$ and $T\left(C_{4}\right)$ hold. Assume for some fixed $b>0$ it holds that $\lambda_{\min }^{*}:=\min _{k \in\{1, \ldots, K\}} \lambda_{\min }\left(\boldsymbol{\Sigma}_{k}\right)>b$, where $\boldsymbol{\Sigma}_{k}:=\mathbb{E}\left[\boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \mid y_{i}=k\right]$. Then PRESTO is a consistent estimator of $\boldsymbol{\beta}_{1}, \ldots, \boldsymbol{\beta}_{K-1}$.

Theorem 3.1 shows that under fairly mild regularity conditions and a sparsity assumption in a high-dimensional setting, PRESTO consistently estimates all of the decision boundaries. That is, it is consistent both if the proportional odds assumption holds and in more flexible settings, where the proportional odds model is unrealistic, under sparsity.

Theorem 2.2 suggests this should be helpful for estimating rare class probabilities. The proof of Theorem 3.1 leverages recent theory developed for $\ell_{1}$-penalized ordinal regression (Ekvall & Bottai, 2022).

![img-0.jpeg](img-0.jpeg)

**Figure 1.** Top left: MSE of estimated rare class probabilities for each method across all *n* = 2500 observations, across 700 simulations, in sparse differences simulation setting of Section 4.1, for intercept setting yielding rare class proportions of about 0.71% on average and sparsity 1/2. Remaining plots: ratios of MSE for PRESTO divided by MSE of each other method for each of three sets of intercepts with sparsity 1/2 (PRESTO performs better if ratio is less than 1). All plots on log scale.

## 4. Experiments

To illustrate the efficacy of PRESTO, we conduct two synthetic experiments and also examine two real data sets. In Section 4.1, we generate random *y* that have conditional probabilities based on a relaxation of the proportional odds model with sparse differences between adjacent decision boundary parameter vectors, rather than parameterizing all decision boundaries with the same β. This setting is well-suited to PRESTO. In Section 4.2, we show that PRESTO also performs well in a less favorable setting, where the differences between adjacent decision boundaries are instead dense; nonetheless, PRESTO still outperforms logistic regression and proportional odds models. In Section 4.3 we compare the performance of PRESTO to logistic regression and the proportional odds model at estimating rare probabilities in a real data experiment. Finally, in Section B of the appendix we conduct a second real data experiment on a data set of patients diagnosed with diabetes, where we vary the rarity of the outcome of interest. See Section H of the appendix for all implementation details. The code generating all plots and tables is available at https://github.com/gregfaletto/presto.

![img-1.jpeg](img-1.jpeg)

**Figure 2.** Same plots as in Figure 1, but for uniform differences synthetic experiment in Section 4.2.

### 4.1. Synthetic Data: Sparse Differences Setting

We repeat the following procedure for 700 simulations. First we generate data using *n* = 2500, *p* = 10, and *K* = 4. We draw a random *X* ∈ [−1, 1]^{*n* × *p*}, where *X*_{*i**j*} ∼ Uniform(−1, 1) for all *i* ∈ {1, ..., *n*} and *j* ∈ {1, ..., *p*}. Then *y* ∈ {1, ..., *K*}^{*n*} is generated according to a relaxation of the proportional odds model; instead of (1), we generate probabilities according to (3) where the β_{k} are generated in the following way for sparsity settings of η ∈ {1/3, 1/2}: first, we generate β_{1} by taking the vector (0.5, ..., 0.5)^T, but setting all of the entries equal to 0 randomly with probability 1 − η for each entry independently. Then we set β_{k} = β_{k−1} + ψ_{k}, *k* ∈ {2, ..., *K* − 1}, where ψ_{k} ∈ ℝ^{*p*} are iid random vectors for each *k* ∈ {2, ..., *K* − 1} generated according to the following distribution:

$$
\psi_{kj} = \begin{cases}
0, & \text{with probability } 1 - \eta, \\
0.5, & \text{with probability } \eta/2, \\
-0.5, & \text{with probability } \eta/2,
\end{cases}, \quad j \in \{1, \dots, p\}.
$$

We consider three possible sets of intercepts: α = (0, 3, 5), (0, 3.5, 5.5), and (0, 4, 6), so that the first two categories are common and the remaining categories are rare. The final rare class is the one of interest; in the three settings, the average proportions of observations falling in the rare class are 1.00%, 0.62%, and 0.37%, respectively, for

the $\eta=1 / 3$ setting and $1.17 \%, 0.71 \%$, and $0.43 \%$ for the $\eta=1 / 2$ setting.

The fact that the decision boundaries may cross in the support of $\boldsymbol{X}$, which would mean that for such $\boldsymbol{x}$ some class probabilities are defined to be negative, puts practical limits on the magnitude of $\boldsymbol{\psi}_{k}$ in simulations. (See Section 3.6.1 of Agresti 2010 for a discussion of this point.) Also, for this reason, in each simulation we check whether or not the conditional probabilities are positive for each class for every sampled $\boldsymbol{x}$; if not, we generate new $\boldsymbol{\psi}_{2}, \ldots, \boldsymbol{\psi}_{K-1}$ for a limited number of iterations, ending the simulation study in failure if no suitable $\boldsymbol{\psi}_{k}$ can be found in a reasonable number of attempts. The parameters we used generated positive probabilities for all observations across all simulations.

We then estimate a model for each method; for logistic regression, we estimate the binary classification problem of whether or not each observation is in class $K$, and for proportional odds and PRESTO, we fit a full model on all $K$ responses. For PRESTO, we use 5 -fold cross-validation to choose a value of $\lambda_{n}$ among 20 choices, selecting the $\lambda_{n}$ with the best out-of-fold Brier score (other metrics, like negative log likelihood, failed because some values of $\lambda_{n}$ in some folds resulted in models yielding negative probabilities, so these other metrics were undefined). The 20 candidate values of $\lambda_{n}$ are generated in the following way: the largest $\lambda_{n}$ value, $\lambda_{n}^{(20)}$, is the smallest $\lambda_{n}$ for which all of the estimated sparse differences equal 0 ; the smallest $\lambda_{n}$ value is set to $\lambda_{n}^{(1)}=0.01 \cdot \lambda_{n}^{(20)}$, and the remaining $\lambda_{n}$ values are generated at equal intervals on a logarithmic scale between these two values.

Each of these models yields estimated probabilities that each observation lies in class $K$. In the final step of each simulation run, we compute the mean squared error of these estimated probabilities for each method.

In Figure 1, we show boxplots of the empirical mean squared errors for each method in the setting where the rare class is observed in $0.71 \%$ of observations when $\eta=1 / 2$. In order to see how the methods compare pairwise on each simulation, we also show boxplots of the ratio between the mean squared error of PRESTO and the other two methods in each of the three simulation settings. We also conduct one-tailed paired $t$-tests of the alternative hypothesis that the mean MSE for PRESTO is lower than each of the competitor methods in each setting; all 12 of the $p$-values (provided in Table 3 of Appendix C) are below 0.01. Finally, in Appendix C we also provide the means and standard errors for the MSE of each method in each simulation setting in Table 4, as well as boxplots like the one in the top left corner of Figure 1 for the other two intercept settings and all boxplots for the $\eta=1 / 3$ setting.

## We see that PRESTO typically estimates these rare proba-
bilities better than logistic regression, which despite being correctly specified struggles with class imbalance and does not draw strength from estimating the easier decision boundary between classes 1 and 2, and the proportional odds model, whose assumptions are not satisfied in this setting.

### 4.2. Synthetic Data: Dense Differences Setting

In real data sets the differences between adjacent decision boundary parameter vectors may not always be exactly sparse, so we conduct another synthetic experiment in the same way as in Section 4.1, except $\boldsymbol{\beta}_{1 j} \sim$ Uniform $(-.5, .5)$ and each $\psi_{k j} \sim \operatorname{Uniform}(-.5, .5)$, iid across $j \in\{1, \ldots, p\}$ and $k \in\{2, \ldots, K-1\}$. We also add an extra intercept setting of $(0,2.5,4.5)$. This yields average rare class proportions of $0.99 \%, 0.62 \%$, and $0.36 \%$ using the same intercepts as the experiments in Section 4.1 and $1.60 \%$ in the new intercept setting. The uniformly distributed differences can be considered "approximately" sparse in the sense that while no deviations will exactly equal 0 , some will be large and important to estimate, and some will be essentially negligible.

Figure 2 and Table 1 summarize the results, along with additional figures and tables in Appendix C. We again see that PRESTO outperforms both competitor methods by statistically significant margins.

Table 1. Calculated $p$-values for one-tailed paired $t$-tests for uniform differences simulation setting of Section 4.2 testing the alternative hypothesis that PRESTO's rare probability MSE is less than each competitor method in each rarity setting. (Statistically significant $p$-values indicate better performance for PRESTO).

| Rare Class Proportion | Logit $p$-value | PO $p$-value |
| :--: | :--: | :--: |
| $1.6 \%$ | $<1 \mathrm{e}-10$ | $<1 \mathrm{e}-10$ |
| $0.99 \%$ | $<1 \mathrm{e}-10$ | $<1 \mathrm{e}-10$ |
| $0.62 \%$ | $<1 \mathrm{e}-10$ | $<1 \mathrm{e}-10$ |
| $0.36 \%$ | $<1 \mathrm{e}-10$ | 0.000242 |

### 4.3. Real Data Experiment

We conduct a real data experiment using the soup data set from the R ordinal package (R. H. B. Christensen, 2019). The data come from a study (Christensen et al., 2011) of participants who tasted soups and responded whether they thought each soup was a reference product they had previously been familiarized with or a new test product. The respondents also stated how sure they were in their response on a three-level scale, yielding a total of $K=6$ possible ordered outcomes for $n=1847$ observations. The outcome of interest corresponds to the respondent being sure the tasted soup was the reference and is observed in 228 observations (about $12 \%$ of the total). All of the features

are categorical, and after one-hot encoding we have $p=22$ binary features related to the soup, the respondent, and the testing environment ${ }^{1}$. This may be a promising setting for PRESTO because, while the responses have a well-defined ordering, it's plausible that different features could have varying impacts at different levels of respondent certainty.

We complete the following procedure 350 times: first, we randomly split the data into training ( $90 \%$ of the data) and test $(10 \%)$ sets. We estimate models using PRESTO, logistic regression, and the proportional odds model on the training data and evaluate on the test set.

We are interested in the accuracy of the rare class probabilities, but we can't evaluate rare probability MSE directly since we don't observe the true probabilities. Brier score could be a reasonable proxy, but it is known to be a poor metric in the presence of class imbalance (Benedetti, 2010). Instead we estimate rare probability MSE using the following procedure. For each method, we sort the estimated test set rare class probabilities in ascending order and assign the observations into 10 bins: the first $1 / 10$ observations go in the first bin, and so on. Then we estimate the mean squared error of the estimated probabilities by $\frac{1}{n} \sum_{i=1}^{n}\left(\hat{\pi}_{1}^{(i)}-o_{b(i)}\right)^{2}$, where $\hat{\pi}_{1}^{(i)}$ is the estimated rare class probability for observation $i$ and $o_{b(i)}$ is the observed rare class proportion in the bin containing observation $i$. This is similar to expected calibration error (Naeini et al., 2015), though we use squared error rather than absolute error. 10 equal frequency bins follows the default of the R CalibratR package that implements expected calibration error (Schwarz \& Heider, 2018).

By this metric, the mean error for PRESTO is 0.0096 , 0.0157 for logistic regression and 0.0135 for proportional odds. Figure 3 displays boxplots of the results as in the synthetic experiments which indicate that PRESTO typically outperforms the other methods. We do not report $p$-values or standard errors since the observed samples are dependent (random splits of the same data set).

## 5. Conclusion

By leveraging data from earlier decision boundaries, but relaxing the rigid proportional odds assumption, PRESTO can substantially improve estimation of the probability of rare events, even when the assumption of sparse differences between adjacent decision boundary weight vectors does not exactly hold. Future work could explore $\ell_{1}$ penalties for the coefficients themselves, not just the differences between the coefficients, to allow for simultaneous feature selection

[^0]![img-2.jpeg](img-2.jpeg)

Figure 3. Left: Estimated MSEs of estimated rare class probabilities for each method across 350 random draws of training and test sets in real data experiment from Section 4.3. Right: ratios of estimated MSE for PRESTO divided by MSE of each other method (PRESTO performs better if ratio is less than 1).
and model estimation. Inference for PRESTO could also be possible by extending the method for exact post-selection inference for the generalized lasso path by Hyun et al. (2018), or similar work on the fused lasso by Chen et al. (2022), to our generalized linear model setting. Future work could also explore the empirical performance of PRESTO in even more depth, perhaps by using large-scale real world data sets like those used in Duncan \& Elkan (2015).

There are other possible extensions that could improve estimation. For example, we set the first decision boundary as the one that is directly penalized, with differences from this boundary assumed to be sparse. This makes sense if the classes become increasingly rare and the first decision boundary is the most balanced. However, it may make more sense to directly penalize whichever decision boundary has the best balance of observed responses on each side. Penalizing the differences from this boundary might improve estimation since this decision boundary ought to be the easiest to estimate. This could improve estimation in settings like the real data experiment from Section 4.3 where the most balanced decision boundary is closer to the center of the responses.

Also, in cases where the final categories are very rare, a better bias/variance tradeoff might be achieved by reimposing the proportional odds assumption, imposing an exact equality constraint for the last few decision boundaries. In these settings, data might be too rare to hope for better estimation by relaxing the proportional odds assumption even with regularization.

Lastly, in Section H of the appendix we discuss possible faster approaches than the one used in the present work for solving the PRESTO optimization problem.

[^0]:    ${ }^{1}$ The categorical predictors PRODID and RESP are omitted because in some splits not all levels of these features are observed in the training set, making it impossible to estimate parameters for these features.

## References

Agresti, A. Analysis of ordinal categorical data, volume 656. John Wiley \& Sons, 2010.

Armstrong, B. G. and Sloan, M. Ordinal regression models for epidemiologic data. American Journal of Epidemiology, 129(1):191-204, 1989.

Arnold, T. B. and Tibshirani, R. J. Efficient Implementations of the Generalized Lasso Dual Path Algorithm. Journal of Computational and Graphical Statistics, 25(1):1-27, 2016. ISSN 15372715. doi: 10.1080/10618600.2015. 1008638 .

Benedetti, R. Scoring rules for forecast verification. Monthly Weather Review, 138(1):203-211, 2010.

Bickel, P. J., Ritov, Y., and Tsybakov, A. B. Simultaneous Analysis of LASSO and Dantzig Selector. The Annals of Statistics, 37(4):17051732, 2009. doi: 10.1214/08-AOS620. URL https://projecteuclid-org.libproxy1. usc.edu/download/pdfview_1/euclid. aos/1245332830.

Bien, J. The simulator: an engine to streamline simulations. arXiv preprint arXiv:1607.00021, 2016.

Brant, R. Assessing proportionality in the proportional odds model for ordinal logistic regression. Biometrics, pp. $1171-1178,1990$.

Cameron, A. C. and Trivedi, P. K. Microeconometrics: methods and applications. Cambridge university press, 2005.

Casella, G. and Berger, R. L. Statistical inference. Cengage Learning, 2021.

Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.

Chen, Y., Jewell, S., and Witten, D. More powerful selective inference for the graph fused lasso. Journal of Computational and Graphical Statistics, pp. 1-11, 2022.

Christensen, R. H. B., Cleaver, G., and Brockhoff, P. B. Statistical and Thurstonian models for the A-not A protocol with and without sureness. Food Quality and Preference, 22(6):542-549, 2011. ISSN 09503293. doi: 10.1016/j. foodqual.2011.03.003. URL http://dx.doi.org/ 10.1016/j.foodqual.2011.03.003.

Cordeiro, G. M. and McCullagh, P. Bias Correction in Generalized Linear Models. Journal of the Royal Statistical Society: Series B (Methodological), 53(3):629-643, 1991. doi: 10.1111/j.2517-6161.1991.tb01852.x.

Duncan, B. A. and Elkan, C. P. Probabilistic modeling of a sales funnel to prioritize leads. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '15, pp. 1751-1758, New York, NY, USA, 2015. Association for Computing Machinery. ISBN 9781450336642. doi: 10.1145/2783258.2788578. URL https://doi. org/10.1145/2783258.2788578.

Ekvall, K. and Bottai, M. Concave likelihood-based regression with finite-support response variables. Biometrics, (March):1-12, 2022. ISSN 0006-341X. doi: 10.1111/biom. 13760.

Fernández, A., Garcia, S., Herrera, F., and Chawla, N. V. Smote for learning from imbalanced data: progress and challenges, marking the 15-year anniversary. Journal of artificial intelligence research, 61:863-905, 2018.

Greene, W. H. Econometric Analysis. Pearson Education, 7th edition, 2012.

Hansen, B. Econometrics. Princeton University Press, 2022. ISBN 9780691235899. URL https://books. google.com/books?id=Pte7zgEACAAJ.

Hastie, T., Tibshirani, R., and Wainwright, M. Statistical learning with sparsity. Monographs on statistics and applied probability, 143:143, 2015.

He, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T., Shi, Y., Atallah, A., Herbrich, R., Bowers, S., et al. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the eighth international workshop on data mining for online advertising, pp. 1-9, 2014.

Hlavac, M. stargazer: Well-Formatted Regression and Summary Statistics Tables. Social Policy Institute, Bratislava, Slovakia, 2022. URL https://CRAN. R-project.org/package=stargazer. R package version 5.2.3.

Höfling, H., Binder, H., and Schumacher, M. A coordinatewise optimization algorithm for the fused lasso. arXiv preprint arXiv:1011.6409, 2010.

Horn, R. A. and Johnson, C. R. Matrix Analysis. Cambridge University Press, 2 edition, 2012. doi: 10.1017/ 9781139020411.

Hutson, G., Laldin, A., and Velásquez, I. MLDataR: Collection of Machine Learning Datasets for Supervised Machine Learning, 2022. URL https://CRAN. R-project.org/package=MLDataR. R package version 0.1.3.

Hyun, S., G'Sell, M., and Tibshirani, R. J. Exact postselection inference for the generalized lasso path. Electronic Journal of Statistics, 12(1):1053-1097, 2018.

Johnson, J. M. and Khoshgoftaar, T. M. Survey on deep learning with class imbalance. Journal of Big Data, 6(1):27, 2019. doi: 10.1186/ s40537-019-0192-5. URL https://doi.org/10. 1186/s40537-019-0192-5.

Ko, S., Yu, D., and Won, J.-H. Easily parallelizable and distributable class of algorithms for structured sparsity, with optimal acceleration. Journal of Computational and Graphical Statistics, 28(4):821-833, 2019.

Lehmann, E. L. Elements of large-sample theory. Springer, 1999.

McCullagh, P. Regression models for ordinal data. Journal of the Royal Statistical Society: Series B (Methodological), 42(2):109-127, 1980.

Naeini, M. P., Cooper, G., and Hauskrecht, M. Obtaining well calibrated probabilities using bayesian binning. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.

Norris, C. M., Ghali, W. A., Saunders, L. D., Brant, R., Galbraith, D., Faris, P., Knudtson, M. L., Investigators, A., et al. Ordinal regression model and the linear regression model were superior to the logistic regression models. Journal of clinical epidemiology, 59(5):448-456, 2006.

Owen, A. B. Infinitely imbalanced logistic regression. Journal of Machine Learning Research, 8(4), 2007.

Peterson, B. and Harrell Jr, F. E. Partial proportional odds models for ordinal response variables. Journal of the Royal Statistical Society: Series C (Applied Statistics), 39(2):205-217, 1990.

Pößnecker, W. and Tutz, G. A general framework for the selection of effect type in ordinal regression, 2016. URL http://nbn-resolving.de/urn/resolver. pl?urn=nbn:de:bvb:19-epub-26912-0.

Pratt, J. W. Concavity of the log likelihood. Journal of the American Statistical Association, 76(373):103-106, 1981. ISSN 1537274X. doi: 10.1080/01621459.1981. 10477613.
R. H. B. Christensen. ordinal—Regression Models for Ordinal Data, 2019. URL https://CRAN.R-project. org/package=ordinal.

Schwarz, J. and Heider, D. GUESS: projecting machine learning scores to well-calibrated probability estimates for clinical decision-making. Bioinformatics, 35(14): 2458-2465, 11 2018. ISSN 1367-4803. doi: 10.1093/ bioinformatics/bty984. URL https://doi.org/10. 1093/bioinformatics/bty984.

Serfling, R. Approximation theorems of mathematical statistics. Wiley series in probability and mathematical statistics : Probability and mathematical statistics. Wiley, New York, NY [u.a.], [nachdr.] edition, 1980. ISBN 0471024031. URL http://gso.gbv.de/DB=2. 1/CMD?ACT=SRCHA\&SRT=YOP\&IKT=1016\&TRM= ppn+024353353\&sourceid=fbw_bibsonomy.

Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., and Knight, K. Sparsity and smoothness via the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(1):91-108, 2005.

Tibshirani, R. J. and Taylor, J. The solution path of the generalized lasso. The annals of statistics, 39(3):13351371, 2011.

Tutz, G. and Gertheiss, J. Regularized regression for categorical data. Statistical Modelling, 16(3):161-200, 2016.

Ugba, E. R., Mörlein, D., and Gertheiss, J. Smoothing in ordinal regression: An application to sensory data. Stats, 4(3):616-633, 2021.
van der Vaart, A. Asymptotic Statistics. Asymptotic Statistics. Cambridge University Press, 2000. ISBN 9780521784504. URL https://books.google. com/books?id=UEuQEM5RjWgC.

Venables, W. N. and Ripley, B. D. Modern Applied Statistics with S. Springer, New York, fourth edition, 2002. URL http://www.stats.ox.ac.uk/pub/ MASS4. ISBN 0-387-95457-0.

Vershynin, R. Introduction to the non-asymptotic analysis of random matrices, pp. 210-268. Cambridge University Press, 2012. doi: 10.1017/CBO9780511794308.006.

Vershynin, R. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. ISBN 9781108415194. URL https://books.google. com/books?id=J-VjswEACAAJ.

Viallon, V., Lambert-Lacroix, S., Höfling, H., and Picard, F. Adaptive generalized fused-lasso: Asymptotic properties and applications. 2013.

Viallon, V., Lambert-Lacroix, S., Hoefling, H., and Picard, F. On the robustness of the generalized fused lasso to prior specifications. Statistics and Computing, 26(1-2): 285-301, 2016.

Von Wachter, T., Bertrand, M., Pollack, H., Rountree, J., and Blackwell, B. Predicting and preventing homelessness in los angeles. California Policy Lab and University of Chicago Poverty Lab, 2019.

Wickham, H. ggplot2 Elegant Graphics for Data Analysis. Use R! Springer International Publishing, Cham, 2016. ISBN 978-3-319-24275-0. doi: 10.1007/ 978-3-319-24277-4. URL http://dx.doi.org/ 10.1007/978-3-319-24277-4.

Wilke, C. cowplot: streamlined plot theme and plot annotations for 'ggplot2'. r package version 1.1.1. https://CRAN. R-project. org/package= cowplot, 2020.

Wooldridge, J. M. Econometric analysis of cross section and panel data. MIT press, 2010.

Wurm, M. J., Rathouz, P. J., and Hanlon, B. M. Regularized ordinal regression and the ordinalnet r package. arXiv preprint arXiv:1706.05003, 2017.

Wurm, M. J., Rathouz, P. J., and Hanlon, B. M. Regularized ordinal regression and the ordinalNet R package. Journal of Statistical Software, 99(6):1-42, 2021.

Xin, B., Kawahara, Y., Wang, Y., and Gao, W. Efficient generalized fused lasso and its application to the diagnosis of alzheimer's disease. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 28, 2014.

Zhang, F. The Schur Complement and Its Applications. Numerical Methods and Algorithms. Springer, 2005. ISBN 9780387242712. URL https://books.google. com/books?id=Wjd8_AwjiIIC.

Zhang, W., Yuan, S., and Wang, J. Optimal real-time bidding for display advertising. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1077-1086, 2014.

Zhu, Y. An augmented admm algorithm with application to the generalized lasso problem. Journal of Computational and Graphical Statistics, 26(1):195-204, 2017.

We state and briefly discuss Theorem A.1, which shows that logistic regression does arbitrarily badly at estimating rare class probabilities as the class imbalance increases, in Section A. We present a second real data experiment on data from patients diagnosed with diabetes in Section B. In Section C, we display summary statistics and additional figures for the observed mean squared errors (MSEs) for each method from the synthetic data experiments from Sections 4.1 and 4.2. We also briefly investigate the effect of implementing PRESTO with a squared $\ell_{2}$ (ridge) penalty rather than an $\ell_{1}$ penalty in Section C.1. We provide the proofs of Theorems 2.2 and A. 1 in Section D. In Section E, we present synthetic data experiments and analysis justifying the validity of one of the assumptions of Theorem 2.3 in Sections E. 1 and E.2, and we then prove Theorem 2.3. Theorems 2.2, A.1, and 2.3 depend on Lemma D.1, which is stated at the beginning of Section D and proven in Section F. We prove Theorem 3.1 in Section G. Finally, in Section H we provide implementation details for estimating PRESTO.

# A. Theorem A. 1

It is well-known that class imbalance poses a major challenge for classifiers. Theorem A. 1 exhibits this concretely for logistic regression.
Theorem A.1. Assume $X\left(\mathbb{R}^{p}, \lambda_{\max }\right)$ and $Y(2)$ hold. Let $\pi(\boldsymbol{x}):=\mathbb{P}(y=2 \mid \boldsymbol{x})$, and assume that $\sup _{\boldsymbol{x} \in \mathcal{S}} \pi(\boldsymbol{x})=\pi_{\text {rare }}$ for some $\pi_{\text {rare }} \leq 1 / 2$. Then

1. for any fixed $\boldsymbol{v} \in \mathbb{R}^{p+1}$,

$$
\frac{1}{\|\boldsymbol{v}\|_{2}^{2}} \operatorname{Asym} . \operatorname{MSE}\left(\left(\hat{\alpha}, \hat{\boldsymbol{\beta}}^{\top}\right) \boldsymbol{v}\right) \geq \frac{1}{\lambda_{\max } \pi_{\text {rare }}}
$$

and
2. for any fixed $\boldsymbol{z} \in \mathcal{S}$,

$$
\operatorname{Asym} . \operatorname{MSE}\left(\frac{\hat{\pi}(\boldsymbol{z})}{\pi(\boldsymbol{z})}\right) \geq \frac{1-\pi_{\text {rare }}}{\pi_{\text {rare }}} \frac{1}{\lambda_{\text {max }}}
$$

Proof. Provided in Section D. 2 .
To give an example of applying part 1 of this result, consider the choice $\boldsymbol{v}=(0,1,0, \ldots, 0)$. Then we have that Asym.MSE $\left(\hat{\beta}_{1}\right) \geq 1 /\left(\lambda_{\max } \pi_{\text {rare }}\right)$, so $\hat{\beta}_{1}$ (or any other estimated coefficient) has arbitrarily large asymptotic mean squared error as $\pi_{\text {rare }}$ vanishes. Part 2 shows that the same thing happens to the asymptotic mean squared error for the estimated probabilities of the logistic regression estimator, when scaled by $\pi(\boldsymbol{z})$.

## B. Real Data Experiment 2: Diabetes

We present another real data experiment using the data set PreDiabetes from the R MLDataR package (Hutson et al., 2022). This data set contains $n=3059$ observations of patients who were eventually diagnosed with diabetes. Each observation consists of the age at which the patient was diagnosed with prediabetes and diabetes as well as $p=5$ covariates. Given an age $a$, we form an ordinal variable based on the patient's status of non-diabetic, prediabetic, or diabetic at age $a-1$. We do this for ages $a \in\{30,35,40, \ldots, 65\}$. The number of patients diagnosed with diabetes increases with $a$, so varying $a$ allows us to change the rarity of the rarest class in a natural way. $0.92 \%$ of patients in the data were diagnosed with diabetes before age $a=30$, and $50.93 \%$ of the patients were diagnosed with diabetes before age $a=65$.

We use PRESTO, logistic regression, and the proportional odds model to estimate the probability that each patient was diagnosed with diabetes before age $a$ for each $a$. Much like our soup tasting data application, in each setting we take repeated random splits of the data, using $90 \%$ of the data selected at random for training and $10 \%$ for testing. In each iteration we again evaluate each method on the test data using the same estimator for mean squared error of the estimated rare class probabilities. We repeat this procedure 49 times in each of the 8 settings.

We display the results in a plot in Figure 4. We also provide the mean MSEs for each method at each age cutoff in Table 2. We see that PRESTO outperforms both logistic regression and the proportional odds model in all of these settings. (For age cutoffs $a=29$ and below we were unable to estimate the proportional odds model on all subsamples because of the

![img-3.jpeg](img-3.jpeg)

Figure 4. Estimated MSEs of estimated rare class probabilities for each method and each age cutoff across 49 random draws of training and test sets in real data experiment from Section B.

Table 2. Estimated rare class MSE for each method at each age cutoff in prediabetes real data experiment from Section B.

| Age cutoff | PRESTO | Logit | PO |
| :-- | :-- | :-- | :-- |
| 30 | 0.000943 | 0.009609 | 0.009217 |
| 35 | 0.005013 | 0.023658 | 0.021740 |
| 40 | 0.017307 | 0.046828 | 0.048123 |
| 45 | 0.060896 | 0.115189 | 0.117525 |
| 50 | 0.124000 | 0.211083 | 0.213059 |
| 55 | 0.234906 | 0.336009 | 0.340130 |
| 60 | 0.353615 | 0.413534 | 0.418179 |
| 65 | 0.345535 | 0.448015 | 0.447290 |

difficulty of having at least one observation from each class in both the training and test sets for 49 random draws.) PRESTO seems to outperform the other methods at all class rarities, and the absolute performance gap increases as the rare class becomes less rare.

# C. More Simulation Results

For more results from the synthetic experiments, see Tables 3, 4, and 5, along with Figures 5, 6, 7, 8, and 9.

Table 3. Similar to Table 1; calculated $p$-values for one-tailed paired $t$-tests for sparse differences simulation setting of Section 4.1 (statistically significant $p$-values indicate better performance for PRESTO).

| Rare Prop. | Sparsity | Logit $p$-value | PO $p$-value |
| :--: | :--: | :--: | :--: |
| $1 \%$ | $1 / 3$ | $1.69 \mathrm{e}-33$ | $6.42 \mathrm{e}-41$ |
| $1.17 \%$ | $1 / 2$ | $1.61 \mathrm{e}-15$ | $2.78 \mathrm{e}-66$ |
| $0.61 \%$ | $1 / 3$ | $5.19 \mathrm{e}-74$ | $4.21 \mathrm{e}-19$ |
| $0.71 \%$ | $1 / 2$ | $8.68 \mathrm{e}-48$ | $3.38 \mathrm{e}-35$ |
| $0.37 \%$ | $1 / 3$ | $3.08 \mathrm{e}-61$ | 0.00165 |
| $0.43 \%$ | $1 / 2$ | $3.75 \mathrm{e}-64$ | $2.57 \mathrm{e}-11$ |

Table 4. Means and standard errors of empirical MSEs for each method in each of three intercept settings in the sparse differences synthetic experiment setting of Section 4.1.

| Rare Class Proportion | Sparsity | PRESTO | Logistic Regression | Proportional Odds |
| :--: | :--: | :--: | :--: | :--: |
| $1 \%$ | $1 / 3$ | $6.05 \mathrm{e}-05(2.1 \mathrm{e}-06)$ | $9.38 \mathrm{e}-05(2.5 \mathrm{e}-06)$ | $8.62 \mathrm{e}-05(3.1 \mathrm{e}-06)$ |
| $1.17 \%$ | $1 / 2$ | $9.87 \mathrm{e}-05(2.9 \mathrm{e}-06)$ | $1.25 \mathrm{e}-04(3.3 \mathrm{e}-06)$ | $1.66 \mathrm{e}-04(5.5 \mathrm{e}-06)$ |
| $0.61 \%$ | $1 / 3$ | $3.03 \mathrm{e}-05(1.1 \mathrm{e}-06)$ | $6.90 \mathrm{e}-05(2.1 \mathrm{e}-06)$ | $3.64 \mathrm{e}-05(1.4 \mathrm{e}-06)$ |
| $0.71 \%$ | $1 / 2$ | $5.22 \mathrm{e}-05(1.9 \mathrm{e}-06)$ | $8.89 \mathrm{e}-05(2.5 \mathrm{e}-06)$ | $7.50 \mathrm{e}-05(3 \mathrm{e}-06)$ |
| $0.37 \%$ | $1 / 3$ | $1.40 \mathrm{e}-05(6 \mathrm{e}-07)$ | $5.66 \mathrm{e}-05(2.4 \mathrm{e}-06)$ | $1.49 \mathrm{e}-05(6.1 \mathrm{e}-07)$ |
| $0.43 \%$ | $1 / 2$ | $2.63 \mathrm{e}-05(1 \mathrm{e}-06)$ | $7.21 \mathrm{e}-05(2.7 \mathrm{e}-06)$ | $3.17 \mathrm{e}-05(1.4 \mathrm{e}-06)$ |

Table 5. Means and standard errors of empirical MSEs for each method in each of four intercept settings in the uniform differences synthetic experiment setting of Section 4.2.

| Rare Class Proportion | PRESTO | Logistic Regression | Proportional Odds |
| :--: | :--: | :--: | :--: |
| $1.6 \%$ | $1.13 \mathrm{e}-04(2.7 \mathrm{e}-06)$ | $1.35 \mathrm{e}-04(2.9 \mathrm{e}-06)$ | $1.85 \mathrm{e}-04(5.2 \mathrm{e}-06)$ |
| $0.99 \%$ | $5.82 \mathrm{e}-05(1.7 \mathrm{e}-06)$ | $9.25 \mathrm{e}-05(2.3 \mathrm{e}-06)$ | $8.53 \mathrm{e}-05(2.6 \mathrm{e}-06)$ |
| $0.62 \%$ | $2.86 \mathrm{e}-05(8.3 \mathrm{e}-07)$ | $6.51 \mathrm{e}-05(1.7 \mathrm{e}-06)$ | $3.43 \mathrm{e}-05(9.8 \mathrm{e}-07)$ |
| $0.36 \%$ | $1.33 \mathrm{e}-05(4.4 \mathrm{e}-07)$ | $5.36 \mathrm{e}-05(2.2 \mathrm{e}-06)$ | $1.43 \mathrm{e}-05(5 \mathrm{e}-07)$ |

## C.1. Ridge PRESTO

We briefly investigate the effect of implementing PRESTO with a ridge penalty instead of an $\ell_{1}$ penalty, similarly to proposals by Tutz \& Gertheiss (2016, Section 4.2.2) and Ugba et al. (2021, Equation 8),

$$
\lambda_{n}\left(\sum_{j=1}^{p} \beta_{j 1}^{2}+\sum_{j=1}^{p} \sum_{k=2}^{K-1}\left(\beta_{j k}-\beta_{j, k-1}\right)^{2}\right)
$$

We implement this method ("PRESTO_L2") in the sparse differences synthetic data experiment of Section 4.1 on the same simulated data that was used for the other methods in the intercept setting $(0,3,5)$ for both sparsity levels. The

![img-4.jpeg](img-4.jpeg)

Figure 5. MSE of predicted rare class probabilities for each method across all $n=2500$ observations, across 700 simulations, in sparse differences synthetic experiment setting of Section 4.1 with sparsity $1 / 3$. (These plots are for the two intercept settings that weren't shown in the main text for the sparsity setting of $1 / 3$ )
implementation is identical to PRESTO in every way except for the ridge penalty-the method is implemented using our modification of the ordinal Net R package and the tuning parameter is selected in the same way.

Figures 10 and 11 display the results. (The results for all methods but PRESTO_L2 are identical to previous plots and are only displayed for reference.) We also present the means and standard deviations of the MSEs for each method in these settings in Table 6, and $p$-values for one-tailed paired $t$-tests of the alternative hypothesis that PRESTO has a lower MSE than the competitor methods in Table 7. We see that in practice, PRESTO and PRESTO_L2 seem to perform similarly in our setting, though the $t$-tests show that PRESTO does outperform PRESTO_L2 at a $5 \%$ significance level in both settings. We might expect PRESTO to better outperform PRESTO_L2 in settings where $p / n$ is larger and where the sparsity is lower. We also note it is not clear if PRESTO_L2 enjoys a high-dimensional consistency guarantee similar to Theorem 2.3.

Table 6. Means and standard errors of empirical MSEs for each method in each of three intercept settings in the sparse differences synthetic experiment setting of Section 4.1 for intercept setting of $(0,3,5)$, with PRESTO_L2 implemented as well, as described in Section C.1.

| Rare Prop. | Sparsity | PRESTO | Logistic Regression | Proportional Odds | PRESTO_L2 |
| :--: | :--: | :--: | :--: | :--: | :--: |
| $1 \%$ | $1 / 3$ | $6.05 \mathrm{e}-05(2.1 \mathrm{e}-06)$ | $9.38 \mathrm{e}-05(2.5 \mathrm{e}-06)$ | $8.62 \mathrm{e}-05(3.1 \mathrm{e}-06)$ | $6.25 \mathrm{e}-05(2.4 \mathrm{e}-06)$ |
| $1.17 \%$ | $1 / 2$ | $9.87 \mathrm{e}-05(2.9 \mathrm{e}-06)$ | $1.25 \mathrm{e}-04(3.3 \mathrm{e}-06)$ | $1.66 \mathrm{e}-04(5.5 \mathrm{e}-06)$ | $1.17 \mathrm{e}-04(4.2 \mathrm{e}-06)$ |

Table 7. Calculated $p$-values for one-tailed paired $t$-tests for sparse differences simulation setting of Section 4.1 for intercept setting of $(0,3,5)$, with PRESTO_L2 implemented as well, as described in Section C.1. (Statistically significant $p$-values indicate better performance for PRESTO).

| Rare Class Proportion | Sparsity | Logistic Regression | Proportional Odds | PRESTO_L2 |
| :--: | :--: | :--: | :--: | :--: |
| $1 \%$ | $1 / 3$ | $1.69 \mathrm{e}-33$ | $6.42 \mathrm{e}-41$ | 0.0358 |
| $1.17 \%$ | $1 / 2$ | $1.61 \mathrm{e}-15$ | $2.78 \mathrm{e}-66$ | $1.51 \mathrm{e}-14$ |

![img-5.jpeg](img-5.jpeg)

Figure 6. Same as Figure 1, but for the simulations with sparsity $1 / 2$.

![img-6.jpeg](img-6.jpeg)

Figure 7. Same as Figure 5, but for the simulations with sparsity $1 / 2$.

Rare Proportion: $0.994 \%$
![img-7.jpeg](img-7.jpeg)

Rare Proportion: $0.364 \%$
![img-8.jpeg](img-8.jpeg)

Figure 8. MSE of predicted rare class probabilities for each method across all $n=2500$ observations, across 700 simulations, in uniform differences synthetic experiment setting of Section 4.2. (These plots are for two of the intercept settings that weren't shown in the main text.)

![img-9.jpeg](img-9.jpeg)

Figure 9. MSE of predicted rare class probabilities for each method across all $n=2500$ observations, across 700 simulations, in uniform differences synthetic experiment setting of Section 4.2 for intercept setting of $(0,2.5,4.5)$.

Rare Proportion: $0.996 \%$
![img-10.jpeg](img-10.jpeg)

Rare Proportion: $0.996 \%$
![img-11.jpeg](img-11.jpeg)

Figure 10. MSE of predicted rare class probabilities for each method across all $n=2500$ observations, across 700 simulations, in sparse differences synthetic experiment setting of Section 4.1 with sparsity $1 / 3$ for intercept setting of $(0,3,5)$, with PRESTO_L2 implemented as well, as described in Section C.1.

![img-12.jpeg](img-12.jpeg)

Figure 11. MSE of predicted rare class probabilities for each method across all $n=2500$ observations, across 700 simulations, in sparse differences synthetic experiment setting of Section 4.1 with sparsity $1 / 2$ for intercept setting of $(0,3,5)$, with PRESTO_L2 implemented as well, as described in Section C.1.

# D. Statement of Lemma D. 1 and Proofs of Theorems A. 1 and 2.2

In Section D. 1 we state Lemma D.1, and we prove Theorems A. 1 and 2.2 in Section D.2.

## D.1. Statement of Lemma D. 1

Theorems A.1, 2.2, and 2.3 relate to the asymptotic covariance matrices of the maximum likelihood estimators of the parameters of the proportional odds and logistic regression models. Under mild regularity conditions, the asymptotic covariance matrix of any maximum likelihood estimator (when scaled by $\sqrt{n}$ ) is known to be the inverse of the Fisher information matrix

$$
-\mathbb{E}\left[\frac{\partial^{2}}{\partial \boldsymbol{\theta} \boldsymbol{\theta}^{\top}} \mathcal{L}(\boldsymbol{\theta})\right]
$$

where $\boldsymbol{\theta}$ are the parameters estimated by the model and $\mathcal{L}(\boldsymbol{\theta})$ is the log likelihood (Serfling, 1980, Section 4.2.2). In the proof of Lemma D.1, we calculate these Fisher information matrices for the proportional odds and logistic regression models and verify the needed regularity conditions.

Lemma D.1. Assume that no class has probability 0 for any $\boldsymbol{x} \in \mathcal{S}$ (equivalently, assume that all of the intercepts in the proportional odds model (1) are not equal, so $\alpha_{1}<\ldots<\alpha_{K-1}$ ). Assume that $d F(\boldsymbol{x})$ has bounded support.

1. The Fisher information matrix for the maximum likelihood estimator of the proportional odds model (1) is

$$
I^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})=\left(\begin{array}{ll}
I_{\alpha \alpha}^{\text {prop. odds }} & \left(I_{\beta \alpha}^{\text {prop. odds }}\right)^{\top} \\
I_{\beta \alpha}^{\text {prop. odds }} & I_{\beta \beta}^{\text {prop. odds }}
\end{array}\right) \in \mathbb{R}^{(K-1+p) \times(K-1+p)}
$$

where

$$
\begin{aligned}
I_{\alpha \alpha}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta}) & =\left(\begin{array}{cccccc}
## M_{1} & -\tilde{M}_{2} & 0 & \cdots & 0 & 0 \\
-\tilde{M}_{2} & M_{2} & -\tilde{M}_{3} & \cdots & 0 & 0 \\
0 & -\tilde{M}_{3} & M_{3} & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & M_{K-2} & -\tilde{M}_{K-1} \\
0 & 0 & 0 & \cdots & -\tilde{M}_{K-1} & M_{K-1}
\end{array}\right) \\
I_{\beta \alpha}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta}) & =\left(\begin{array}{c}
## J_{1}^{\boldsymbol{x}}+\tilde{J}_{2}^{\boldsymbol{x}} \\
## J_{2}^{\boldsymbol{x}}+\tilde{J}_{3}^{\boldsymbol{x}} \\
\vdots \\
## J_{K-1}^{\boldsymbol{x}}+\tilde{J}_{K}^{\boldsymbol{x}}
\end{array}\right), \quad \text { and } \\
I_{\beta \beta}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta}) & =\sum_{k=1}^{K}\left(J_{k}^{\boldsymbol{x} \boldsymbol{x}^{\top}}+\tilde{J}_{k}^{\boldsymbol{x} \boldsymbol{x}^{\top}}\right)
\end{aligned}
$$

where

$$
\begin{aligned}
& M_{k}:=\int\left[p_{k}(\boldsymbol{x})\left(1-p_{k}(\boldsymbol{x})\right)\right]^{2}\left(\frac{1}{\pi_{k}(\boldsymbol{x})}+\frac{1}{\pi_{k+1}(\boldsymbol{x})}\right) d F(\boldsymbol{x}), \quad k \in\{1, \ldots, K-1\} \\
& \tilde{M}_{k}:=\int p_{k}(\boldsymbol{x})\left(1-p_{k}(\boldsymbol{x})\right) p_{k-1}(\boldsymbol{x})\left(1-p_{k-1}(\boldsymbol{x})\right) \cdot \frac{1}{\pi_{k}(\boldsymbol{x})} d F(\boldsymbol{x}), \quad k \in\{2, \ldots, K-1\}
\end{aligned}
$$

and

$$
\begin{aligned}
J_{k} & :=\int \pi_{k}(\boldsymbol{x}) p_{k}(\boldsymbol{x})\left[1-p_{k}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \in \mathbb{R} \\
J_{k}^{\boldsymbol{x}} & :=\int \boldsymbol{x} \pi_{k}(\boldsymbol{x}) p_{k}(\boldsymbol{x})\left[1-p_{k}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \in \mathbb{R}^{p} \\
J_{k}^{\boldsymbol{x} \boldsymbol{x}^{\top}} & :=\int \boldsymbol{x} \boldsymbol{x}^{\top} \pi_{k}(\boldsymbol{x}) p_{k}(\boldsymbol{x})\left[1-p_{k}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \in \mathbb{R}^{p \times p} \\
\tilde{J}_{k} & :=\int \pi_{k}(\boldsymbol{x}) p_{k-1}(\boldsymbol{x})\left[1-p_{k-1}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \in \mathbb{R} \\
\tilde{J}_{k}^{\boldsymbol{x}} & :=\int \boldsymbol{x} \pi_{k}(\boldsymbol{x}) p_{k-1}(\boldsymbol{x})\left[1-p_{k-1}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \in \mathbb{R}^{p}, \quad \text { and } \\
\tilde{J}_{k}^{\boldsymbol{x} \boldsymbol{x}^{\top}} & :=\int \boldsymbol{x} \boldsymbol{x}^{\top} \pi_{k}(\boldsymbol{x}) p_{k-1}(\boldsymbol{x})\left[1-p_{k-1}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \in \mathbb{R}^{p \times p}
\end{aligned}
$$

for all $k \in[K]$.
2. The Fisher information matrix for the maximum likelihood estimator of the logistic regression model predicting whether or not each observation is in class 1 is

$$
## I^{\text {logistic }}\left(\alpha_{1}, \boldsymbol{\beta}\right)=\left(\begin{array}{ll}
## I_{\alpha \alpha}^{\text {logistic }} & \left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top} \\
## I_{\beta \alpha}^{\text {logistic }} & I_{\beta \beta}^{\text {logistic }}
\end{array}\right)=\mathbb{E}\left[\pi_{1}(\boldsymbol{X})\left[1-\pi_{1}(\boldsymbol{X})\right] \widehat{\boldsymbol{X}} \widehat{\boldsymbol{X}}^{\top}\right] \in \mathbb{R}^{(p+1) \times(p+1)}
$$

where $\widehat{\boldsymbol{X}}:=\left(\begin{array}{ll}1 & \boldsymbol{X}\end{array}\right)$ (an n-vector of all ones followed by $\boldsymbol{X}$ ) and

$$
\begin{aligned}
I_{\alpha \alpha}^{\text {logistic }}\left(\alpha_{1}, \boldsymbol{\beta}\right) & =M_{1}^{\text {logistic }} \\
I_{\beta \alpha}^{\text {logistic }}\left(\alpha_{1}, \boldsymbol{\beta}\right) & =J_{1}^{\boldsymbol{x} ; \text { logistic }}+\tilde{J}_{2}^{\boldsymbol{x} ; \text { logistic }}, \quad \text { and } \\
I_{\beta \beta}^{\text {logistic }}\left(\alpha_{1}, \boldsymbol{\beta}\right) & =J_{1}^{\boldsymbol{x} \boldsymbol{x}^{\top} ; \text { logistic }}+\tilde{J}_{2}^{\boldsymbol{x} \boldsymbol{x}^{\top} ; \text { logistic }}
\end{aligned}
$$

where we define

$$
M_{1}^{\text {logistic }}:=\int \pi_{1}(\boldsymbol{x})\left(1-\pi_{1}(\boldsymbol{x})\right) d F(\boldsymbol{x})
$$

and

$$
\begin{aligned}
J_{1}^{\boldsymbol{x}: \text { logistic }} & :=\int \boldsymbol{x} \pi_{1}(\boldsymbol{x})^{2}\left[1-\pi_{1}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \in \mathbb{R}^{p}=J_{1}^{\boldsymbol{x}} \\
J_{1}^{\boldsymbol{x} \boldsymbol{x}^{\top}: \text { logistic }} & :=\int \boldsymbol{x} \boldsymbol{x}^{\top} \pi_{1}(\boldsymbol{x})^{2}\left[1-\pi_{1}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \in \mathbb{R}^{p \times p}=J_{1}^{\boldsymbol{x} \boldsymbol{x}^{\top}} \\
\tilde{J}_{2}^{\boldsymbol{x}: \text { logistic }} & :=\int \boldsymbol{x} \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right]^{2} d F(\boldsymbol{x}) \in \mathbb{R}^{p}, \quad \text { and } \\
\tilde{J}_{2}^{\boldsymbol{x} \boldsymbol{x}^{\top}: \text { logistic }} & :=\int \boldsymbol{x} \boldsymbol{x}^{\top} \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right]^{2} d F(\boldsymbol{x}) \in \mathbb{R}^{p \times p}
\end{aligned}
$$

3. The information matrices $I^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})$ and $I^{\text {logistic }}\left(\alpha_{1}, \boldsymbol{\beta}\right)$ are finite and positive definite, and the following convergences hold:

$$
\sqrt{n} \cdot\left(\tilde{\boldsymbol{\theta}}_{1}^{\text {prop. odds }}-\boldsymbol{\theta}_{1}\right) \xrightarrow{d} \mathcal{N}\left(\mathbf{0},\left(I^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right)^{-1}\right)
$$

and

$$
\sqrt{n} \cdot\left(\tilde{\boldsymbol{\theta}}_{1}^{\text {logistic }}-\boldsymbol{\theta}_{1}\right) \xrightarrow{d} \mathcal{N}\left(\mathbf{0},\left(I^{\text {logistic }}\left(\alpha_{1}, \boldsymbol{\beta}\right)\right)^{-1}\right)
$$

Further, because these information matrices are symmetric and positive definite, by Observation 7.1.2 in Horn \& Johnson (2012) the principal submatrices $I_{\alpha \alpha}^{\text {prop. odds }}:=I_{\alpha \alpha}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta}), I_{\beta \beta}^{\text {prop. odds }}:=I_{\beta \beta}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})$, and $I_{\beta \beta}^{\text {logistic }}:=$ $I_{\beta \beta}^{\text {logistic }}\left(\alpha_{1}, \boldsymbol{\beta}\right)$ are all positive definite. Finally, we can characterize the finite-sample bias: for any $\boldsymbol{v} \in \mathbb{R}^{p+1}$, $\left[\left(\hat{\alpha}, \hat{\boldsymbol{\beta}}^{\top}\right)-(\alpha, \boldsymbol{\beta})^{\top}\right] \boldsymbol{v}=\mathcal{O}(1 / n)$.

Proof. Provided in Section F.

Remark D.2. Note that $J_{K}=0$ because $1-p_{K}(\boldsymbol{x})=1-\mathbb{P}(y(\boldsymbol{x}) \leq K \mid \boldsymbol{x})=0$ for all $\boldsymbol{x}$, and similarly for $J_{K}^{\boldsymbol{x}}$ and $J_{K}^{\boldsymbol{x} \boldsymbol{x}^{\top}}$. Likewise, $\tilde{J}_{1}=0$ because $p_{0}(\boldsymbol{x})=\mathbb{P}(y(\boldsymbol{x}) \leq 0 \mid \boldsymbol{x})=0$ for all $\boldsymbol{x}$, and similarly for $\tilde{J}_{1}^{\boldsymbol{x}}$ and $\tilde{J}_{1}^{\boldsymbol{x} \boldsymbol{x}^{\top}}$.
We also take a moment to briefly establish some identities we will use later. For any $k \in\{1, \ldots, K-1\}$,

$$
\begin{aligned}
J_{k}^{\boldsymbol{x}}+\tilde{J}_{k+1}^{\boldsymbol{x}} & =\int \boldsymbol{x} \pi_{k}(\boldsymbol{x}) p_{k}(\boldsymbol{x})\left[1-p_{k}(\boldsymbol{x})\right] d F(\boldsymbol{x})+\int \boldsymbol{x} \pi_{k+1}(\boldsymbol{x}) p_{k}(\boldsymbol{x})\left[1-p_{k}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \\
& =\int \boldsymbol{x}\left[\pi_{k}(\boldsymbol{x})+\pi_{k+1}(\boldsymbol{x})\right] p_{k}(\boldsymbol{x})\left[1-p_{k}(\boldsymbol{x})\right] d F(\boldsymbol{x})
\end{aligned}
$$

## Similarly,

$$
J_{k}^{\boldsymbol{x} \boldsymbol{x}^{\top}}+\tilde{J}_{k+1}^{\boldsymbol{x} \boldsymbol{x}^{\top}}=\int \boldsymbol{x} \boldsymbol{x}^{\top}\left[\pi_{k}(\boldsymbol{x})+\pi_{k+1}(\boldsymbol{x})\right] p_{k}(\boldsymbol{x})\left[1-p_{k}(\boldsymbol{x})\right] d F(\boldsymbol{x})
$$

so from (10) we have

$$
\begin{aligned}
I_{\beta \beta}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})= & \sum_{k=1}^{K}\left(J_{k}^{\boldsymbol{x} \boldsymbol{x}^{\top}}+\tilde{J}_{k}^{\boldsymbol{x} \boldsymbol{x}^{\top}}\right) \\
= & \tilde{J}_{1}^{\boldsymbol{x} \boldsymbol{x}^{\top}}+\sum_{k=1}^{K-1}\left(J_{k}^{\boldsymbol{x} \boldsymbol{x}^{\top}}+\tilde{J}_{k+1}^{\boldsymbol{x} \boldsymbol{x}^{\top}}\right)+J_{K}^{\boldsymbol{x} \boldsymbol{x}^{\top}} \\
= & \int \boldsymbol{x} \boldsymbol{x}^{\top} \pi_{1}(\boldsymbol{x}) \underbrace{p_{0}(\boldsymbol{x})}_{=\mathbb{P}(y \leq 0 \mid \boldsymbol{x})=0}\left[1-p_{0}(\boldsymbol{x})\right] d F(\boldsymbol{x})+\sum_{k=1}^{K-1}\left(J_{k}^{\boldsymbol{x} \boldsymbol{x}^{\top}}+\tilde{J}_{k+1}^{\boldsymbol{x} \boldsymbol{x}^{\top}}\right) \\
& +\int \boldsymbol{x} \boldsymbol{x}^{\top} \pi_{K}(\boldsymbol{x}) p_{K}(\boldsymbol{x})\left[1-\underbrace{p_{K}(\boldsymbol{x})}_{=\mathbb{P}(y \leq K \mid \boldsymbol{x})=1}\right] d F(\boldsymbol{x}) \\
= & \sum_{k=1}^{K-1} \int \boldsymbol{x} \boldsymbol{x}^{\top}\left[\pi_{k}(\boldsymbol{x})+\pi_{k+1}(\boldsymbol{x})\right] p_{k}(\boldsymbol{x})\left[1-p_{k}(\boldsymbol{x})\right] d F(\boldsymbol{x})
\end{aligned}
$$

# D.2. Proofs of Theorems A. 1 and 2.2

Equipped with the results of Lemma D.1, we proceed to prove Theorems A. 1 and 2.2. (Recall that for square matrices $\boldsymbol{A}$ and $\boldsymbol{B}$ of equal dimension $p$, we say $\boldsymbol{A} \preceq \boldsymbol{B}$ if $\boldsymbol{B}-\boldsymbol{A}$ is positive semidefinite.)

Proof of Theorem A.1. 1. Note that the assumptions of Lemma D. 1 are satisfied. Since the inverse of the asymptotic covariance matrix is

$$
\mathbb{E}\left[\pi(\boldsymbol{X})[1-\pi(\boldsymbol{X})] \tilde{\boldsymbol{X}} \tilde{\boldsymbol{X}}^{\top}\right] \preceq \pi_{\text {rare }}\left(1-\pi_{\text {rare }}\right) \mathbb{E}\left[\tilde{\boldsymbol{X}} \tilde{\boldsymbol{X}}^{\top}\right]
$$

(where the second step is valid because $t \mapsto t(1-t)$ is monotone increasing in $t$ for $t \in[0,1 / 2]$ ), by Corollary 7.7.4 in Horn \& Johnson (2012) the largest eigenvalue of the inverse of the asymptotic covariance matrix is no larger than $\pi_{\text {rare }}\left(1-\pi_{\text {rare }}\right) \lambda_{\text {max }}$. Therefore

$$
\operatorname{Asym} . \operatorname{Cov}\left((\sqrt{n} \cdot \hat{\alpha}, \sqrt{n} \cdot \hat{\boldsymbol{\beta}})\right)=\left(\mathbb{E}\left[\pi(\boldsymbol{X})[1-\pi(\boldsymbol{X})] \tilde{\boldsymbol{X}} \tilde{\boldsymbol{X}}^{\top}\right]\right)^{-1} \succeq\left(\pi_{\text {rare }}\left(1-\pi_{\text {rare }}\right) \mathbb{E}\left[\tilde{\boldsymbol{X}} \tilde{\boldsymbol{X}}^{\top}\right]\right)^{-1}
$$

has smallest eigenvalue at least $1 /\left[\lambda_{\max } \pi_{\text {rare }}\left(1-\pi_{\text {rare }}\right)\right]$, which is larger than $1 /\left[\lambda_{\max } \pi_{\text {rare }}\right]$, again using Corollary 7.7.4 in Horn \& Johnson (2012). So for any $\boldsymbol{v} \in \mathbb{R}^{p+1}$, by Theorem 5.1.8 in Lehmann (1999)

$$
\operatorname{Asym} . \operatorname{Var}\left(\left(\hat{\alpha}, \hat{\boldsymbol{\beta}}^{\top}\right) \boldsymbol{v}\right)=\boldsymbol{v}^{\top} \operatorname{Asym} . \operatorname{Cov}\left((\sqrt{n} \cdot \hat{\alpha}, \sqrt{n} \cdot \hat{\boldsymbol{\beta}})\right) \boldsymbol{v} \geq \frac{\boldsymbol{v}^{\top} \boldsymbol{v}}{\lambda_{\max } \pi_{\text {rare }}}
$$

Finally, since we have already shown that $\left(\hat{\alpha}, \hat{\boldsymbol{\beta}}^{\top}\right)$ is asymptotically unbiased, the asymptotic MSE is equal to this asymptotic variance:

$$
\begin{aligned}
\operatorname{Asym} . \operatorname{MSE}\left(\left(\hat{\alpha}, \hat{\boldsymbol{\beta}}^{\top}\right) \boldsymbol{v}\right))= & \mathbb{E}\left[\lim _{n \rightarrow \infty}\left(\sqrt{n} \cdot\left[\left(\hat{\alpha}, \hat{\boldsymbol{\beta}}^{\top}\right) \boldsymbol{v}-\left(\alpha, \boldsymbol{\beta}^{\top}\right) \boldsymbol{v}\right]\right)^{2}\right] \\
= & \mathbb{E}\left[\lim _{n \rightarrow \infty}\left(\sqrt{n} \cdot\left[\mathbb{E}\left[\left(\hat{\alpha}, \hat{\boldsymbol{\beta}}^{\top}\right) \boldsymbol{v}\right]-\left(\hat{\alpha}, \hat{\boldsymbol{\beta}}^{\top}\right) \boldsymbol{v}\right]\right)^{2}\right. \\
& \left.+\lim _{n \rightarrow \infty}\left(\sqrt{n} \cdot\left[\left(\alpha, \boldsymbol{\beta}^{\top}\right) \boldsymbol{v}-\mathbb{E}\left[\left(\hat{\alpha}, \hat{\boldsymbol{\beta}}^{\top}\right) \boldsymbol{v}\right]\right)^{2}\right]\right] \\
= & \operatorname{Asym} . \operatorname{Var}\left(\sqrt{n}\left[\left(\alpha, \boldsymbol{\beta}^{\top}\right) \boldsymbol{v}-\left(\hat{\alpha}, \hat{\boldsymbol{\beta}}^{\top}\right) \boldsymbol{v}\right]\right)+0 \\
\geq & \frac{\boldsymbol{v}^{\top} \boldsymbol{v}}{\lambda_{\max } \pi_{\text {rare }}}
\end{aligned}
$$

where in the second-to-last step we used $\left[\left(\hat{\alpha}, \hat{\boldsymbol{\beta}}^{\top}\right)-(\alpha, \boldsymbol{\beta})^{\top}\right] \boldsymbol{v}=\mathcal{O}(1 / n)$ from Lemma D.1.

2. Because $(\hat{\alpha}, \hat{\boldsymbol{\beta}}) \mapsto \hat{\pi}(\boldsymbol{z})$ is differentiable for all $\boldsymbol{z} \in \mathbb{R}^{p}$, by the delta method (Theorem 3.1 in van der Vaart 2000)

$$
\sqrt{n} \cdot[\hat{\pi}(\boldsymbol{z})-\pi(\boldsymbol{z})] \xrightarrow{d} \mathcal{N}\left(0, \pi(\boldsymbol{z})^{2}[1-\pi(\boldsymbol{z})]^{2}\left(1, \boldsymbol{z}^{\top}\right)\left(I^{\text {logistic }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right)^{-1}\left(1, \boldsymbol{z}^{\top}\right)^{\top}\right)
$$

for any $\boldsymbol{z} \in \mathbb{R}^{p}$. Therefore

$$
\begin{aligned}
\operatorname{Asym} . \operatorname{Var}(\sqrt{n} \cdot \hat{\pi}(\boldsymbol{z})) & =\pi(\boldsymbol{z})^{2}[1-\pi(\boldsymbol{z})]^{2}\left(1, \boldsymbol{z}^{\top}\right)\left(I^{\text {logistic }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right)^{-1}\left(1, \boldsymbol{z}^{\top}\right)^{\top} \\
& \geq \pi(\boldsymbol{z})^{2}[1-\pi(\boldsymbol{z})]^{2}\left\|\left(1, \boldsymbol{z}^{\top}\right)\right\|_{2}^{2} \lambda_{\min }\left(\left(I^{\text {logistic }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right)^{-1}\right) \\
& =\frac{\pi(\boldsymbol{z})^{2}[1-\pi(\boldsymbol{z})]^{2}\left\|\left(1, \boldsymbol{z}^{\top}\right)\right\|_{2}^{2}}{\left\|I^{\text {logistic }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right\|_{\text {op }}} \\
& \geq \frac{\pi(\boldsymbol{z})^{2}[1-\pi(\boldsymbol{z})]^{2}\left\|\left(1, \boldsymbol{z}^{\top}\right)\right\|_{2}^{2}}{\pi_{\text {rare }}\left(1-\pi_{\text {rare }}\right)\left\|\mathbb{E}\left[\hat{\boldsymbol{X}} \hat{\boldsymbol{X}}^{\top}\right]\right\|_{\text {op }}} \\
& \stackrel{(*)}{=} \frac{\pi(\boldsymbol{z})^{2}\left[1-\pi_{\text {rare }}\right]^{2}}{\pi_{\text {rare }}\left(1-\pi_{\text {rare }}\right) \lambda_{\max }} \\
& =\frac{\pi(\boldsymbol{z})^{2}\left[1-\pi_{\text {rare }}\right]}{\pi_{\text {rare }} \lambda_{\max }}
\end{aligned}
$$

where $\lambda_{\min }(\cdot)$ denotes the minimum eigenvalue of $\cdot$ and $(*)$ uses $\left\|\left(1, \boldsymbol{z}^{\top}\right)\right\|_{2}^{2} \geq 1$ and $\pi(\boldsymbol{z}) \leq \pi_{\text {rare }}$ for all $\boldsymbol{z} \in \mathcal{S}$. This yields

$$
\operatorname{Asym} . \operatorname{Var}\left(\sqrt{n} \frac{\pi(\boldsymbol{z})-\hat{\pi}_{n}(\boldsymbol{z})}{\pi(\boldsymbol{z})}\right)=\frac{1}{\pi(\boldsymbol{z})^{2}} \operatorname{Asym} . \operatorname{Var}(\sqrt{n} \cdot \hat{\pi}(\boldsymbol{z})) \geq \frac{1-\pi_{\text {rare }}}{\pi_{\text {rare }} \lambda_{\max }}
$$

Similarly to the previous result, $\hat{\pi}(\boldsymbol{z})$ is asymptotically unbiased, its finite sample bias $\pi(\boldsymbol{z})-\mathbb{E}\left[\hat{\pi}_{n}(\boldsymbol{z})\right]$ is $\mathcal{O}(1 / n)$ by standard maximum likelihood theory (Cordeiro \& McCullagh, 1991) since it is a maximum likelihood estimator by the functional equivariance of maximum likelihood esimators, and its asymptotic MSE is equal to its asymptotic variance:

$$
\begin{aligned}
\operatorname{Asym} . \operatorname{MSE}(\hat{\pi}(\boldsymbol{z})) & =\mathbb{E}\left[\lim _{n \rightarrow \infty}\left(\sqrt{n} \cdot \frac{\pi(\boldsymbol{z})-\hat{\pi}_{n}(\boldsymbol{z})}{\pi(\boldsymbol{z})}\right)^{2}\right] \\
& =\mathbb{E}\left[\lim _{n \rightarrow \infty}\left(\sqrt{n} \cdot \frac{\mathbb{E}\left[\hat{\pi}_{n}(\boldsymbol{z})\right]-\hat{\pi}_{n}(\boldsymbol{z})}{\pi(\boldsymbol{z})}\right)^{2}+\lim _{n \rightarrow \infty}\left(\sqrt{n} \cdot \frac{\pi(\boldsymbol{z})-\mathbb{E}\left[\hat{\pi}_{n}(\boldsymbol{z})\right]}{\pi(\boldsymbol{z})}\right)^{2}\right] \\
& =\operatorname{Asym} . \operatorname{Var}\left(\sqrt{n} \frac{\pi(\boldsymbol{z})-\hat{\pi}_{n}(\boldsymbol{z})}{\pi(\boldsymbol{z})}\right)+0 \\
& \geq \frac{1-\pi_{\text {rare }}}{\pi_{\text {rare }} \lambda_{\max }}
\end{aligned}
$$

Proof of Theorem 2.2. 1. Again, the assumptions of Lemma D. 1 are satisfied. Lemma D. 1 shows that the asymptotic covariance matrix of the scaled maximum likelihood estimates of the parameters of logistic regression (a special case of the proportional odds model with $K=2$ categories) is

$$
\operatorname{Asym} . \operatorname{Cov}(\sqrt{n} \cdot(\hat{\alpha}, \hat{\boldsymbol{\beta}})^{\top})=\left(I^{\text {logistic }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right)^{-1}=\left(\begin{array}{ll}
## I_{\alpha \alpha}^{\text {logistic }} & \left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top} \\
## I_{\beta \alpha}^{\text {logistic }} & I_{\beta \beta}^{\text {logistic }}
\end{array}\right)^{\top}
$$

so in the case that $\boldsymbol{\beta}$ is known, we have

$$
\operatorname{Asym} . \operatorname{Var}\left(\sqrt{n} \cdot \hat{\alpha}_{q}\right)=\left(I_{\alpha \alpha}^{\text {logistic }}\right)^{-1}=\frac{1}{I_{\alpha \alpha}^{\text {logistic }}}
$$

If $\boldsymbol{\beta}$ is not known, then if $I_{\beta \beta}^{\text {logistic }}$ is positive definite (and therefore invertible) the formula for block matrix inversion yields

$$
\operatorname{Asym} . \operatorname{Var}(\sqrt{n} \cdot \hat{\alpha})=\frac{1}{I_{\alpha \alpha}^{\text {logistic }}-\left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top}\left(I_{\beta \beta}^{\text {logistic }}\right)^{-1} I_{\beta \alpha}^{\text {logistic }}}
$$

We know that $I_{\beta \beta}^{\text {logistic }}$ is positive definite because $I^{\text {logistic }}(\boldsymbol{\alpha}, \boldsymbol{\beta})$ is finite and positive definite from Lemma D.1, so the principal submatrix $I_{\beta \beta}^{\text {logistic }}$ is positive definite by Observation 7.1.2 in Horn \& Johnson (2012). Further, since we know from Lemma D. 1 that the covariance matrix of $(\hat{\alpha}, \hat{\boldsymbol{\beta}})$ is finite and positive definite under our conditions, this also implies that

$$
0<I_{\alpha \alpha}^{\text {logistic }}-\left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top}\left(I_{\beta \beta}^{\text {logistic }}\right)^{-1} I_{\beta \alpha}^{\text {logistic }}<\infty
$$

Now we seek a lower bound for Asym. $\operatorname{Var}(\sqrt{n} \cdot \hat{\alpha})$. We see from (20) that we can get such a bound by lower-bounding $\left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top}\left(I_{\beta \beta}^{\text {logistic }}\right)^{-1} I_{\beta \alpha}^{\text {logistic }}$. Because $t \mapsto t(1-t)$ is upper-bounded by $1 / 4$ for all $t \in[0,1]$,

$$
I_{\beta \beta}^{\text {logistic }}=\int \boldsymbol{x} \boldsymbol{x}^{\top} \pi_{2}(\boldsymbol{x})\left[1-\pi_{2}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \preceq \int \boldsymbol{x} \boldsymbol{x}^{\top} \cdot \frac{1}{4} d F(\boldsymbol{x})=\frac{1}{4} \mathbb{E}\left[\boldsymbol{X} \boldsymbol{X}^{\top}\right]
$$

## Then

$$
\begin{aligned}
\left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top}\left(I_{\beta \beta}^{\text {logistic }}\right)^{-1} I_{\beta \alpha}^{\text {logistic }} & \geq\left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top}\left(\frac{1}{4} \mathbb{E}\left[\boldsymbol{X} \boldsymbol{X}^{\top}\right]\right)^{-1} I_{\beta \alpha}^{\text {logistic }} \\
& \geq 4 \lambda_{\min }\left(\left(\mathbb{E}\left[\boldsymbol{X} \boldsymbol{X}^{\top}\right]\right)^{-1}\right)\left\|I_{\beta \alpha}^{\text {logistic }}\right\|_{2}^{2} \\
& =\frac{4\left\|I_{\beta \alpha}^{\text {logistic }}\right\|_{2}^{2}}{\left\|\mathbb{E}\left[\boldsymbol{X} \boldsymbol{X}^{\top}\right]\right\|_{\text {op }}} \\
& \geq \frac{4 \pi_{\min }^{2}\left(1-\pi_{\min }\right)^{2}\|\mathbb{E}[\boldsymbol{X}]\|_{2}^{2}}{\lambda_{\max }}=: \Delta
\end{aligned}
$$

where $\lambda_{\min }(\cdot)$ denotes the minimum eigenvalue of $\cdot$ and the last step follows because

$$
\begin{aligned}
\left\|I_{\beta \alpha}^{\text {logistic }}\right\|_{2} & =\left\|\int \boldsymbol{x} \pi_{2}(\boldsymbol{x})\left[1-\pi_{2}(\boldsymbol{x})\right] d F(\boldsymbol{x})\right\|_{2} \\
& \geq\left\|\int \boldsymbol{x} \pi_{\min }\left(1-\pi_{\min }\right) d F(\boldsymbol{x})\right\|_{2} \\
& =\pi_{\min }\left(1-\pi_{\min }\right)\|\mathbb{E}[\boldsymbol{X}]\|_{2}
\end{aligned}
$$

(where we used the fact that $\boldsymbol{X}$ has support only over nonnegative numbers). Therefore (20) and (22) yield

$$
\text { Asym. } \operatorname{Var}(\sqrt{n} \cdot \hat{\alpha}) \geq\left(\frac{1}{\operatorname{Asym} . \operatorname{Var}\left(\sqrt{n} \cdot \hat{\alpha}_{q}\right)}-\Delta\right)^{-1}
$$

The remainder of the argument is similar to the end of the proof of Theorem A.1: the asymptotic unbiasedness and $\mathcal{O}(1 / n)$ finite-sample bias of these estimators yields

$$
\operatorname{Asym} . \operatorname{MSE}(\hat{\alpha})=\operatorname{Asym} . \operatorname{Var}(\sqrt{n} \cdot[\alpha-\hat{\alpha}])
$$

and

$$
\operatorname{Asym} . \operatorname{MSE}\left(\hat{\alpha}_{q}\right)=\operatorname{Asym} . \operatorname{Var}\left(\sqrt{n} \cdot\left[\alpha-\hat{\alpha}_{q}\right]\right)
$$

## Then from (21) we know that

$$
I_{\alpha \alpha}^{\text {logistic }}>\left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top}\left(I_{\beta \beta}^{\text {logistic }}\right)^{-1} I_{\beta \alpha}^{\text {logistic }} \geq \frac{4 \pi_{\min }^{2}\left(1-\pi_{\min }\right)^{2}\|\mathbb{E}[\boldsymbol{X}]\|_{2}^{2}}{\lambda_{\max }}
$$

## Making the appropriate substitutions into (24) yields

$$
\frac{1}{\operatorname{Asym} \cdot \operatorname{MSE}\left(\hat{\alpha}_{q}\right)}-\Delta>0
$$

and then substituting into (23) yields

$$
\begin{aligned}
\operatorname{Asym} \cdot \operatorname{MSE}(\hat{\alpha}) & \geq\left(\frac{1}{\operatorname{Asym} \cdot \operatorname{MSE}\left(\hat{\alpha}_{q}\right)}-\Delta\right)^{-1} \\
& =\frac{\operatorname{Asym} \cdot \operatorname{MSE}\left(\hat{\alpha}_{q}\right)}{1-\Delta \cdot \operatorname{Asym} \cdot \operatorname{MSE}\left(\hat{\alpha}_{q}\right)} \\
& \stackrel{(*)}{\geq} \operatorname{Asym} \cdot \operatorname{MSE}\left(\hat{\alpha}_{q}\right) \cdot\left(1+\Delta \cdot \operatorname{Asym} \cdot \operatorname{MSE}\left(\hat{\alpha}_{q}\right)\right) \\
\Longleftrightarrow \quad \frac{\operatorname{Asym} \cdot \operatorname{MSE}(\hat{\alpha})-\operatorname{Asym} \cdot \operatorname{MSE}\left(\hat{\alpha}_{q}\right)}{\left[\operatorname{Asym} \cdot \operatorname{MSE}\left(\hat{\alpha}_{q}\right)\right]^{2}} & \geq \Delta
\end{aligned}
$$

where in $(*)$ we used the inequality $c /(1-c t) \leq c(1+c t)$ for any $c>0, t<\frac{1}{c}$.
2. Because $(\hat{\alpha}, \hat{\boldsymbol{\beta}}) \mapsto \hat{\pi}(\boldsymbol{z})$ is differentiable for all $\boldsymbol{z} \in \mathbb{R}^{p}$, by the delta method (Theorem 3.1 in van der Vaart 2000)

$$
\sqrt{n} \cdot[\hat{\pi}(\boldsymbol{z})-\pi(\boldsymbol{z})] \xrightarrow{d} \mathcal{N}\left(0, \pi(\boldsymbol{z})^{2}[1-\pi(\boldsymbol{z})]^{2}\left(1, \boldsymbol{z}^{\top}\right)\left(I^{\text {logistic }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right)^{-1}\left(1, \boldsymbol{z}^{\top}\right)^{\top}\right)
$$

for any $\boldsymbol{z} \in \mathbb{R}^{p}$, and similarly

$$
\sqrt{n} \cdot\left[\hat{\pi}_{q}(\boldsymbol{z})-\pi(\boldsymbol{z})\right] \xrightarrow{d} \mathcal{N}\left(0, \frac{\pi(\boldsymbol{z})^{2}[1-\pi(\boldsymbol{z})]^{2}}{I_{\alpha \hat{\alpha}}^{\text {logistic }}}\right)
$$

We can find $\left(I^{\text {logistic }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right)^{-1}$ using the formula for block matrix inversion if

$$
\boldsymbol{D}:=I_{\beta \beta}^{\text {logistic }}-\frac{I_{\beta \alpha}^{\text {logistic }}\left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top}}{I_{\alpha \alpha}^{\text {logistic }}}
$$

is positive definite (and therefore invertible; note that $\boldsymbol{D}$ is symmetric) and $I_{\alpha \alpha}^{\text {logistic }}>0$. We have

$$
I_{\alpha \alpha}^{\text {logistic }}=\int \pi_{2}(\boldsymbol{x})\left[1-\pi_{2}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \geq \int \pi_{\min }\left[1-\pi_{\min }\right] d F(\boldsymbol{x})=\pi_{\min }\left[1-\pi_{\min }\right]>0
$$

and by Theorem 1.12 in Zhang (2005), we then know $\boldsymbol{D}$ is positive definite (and invertible) since $I^{\text {logistic }}(\boldsymbol{\alpha}, \boldsymbol{\beta})$ is by Lemma D. 1 and $I_{\alpha \alpha}^{\text {logistic }}>0$. Let $\lambda_{\max }^{\boldsymbol{D}}:=\|\boldsymbol{D}\|_{\text {op }}$ be the largest eigenvalue of $\boldsymbol{D}$; then $1 / \lambda_{\max }^{\boldsymbol{D}}$ is the smallest eigenvalue of $\boldsymbol{D}^{-1}$. Then for any $\boldsymbol{z} \in \mathbb{R}^{p}$, we have

$$
\begin{aligned}
& \left(1 \quad \boldsymbol{z}^{\top}\right)\left(I^{\text {logistic }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right)^{-1}\binom{1}{\boldsymbol{z}} \\
= & \left(1 \quad \boldsymbol{z}^{\top}\right)\left(\frac{1}{I_{\alpha \alpha}^{\text {logistic }}}+\frac{1}{\left(I_{\alpha \alpha}^{\text {logistic }}\right)^{2}}\left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top} \boldsymbol{D}^{-1} I_{\beta \alpha}^{\text {logistic }}-\frac{1}{I_{\alpha \alpha}^{\text {logistic }}}\left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top} \boldsymbol{D}^{-1}\right)\binom{1}{\boldsymbol{z}} \\
= & \frac{1}{I_{\alpha \alpha}^{\text {logistic }}}+\frac{1}{\left(I_{\alpha \alpha}^{\text {logistic }}\right)^{2}}\left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top} \boldsymbol{D}^{-1} I_{\beta \alpha}^{\text {logistic }}+\boldsymbol{z}^{\top} \boldsymbol{D}^{-1} \boldsymbol{z}-2 \frac{1}{I_{\alpha \alpha}^{\text {logistic }}} I_{\beta \alpha}^{\text {logistic }} \boldsymbol{D}^{-1} \boldsymbol{z} \\
\stackrel{(a)}{=} & \frac{1}{I_{\alpha \alpha}^{\text {logistic }}}+\frac{1}{\left(I_{\alpha \alpha}^{\text {logistic }}\right)^{2}}\left\|\boldsymbol{D}^{-1 / 2}\left(I_{\beta \alpha}^{\text {logistic }}-I_{\alpha \alpha}^{\text {logistic }} \boldsymbol{z}\right)\right\|_{2}^{2} \\
\stackrel{(b)}{=} & \frac{1}{I_{\alpha \alpha}^{\text {logistic }}}+\frac{1}{\left(I_{\alpha \alpha}^{\text {logistic }}\right)^{2} \lambda_{\max }^{\boldsymbol{D}}}\left\|I_{\beta \alpha}^{\text {logistic }}-I_{\alpha \alpha}^{\text {logistic }} \boldsymbol{z}\right\|_{2}^{2} \\
\geq & \frac{1}{I_{\alpha \alpha}^{\text {logistic }}},
\end{aligned}
$$

where $(a)$ follows from

$$
\begin{aligned}
& \frac{1}{\left(I_{\alpha \alpha}^{\text {logistic }}\right)^{2}}\left\|\boldsymbol{D}^{-1 / 2}\left(I_{\beta \alpha}^{\text {logistic }}-I_{\alpha \alpha}^{\text {logistic }} \boldsymbol{z}\right)\right\|_{2}^{2} \\
= & \frac{1}{\left(I_{\alpha \alpha}^{\text {logistic }}\right)^{2}}\left(\left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top} \boldsymbol{D}^{-1} I_{\beta \alpha}^{\text {logistic }}+\left(I_{\alpha \alpha}^{\text {logistic }}\right)^{2} \boldsymbol{z}^{\top} \boldsymbol{D}^{-1} \boldsymbol{z}-2 I_{\alpha \alpha}^{\text {logistic }} I_{\beta \alpha}^{\text {logistic }} \boldsymbol{D}^{-1} \boldsymbol{z}\right)
\end{aligned}
$$

and $(b)$ uses the fact that $1 / \sqrt{\lambda_{\max }^{\boldsymbol{D}}}$ is the smallest eigenvalue of $\boldsymbol{D}^{-1 / 2}$. If we can show that $\left\|I_{\beta \alpha}^{\text {logistic }}-I_{\alpha \alpha}^{\text {logistic }} \boldsymbol{z}\right\|_{2} \neq 0$, then (25) is enough to establish the strict inequality in the result. Using (13), note that

$$
\begin{aligned}
0 & =I_{\beta \alpha}^{\text {logistic }}-I_{\alpha \alpha}^{\text {logistic }} \boldsymbol{z} \\
& =\int \boldsymbol{x} \pi(\boldsymbol{x})[1-\pi(\boldsymbol{x})] d F(\boldsymbol{x})-\boldsymbol{z} \int \pi(\boldsymbol{x})[1-\pi(\boldsymbol{x})] d F(\boldsymbol{x}) \\
\Longleftrightarrow \quad \boldsymbol{z} & =\frac{\int \boldsymbol{x} \pi(\boldsymbol{x})[1-\pi(\boldsymbol{x})] d F(\boldsymbol{x})}{\int \pi(\boldsymbol{x})[1-\pi(\boldsymbol{x})] d F(\boldsymbol{x})}
\end{aligned}
$$

so for all $\boldsymbol{z} \neq \mathbb{E}[\boldsymbol{X} \pi(\boldsymbol{X})[1-\pi(\boldsymbol{X})]] / \mathbb{E}[\pi(\boldsymbol{X})[1-\pi(\boldsymbol{X})]]$, we have $\left\|I_{\beta \alpha}^{\text {logistic }}-I_{\alpha \alpha}^{\text {logistic }} \boldsymbol{z}\right\|_{2}^{2}>0$. So for any $\boldsymbol{z} \in$ $\mathbb{R}^{p} \backslash \mathbb{E}[\boldsymbol{X} \pi(\boldsymbol{X})[1-\pi(\boldsymbol{X})]] / \mathbb{E}[\pi(\boldsymbol{X})[1-\pi(\boldsymbol{X})]]$, we have

$$
\begin{aligned}
& \pi(\boldsymbol{z})^{2}[1-\pi(\boldsymbol{z})]^{2}\left(\begin{array}{ll}
1 & \boldsymbol{z}^{\top}
\end{array}\right)\left(\begin{array}{ll}
## I_{\alpha \alpha}^{\text {logistic }} & \left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top} \\
## I_{\beta \alpha}^{\text {logistic }} & I_{\beta \beta}^{\text {logistic }}
\end{array}\right)^{-1}\binom{1}{\boldsymbol{z}}>\frac{\pi(\boldsymbol{z})^{2}[1-\pi(\boldsymbol{z})]^{2}}{I_{\alpha \alpha}^{\text {logistic }}} \\
& \Longleftrightarrow \quad \text { Asym. } \operatorname{Var}\left(\sqrt{n} \cdot \hat{\pi}_{q}(\boldsymbol{z})\right)<\operatorname{Asym} . \operatorname{Var}(\sqrt{n} \cdot(\hat{\pi}(\boldsymbol{z}))
\end{aligned}
$$

# E. Investigation of Plausibility of Assumption (5) and Proof of Theorem 2.3

Before we prove Theorem 2.3, we begin by investigating the plausibility of Assumption (5). We start by investigating whether $\lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)$ is bounded away from 0 in Section E.1. Then in Section E. 2 we directly investigate whether Assumption (5) seems to hold in a variety of contexts. We prove Theorem 2.3 in Section E.3, and we prove the supporting result Lemma E. 1 in Section E. 4 .

## E.1. Investigating Whether $\lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)$ is Bounded Away From Zero

To investigate the plausibility of the assumption that $I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}$ is positive definite (and that the upper bound for $\pi_{\text {rare }}$ in Equation 5 can hold) empirically, we perform two simulation studies, with setups similar to those of our synthetic data experiments in Section 4 of the paper. We repeat Simulation Study A 25 times. Using $n=10^{6}, p=10$, and $K=3$, we generate $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ with $\boldsymbol{X}_{i j} \sim \operatorname{Uniform}(-1,1)$ iid for all $i \in\{1, \ldots, n\}$ and $j \in\{1, \ldots, p\}$. We then generate $y_{i} \in\{1,2,3\}$ from $\boldsymbol{x}_{i}$ for each $i \in\{1, \ldots, n\}$ according to the proportional odds model (1), using $\boldsymbol{\beta}=(1, \ldots, 1)^{\top}$ and $\boldsymbol{\alpha}=(0,20)$ (so that class 3 is very rare, with $\pi_{\text {rare }} \approx 4.54 \cdot 10^{-5}$ ). We estimate $I_{\beta \beta}, I_{\beta \alpha_{1}}$, and $I_{\alpha_{1} \alpha_{1}}$ using empirical ("plug-in") estimates of the expressions in (8), (9), and (10); for example, using (19) we estimated $I_{\beta \beta}$ by

$$
\frac{1}{n} \sum_{i=1}^{n} \sum_{k=1}^{K-1} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}\left[\pi_{k}\left(\boldsymbol{x}_{i}\right)+\pi_{k+1}\left(\boldsymbol{x}_{i}\right)\right] p_{k}\left(\boldsymbol{x}_{i}\right)\left[1-p_{k}\left(\boldsymbol{x}_{i}\right)\right]
$$

(Note that we used the true $\pi_{k}(\cdot)$ and $p_{k}(\cdot)$ functions in these estimates.) Finally, we use these estimated quantities to estimate $I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}$, and we calculate the minimum eigenvalue of this estimated matrix. Across all 25 simulations, the

sample mean of this minimum eigenvalue is 0.02361 , and the minimum is 0.02359 . The standard error is $2.94 \cdot 10^{-6}$, and the $95 \%$ confidence interval for the mean of the minimum eigenvalue is $(0.02360,0.02362)$. See Figure 12 for a boxplot of the 25 estimated minimum eigenvalues. These results seem to suggest that the assumption that $\lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)>0$ is reasonable under the assumptions of Theorem 2.3 for $\boldsymbol{X}$ with this iid uniform distribution.
![img-13.jpeg](img-13.jpeg)

Figure 12. Boxplot of the estimated minimum eigenvalues of $I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}$ in Simulation Study A as described in Section E.1.

Next, we conduct another simulation study to investigate whether $\lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)$ is bounded away from 0 if the features in $\boldsymbol{X}$ are correlated, and over a wider range of $\pi_{\text {rare }}$. In Simulation Study B, we generate matrices of standard multivariate Gaussian data truncated entrywise between -3 and 3 , with $n=10^{6}$ and $p=10$. We generate matrices with covariance matrices

$$
\left(\begin{array}{cccc}
1 & \rho & \cdots & \rho \\
\rho & 1 & \cdots & \rho \\
\vdots & \vdots & \ddots & \vdots \\
\rho & \rho & \cdots & 1
\end{array}\right)
$$

for $\rho \in\{0,0.25,0.5,0.75\}$. We then generate ordinal responses from the proportional odds model with $K=3$ and $\boldsymbol{\beta}=(1,1, \ldots, 1)^{\top}$. The intercept for the first separating hyperplane is 0 (so that the expected class probabilities for classes 1 and 2 are both close to $1 / 2$ ) and we choose three different values for the second intercept by analytically solving for intercepts such that $\pi_{\text {rare }}=\sup _{x \in[-3,3]^{p}}\left\{\pi_{3}(\boldsymbol{x})\right\}$ equals $10^{-5}, 10^{-6}$, or $10^{-7}$. To see the behavior as $\pi_{\text {rare }}$ vanishes, we also investigate a fourth setting where all of the $\pi_{3}(\boldsymbol{x})$ terms in the Fisher information are simply set to 0 (and $\pi_{2}(\boldsymbol{x})$ is coerced to equal $1-\pi_{1}(\boldsymbol{x})$ pointwise), as if the intercept for the second separating hyperplane were infinity. In each setting, we generate 7 random matrices and estimate $\lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)$ in the same way as in Simulation Study A.

The results are displayed in Figures 13, 14, 15, and 16 for correlations of $0,0.25,0.5$, and 0.75 , respectively. In each plot the rare class probability is displayed on the horizontal axis on a log scale (with a line break after the far left result for the asymptotic case with $\pi_{\text {rare }}=0$ ), and the average minimum eigenvalue for each of the seven random matrices is displayed on the vertical axis. We see that the means of the minimum eigenvalues are well above 0 ; we also note that in every generated matrix in every setting, the minimum eigenvalue was strictly positive.
![img-14.jpeg](img-14.jpeg)

Figure 13. Plot of the average estimated minimum eigenvalues of $I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}$ in Simulation Study B as described in Section E. 1 with each column of $\boldsymbol{X}$ having correlation 0 with all of the others.

Finally, we also investigate analytically whether $\lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)$ is bounded away from 0 as $\pi_{\text {rare }}$ becomes arbitrarily small. To see that this seems reasonable, note that from (19) we have

$$
\begin{aligned}
I_{\beta \beta} & =\sum_{k=1}^{2} \int \boldsymbol{x} \boldsymbol{x}^{\top}\left[\pi_{k}(\boldsymbol{x})+\pi_{k+1}(\boldsymbol{x})\right] p_{k}(\boldsymbol{x})\left[1-p_{k}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \\
& =\int \boldsymbol{x} \boldsymbol{x}^{\top}\left(\left[\pi_{1}(\boldsymbol{x})+\pi_{2}(\boldsymbol{x})\right] \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right]+\left[\pi_{2}(\boldsymbol{x})+\pi_{3}(\boldsymbol{x})\right] \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right]\right) d F(\boldsymbol{x}) \\
& =\int \boldsymbol{x} \boldsymbol{x}^{\top}\left(\left[1-\pi_{3}(\boldsymbol{x})\right] \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right]+\left[1-\pi_{1}(\boldsymbol{x})\right] \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right]\right) d F(\boldsymbol{x}) \\
& =\int \boldsymbol{x} \boldsymbol{x}^{\top}\left[1-\pi_{1}(\boldsymbol{x})\right]\left[1-\pi_{3}(\boldsymbol{x})\right]\left(\pi_{1}(\boldsymbol{x})+\pi_{3}(\boldsymbol{x})\right) d F(\boldsymbol{x}) \\
& =\int \boldsymbol{x} \boldsymbol{x}^{\top}\left[\pi_{2}(\boldsymbol{x})+\pi_{3}(\boldsymbol{x})\right]\left[1-\pi_{3}(\boldsymbol{x})\right]\left[\pi_{1}(\boldsymbol{x})+\pi_{3}(\boldsymbol{x})\right] d F(\boldsymbol{x})
\end{aligned}
$$

which is non-vanishing in $\pi_{\text {rare }}$. (In particular, there seems to be no reason to suspect that the eigenvalues of $I_{\beta \beta}$ change drastically for, say $\pi_{3}(\boldsymbol{x}) \leq 10^{-7}$ for all $\boldsymbol{x} \in \mathcal{S}$, as in Simulation Study B, versus $\pi_{3}(\boldsymbol{x}) \leq 10^{-20}$ for all $\boldsymbol{x} \in \mathcal{S}$.) Further, (32) below shows that

$$
\left\|\frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right\|_{\mathrm{op}}=\frac{\left\|I_{\beta \alpha_{1}}\right\|_{2}^{2}}{I_{\alpha_{1} \alpha_{1}}}
$$

![img-15.jpeg](img-15.jpeg)

Figure 14. Plot of the average estimated minimum eigenvalues of $I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}$ in Simulation Study B as described in Section E. 1 with each column of $\boldsymbol{X}$ having correlation 0.25 with all of the others.
![img-16.jpeg](img-16.jpeg)

Figure 15. Plot of the average estimated minimum eigenvalues of $I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}$ in Simulation Study B as described in Section E. 1 with each column of $\boldsymbol{X}$ having correlation 0.5 with all of the others.

![img-17.jpeg](img-17.jpeg)

Figure 16. Plot of the average estimated minimum eigenvalues of $I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}$ in Simulation Study B as described in Section E. 1 with each column of $\boldsymbol{X}$ having correlation 0.75 with all of the others.
(where $\|\cdot\|_{\text {op }}$ is the operator norm) is bounded from above by a constant not depending on $\pi_{\text {rare }}$. This also suggests that $\lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)$ should be insensitive to $\pi_{\text {rare }}$ becoming vanishingly small. Taken together, this suggests that Assumption (26) does not become more implausible as $\pi_{\text {rare }}$ becomes arbitrarily small.

# E.2. Investigating Whether Assumption (5) is Plausible

Having developed evidence that $\lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)$ is bounded away from 0 , we now directly investigate the plausibility of the assumption

$$
\pi_{\text {rare }} \leq \frac{\lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)}{3 M^{2}(2+M)}
$$

First we examine this in the context of Simulation Study A. Note that $M=\|(1, \ldots, 1)\|_{2}=\sqrt{p}, \mathcal{S}=[-1,1]^{p}$ and

$$
\sup _{\boldsymbol{x} \in \mathcal{S}}\left\{\pi_{3}(\boldsymbol{x})\right\}=\pi_{3}\left(\boldsymbol{x}^{*}\right)=1-\frac{1}{1+\exp \left\{-\left(20+\sum_{j=1}^{p} x_{j}^{*}\right)\right\}}
$$

for $\boldsymbol{x}^{*}=(-1, \ldots,-1)$. So all of the quantities in (26) are known except the minimum eigenvalue, which we were able to estimate with seemingly high precision. It turns out that (26) is satisfied even when we use the minimum value of $\lambda_{\min }$ across all 25 simulations as our estimate; in this case, the left side of (26) is $4.54 \cdot 10^{-5}$ and the right side is $1.52 \cdot 10^{-4}$.

Next we investigate (26) in the context of Simulation Study B. In each of the four correlation settings, we calculate the proportion of generated random matrices in which (26) is satisfied at each rarity level (according to our minimum eigenvalue estimates). The results are displayed in Figures 17, 18, 19, and 20. We see that (26) is not satisfied for any of the random matrices where $\pi_{\text {rare }}=10^{-5}$, but it is satisfied for all of the matrices with all of the remaining values of $\pi_{\text {rare }}$ regardless of correlation of the features. This suggests that assumption (5) is indeed reasonable for $\pi_{\text {rare }}$ small enough.

![img-18.jpeg](img-18.jpeg)

Figure 17. Plot of the proportion of random matrices in Simulation Study B (as described in Section E.1) in which (26) is satisfied in the setting where each column of $\boldsymbol{X}$ has correlation 0 with all of the others.
![img-19.jpeg](img-19.jpeg)

Figure 18. Plot of the proportion of random matrices in Simulation Study B (as described in Section E.1) in which (26) is satisfied in the setting where each column of $\boldsymbol{X}$ has correlation 0.25 with all of the others.

![img-20.jpeg](img-20.jpeg)

Figure 19. Plot of the proportion of random matrices in Simulation Study B (as described in Section E.1) in which (26) is satisfied in the setting where each column of $\boldsymbol{X}$ has correlation 0.5 with all of the others.

![img-21.jpeg](img-21.jpeg)

Figure 20. Plot of the proportion of random matrices in Simulation Study B (as described in Section E.1) in which (26) is satisfied in the setting where each column of $\boldsymbol{X}$ has correlation 0.75 with all of the others.

# E.3. Proof of Theorem 2.3

Before we proceed with the proof of Theorem 2.3, we state a lemma with inequalities we will use.
Lemma E.1. The following inequalities hold under the assumptions of Theorem 2.3:

$$
\begin{aligned}
I_{\alpha_{2} \alpha_{2}} & \leq \pi_{\text {rare }} I_{\alpha_{1} \alpha_{1}} \cdot \frac{1}{1 / 4-\Delta^{2}} \\
I_{\alpha_{2} \alpha_{2}} & \leq \pi_{\text {rare }}\left(1+\frac{\pi_{\text {rare }}}{1 / 2-\Delta}\right) \\
\left|I_{\alpha_{1} \alpha_{2}}\right| & \leq I_{\alpha_{2} \alpha_{2}} \\
\left\|I_{\beta \alpha_{2}}\right\|_{2} & \leq M \cdot I_{\alpha_{2} \alpha_{2}} \\
\frac{\left\|I_{\beta \alpha_{2}}\right\|_{2}^{2}}{I_{\alpha_{2} \alpha_{2}}} & \leq M^{2} \pi_{\text {rare }}\left(1+\frac{\pi_{\text {rare }}}{1 / 2-\Delta}\right), \quad \text { and } \\
\frac{\left\|I_{\beta \alpha_{1}}\right\|_{2}^{2}}{I_{\alpha_{1} \alpha_{1}}} & \leq \frac{M^{2}}{4}
\end{aligned}
$$

Proof. Provided immediately after the proof of Theorem 2.3, in Section E.4.

By an argument analogous to the one used at the end of the proof of Theorem A.1, it is enough to upper-bound

$$
\left\|\operatorname{Cov}\left(\lim _{n \rightarrow \infty} \sqrt{n}\left[\hat{\boldsymbol{\beta}}^{\text {prop. odds }}-\boldsymbol{\beta}\right]\right)\right\|_{\mathrm{op}}=\left\|\operatorname{Asym} . \operatorname{Cov}\left(\sqrt{n} \cdot \hat{\boldsymbol{\beta}}^{\text {prop. odds }}\right)\right\|_{\mathrm{op}}
$$

Using Lemma D. 1 and the block matrix inversion formula,

$$
\begin{aligned}
\operatorname{Asym} . \operatorname{Cov}\left(\sqrt{n} \cdot \hat{\boldsymbol{\beta}}^{\text {prop. odds }}\right) & =\left(I_{\beta \beta}-I_{\alpha \beta}^{\top} I_{\alpha \alpha}^{-1} I_{\alpha \beta}\right)^{-1} \\
\Longrightarrow \quad\left\|\operatorname{Asym} . \operatorname{Cov}\left(\sqrt{n} \cdot \hat{\boldsymbol{\beta}}^{\text {prop. odds }}\right)\right\|_{\mathrm{op}} & =\frac{1}{\lambda_{\min }\left(I_{\beta \beta}-I_{\alpha \beta}^{\top} I_{\alpha \alpha}^{-1} I_{\alpha \beta}\right)}
\end{aligned}
$$

where $\lambda_{\min }(\cdot)$ is the minimum eigenvalue of $\cdot$ and $\|\cdot\|_{\text {op }}$ is the operator norm. $I_{\alpha \alpha}$ is a $2 \times 2$ matrix, so

$$
I_{\alpha \alpha}^{-1}=\frac{1}{\operatorname{det}\left(I_{\alpha \alpha}\right)}\left(\begin{array}{cc}
## I_{\alpha_{2} \alpha_{2}} & -I_{\alpha_{1} \alpha_{2}} \\
-I_{\alpha_{2} \alpha_{1}} & I_{\alpha_{1} \alpha_{1}}
\end{array}\right)
$$

and

$$
\begin{aligned}
I_{\alpha \beta}^{\top} I_{\alpha \alpha}^{-1} I_{\alpha \beta} & =\frac{1}{\operatorname{det}\left(I_{\alpha \alpha}\right)}\left(I_{\beta \alpha_{1}} \quad I_{\beta \alpha_{2}}\right)\left(\begin{array}{cc}
## I_{\alpha_{2} \alpha_{2}} & -I_{\alpha_{1} \alpha_{2}} \\
-I_{\alpha_{2} \alpha_{1}} & I_{\alpha_{1} \alpha_{1}}
\end{array}\right)\binom{I_{\alpha_{1} \beta}}{I_{\alpha_{2} \beta}} \\
& =\frac{1}{\operatorname{det}\left(I_{\alpha \alpha}\right)}\left(I_{\alpha_{2} \alpha_{2}} I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}+I_{\alpha_{1} \alpha_{1}} I_{\beta \alpha_{2}} I_{\beta \alpha_{2}}^{\top}-I_{\alpha_{1} \alpha_{2}} I_{\beta \alpha_{1}} I_{\beta \alpha_{2}}^{\top}-I_{\alpha_{1} \alpha_{2}} I_{\beta \alpha_{2}} I_{\beta \alpha_{1}}^{\top}\right) \\
& =\frac{1}{\operatorname{det}\left(I_{\alpha \alpha}\right)}\left(I_{\alpha_{2} \alpha_{2}} I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}+I_{\alpha_{1} \alpha_{1}} I_{\beta \alpha_{2}} I_{\beta \alpha_{2}}^{\top}+\left|I_{\alpha_{1} \alpha_{2}}\right| I_{\beta \alpha_{1}} I_{\beta \alpha_{2}}^{\top}+\left|I_{\alpha_{1} \alpha_{2}}\right| I_{\beta \alpha_{2}} I_{\beta \alpha_{1}}^{\top}\right)
\end{aligned}
$$

where in the last step we used that $I_{\alpha_{1} \alpha_{2}}=-\tilde{M}_{2}<0$, which is clear from (9) and (12). Because we know from Lemma D. 1 that $I_{\alpha \alpha}$ (and therefore also $I_{\alpha \alpha}^{-1}$ ) is positive definite, by Observation 7.1.6 in Horn \& Johnson (2012) $I_{\alpha \beta}^{\top} I_{\alpha \alpha}^{-1} I_{\alpha \beta}$ is

positive definite as well. Therefore we can use an upper bound on $1 / \operatorname{det}\left(I_{\alpha \alpha}\right)$ to upper bound $I_{\alpha \beta}^{\top} I_{\alpha \alpha}^{-1} I_{\alpha \beta}$. We have

$$
\begin{aligned}
\operatorname{det}\left(I_{\alpha \alpha}\right) & =I_{\alpha_{1} \alpha_{1}} I_{\alpha_{2} \alpha_{2}}-I_{\alpha_{1} \alpha_{2}}^{2} \\
& \stackrel{(a)}{\geq} I_{\alpha_{1} \alpha_{1}} I_{\alpha_{2} \alpha_{2}}-I_{\alpha_{2} \alpha_{2}}^{2} \\
& =I_{\alpha_{2} \alpha_{2}}\left(I_{\alpha_{1} \alpha_{1}}-I_{\alpha_{2} \alpha_{2}}^{\prime}\right) \\
& \stackrel{(b)}{\geq} I_{\alpha_{2} \alpha_{2}}\left(I_{\alpha_{1} \alpha_{1}}-\pi_{\text {rare }} I_{\alpha_{1} \alpha_{1}} \cdot \frac{1}{1 / 4-\Delta^{2}}\right) \\
& =\left(1-\frac{\pi_{\text {rare }}}{1 / 4-\Delta^{2}}\right) I_{\alpha_{1} \alpha_{1}} I_{\alpha_{2} \alpha_{2}} \\
& \stackrel{(c)}{\geq} \frac{1}{2} I_{\alpha_{1} \alpha_{1}} I_{\alpha_{2} \alpha_{2}}
\end{aligned}
$$

where in (a) we used (29), in (b) we used (27), and (c) uses that from the upper bound (5) for $\pi_{\text {rare }}$ we have

$$
\begin{aligned}
& \pi_{\text {rare }} \leq \frac{1}{2}\left(\frac{1}{2}-\Delta\right)\left(\frac{1}{2}+\Delta\right)=\frac{1}{2}\left(\frac{1}{4}-\Delta^{2}\right) \\
& \Longleftrightarrow \quad \frac{\pi_{\text {rare }}}{1 / 4-\Delta^{2}} \leq \frac{1}{2} \\
& 1-\frac{\pi_{\text {rare }}}{1 / 4-\Delta^{2}} \geq \frac{1}{2} .
\end{aligned}
$$

## Now we can bound $I_{\alpha \beta}^{\top} I_{\alpha \alpha}^{-1} I_{\alpha \beta}$ using (34):

$$
\begin{aligned}
I_{\alpha \beta}^{\top} I_{\alpha \alpha}^{-1} I_{\alpha \beta} & \leq \frac{2}{I_{\alpha_{1} \alpha_{1}} I_{\alpha_{2} \alpha_{2}}}\left(I_{\alpha_{2} \alpha_{2}} I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}+I_{\alpha_{1} \alpha_{1}} I_{\beta \alpha_{2}} I_{\beta \alpha_{2}}^{\top}+\left|I_{\alpha_{1} \alpha_{2}}\right| I_{\beta \alpha_{1}} I_{\beta \alpha_{2}}^{\top}+\left|I_{\alpha_{1} \alpha_{2}}\right| I_{\beta \alpha_{2}} I_{\beta \alpha_{1}}^{\top}\right) \\
& =2\left(\frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}+\frac{I_{\beta \alpha_{2}} I_{\beta \alpha_{2}}^{\top}}{I_{\alpha_{2} \alpha_{2}}}+\frac{\left|I_{\alpha_{1} \alpha_{2}}\right|}{I_{\alpha_{1} \alpha_{1}} I_{\alpha_{2} \alpha_{2}}}\left(I_{\beta \alpha_{1}} I_{\beta \alpha_{2}}^{\top}+I_{\beta \alpha_{2}} I_{\beta \alpha_{1}}^{\top}\right)\right)
\end{aligned}
$$

and

$$
I_{\beta \beta}-I_{\alpha \beta}^{\top} I_{\alpha \alpha}^{-1} I_{\alpha \beta} \succeq I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}-2\left(\frac{I_{\beta \alpha_{2}} I_{\beta \alpha_{2}}^{\top}}{I_{\alpha_{2} \alpha_{2}}}+\frac{\left|I_{\alpha_{1} \alpha_{2}}\right|}{I_{\alpha_{1} \alpha_{1}} I_{\alpha_{2} \alpha_{2}}}\left(I_{\beta \alpha_{1}} I_{\beta \alpha_{2}}^{\top}+I_{\beta \alpha_{2}} I_{\beta \alpha_{1}}^{\top}\right)\right)
$$

Note that $I_{\beta \beta}, I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}, I_{\beta \alpha_{2}} I_{\beta \alpha_{2}}^{\top}$, and $I_{\beta \alpha_{1}} I_{\beta \alpha_{2}}^{\top}+I_{\beta \alpha_{2}} I_{\beta \alpha_{1}}^{\top}$ are all symmetric. By Weyl's theorem, it holds that for symmetric matrices with matching dimensions $\boldsymbol{A}$ and $\boldsymbol{B}$,

$$
\lambda_{\min }(\boldsymbol{A}-\boldsymbol{B}) \geq \lambda_{\min }(\boldsymbol{A})-\|\boldsymbol{B}\|_{\mathrm{op}}
$$

because for any $\boldsymbol{v}$

$$
(\boldsymbol{A}-\boldsymbol{B}) \boldsymbol{v}=\boldsymbol{A} \boldsymbol{v}-\boldsymbol{B} \boldsymbol{v} \geq \lambda_{\min }(\boldsymbol{A}) \boldsymbol{v}-\|\boldsymbol{B}\|_{\mathrm{op}} \boldsymbol{v}=\left(\lambda_{\min }(\boldsymbol{A})-\|\boldsymbol{B}\|_{\mathrm{op}}\right) \boldsymbol{v}
$$

## So

$$
\begin{aligned}
& \lambda_{\min }\left(I_{\beta \beta}-I_{\alpha \beta}^{\top} I_{\alpha \alpha}^{-1} I_{\alpha \beta}\right) \\
\geq & \lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)-\frac{2}{I_{\alpha_{2} \alpha_{2}}}\left\|I_{\beta \alpha_{2}} I_{\beta \alpha_{2}}^{\top}\right\|_{\mathrm{op}}-2 \frac{\left|I_{\alpha_{1} \alpha_{2}}\right|}{I_{\alpha_{1} \alpha_{1}} I_{\alpha_{2} \alpha_{2}}}\left\|I_{\beta \alpha_{1}} I_{\beta \alpha_{2}}^{\top}+I_{\beta \alpha_{2}} I_{\beta \alpha_{1}}^{\top}\right\|_{\mathrm{op}} \\
\stackrel{(a)}{\geq} & \lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)-\frac{2}{I_{\alpha_{2} \alpha_{2}}}\left\|I_{\beta \alpha_{2}} I_{\beta \alpha_{2}}^{\top}\right\|_{\mathrm{op}}-2 \frac{\left|I_{\alpha_{1} \alpha_{2}}\right|}{I_{\alpha_{1} \alpha_{1}} I_{\alpha_{2} \alpha_{2}}}\left(\left\|I_{\beta \alpha_{1}} I_{\beta \alpha_{2}}^{\top}\right\|_{\mathrm{op}}+\left\|I_{\beta \alpha_{2}} I_{\beta \alpha_{1}}^{\top}\right\|_{\mathrm{op}}\right) \\
\stackrel{(b)}{\geq} & \lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)-2 \frac{\left\|I_{\beta \alpha_{2}}\right\|_{2}^{2}}{I_{\alpha_{2} \alpha_{2}}}-2 \frac{\left|I_{\alpha_{1} \alpha_{2}}\right|}{I_{\alpha_{1} \alpha_{1}} I_{\alpha_{2} \alpha_{2}}}\left(2\left\|I_{\beta \alpha_{1}}\right\|_{2}\left\|I_{\beta \alpha_{2}}\right\|_{2}\right) \\
\stackrel{(c)}{\geq} & \lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)-2 M^{2} \pi_{\text {rare }}\left(1+\frac{\pi_{\text {rare }}}{1 / 2-\Delta}\right)-2 \frac{I_{\alpha_{2} \alpha_{2}}}{I_{\alpha_{1} \alpha_{1}} I_{\alpha_{2} \alpha_{2}}}\left(2\left\|I_{\beta \alpha_{1}}\right\|_{2} \cdot M \cdot I_{\alpha_{2} \alpha_{2}}\right) \\
= & \lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)-2 M\left(M \pi_{\text {rare }}\left(1+\frac{\pi_{\text {rare }}}{1 / 2-\Delta}\right)+2 \frac{I_{\alpha_{2} \alpha_{2}}}{I_{\alpha_{1} \alpha_{1}}}\left\|I_{\beta \alpha_{1}}\right\|_{2}\right) \\
\stackrel{(d)}{\geq} & \lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)-2 M\left(M \pi_{\text {rare }}\left(1+\frac{\pi_{\text {rare }}}{1 / 2-\Delta}\right)+2 \frac{1}{I_{\alpha_{1} \alpha_{1}}}\left\|I_{\beta \alpha_{1}}\right\|_{2} \cdot \pi_{\text {rare }}\left(1+\frac{\pi_{\text {rare }}}{1 / 2-\Delta}\right)\right) \\
= & \lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)-2 M \pi_{\text {rare }}\left(1+\frac{\pi_{\text {rare }}}{1 / 2-\Delta}\right)\left(M+2 \frac{\left\|I_{\beta \alpha_{1}}\right\|_{2}}{I_{\alpha_{1} \alpha_{1}}}\right) \\
\stackrel{(e)}{\geq} & \lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)-2 M \pi_{\text {rare }}\left(1+\frac{1}{2}\right)\left(M+\frac{M^{2}}{2}\right) \\
\stackrel{(f)}{\geq} & \frac{1}{2} \lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)
\end{aligned}
$$

where in $(a)$ we used the triangle inequality, in $(b)$ we used that for any $\boldsymbol{a}, \boldsymbol{b} \in \mathbb{R}^{n}$ it holds that $\left\|\boldsymbol{a} \boldsymbol{b}^{\top}\right\|_{\text {op }}=\left|\boldsymbol{b}^{\top} \boldsymbol{a}\right| \leq$ $\|\boldsymbol{a}\|_{2}\|\boldsymbol{b}\|_{2}$ (note that $\boldsymbol{a} \boldsymbol{b}^{\top}$ is rank one with eigenvector $\boldsymbol{a}$ ) as well as the triangle inequality, in (c) we used (29), (30), and (31), in $(d)$ we used (28), (e) follows from (32) and

$$
\frac{\pi_{\text {rare }}}{1 / 2-\Delta} \leq \frac{1 / 2(1 / 2-\Delta)(1 / 2+\Delta)}{1 / 2-\Delta}=\frac{1}{2}\left(\frac{1}{2}+\Delta\right) \leq \frac{1}{2}\left(\frac{1}{2}+\frac{1}{2}\right) \leq \frac{1}{2}
$$

(since $\Delta \leq 1 / 2$ ), and in $(f)$ we used our assumptions that

$$
\lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)>0
$$

and

$$
\begin{aligned}
\pi_{\text {rare }} & \leq \frac{\lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)}{3 M^{2}(2+M)} \\
\Longleftrightarrow \quad 3 M \pi_{\text {rare }}\left(M+\frac{M^{2}}{2}\right) & \leq \frac{1}{2} \lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)
\end{aligned}
$$

## Substituting this into (33) shows that

$$
\left\|\operatorname{Cov}\left(\lim _{\alpha \rightarrow \infty} \sqrt{n}\left[\boldsymbol{\beta}-\boldsymbol{\beta}^{\text {prop. odds }}\right]\right)\right\|_{\mathrm{op}} \leq \frac{2}{\lambda_{\min }\left(I_{\beta \beta}-2 \frac{I_{\beta \alpha_{1}} I_{\beta \alpha_{1}}^{\top}}{I_{\alpha_{1} \alpha_{1}}}\right)}
$$

# E.4. Proof of Lemma E. 1

We omit x in integrals, when it is clear (e.g., $\pi_{1}$ stands for $\pi_{1}(\mathrm{x})$ ).
## Proof of (27):
From (9) and (11), using $\pi_{1}(\boldsymbol{x})+\pi_{2}(\boldsymbol{x})+\pi_{3}(\boldsymbol{x})=1$ we have

$$
\begin{aligned}
## I_{\alpha_{2} \alpha_{2}} & =M_{2} \\
& =\int\left[\pi_{3}(\boldsymbol{x})\left(1-\pi_{3}(\boldsymbol{x})\right)\right]^{2}\left(\frac{1}{\pi_{2}(\boldsymbol{x})}+\frac{1}{\pi_{3}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& =\int\left[\pi_{3}(\boldsymbol{x})\left(1-\pi_{3}(\boldsymbol{x})\right)\right]^{2}\left(\frac{\pi_{2}(\boldsymbol{x})+\pi_{3}(\boldsymbol{x})}{\pi_{2}(\boldsymbol{x}) \pi_{3}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& =\int \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right]^{2}\left(\frac{1-\pi_{1}(\boldsymbol{x})}{\pi_{2}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& \leq \pi_{\text {rare }} \int\left(1-\pi_{3}\right)^{2}\left(1-\pi_{1}\right) / \pi_{2} d F \\
& =\pi_{\text {rare }} \int \pi_{1}\left(1-\pi_{1}\right)^{2} \frac{\left(1-\pi_{3}\right)^{2}}{\pi_{2}}\left[\frac{1}{\pi_{1}\left(1-\pi_{1}\right)}\right] d F \\
& \stackrel{(a)}{\leq} \pi_{\text {rare }} \int \pi_{1}\left(1-\pi_{1}\right)^{2} \frac{1-\pi_{3}}{\pi_{2}}\left[\frac{1}{(1 / 2-\Delta)(1 / 2+\Delta)}\right] d F \\
& =\pi_{\text {rare }} \int \pi_{1}^{2}\left(1-\pi_{1}\right)^{2} \frac{\pi_{1}+\pi_{2}}{\pi_{1} \pi_{2}} \frac{1}{1 / 4-\Delta^{2}} \\
& =\pi_{\text {rare }} \int \pi_{1}^{2}\left(1-\pi_{1}\right)^{2}\left(\frac{1}{\pi_{1}}+\frac{1}{\pi_{2}}\right) \frac{1}{1 / 4-\Delta^{2}} \\
& =\pi_{\text {rare }} M_{1} \frac{1}{1 / 4-\Delta^{2}} \\
& =\pi_{\text {rare }} I_{\alpha_{1} \alpha_{1}} \frac{1}{1 / 4-\Delta^{2}}
\end{aligned}
$$

where in $(a)$ we used the fact that $t \mapsto 1 /[t(1-t)]$ is nonincreasing in $t$ for $t \in(0,1 / 2]$, so it is largest when $t$ is small as possible, and by assumption $\inf _{\boldsymbol{x} \in \mathcal{S}}\left\{\pi_{1} \wedge 1-\pi_{1}\right\} \geq 1 / 2-\Delta$.
## Proof of (28): Using (35) we have

$$
\begin{aligned}
I_{\alpha_{2} \alpha_{2}} & =\int \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right]^{2}\left(\frac{\pi_{2}(\boldsymbol{x})+\pi_{3}(\boldsymbol{x})}{\pi_{2}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& =\int \pi_{3}\left(1-\pi_{3}\right)^{2}\left(1+\pi_{3} / \pi_{2}\right) d F \\
& \leq \pi_{\text {rare }} \int(1-0)^{2}\left(1+\frac{\pi_{\text {rare }}}{1 / 2-\Delta}\right) d F
\end{aligned}
$$

## Proof of (29): Using (35) along with (8) and (12) we have

$$
\begin{aligned}
I_{\alpha_{2} \alpha_{2}} & =\int \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right]^{2}\left(\frac{1-\pi_{1}(\boldsymbol{x})}{\pi_{2}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& =\int \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right]\left[1-\pi_{1}(\boldsymbol{x})\right]\left(\frac{\pi_{1}(\boldsymbol{x})+\pi_{2}(\boldsymbol{x})}{\pi_{2}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& =\int \pi_{3}\left(1-\pi_{3}\right)\left(1-\pi_{1}\right) \frac{\pi_{1}}{\pi_{2}} d F+\int \pi_{3}\left(1-\pi_{3}\right)\left(1-\pi_{1}\right) d F \\
& =\tilde{M}_{2}+\int \pi_{3}\left(1-\pi_{3}\right)\left(1-\pi_{1}\right) d F \\
& =\left|I_{\alpha_{1} \alpha_{2}}\right|+\int \pi_{3}\left(1-\pi_{3}\right)\left(1-\pi_{1}\right) d F \\
& \geq\left|I_{\alpha_{1} \alpha_{2}}\right|
\end{aligned}
$$

## Proof of (30): From (9) and (18) we have

$$
\begin{aligned}
\left\|I_{\beta \alpha_{2}}\right\|_{2} & =\left\|\int \boldsymbol{x}\left[\pi_{2}(\boldsymbol{x})+\pi_{3}(\boldsymbol{x})\right] \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right] d F(\boldsymbol{x})\right\|_{2} \\
& =\left\|\int \boldsymbol{x}\left[1-\pi_{1}(\boldsymbol{x})\right] \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right] d F(\boldsymbol{x})\right\|_{2} \\
& \leq \int\|\boldsymbol{x}\|_{2} \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right]\left[1-\pi_{1}(\boldsymbol{x})\right] d F \\
& \leq M \int \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right]\left[1-\pi_{1}(\boldsymbol{x})\right] d F \\
& \leq M \cdot \int \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right]\left[1-\pi_{1}(\boldsymbol{x})\right]\left(1+\frac{\pi_{3}(\boldsymbol{x})}{\pi_{2}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& =M \cdot \int \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right]\left[1-\pi_{1}(\boldsymbol{x})\right]\left(\frac{\pi_{2}(\boldsymbol{x})+\pi_{3}(\boldsymbol{x})}{\pi_{2}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& =M \cdot \int \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right]\left[1-\pi_{1}(\boldsymbol{x})\right]\left(\frac{1-\pi_{1}(\boldsymbol{x})}{\pi_{2}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& \leq M \cdot \int \pi_{3}(\boldsymbol{x})\left[1-\pi_{3}(\boldsymbol{x})\right]^{2}\left(\frac{1-\pi_{1}(\boldsymbol{x})}{\pi_{2}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& =M \cdot I_{\alpha_{2} \alpha_{2}}
\end{aligned}
$$

where in the last inequality we used $\pi_{3}(\boldsymbol{x}) \leq \pi_{\text {rare }}<1 / 2-\Delta \leq \pi_{1}(\boldsymbol{x})$ for all $\boldsymbol{x} \in \mathcal{S}$ and in the last step we used (35).
Proof of (31): This follows from (28) and (30).
## Proof of (32): Using from (9)

$$
\begin{aligned}
\left\|I_{\beta \alpha_{1}}\right\|_{2} & =\left\|\int \boldsymbol{x} \pi_{1}^{2}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right] d F(\boldsymbol{x})+\int \boldsymbol{x} \pi_{2}(\boldsymbol{x}) \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right] d F(\boldsymbol{x})\right\|_{2} \\
& =\left\|\int \boldsymbol{x}\left[\pi_{1}(\boldsymbol{x})+\pi_{2}(\boldsymbol{x})\right] \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right] d F(\boldsymbol{x})\right\|_{2} \\
& \leq \int\|\boldsymbol{x}\|_{2}\left[\pi_{1}(\boldsymbol{x})+\pi_{2}(\boldsymbol{x})\right] \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \\
& \leq M \int\left[\pi_{1}(\boldsymbol{x})+\pi_{2}(\boldsymbol{x})\right] \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right] d F(\boldsymbol{x})
\end{aligned}
$$

and from (8)

$$
\begin{aligned}
I_{\alpha_{1} \alpha_{1}}=M_{1} & =\int\left(\pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right]\right)^{2}\left(\frac{1}{\pi_{1}(\boldsymbol{x})}+\frac{1}{\pi_{2}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& =\int\left(\pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right]\right)^{2} \frac{\pi_{1}(\boldsymbol{x})+\pi_{2}(\boldsymbol{x})}{\pi_{1}(\boldsymbol{x}) \pi_{2}(\boldsymbol{x})} d F(\boldsymbol{x}) \\
& =\int \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right]\left[\pi_{1}(\boldsymbol{x})+\pi_{2}(\boldsymbol{x})\right] \frac{1-\pi_{1}(\boldsymbol{x})}{\pi_{2}(\boldsymbol{x})} d F(\boldsymbol{x}) \\
& \geq \int \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right]\left[\pi_{1}(\boldsymbol{x})+\pi_{2}(\boldsymbol{x})\right] d F(\boldsymbol{x})
\end{aligned}
$$

we have

$$
\frac{\left\|I_{\beta \alpha_{1}}\right\|_{2}^{2}}{I_{\alpha_{1} \alpha_{1}}} \leq M^{2} \int\left[\pi_{1}(\boldsymbol{x})+\pi_{2}(\boldsymbol{x})\right] \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \leq M^{2} \int \frac{1}{4}\left[\pi_{1}(\boldsymbol{x})+\pi_{2}(\boldsymbol{x})\right] d F(\boldsymbol{x}) \leq \frac{M^{2}}{4}
$$

where in the second-to-last step we used that $t \mapsto t(1-t) \leq 1 / 4$ for all $t \in[0,1]$.

# F. Proof of Lemma D. 1

First we will calculate the Fisher information matrices, then we will show the convergence results.
Since the logistic regression model can be considered a special case of the proportional odds model with $K=2$ categories, we will mostly focus our calculations on the proportional odds model.

## F.1. Calculating the Log Likelihood and Gradients

## In the proportional odds model, the likelihood can be expressed as

$$
\prod_{i=1}^{n} \prod_{k=1}^{K}\left(\frac{1}{1+\exp \left\{-\left(\alpha_{k}+\boldsymbol{\beta}^{\top} \boldsymbol{x}_{i}\right)\right\}}-\frac{1}{1+\exp \left\{-\left(\alpha_{k-1}+\boldsymbol{\beta}^{\top} \boldsymbol{x}_{i}\right)\right\}}\right)^{\mathbb{1}\left\{y_{i}=k\right\}}
$$

so the log likelihood can be expressed as

$$
\begin{aligned}
\mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta}) & =\sum_{i=1}^{n} \sum_{k=1}^{K} \mathbb{1}\left\{y_{i}=k\right\} \log \left(\frac{1}{1+\exp \left\{-\left(\alpha_{k}+\boldsymbol{\beta}^{\top} \boldsymbol{x}_{i}\right)\right\}}-\frac{1}{1+\exp \left\{-\left(\alpha_{k-1}+\boldsymbol{\beta}^{\top} \boldsymbol{x}_{i}\right)\right\}}\right) \\
& =\sum_{i=1}^{n} \sum_{k=1}^{K} \mathbb{1}\left\{y_{i}=k\right\} \log \left(p_{i k}-p_{i, k-1}\right) \\
& =\sum_{i=1}^{n} \sum_{k=1}^{K} \mathbb{1}\left\{y_{i}=k\right\} \log \left(\pi_{i k}\right)
\end{aligned}
$$

where

$$
\begin{aligned}
& p_{i k}:=p_{k}\left(\boldsymbol{x}_{i}\right)=\mathbb{P}\left(y_{i} \leq k \mid \boldsymbol{x}_{i}\right) \\
& \pi_{i k}:=\mathbb{P}\left(y_{i}=k \mid \boldsymbol{x}_{i}\right)=p_{i k}-p_{i, k-1}=p_{k}\left(\boldsymbol{x}_{i}\right)-p_{k-1}\left(\boldsymbol{x}_{i}\right)
\end{aligned}
$$

$\alpha_{0}:=-\infty$, and $\alpha_{K}:=\infty$ (while $\boldsymbol{\alpha}:=\left(\alpha_{1}, \ldots, \alpha_{K-1}\right)^{\top}$ are parameters to be estimated). Using

$$
\frac{\partial}{\partial t} \frac{1}{1+\exp \{-t\}}=\frac{1}{1+\exp \{-t\}}\left(1-\frac{1}{1+\exp \{-t\}}\right)
$$

the gradient has entries corresponding to $\boldsymbol{\beta}$ equal to

$$
\begin{aligned}
\nabla_{\boldsymbol{\beta}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta}) & =\sum_{i=1}^{n} \sum_{k=1}^{K} \mathbb{1}\left\{y_{i}=k\right\} \boldsymbol{x}_{i}\left(\frac{p_{i k}\left(1-p_{i k}\right)-p_{i, k-1}\left(1-p_{i, k-1}\right)}{p_{i k}-p_{i, k-1}}\right) \\
& =\sum_{i=1}^{n} \sum_{k=1}^{K} \mathbb{1}\left\{y_{i}=k\right\} \boldsymbol{x}_{i}\left(\frac{p_{i k}-p_{i, k-1}-\left(p_{i k}^{2}-p_{i, k-1}^{2}\right)}{p_{i k}-p_{i, k-1}}\right) \\
& =\sum_{i=1}^{n} \sum_{k=1}^{K} \mathbb{1}\left\{y_{i}=k\right\} \boldsymbol{x}_{i}\left(1-p_{i k}-p_{i, k-1}\right)
\end{aligned}
$$

and using

$$
\begin{aligned}
\frac{\partial}{\partial \alpha_{k}} \pi_{i k} & =\frac{\partial}{\partial \alpha_{k}}\left(p_{i k}-p_{i, k-1}\right)=p_{i k}\left(1-p_{i k}\right) \quad \text { and } \\
\frac{\partial}{\partial \alpha_{k}} \pi_{i, k+1} & =\frac{\partial}{\partial \alpha_{k}}\left(p_{i, k+1}-p_{i k}\right)=-p_{i k}\left(1-p_{i k}\right)
\end{aligned}
$$

the entries corresponding to $\boldsymbol{\alpha}$ equal

$$
\begin{aligned}
\frac{\partial}{\partial \alpha_{k}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta}) & =\sum_{i=1}^{n} \frac{\partial}{\partial \alpha_{k}}\left(\mathbb{1}\left\{y_{i}=k\right\} \log \left(\pi_{i k}\right)+\mathbb{1}\left\{y_{i}=k+1\right\} \log \left(\pi_{i, k+1}\right)\right) \\
& =\sum_{i=1}^{n}\left(\mathbb{1}\left\{y_{i}=k\right\} \frac{p_{i k}\left(1-p_{i k}\right)}{\pi_{i k}}-\mathbb{1}\left\{y_{i}=k+1\right\} \frac{p_{i k}\left(1-p_{i k}\right)}{\pi_{i, k+1}}\right) \\
\Longrightarrow \quad \nabla_{\boldsymbol{\alpha}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta}) & =\sum_{i=1}^{n} \boldsymbol{e}_{k} p_{i k}\left(1-p_{i k}\right)\left(\frac{\mathbb{1}\left\{y_{i}=k\right\}}{\pi_{i k}}-\frac{\mathbb{1}\left\{y_{i}=k+1\right\}}{\pi_{i, k+1}}\right)
\end{aligned}
$$

where $\boldsymbol{e}_{k} \in\{0,1\}^{K-1}$ has the $k^{\text {th }}$ entry equal to 1 and the rest equal to 0 . (Note that since $\alpha_{0}=-\infty, p_{i 0}=0$, and similarly $p_{i K}=1$ as expected.)

# F.2. Calculating the Hessian Matrices

The entries of the Hessian corresponding to $\boldsymbol{\beta}, H_{\beta \beta}^{\text {prop. odds }}$ are

$$
\begin{aligned}
\nabla_{\boldsymbol{\beta}}^{2} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta}) & =-\sum_{i=1}^{n} \sum_{k=1}^{K} \mathbb{1}\left\{y_{i}=k\right\} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}\left[p_{i k}\left(1-p_{i k}\right)+p_{i, k-1}\left(1-p_{i, k-1}\right)\right] \\
& =-\sum_{i=1}^{n} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \sum_{k=1}^{K}\left(\mathbb{1}\left\{y_{i}=k+1\right\}+\mathbb{1}\left\{y_{i}=k\right\}\right) \operatorname{Var}\left(\mathbb{1}\left\{y_{i} \leq k\right\}\right)
\end{aligned}
$$

## Using

$$
\frac{\partial}{\partial \alpha_{k}} p_{i k}\left(1-p_{i k}\right)=p_{i k}\left(1-p_{i k}\right)-2 p_{i k}^{2}\left(1-p_{i k}\right)=p_{i k}\left(1-p_{i k}\right)\left(1-2 p_{i k}\right)
$$

the entries corresponding to the $\boldsymbol{\alpha}$ block of the Hessian $H_{\alpha \alpha}^{\text {prop. odds }}$ are as follows:

$$
\begin{aligned}
\frac{\partial^{2}}{\partial \alpha_{k}^{2}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})= & \sum_{i=1}^{n} \frac{\partial}{\partial \alpha_{k}}\left(p_{i k}\left(1-p_{i k}\right)\left(\frac{\mathbb{1}\left\{y_{i}=k\right\}}{\pi_{i k}}-\frac{\mathbb{1}\left\{y_{i}=k+1\right\}}{\pi_{i, k+1}}\right)\right) \\
= & \sum_{i=1}^{n}\left(\mathbb{1}\left\{y_{i}=k\right\} \frac{\pi_{i k} p_{i k}\left(1-p_{i k}\right)\left(1-2 p_{i k}\right)-p_{i k}^{2}\left(1-p_{i k}\right)^{2}}{\pi_{i k}^{2}}\right. \\
& \left.-\mathbb{1}\left\{y_{i}=k+1\right\} \frac{\pi_{i, k+1} p_{i k}\left(1-p_{i k}\right)\left(1-2 p_{i k}\right)+p_{i k}^{2}\left(1-p_{i k}\right)^{2}}{\pi_{i, k+1}^{2}}\right) \\
= & \sum_{i=1}^{n} p_{i k}\left(1-p_{i k}\right)\left(\mathbb{1}\left\{y_{i}=k\right\} \frac{\pi_{i k}\left(1-2 p_{i k}\right)-p_{i k}\left(1-p_{i k}\right)}{\pi_{i k}^{2}}\right. \\
& \left.-\mathbb{1}\left\{y_{i}=k+1\right\} \frac{\pi_{i, k+1}\left(1-2 p_{i k}\right)+p_{i k}\left(1-p_{i k}\right)}{\pi_{i, k+1}^{2}}\right) \\
\frac{\partial^{2}}{\partial \alpha_{k} \partial \alpha_{k-1}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})= & \sum_{i=1}^{n} p_{i k}\left(1-p_{i k}\right) \frac{\partial}{\partial \alpha_{k-1}}\left(\frac{\mathbb{1}\left\{y_{i}=k\right\}}{\pi_{i k}}-\frac{\mathbb{1}\left\{y_{i}=k+1\right\}}{\pi_{i, k+1}}\right) \\
= & -\sum_{i=1}^{n} \mathbb{1}\left\{y_{i}=k\right\} p_{i k}\left(1-p_{i k}\right)\left(\frac{-p_{i, k-1}\left(1-p_{i, k-1}\right)}{\pi_{i k}^{2}}\right) \\
= & \sum_{i=1}^{n} \mathbb{1}\left\{y_{i}=k\right\} \cdot \frac{p_{i k}\left(1-p_{i k}\right) p_{i, k-1}\left(1-p_{i, k-1}\right)}{\pi_{i k}^{2}}, \quad \text { and } \\
\frac{\partial^{2}}{\partial \alpha_{k} \partial \alpha_{k^{\prime}}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})= & 0, \quad \text { all other } k \neq k^{\prime}
\end{aligned}
$$

where $\pi_{i, K+1}=0$. (Note that $\frac{\partial^{2}}{\partial \alpha_{k} \partial \alpha_{k+1}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})$ is nonzero as well, but it matches the expression for $\frac{\partial^{2}}{\partial \alpha_{k} \partial \alpha_{k-1}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})$ with $\tilde{k}:=k+1$.)

Finally, the entries corresponding to the $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ mixed blocks $H_{\alpha \beta}^{\text {prop. odds }}$ of the Hessian are

$$
\begin{aligned}
\frac{\partial}{\partial \alpha_{k}} \nabla_{\boldsymbol{\beta}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})= & \sum_{i=1}^{n}\left(\mathbb{1}\left\{y_{i}=k\right\} \boldsymbol{x}_{i} \frac{\partial}{\partial \alpha_{k}}\left(1-p_{i k}-p_{i, k-1}\right)\right. \\
& \left.+\mathbb{1}\left\{y_{i}=k+1\right\} \boldsymbol{x}_{i} \frac{\partial}{\partial \alpha_{k}}\left(1-p_{i, k+1}-p_{i, k}\right)\right) \\
= & -\sum_{i=1}^{n} \boldsymbol{x}_{i} p_{i k}\left(1-p_{i k}\right)\left(\mathbb{1}\left\{y_{i}=k\right\}+\mathbb{1}\left\{y_{i}=k+1\right\}\right), k \in[K-1]
\end{aligned}
$$

# F.3. Calculation of the Fisher Information Matrices

## Now we find the Fisher information matrices

$$
I^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})=\left(\begin{array}{cc}
I_{\alpha \alpha}^{\text {prop. odds }} & I_{\beta \alpha}^{\text {prop. odds }} \\
I_{\beta \alpha}^{\text {prop. odds }} & I_{\beta \beta}^{\text {prop. odds }}
\end{array}\right)
$$

and

$$
## I^{\text {logistic }}(\boldsymbol{\alpha}, \boldsymbol{\beta})=\left(\begin{array}{ll}
## I_{\alpha \alpha}^{\text {logistic }} & \left(I_{\beta \alpha}^{\text {logistic }}\right)^{\top} \\
## I_{\beta \alpha}^{\text {logistic }} & I_{\beta \beta}^{\text {logistic }}
\end{array}\right)
$$

by taking the negative expectation of each block (using a single observation). For the $\alpha$ block, we have

$$
\begin{aligned}
-\mathbb{E}\left[\frac{\partial^{2}}{\partial \alpha_{k}^{2}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right]= & -\mathbb{E}\left[p_{i k}\left(1-p_{i k}\right)\left(\mathbb{1}\left\{y_{i}=k\right\} \frac{\pi_{i k}\left(1-2 p_{i k}\right)-p_{i k}\left(1-p_{i k}\right)}{\pi_{i k}^{2}}\right.\right. \\
& \left.\left.-\mathbb{1}\left\{y_{i}=k+1\right\} \frac{\pi_{i, k+1}\left(1-2 p_{i k}\right)+p_{i k}\left(1-p_{i k}\right)}{\pi_{i, k+1}^{2}}\right)\right] \\
= & -\mathbb{E}\left[p_{i k}\left(1-p_{i k}\right)\left(\frac{\pi_{i k}\left(1-2 p_{i k}\right)-p_{i k}\left(1-p_{i k}\right)}{\pi_{i k}}\right.\right. \\
& \left.\left.-\frac{\pi_{i, k+1}\left(1-2 p_{i k}\right)+p_{i k}\left(1-p_{i k}\right)}{\pi_{i, k+1}}\right)\right] \\
= & \mathbb{E}\left[p_{i k}^{2}\left(1-p_{i k}\right)^{2}\left(\frac{1}{\pi_{i k}}+\frac{1}{\pi_{i, k+1}}\right)\right] \\
= & M_{k}, \quad k \in[K-1] \\
-\mathbb{E}\left[\frac{\partial^{2}}{\partial \alpha_{k} \partial \alpha_{k-1}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right] & =-\mathbb{E}\left[\mathbb{1}\left\{y_{i}=k\right\} \cdot \frac{p_{i k}\left(1-p_{i k}\right) p_{i, k-1}\left(1-p_{i, k-1}\right)}{\pi_{i k}^{2}}\right] \\
= & -\mathbb{E}\left[\frac{p_{i k}\left(1-p_{i k}\right) p_{i, k-1}\left(1-p_{i, k-1}\right)}{\pi_{i k}}\right] \\
= & -\tilde{M}_{k}, \quad k \in\{2, \ldots, K-1\}, \quad \text { and } \\
\mathbb{E}\left[\frac{\partial^{2}}{\partial \alpha_{k} \partial \alpha_{k^{\prime}}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right] & =0, \quad \text { all other } k \neq k^{\prime}
\end{aligned}
$$

where we used the definitions of $M_{k}$ and $\tilde{M}_{k}$ in (11) and (12). Therefore $I_{\alpha \alpha}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta}) \in \mathbb{R}^{(K-1) \times(K-1)}$ has tridiagonal form

$$
I_{\alpha \alpha}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})=\left(\begin{array}{ccccccc}
## M_{1} & -\tilde{M}_{2} & 0 & \cdots & 0 & 0 \\
-\tilde{M}_{2} & M_{2} & -\tilde{M}_{3} & \cdots & 0 & 0 \\
0 & -\tilde{M}_{3} & M_{3} & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & M_{K-2} & -\tilde{M}_{K-1} \\
0 & 0 & 0 & \cdots & -\tilde{M}_{K-1} & M_{K-1}
\end{array}\right)
$$

verifying (8). Observe that in the case of logistic regression $(K=2)$, we have

$$
\begin{aligned}
I_{\alpha \alpha}^{\text {logistic }}\left(\alpha_{1}, \boldsymbol{\beta}\right)=M_{1} & =\int\left[p_{1}(\boldsymbol{x})\left(1-p_{1}(\boldsymbol{x})\right)\right]^{2}\left(\frac{1}{\pi_{1}(\boldsymbol{x})}+\frac{1}{\pi_{2}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& =\int\left[\pi_{1}(\boldsymbol{x})\left(1-\pi_{1}(\boldsymbol{x})\right)\right]^{2}\left(\frac{1}{\pi_{1}(\boldsymbol{x})}+\frac{1}{1-\pi_{1}(\boldsymbol{x})}\right) d F(\boldsymbol{x}) \\
& =\int\left[\pi_{1}(\boldsymbol{x})\left(1-\pi_{1}(\boldsymbol{x})\right)\right]^{2} \cdot \frac{1}{\pi_{1}(\boldsymbol{x})\left(1-\pi_{1}(\boldsymbol{x})\right)} d F(\boldsymbol{x}) \\
& =\int \pi_{1}(\boldsymbol{x})\left(1-\pi_{1}(\boldsymbol{x})\right) d F(\boldsymbol{x}) \\
& =M_{1}^{\text {logistic }}
\end{aligned}
$$

which is (14). (This is equivalent to a logistic regression predicting whether $y_{i}=1$ regardless of $K$.)

# F.4. Mixed block

## For the $\alpha-\beta$ mixed block, we have for all $k \in\{1, \ldots, K-1\}$

$$
-\mathbb{E}\left[\frac{\partial^{2}}{\partial \alpha_{k} \partial \boldsymbol{\beta}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right]=\mathbb{E}\left[\boldsymbol{X}_{1} p_{k}\left(\boldsymbol{X}_{1}\right)\left(1-p_{k}\left(\boldsymbol{X}_{1}\right)\right)\left(\pi_{k}\left(\boldsymbol{X}_{1}\right)+\pi_{k+1}\left(\boldsymbol{X}_{1}\right)\right)\right]
$$

## Then

$$
-\mathbb{E}\left[\frac{\partial^{2}}{\partial \alpha_{k} \partial \boldsymbol{\beta}} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right]=J_{k}^{\boldsymbol{x}}+\tilde{J}_{k+1}^{\boldsymbol{x}}, \quad k \in\{1, \ldots, K-1\}
$$

so $I_{\beta \alpha}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta}) \in \mathbb{R}^{(K-1) \times p}$ has the form

$$
I_{\beta \alpha}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})=\left(\begin{array}{c}
## J_{1}^{\boldsymbol{x}}+\tilde{J}_{2}^{\boldsymbol{x}} \\
## J_{2}^{\boldsymbol{x}}+\tilde{J}_{3}^{\boldsymbol{x}} \\
\vdots \\
## J_{K-1}^{\boldsymbol{x}}+\tilde{J}_{K}^{\boldsymbol{x}}
\end{array}\right)
$$

as in (9). In the case of logistic regression predicting whether $y_{i}=1$,

$$
I_{\beta \alpha}^{\text {logistic }}\left(\alpha_{1}, \boldsymbol{\beta}\right)=J_{1}^{\boldsymbol{x} ; \text { logistic }}+\tilde{J}_{2}^{\boldsymbol{x} ; \text { logistic }}=\int \boldsymbol{x} \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right] d F(\boldsymbol{x})
$$

verifying (15).

# F.5. Beta block

## Finally, for the $\beta$ block, we have

$$
\begin{aligned}
I_{\beta \beta}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta}) & =-\mathbb{E}\left[\nabla_{\boldsymbol{\beta}}^{2} \mathcal{L}^{\text {prop. odds }}(\boldsymbol{\alpha}, \boldsymbol{\beta})\right] \\
& =\mathbb{E}\left[\mathbb{E}\left[\boldsymbol{X}_{1} \boldsymbol{X}_{1}^{\top} \sum_{k=1}^{K}\left(\mathbb{1}\left\{y_{i}=k+1\right\}+\mathbb{1}\left\{y_{i}=k\right\}\right) p_{k}\left(\boldsymbol{X}_{1}\right)\left(1-p_{k}\left(\boldsymbol{X}_{1}\right)\right) \mid \boldsymbol{X}\right]\right] \\
& =\mathbb{E}\left[\boldsymbol{X}_{1} \boldsymbol{X}_{1}^{\top} \sum_{k=1}^{K}\left(\pi_{k+1}\left(\boldsymbol{X}_{1 .}\right)+\pi_{k}\left(\boldsymbol{X}_{1 .}\right)\right) p_{k}\left(\boldsymbol{X}_{1}\right)\left(1-p_{k}\left(\boldsymbol{X}_{1}\right)\right)\right] \\
& =\sum_{k=1}^{K}\left(J_{k}^{\boldsymbol{x} \boldsymbol{x}^{\top}}+\tilde{J}_{k}^{\boldsymbol{x} \boldsymbol{x}^{\top}}\right)
\end{aligned}
$$

as in (10) (recall that $J_{K}^{\boldsymbol{x} \boldsymbol{x}^{\top}}=0$ and $\tilde{J}_{1}^{\boldsymbol{x} \boldsymbol{x}^{\top}}=0$ for all $\boldsymbol{x}$ ). In the case of logistic regression predicting whether $y_{i}=1$ (for any $K$ ),

$$
I_{\beta \beta}^{\text {logistic }}\left(\alpha_{1}, \boldsymbol{\beta}\right)=J_{1}^{\boldsymbol{x} \boldsymbol{x}^{\top} ; \text { logistic }}+\tilde{J}_{2}^{\boldsymbol{x} \boldsymbol{x}^{\top} ; \text { logistic }}=\int \boldsymbol{x} \boldsymbol{x}^{\top} \pi_{1}(\boldsymbol{x})\left[1-\pi_{1}(\boldsymbol{x})\right] d F(\boldsymbol{x})
$$

matching (16).

## F.6. Verifying the Asymptotic Distribution of Each Estimator

By standard maximum likelihood theory (for example, the theorem on p. 145 of Serfling (1980, Section 4.2.2, multidimensional generalization on p. 148), or Theorem 13.1 in Wooldridge 2010), the result holds if we can verify three regularity conditions. Before we do, we will define the set $\Theta$ of feasible parameters $(\boldsymbol{\alpha}, \boldsymbol{\beta})$. Since

$$
\alpha_{1}<\alpha_{2}<\ldots<\alpha_{K-1}
$$

where the strict inequality follows from our assumption that no class has probability 0 for any $x \in[-1,1]$, define the set $\mathcal{A} \subset \mathbb{R}^{K-1}$ of points satisfying this constraint. Then define $\boldsymbol{\Theta}:=\mathcal{A} \times \mathbb{R}^{p}$, so that $(\boldsymbol{\alpha}, \boldsymbol{\beta}) \in \boldsymbol{\Theta}$.
Now we state and verify the needed regularity conditions.

1. (R1) The third derivatives of the log likelihood with respect to each parameter $(\boldsymbol{\alpha}, \boldsymbol{\beta})$ exist for all $\boldsymbol{x} \in \mathcal{S}$. This condition holds for both the proportional odds model and logistic regression because every entry of the Hessian matrices in Section (F.2) are differentiable in every parameter for any $K \geq 2$.

2. (R2) For each $\left(\boldsymbol{\alpha}_{0}, \boldsymbol{\beta}_{0}\right) \in \boldsymbol{\Theta}$, for all $(\boldsymbol{\alpha}, \boldsymbol{\beta})$ in a neighborhood of $\left(\boldsymbol{\alpha}_{0}, \boldsymbol{\beta}_{0}\right)$ it holds that (i) the element-wise absolute values of the gradients and Hessians of the likelihood are bounded by functions of $\boldsymbol{x}$ with finite integral over $\boldsymbol{x} \in \mathcal{S}$, and (ii) the element-wise absolute values of the third derivatives of the log likelihood is bounded by a function of $\boldsymbol{x}$ with finite expectation with respect to $\boldsymbol{X}$. Because $\boldsymbol{X}$ has bounded support, for these integrals and expectations to be finite it is enough for the bounding functions to over $\mathcal{S}$ to be finite constants-that is, it is enough to find upper bounds on the absolute values of the gradients and Hessians of the likelihoods and the third derivatives of the log likelihoods. The logistic regression likelihood

$$
\prod_{i=1}^{n} \frac{\exp \left\{-\mathbb{1}\left\{y_{i}=1\right\}\left(\alpha_{1}+\boldsymbol{\beta}^{\top} \boldsymbol{x}_{i}\right)\right\}}{1+\exp \left\{-\left(\alpha_{1}+\boldsymbol{\beta}^{\top} \boldsymbol{x}_{i}\right)\right\}}
$$

has continuous second derivatives and therefore its gradient and Hessian both have finite element-wise absolute value on the bounded support. The same is true of the proportional odds likelihood (36) when all outcomes have positive probability for all $\boldsymbol{x} \in \mathcal{S}$, that is, $\alpha_{1}<\ldots<\alpha_{K-1}$. Finally, examining again the Hessian matrices in Section F.2, we see that they have continuous derivatives in every parameter for any $K \geq 2$, so the third derivatives of the log likelihoods are bounded for any $\left(\boldsymbol{\alpha}_{0}, \boldsymbol{\beta}_{0}\right) \in \boldsymbol{\Theta}$ for all $\boldsymbol{x} \in \mathcal{S}$.
3. (R3) The Fisher information matrices exist and are finite and positive definite. One can see that both of the Fisher information matrices are finite entrywise for all $(\boldsymbol{\alpha}, \boldsymbol{\beta}) \in \boldsymbol{\Theta}$ by examining the matrices and noting that the probabilities for all of the classes are strictly greater than 0 over $\mathcal{S}$ by assumption. To verify the positive definiteness of the Fisher information matrices

$$
-\mathbb{E}\left[\frac{\partial^{2}}{\partial \boldsymbol{\theta} \boldsymbol{\theta}^{\top}} \mathcal{L}(\boldsymbol{\theta})\right]
$$

it is enough to show that the log likelihood for each model is strictly concave, which implies that $\frac{\partial^{2}}{\partial \boldsymbol{\theta} \boldsymbol{\theta}^{\top}} \mathcal{L}(\boldsymbol{\theta})$ is almost surely negative definite (since the log likelihood is twice differentiable). Strict concavity of the logistic regression log likelihood

$$
\sum_{i=1}^{n}\left[-\mathbb{1}\left\{y_{i}=1\right\}\left(\alpha_{1}+\boldsymbol{\beta}^{\top} \boldsymbol{x}_{i}\right)-\log \left(1+e^{-\left(\alpha_{1}+\boldsymbol{\beta}^{\top} \boldsymbol{x}_{i}\right)}\right)\right]
$$

is easily seen, and Pratt (1981) provides a proof that the log likelihood for the proportional odds model is strictly concave when the intercepts $\alpha_{1}, \ldots, \alpha_{K-1}$ are not equal.

Lastly, the asymptotic $\mathcal{O}(1 / n)$ bias is a consequence of standard maximum likelihood theory; see, for example, Cordeiro \& McCullagh (1991).

# G. Proof of Theorem 3.1

We prove Theorem 3.1 in Section G.1, and Section G. 2 contains proofs of the supporting lemmas.

## G.1. Proof of Theorem 3.1

The proof will proceed as follows. First we will show that PRESTO is a member of a class of models described by Ekvall \& Bottai (2022), which means we can bound the estimation error of the parameters of PRESTO in a finite sample under their Theorem 3 once we show that its assumptions are satisfied. Their result depends on the probability of a particular event $\mathcal{C}_{n, n, p_{n}(K-1)}$, and in Proposition G. 1 we prove a lower bound on $\mathbb{P}\left(\mathcal{C}_{n, n, p_{n}(K-1)}\right)$ that tends towards 1 as $n \rightarrow \infty$. This establishes the consistency of PRESTO.

In the notation of Ekvall \& Bottai (2022), we can express the objective function for the PRESTO estimator from (7) as $R\left\{b\left(y_{i}, \boldsymbol{x}_{i}, \boldsymbol{\theta}\right)\right\}-R\left\{a\left(y_{i}, \boldsymbol{x}_{i}, \boldsymbol{\theta}\right)\right\}+\lambda_{n}\|\boldsymbol{\theta}\|_{1}$, where $R(\cdot)=F(\cdot)$, the logistic cumulative distribution function;

$\left(a\left(y_{i}, \boldsymbol{x}_{i}, \boldsymbol{\theta}\right) ; b\left(y_{i}, \boldsymbol{x}_{i}, \boldsymbol{\theta}\right)\right)^{\top}=\boldsymbol{Z}_{i}^{\top} \boldsymbol{\theta}+\boldsymbol{m}_{i}$ where

$$
\begin{aligned}
\boldsymbol{Z}_{i} & :=\left(\begin{array}{cc}
\mathbb{1}\left\{y_{i} \geq 2\right\} \boldsymbol{x}_{i} & \mathbb{1}\left\{y_{i} \leq K-1\right\} \boldsymbol{x}_{i} \\
\mathbb{1}\left\{y_{i} \geq 3\right\} \boldsymbol{x}_{i} & \mathbb{1}\left\{2 \leq y_{i} \leq K-1\right\} \boldsymbol{x}_{i} \\
\vdots & \vdots \\
\mathbb{1}\left\{y_{i}=K\right\} \boldsymbol{x}_{i} & \mathbb{1}\left\{y_{i}=K-1\right\} \boldsymbol{x}_{i}
\end{array}\right) \in \mathbb{R}^{p_{n}(K-1) \times 2} \\
\boldsymbol{\theta} & :=\left(\begin{array}{c}
\boldsymbol{\beta}_{1} \\
\boldsymbol{\psi}_{2} \\
\vdots \\
\boldsymbol{\psi}_{K-1}
\end{array}\right) \in \mathbb{R}^{p_{n}(K-1)}, \quad \text { and } \\
\boldsymbol{m}_{i} & :=\left(\begin{array}{cc}
\left\{\begin{array}{cc}
-\infty, & y_{i}=1 \\
\alpha_{k-1}, & y_{i}=k(k \geq 2) \\
\alpha_{k}, & y_{i}=k(k<K-1) \\
\infty, & y_{i}=K
\end{array}\right\} \in \mathbb{R}^{2}\right.
\end{aligned}
$$

where $\overline{\mathbb{R}}:=\mathbb{R} \cup\{-\infty, \infty\}$ denotes the extended real number system; and, as elsewhere in the paper, $\boldsymbol{\psi}_{k}=\boldsymbol{\beta}_{k}-\boldsymbol{\beta}_{k-1}$ for $k \in\{2, \ldots, K-1\}$. Observe that this is a special case of model (1) of Ekvall \& Bottai (2022). Now, since $\boldsymbol{\Theta}=\mathbb{R}^{p_{n}(K-1)}$ is open and the standard logistic density $r(t)=\exp \{-t\} /(1+\exp \{-t\})^{2}$ is strictly log-concave, strictly positive, and continuously differentiable on $\mathbb{R}$, assumptions $(a)$ and $(b)$ of Theorem 3 in Ekvall \& Bottai (2022) are satisfied, assumption $S\left(C_{4}\right)$ is sufficient for assumption $(c)$, and assumption (e) is satisfied for $C_{3}=c_{2}$.
We now discuss Assumption 1 from Ekvall \& Bottai (2022). Note that a decision boundary crosses at some $\boldsymbol{x} \in \mathcal{S}$ if and only if for some $k \in\{2, \ldots, K-1\}$ it holds that

$$
\begin{aligned}
F\left(\alpha_{k}+\boldsymbol{\beta}_{k}^{\top} \boldsymbol{x}\right)-F\left(\alpha_{k-1}+\boldsymbol{\beta}_{k-1}^{\top} \boldsymbol{x}\right) & \leq 0 \\
\Longleftrightarrow \quad b(k, \boldsymbol{x}, \boldsymbol{\theta})-a(k, \boldsymbol{x}, \boldsymbol{\theta}) & \leq 0
\end{aligned}
$$

Observe that for $k=1$ it holds that $m_{i 1}=-\infty$ and for $k=K$ it holds that $m_{i 2}=\infty$; that is, an element of $\boldsymbol{m}_{i}$ is infinite. So Assumption $T\left(C_{4}\right)$ is sufficient for it to either hold that $\boldsymbol{Z}_{i}^{\top} \boldsymbol{\theta}+\boldsymbol{m}_{i} \in E$ where $E \subseteq\left\{\boldsymbol{t} \in \mathbb{R}^{2}: t_{1}<t_{2}\right\}$ or an element of $\boldsymbol{m}_{i}$ is infinite. To satisfy Assumption 1, it only remains to show that there exists such an $E$ that is compact. Note that $\left\|\boldsymbol{\theta}_{*}\right\|_{\infty}$ is bounded under Assumption $S\left(s, C_{4}\right)$ and $\left\|\boldsymbol{\theta}_{*}\right\|_{0} \leq s$ under our sparsity assumption, so $\left\|\boldsymbol{\theta}_{*}\right\|_{1}$ is bounded due to Hölder's inequality. Lastly $\boldsymbol{m}_{i}$ is bounded because $\max _{k \in\{1, \ldots, K-1\}}\left|\alpha_{k}\right| \leq C_{4}$ under Assumption $T\left(C_{4}\right)$, so we have shown that we can find such an $E$ that is bounded. Choose one that is closed and we have $E$ compact by Bolzano-Weierstrass.

We have shown that the assumptions of Theorem 3 of Ekvall \& Bottai (2022) are satisfied, so we have that for $n$ large enough that $p_{n} \geq K-1$, with probability at least $\mathbb{P}\left(\mathcal{C}_{\kappa, n, p_{n}(K-1)}\right)-\left[p_{n}(K-1)\right]^{-c_{3}} \geq \mathbb{P}\left(\mathcal{C}_{\kappa, n, p_{n}(K-1)}\right)-\left(p_{n}^{2}\right)^{-c_{3}}$

$$
\left\|\hat{\boldsymbol{\theta}}^{\lambda_{n}}-\boldsymbol{\theta}_{*}\right\|_{1} \leq c_{5} \sqrt{\frac{\log \left(p_{n}(K-1)\right)}{n}} \leq c_{5} \sqrt{\frac{\log \left(p_{n}^{2}\right)}{n}}=c_{5} \sqrt{2} \sqrt{\frac{\log p_{n}}{n}}
$$

where $c_{3}$ and $c_{5}$ are constants from Ekvall \& Bottai (2022), and $\mathcal{C}_{\kappa, n, p_{n}(K-1)}$ is defined as follows. For a set $\mathcal{A} \subseteq$ $\left\{1, \ldots, p_{n}(K-1)\right\}$, define $\boldsymbol{\theta}_{\mathcal{A}} \in \mathbb{R}^{p_{n}(K-1)}$ to have $j^{\text {th }}$ entry

$$
\left(\boldsymbol{\theta}_{\mathcal{A}}\right)_{j}:= \begin{cases}\boldsymbol{\theta}_{j}, & j \in \mathcal{A} \\ 0, & j \notin \mathcal{A}\end{cases}
$$

and define $\boldsymbol{\theta}_{\mathcal{A}^{c}}:=\boldsymbol{\theta}-\boldsymbol{\theta}_{\mathcal{A}}$. Then for an $s$-sparse (in the sense of Assumption $S(s, c)$ ) $\boldsymbol{\theta}_{*}$ with support set $\mathcal{S}$, define

$$
\mathbb{C}(\mathcal{S}):=\left\{\boldsymbol{\theta} \in \mathbb{R}^{p_{n}(K-1)}:\left\|\boldsymbol{\theta}_{\mathcal{S}^{c}}\right\|_{1} \leq 3\left\|\boldsymbol{\theta}_{\mathcal{S}}\right\|_{1}\right\}
$$

We interpret $\mathbb{C}(\mathcal{S})$ to be a set of approximately $s$-sparse vectors (the vectors would be exactly $s$-sparse if $\left\|\boldsymbol{\theta}_{\mathcal{S}^{c}}\right\|_{1}=0$ ). Then for $\kappa>0$ and $n \in \mathbb{N}$, define

$$
\mathcal{C}_{\kappa, n, p_{n}(K-1)}:=\left\{(\boldsymbol{X}, \boldsymbol{y}): \inf _{\{\boldsymbol{\theta} \in \mathbb{C}(\mathcal{S}):\|\boldsymbol{\theta}\|_{2}=1\}}\left\{\boldsymbol{\theta}^{\top}\left(\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{Z}_{i} \boldsymbol{Z}_{i}^{\top}\right) \boldsymbol{\theta}\right\} \geq \kappa\right\}
$$

If the set of $\boldsymbol{\theta}$ over which this condition must hold were $\mathbb{R}^{p_{n}(K-1)}$, this would be a minimum eigenvalue condition on $\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{Z}_{i} \boldsymbol{Z}_{i}^{\top}$. This condition is sometimes called a restricted eigenvalue condition (Bickel et al., 2009). We bound $\mathbb{P}\left(\mathcal{C}_{n, n, p_{n}(K-1)}\right)$ in Proposition G.1.
Proposition G.1. Suppose the assumptions of Theorem 3.1 hold. Let

$$
\pi_{\text {rare,min }}:=\inf _{\boldsymbol{x} \in \mathcal{S}, k \in\{1, \ldots, K\}}\left\{\mathbb{P}\left(y_{i}=k \mid \boldsymbol{x}\right)\right\}
$$

and observe that $\pi_{\text {rare,min }}>0$ under Assumptions $X\left(\mathbb{R}^{p_{n}}\right)$, $S\left(s, C_{4}\right)$, and $T\left(C_{4}\right)$. Assume $n$ is large enough so that

$$
n \pi_{\text {rare, min }}>\max \left\{2\left(C \sqrt{p_{n}}+\sqrt{\frac{a}{\lambda_{\text {min }}^{*}}}\right)^{2}, 2\right\}
$$

for some $a>0$ (recall from the statement of Theorem 3.1 that we assumed $\lambda_{\text {min }}^{*}:=$ $\min _{k \in\{1, \ldots, K\}} \lambda_{\text {min }}\left(\mathbb{E}\left[\boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \mid y_{i}=k\right]\right)>b$ for a fixed $b>0$ ). Then

$$
\mathbb{P}\left(\mathcal{C}_{a \pi_{\text {rare,min }} / 4, n, p_{n}(K-1)}\right) \geq 1-2 K \exp \left\{-\frac{1}{2} \pi_{\text {rare,min }}^{2} n\right\}-2 K \exp \left\{-c\left(\sqrt{\frac{n \pi_{\text {rare,min }}}{2}}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\text {min }}^{*}}}\right)^{2}\right\}
$$

Proof. Provided in Section G.2.

Observe that since $p_{n} \leq C_{1} n^{C_{2}}$ for some $C_{1}>0, C_{2} \in(0,1)$ this probability tends to 1 as $n \rightarrow \infty$. Lemma G.2, below, along with (37) then shows that with probability at least

$$
1-p_{n}^{-C_{5}}-2 K \exp \left\{-\frac{1}{2} \pi_{\text {rare,min }}^{2} n\right\}-2 K \exp \left\{-c\left(\sqrt{\frac{n \pi_{\text {rare,min }}}{2}}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\text {min }}^{*}}}\right)^{2}\right\}
$$

it holds that

$$
\left\|\hat{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{2} \leq\left\|\hat{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{1} \leq C_{6} \sqrt{\frac{\log p_{n}}{n}}
$$

where $C_{5}:=2 c_{3}$ and $C_{6}:=c_{5} \sqrt{2}(K-1)$ (where $c_{5}$ depends on the sparsity level $s$ ).
Lemma G.2. $\left\|\hat{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{2} \leq\left\|\hat{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{1} \leq(K-1)\left\|\hat{\boldsymbol{\theta}}^{\lambda_{n}}-\boldsymbol{\theta}_{*}\right\|_{1}$.

Proof. Provided in Section G.2.

Finally, we can now show consistency by showing that the random variable $\left\|\hat{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{2}$ converges in probability to 0 . For any $\epsilon>0$,

$$
\begin{aligned}
& \lim _{n \rightarrow \infty} \mathbb{P}\left(\left\|\hat{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{2}<\epsilon\right) \\
\stackrel{(*)}{\geq} & \lim _{n \rightarrow \infty} \mathbb{P}\left(\left\|\hat{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{2}<C_{6} \sqrt{\frac{\log p_{n}}{n}}\right) \\
\geq & \lim _{n \rightarrow \infty}\left(1-p_{n}^{-C_{5}}-2 K \exp \left\{-\frac{1}{2} \pi_{\text {rare,min }}^{2} n\right\}-2 K \exp \left\{-c\left(\sqrt{\frac{n \pi_{\text {rare,min }}}{2}}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\text {min }}^{*}}}\right)^{2}\right\}\right) \\
= & 1
\end{aligned}
$$

where $(*)$ follows because for large enough $n, \epsilon>C_{6} \sqrt{\log \left(p_{n}\right) / n}$. This establishes Theorem 3.1. All that remains is to provide proofs for the supporting lemmas and proposition, which we do in the following section.

Remark G.3. Observe that for finite $n$ we have that for $n$ large enough, with probability at least equal to the expression in (39) it holds that

$$
\sqrt{n}\left\|\hat{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{2} \leq \sqrt{n}\left\|\hat{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{1} \leq C_{6} \sqrt{\log p_{n}}
$$

This establishes a high-probability finite sample relationship between our theoretical guarantee and the asymptotic mean squared error from Definition 2.1.

Also note that for any $\epsilon>0$, there exists an $n_{\epsilon}$ such that the probability in (39) is less than $\epsilon$ for all $n \geq n_{\epsilon}$. Since there also exists an $n_{2}$ large enough so that for all $n \geq n_{2}$ it holds that

$$
\epsilon \geq C_{6} \sqrt{\frac{\log p_{n}}{n}}
$$

we have for any $\epsilon>0$, for all $n \geq \max \left\{n_{\epsilon}, n_{2}\right\}$

$$
\begin{aligned}
\mathbb{P}\left(\frac{\left\|\hat{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{1}}{\sqrt{\log \left(p_{n}\right) / n}} \geq C_{6}\right) & \leq 1-p_{n}^{-C_{5}}-2 K \exp \left\{-\frac{1}{2} \pi_{\text {rare,min }}^{2} n\right\}-2 K \exp \left\{-c\left(\sqrt{\frac{n \pi_{\text {rare,min }}}{2}}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\text {min }}^{*}}}\right)^{2}\right\} \\
& <\epsilon
\end{aligned}
$$

that is, $\left\|\hat{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{1} \in \mathcal{O}_{p}\left(\sqrt{\log \left(p_{n}\right) / n}\right)$, and likewise for $\left\|\hat{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{2}$. Again, we emphasize that $C_{6}$ depends on the sparsity $s$ (though the above holds since we have assumed that $s$ is fixed).

# G.2. Supporting Results for Proof of Theorem 3.1

Proof of Proposition G.1. We will show that there is a high probability event such that for $a>0$ it holds that

$$
\inf _{\{\boldsymbol{\theta} \in \mathbb{C}(\mathcal{S}):\|\boldsymbol{\theta}\|_{2}=1\}}\left\{\boldsymbol{\theta}^{\top}\left(\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{Z}_{i} \boldsymbol{Z}_{i}^{\top}\right) \boldsymbol{\theta}\right\}>\frac{1}{2} a \pi_{\text {rare,min }}
$$

## Let

$$
\begin{gathered}
\boldsymbol{A}^{(1)}=\left(\mathbf{0}_{K-1} \quad \boldsymbol{e}_{1}\right) \in \mathbb{R}^{(K-1) \times 2} \\
\boldsymbol{A}^{(k)}=\left(\begin{array}{cc}
\mathbf{1}_{k-1} & \mathbf{1}_{k-1} \\
0 & 1 \\
\mathbf{0}_{K-k-1} & \mathbf{0}_{K-k-1}
\end{array}\right)=\left(\begin{array}{ll}
\sum_{k^{\prime}=1}^{k-1} \boldsymbol{e}_{k^{\prime}} & \sum_{k^{\prime}=1}^{k} \boldsymbol{e}_{k^{\prime}}
\end{array}\right) \in \mathbb{R}^{(K-1) \times 2}, k \in\{2, \ldots, K-1\}
\end{gathered}
$$

and

$$
\boldsymbol{A}^{(K)}=\left(\begin{array}{ll}
\mathbf{1}_{K-1} & \mathbf{0}_{K-1}
\end{array}\right)=\left(\begin{array}{ll}
\sum_{k^{\prime}=1}^{K-1} \boldsymbol{e}_{k^{\prime}} & \mathbf{0}_{K-1}
\end{array}\right) \in \mathbb{R}^{(K-1) \times 2}
$$

where $\mathbf{0}_{n}$ and $\mathbf{1}_{n}$ are $n$-vectors of zeroes and ones (respectively) and $\boldsymbol{e}_{k}$ is the standard basis vector in $\mathbb{R}^{K-1}$ with a 1 in the $k^{\text {th }}$ entry and zeroes elsewhere, and $\boldsymbol{A}^{(k)} \in \mathbb{R}^{(K-1) \times 2}$ for all $k$. Note that

$$
\boldsymbol{Z}_{i}=\boldsymbol{A}^{\left(\varrho_{i}\right)} \otimes \boldsymbol{x}_{i}
$$

## Let

$$
\boldsymbol{B}=\left(\begin{array}{ccccc}
1 & 1 & 1 & \cdots & 1 \\
0 & 1 & 1 & \cdots & 1 \\
0 & 0 & 1 & \cdots & 1 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{array}\right)=\left(\begin{array}{llll}
\boldsymbol{e}_{1} & \sum_{k^{\prime}=1}^{2} \boldsymbol{e}_{k^{\prime}} & \cdots & \sum_{k^{\prime}=1}^{K-1} \boldsymbol{e}_{k^{\prime}}
\end{array}\right) \in \mathbb{R}^{(K-1) \times(K-1)}
$$

that is, the columns of $\boldsymbol{B}$ are $\boldsymbol{B}_{k}=\sum_{\ell=1}^{k} \boldsymbol{e}_{\ell}$. We will make use of the following lemmas, which we prove later in this section:

Lemma G.4.

$$
\sum_{k=1}^{K} \frac{n_{k}}{n} \boldsymbol{A}^{(k)}\left(\boldsymbol{A}^{(k)}\right)^{\top}=\boldsymbol{B} \boldsymbol{D} \boldsymbol{B}^{\top}
$$

where

$$
\boldsymbol{D}:=\operatorname{diag}\left(\frac{n_{1}+n_{2}}{n}, \ldots, \frac{n_{K-1}+n_{K}}{n}\right) \in \mathbb{R}^{(K-1) \times(K-1)}
$$

Lemma G.5. $\sigma_{\min }^{2}(\boldsymbol{B}) \geq 1 / 2$.

Lemma G.6. For arbitrary matrices $\overline{\boldsymbol{A}}, \overline{\boldsymbol{B}}$, and $\overline{\boldsymbol{C}}$, if $\overline{\boldsymbol{B}} \succeq \overline{\boldsymbol{C}}$ and $\overline{\boldsymbol{A}} \succeq \mathbf{0}$ then $\overline{\boldsymbol{A}} \otimes \overline{\boldsymbol{B}} \succeq \overline{\boldsymbol{A}} \otimes \overline{\boldsymbol{C}}$.

Lemma G.7. With probability at least $1-K \exp \left\{-\frac{1}{2} \pi_{\text {rare,min }}^{2} n\right\}$ it holds that $\min \left\{n_{1}, \ldots, n_{K}\right\}>\pi_{\text {rare,min }} n / 2$, where $n_{k}:=\sum_{i=1}^{n} \mathbb{1}\left\{y_{i}=k\right\}$ is the number of observations in class $k$.

Lemma G.8. Under the assumptions of Proposition G.1, there exist constants $c, C>0$ such that $\lambda_{\text {min }}\left(\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}\right) \geq$ a for all $k \in\{1, \ldots, K\}$ with probability at least

$$
1-2 K \exp \left\{-c\left(\sqrt{\frac{n \pi_{\text {rare }, \text { min }}}{2}}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\min }^{*}}}\right)^{2}\right\}-K \exp \left\{-\frac{1}{2} \pi_{\text {rare,min }}^{2} n\right\}
$$

where $n_{k}$ is the number of observations in class $k$ and $\lambda_{\text {min }}^{*}:=\min _{k \in\{1, \ldots, K\}}\left\{\lambda_{\text {min }}\left(\mathbb{E}\left[\boldsymbol{X}^{\top} \boldsymbol{X} \mid y=k\right]\right)\right\}$.

## Using a union bound, there exists an event with probability at least

$$
1-2 K \exp \left\{-\frac{1}{2} \pi_{\text {rare,min }}^{2} n\right\}-2 K \exp \left\{-c\left(\sqrt{\frac{n \pi_{\text {rare,min }}}{2}}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\min }^{*}}}\right)^{2}\right\}
$$

on which the conclusions of all of the above lemmas hold for $n$ large enough. On this event we have for $n$ sufficiently large

$$
\begin{aligned}
\inf _{\left\{\boldsymbol{\theta} \in \mathbb{C}(\mathcal{S}):\|\boldsymbol{\theta}\|_{2}=1\right\}}\left\{\boldsymbol{\theta}^{\top}\left(\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{Z}_{i} \boldsymbol{Z}_{i}^{\top}\right) \boldsymbol{\theta}\right\} & \geq \inf _{\left\{\boldsymbol{\theta} \in \mathbb{R}^{p_{n}(K-1)}:\|\boldsymbol{\theta}\|_{2}=1\right\}}\left\{\boldsymbol{\theta}^{\top}\left(\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{Z}_{i} \boldsymbol{Z}_{i}^{\top}\right) \boldsymbol{\theta}\right\} \\
& =\lambda_{\min }\left(\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{Z}_{i} \boldsymbol{Z}_{i}^{\top}\right) \\
\stackrel{(a)}{=} & \lambda_{\min }\left(\frac{1}{n} \sum_{i=1}^{n}\left(\boldsymbol{A}^{\left(y_{i}\right)} \otimes \boldsymbol{x}_{i}\right)\left(\boldsymbol{A}^{\left(y_{i}\right)} \otimes \boldsymbol{x}_{i}\right)^{\top}\right) \\
& =\lambda_{\min }\left(\frac{1}{n} \sum_{i=1}^{n}\left(\boldsymbol{A}^{\left(y_{i}\right)}\left(\boldsymbol{A}^{\left(y_{i}\right)}\right)^{\top}\right) \otimes\left(\boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}\right)\right) \\
& =\lambda_{\min }\left(\frac{1}{n} \sum_{k=1}^{K} \sum_{i: y_{i}=k}\left(\boldsymbol{A}^{\left(y_{i}\right)}\left(\boldsymbol{A}^{\left(y_{i}\right)}\right)^{\top}\right) \otimes\left(\boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}\right)\right) \\
& =\lambda_{\min }\left(\frac{1}{n} \sum_{k=1}^{K}\left(\boldsymbol{A}^{(k)}\left(\boldsymbol{A}^{(k)}\right)^{\top}\right) \otimes\left(\sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}\right)\right) \\
& =\lambda_{\min }\left(\sum_{k=1}^{K}\left(\frac{n_{k}}{n} \boldsymbol{A}^{(k)}\left(\boldsymbol{A}^{(k)}\right)^{\top}\right) \otimes\left(\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}\right)\right) \\
& \stackrel{(k)}{\geq} \lambda_{\min }\left(\sum_{k=1}^{K}\left(\frac{n_{k}}{n} \boldsymbol{A}^{(k)}\left(\boldsymbol{A}^{(k)}\right)^{\top}\right) \otimes\left(a \boldsymbol{I}_{p}\right)\right) \\
& \stackrel{(c)}{=} \lambda_{\min }\left(\sum_{k=1}^{K} \frac{n_{k}}{n} \boldsymbol{A}^{(k)}\left(\boldsymbol{A}^{(k)}\right)^{\top}\right) \lambda_{\min }\left(a \boldsymbol{I}_{p}\right) \\
& \stackrel{(d)}{=} \lambda_{\min }\left(\boldsymbol{B} \boldsymbol{D} \boldsymbol{B}^{\top}\right) \lambda_{\min }\left(a \boldsymbol{I}_{p}\right) \\
& \stackrel{(e)}{\geq} a_{\min _{k \in\{1, \ldots, K-1\}}}\left\{\frac{n_{k}+n_{k+1}}{n}\right\} \lambda_{\min }\left(\boldsymbol{B} \boldsymbol{B}^{\top}\right) \\
& \stackrel{(f)}{\geq} \frac{a}{4} \min _{k \in\{1, \ldots, K-1\}}\left\{\frac{n_{k}+n_{k+1}}{n}\right\} \\
& \stackrel{(g)}{>} \frac{a \pi_{\text {rare,min }}}{4},
\end{aligned}
$$

where in (a) we used (40), (b) holds on the high probability event by Lemmas G. 6 and G. 8 because $\lambda_{\min }\left(\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}\right) \geq a$ implies $\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \succeq a \boldsymbol{I}_{p}$ for all $k$, in ( $c$ ) we used the fact that for matrices $\boldsymbol{M}, \boldsymbol{N}$ the eigenvalues of $\boldsymbol{M} \otimes \boldsymbol{N}$ are the products of the eigenvalues of $\boldsymbol{M}$ and $\boldsymbol{N}$ and both of the factor matrices are positive semidefinite, in $(d)$ we applied Lemma G.4, (e) follows from $\boldsymbol{B} \boldsymbol{D} \boldsymbol{B}^{\top} \succeq \min _{k \in\{1, \ldots, K-1\}}\left\{\frac{n_{k}+n_{k+1}}{n}\right\} \boldsymbol{B} \boldsymbol{B}^{\top}$, in $(f)$ we applied Lemma G.5, and $(g)$ follows from Lemma G.7. That is, on this high probability event it holds that

$$
\inf _{\left\{\boldsymbol{\theta} \in \mathbb{C}(\mathcal{S}):\|\boldsymbol{\theta}\|_{2}=1\right\}}\left\{\boldsymbol{\theta}^{\top}\left(\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{Z}_{i} \boldsymbol{Z}_{i}^{\top}\right) \boldsymbol{\theta}\right\}>\frac{a \pi_{\text {rare,min }}}{4}
$$

and the conclusion follows.

Proof of Lemma G.2. For convenience, denote $\boldsymbol{\theta}_{1}=\boldsymbol{\beta}_{1}$ and $\boldsymbol{\theta}_{k}=\boldsymbol{\psi}_{k}, k \in\{2, \ldots, K-1\}$. Let

$$
\check{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}}^{\lambda_{n}}=\left(\left(\check{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}, 1}^{\lambda_{n}}\right)^{\top},\left(\check{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}, 2}^{\lambda_{n}}\right)^{\top}, \ldots,\left(\check{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}, K-1}^{\lambda_{n}}\right)^{\top}\right)^{\top}:=\left(\left(\check{\boldsymbol{\theta}}_{1}^{\lambda_{n}}-\boldsymbol{\theta}_{1}\right)^{\top},\left(\check{\boldsymbol{\theta}}_{2}^{\lambda_{n}}-\boldsymbol{\theta}_{2}\right)^{\top}, \ldots,\left(\check{\boldsymbol{\theta}}_{K-1}^{\lambda_{n}}-\boldsymbol{\theta}_{K-1}\right)^{\top}\right)^{\top}
$$

Note that $\boldsymbol{\beta}_{k}=\sum_{k^{\prime} \leq k} \boldsymbol{\theta}_{k^{\prime}}$. Let $\boldsymbol{\beta}:=\left(\boldsymbol{\beta}_{1}^{\top}, \boldsymbol{\beta}_{2}^{\top}, \ldots, \boldsymbol{\beta}_{K-1}^{\top}\right)^{\top}$, and denote by

$$
\check{\boldsymbol{\beta}}_{k}^{\lambda_{n}}:=\sum_{k^{\prime} \leq k} \check{\boldsymbol{\theta}}_{k^{\prime}}, \quad k \in\{1, \ldots, K-1\}
$$

the estimates of $\boldsymbol{\beta}$ yielded from the estimates of $\boldsymbol{\theta}$. Let

$$
\begin{aligned}
\check{\boldsymbol{\epsilon}}_{\boldsymbol{\beta}}^{\lambda_{n}} & =\left(\left(\check{\boldsymbol{\epsilon}}_{\boldsymbol{\beta}, 1}^{\lambda_{n}}\right)^{\top},\left(\check{\boldsymbol{\epsilon}}_{\boldsymbol{\beta}, 2}^{\lambda_{n}}\right)^{\top}, \ldots,\left(\check{\boldsymbol{\epsilon}}_{\boldsymbol{\beta}, K-1}^{\lambda_{n}}\right)^{\top}\right)^{\top} \\
& :=\left(\left(\check{\boldsymbol{\beta}}_{1}^{\lambda_{n}}-\boldsymbol{\beta}_{1}\right)^{\top},\left(\check{\boldsymbol{\beta}}_{2}^{\lambda_{n}}-\boldsymbol{\beta}_{2}\right)^{\top}, \ldots,\left(\check{\boldsymbol{\beta}}_{K-1}^{\lambda_{n}}-\boldsymbol{\beta}_{K-1}\right)^{\top}\right)^{\top}
\end{aligned}
$$

and observe that

$$
\begin{aligned}
\check{\boldsymbol{\epsilon}}_{\boldsymbol{\beta}}^{\lambda_{n}} & =\left(\left(\check{\boldsymbol{\beta}}_{1}^{\lambda_{n}}-\boldsymbol{\beta}_{1}\right)^{\top},\left(\check{\boldsymbol{\beta}}_{2}^{\lambda_{n}}-\boldsymbol{\beta}_{2}\right)^{\top}, \ldots,\left(\check{\boldsymbol{\beta}}_{K-1}^{\lambda_{n}}-\boldsymbol{\beta}_{K-1}\right)^{\top}\right)^{\top} \\
& =\left(\left(\check{\boldsymbol{\theta}}_{1}^{\lambda_{n}}-\boldsymbol{\theta}_{1}\right)^{\top},\left(\sum_{k^{\prime} \leq 2} \check{\boldsymbol{\theta}}_{k^{\prime}}-\sum_{k^{\prime} \leq 2} \boldsymbol{\theta}_{k^{\prime}}\right)^{\top}, \ldots,\left(\sum_{k^{\prime} \leq K-1} \check{\boldsymbol{\theta}}_{k^{\prime}}-\sum_{k^{\prime} \leq K-1} \boldsymbol{\theta}_{k^{\prime}}\right)^{\top}\right)^{\top} \\
& =\left(\left(\check{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}, 1}^{\lambda_{n}}\right)^{\top},\left(\sum_{k^{\prime} \leq 2} \check{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}, k^{\prime}}^{\lambda_{n}}\right)^{\top}, \ldots,\left(\sum_{k^{\prime} \leq K-1} \check{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}, k^{\prime}}^{\lambda_{n}}\right)^{\top}\right)^{\top}
\end{aligned}
$$

## Then

$$
\begin{aligned}
& \left\|\check{\boldsymbol{\epsilon}}_{\boldsymbol{\beta}, 1}^{\lambda_{n}}\right\|_{1}=\sum_{k=1}^{K-1}\left\|\check{\boldsymbol{\epsilon}}_{\boldsymbol{\beta}, k}^{\lambda_{n}}\right\|_{1} \stackrel{(a)}{=} \sum_{k=1}^{K-1}\left\|\sum_{k^{\prime} \leq k} \check{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}, k^{\prime}}^{\lambda_{n}}\right\|_{1} \stackrel{(b)}{\leq} \sum_{k=1}^{K-1} \sum_{k^{\prime} \leq k}\left\|\check{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}, k^{\prime}}^{\lambda_{n}}\right\|_{1}=\sum_{k=1}^{K-1}(K-k)\left\|\check{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}, k}^{\lambda_{n}}\right\|_{1} \\
& \leq(K-1) \sum_{k=1}^{K-1}\left\|\check{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}, k}^{\lambda_{n}}\right\|_{1}=(K-1)\left\|\check{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}}^{\lambda_{n}}\right\|_{1}
\end{aligned}
$$

where in (a) we used (41) and in (b) we used the triangle inequality. Lastly, $\left\|\check{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{2} \leq\left\|\check{\boldsymbol{\beta}}^{\lambda_{n}}-\boldsymbol{\beta}\right\|_{1}$ is a property of the $\ell_{1}$ and $\ell_{2}$ norms.

Proof of Lemma G.4.

$$
\begin{aligned}
& \sum_{k=1}^{K} \frac{n_{k}}{n} \boldsymbol{A}^{(k)}\left(\boldsymbol{A}^{(k)}\right)^{\top} \\
= & \frac{n_{1}}{n} \boldsymbol{A}^{(1)}\left(\boldsymbol{A}^{(1)}\right)^{\top}+\sum_{k=2}^{K-1} \frac{n_{k}}{n} \boldsymbol{A}^{(k)}\left(\boldsymbol{A}^{(k)}\right)^{\top}+\frac{n_{K}}{n} \boldsymbol{A}^{(K)}\left(\boldsymbol{A}^{(K)}\right)^{\top} \\
= & \frac{n_{1}}{n}\left(\mathbf{0}_{K-1} \quad \boldsymbol{e}_{1}\right)\left(\begin{array}{c}
\mathbf{0}_{K-1}^{\top} \\
\boldsymbol{e}_{1}^{\top}
\end{array}\right)+\sum_{k=2}^{K-1} \frac{n_{k}}{n}\left(\sum_{k^{\prime}=1}^{k-1} \boldsymbol{e}_{k^{\prime}} \quad \sum_{k^{\prime}=1}^{k} \boldsymbol{e}_{k^{\prime}}\right)\left(\begin{array}{c}
\sum_{k^{\prime}=1}^{k-1} \boldsymbol{e}_{k^{\prime}}^{\top} \\
\sum_{k^{\prime}=1}^{k} \boldsymbol{e}_{k^{\prime}}^{\top}
\end{array}\right) \\
& +\frac{n_{K}}{n}\left(\sum_{k^{\prime}=1}^{K-1} \boldsymbol{e}_{k^{\prime}} \quad \mathbf{0}_{K-1}\right)\left(\begin{array}{c}
\sum_{k^{\prime}=1}^{K-1} \boldsymbol{e}_{k^{\prime}}^{\top} \\
\mathbf{0}_{K-1}^{\top}
\end{array}\right) \\
= & \frac{n_{1}}{n}\left(\mathbf{0}_{K-1} \mathbf{0}_{K-1}^{\top}+\boldsymbol{e}_{1} \boldsymbol{e}_{1}^{\top}\right)+\sum_{k=2}^{K-1} \frac{n_{k}}{n}\left(\sum_{k^{\prime}=1}^{k-1} \boldsymbol{e}_{k^{\prime}} \sum_{k^{\prime}=1}^{k-1} \boldsymbol{e}_{k^{\prime}}^{\top}+\sum_{k^{\prime}=1}^{k} \boldsymbol{e}_{k^{\prime}} \sum_{k^{\prime}=1}^{k} \boldsymbol{e}_{k^{\prime}}^{\top}\right) \\
& +\frac{n_{K}}{n}\left(\sum_{k^{\prime}=1}^{K-1} \boldsymbol{e}_{k^{\prime}} \sum_{k^{\prime}=1}^{K-1} \boldsymbol{e}_{k^{\prime}}^{\top}+\mathbf{0}_{K-1} \mathbf{0}_{K-1}^{\top}\right) \\
= & \frac{n_{1}}{n} \boldsymbol{B}_{1} \boldsymbol{B}_{1}^{\top}+\sum_{k=2}^{K-1} \frac{n_{k}}{n}\left(\boldsymbol{B}_{k-1} \boldsymbol{B}_{k-1}^{\top}+\boldsymbol{B}_{k} \boldsymbol{B}_{k}^{\top}\right)+\frac{n_{K}}{n} \boldsymbol{B}_{K-1} \boldsymbol{B}_{K-1}^{\top} \\
= & \sum_{k=1}^{K-1} \frac{n_{k}+n_{k+1}}{n} \boldsymbol{B}_{k} \boldsymbol{B}_{k}^{\top} \\
= & \boldsymbol{B} \boldsymbol{D} \boldsymbol{B}^{\top} .
\end{aligned}
$$

Proof of Lemma G.5. $\boldsymbol{B}$ is full rank with inverse

$$
\boldsymbol{B}^{-1}=\left(\begin{array}{cccccc}
1 & -1 & 0 & 0 & \cdots & 0 \\
0 & 1 & -1 & 0 & \cdots & 0 \\
0 & 0 & 1 & -1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & 0 & \cdots & 1
\end{array}\right)
$$

## We have

$$
\left\|\boldsymbol{B}^{-1}\right\|_{\mathrm{op}} \leq \sqrt{\left\|\boldsymbol{B}^{-1}\right\|_{1}\left\|\boldsymbol{B}^{-1}\right\|_{\infty}}=\sqrt{2 \cdot 2}=2
$$

where we have used that the $\ell_{1}$ norm of each column and row is at most 2 . Then the result follows from $\sigma_{\min }(\boldsymbol{B})=$ $1 /\left\|\boldsymbol{B}^{-1}\right\|_{\text {op }}$.

Proof of Lemma G.6. The matrix $\overline{\boldsymbol{A}} \otimes(\overline{\boldsymbol{B}}-\overline{\boldsymbol{C}})$ is the Kronecker product of two positive semidefinite matrices and is therefore positive semidefinite.

Proof of Lemma G.7. First we state a lemma we will use.
Lemma G.9. Suppose that for all $\boldsymbol{x} \in \mathcal{S}$ and $k \in\{1, \ldots, K\}$ it holds that $\mathbb{P}\left(y_{i}=k \mid \boldsymbol{x}\right) \geq \pi_{\text {rare,min }}$. Let $n_{k}$ be the number of observations in class $k$ from a data set of size $n$. Then for any $q \in\left\{1, \ldots,\left\lfloor n \pi_{\text {rare,min }}\right\rfloor\right\}$,

$$
\mathbb{P}\left(\bigcap_{k=1}^{K}\left\{n_{k}>q\right\}\right) \geq 1-K \exp \left\{-2 n\left(\pi_{\text {rare,min }}-\frac{q}{n}\right)^{2}\right\}
$$

Proof. Provided later in this section.

Note that $n \pi_{\text {rare,min }} / 2 \leq\left\lfloor n \pi_{\text {rare,min }}\right\rfloor$ because $x / 2 \leq\lfloor x\rfloor$ for all $x \geq 2$ and $n \pi_{\text {rare,min }} \geq 2$ due to assumption (38). So the assumptions of Lemma G. 9 are satisfied for $q=n \pi_{\text {rare,min }} / 2$, and there are more than $n \pi_{\text {rare,min }} / 2$ observations in each class with probability at least

$$
1-K \exp \left\{-2 n\left(\pi_{\text {rare,min }}-\pi_{\text {rare,min }} / 2\right\}\right)^{2}\right\}=1-K \exp \left\{-\frac{1}{2} \pi_{\text {rare,min }}^{2} n\right\}
$$

Proof of Lemma G.8. We will prove the result by using a concentration inequality on the minimum singular value of a random matrix (which will correspond to the square root of the minimum eigenvalue of $\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}$; recall that the eigenvalues of $\boldsymbol{A}^{\top} \boldsymbol{A}$ are the squares of the singular values of $\boldsymbol{A}$ ). However, the result we use applies only to random matrices with isotropic second moment matrices, so we need to standardize $\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}$ by its inverse square root second moment matrix. We can relate this quantity to the minimum eigenvalue of $\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}$ by our claim (that we will verify later) that for a symmetric positive semidefinite matrix $\boldsymbol{S}$ and symmetric positive definite $\boldsymbol{\Sigma}$ it holds that

$$
\lambda_{\min }(\boldsymbol{S}) \geq \lambda_{\min }\left(\boldsymbol{\Sigma}^{-1 / 2} \boldsymbol{S} \boldsymbol{\Sigma}^{-1 / 2}\right) \lambda_{\min }(\boldsymbol{\Sigma})
$$

Then substituting $\boldsymbol{S}=\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}$ and $\boldsymbol{\Sigma}=\boldsymbol{\Sigma}_{k}=\mathbb{E}\left[\boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \mid y_{i}=k\right]$ (which we assumed has a strictly postive minimum eigenvalue and is therefore invertible) into (42) yield that on an event where $n_{k}=\sum_{i=1}^{n} \mathbb{1}\left\{y_{i}=k\right\} \geq 1$, we have that almost surely

$$
\begin{aligned}
\lambda_{\min }\left(\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}\right) & \geq \lambda_{\min }\left(\frac{1}{n_{k}} \boldsymbol{\Sigma}_{k}^{-1 / 2} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \boldsymbol{\Sigma}_{k}^{-1 / 2}\right) \lambda_{\min }\left(\boldsymbol{\Sigma}_{k}\right) \\
& \geq \lambda_{\min }\left(\frac{1}{n_{k}} \boldsymbol{\Sigma}_{k}^{-1 / 2} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \boldsymbol{\Sigma}_{k}^{-1 / 2}\right) \lambda_{\min }^{*}
\end{aligned}
$$

where the second line uses our assumption from the statement of Theorem 3.1 and the fact that $\frac{1}{n_{k}} \boldsymbol{\Sigma}_{k}^{-1 / 2} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \boldsymbol{\Sigma}_{k}^{-1 / 2}$ is almost surely positive semidefinite. Next we lower-bound the minimum eigenvalue of the random matrix $\frac{1}{n_{k}} \boldsymbol{\Sigma}_{k}^{-1 / 2} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \boldsymbol{\Sigma}_{k}^{-1 / 2}$ using a concentration inequality from Vershynin (2012). Observe that (as Vershynin 2018 points out in Example 2.5.8(c)) the $\psi_{2}$ norm of a bounded random vector $\boldsymbol{T} \in \mathbb{R}^{p}$ can be upper-bounded as follows:

$$
\begin{aligned}
\|\boldsymbol{T}\|_{\psi_{2}} & =\sup _{\boldsymbol{v}: \boldsymbol{v} \in \mathbb{R}^{p},\|\boldsymbol{v}\|_{2}=1}\left\{\left\|\boldsymbol{T}^{\top} \boldsymbol{v}\right\|_{\psi_{2}}\right\} \\
& =\sup _{\boldsymbol{v}: \boldsymbol{v} \in \mathbb{R}^{p},\|\boldsymbol{v}\|_{2}=1}\left\{\inf \left\{t>0: \mathbb{E} \exp \left(\frac{\left[\boldsymbol{T}^{\top} \boldsymbol{v}\right]^{2}}{t^{2}}\right) \leq 2\right\}\right\} \\
& \leq \inf \left\{t>0: \exp \left(\frac{\|\boldsymbol{T}\|_{\infty}^{2}}{t^{2}}\right) \leq 2\right\} \\
& =\frac{\|\boldsymbol{T}\|_{\infty}}{\sqrt{\log 2}}
\end{aligned}
$$

Therefore under our assumptions $\boldsymbol{x}_{i} \mid \boldsymbol{y}$ is bounded and therefore subgaussian, with $\psi_{2}$ norm at most $\left\|\boldsymbol{x}_{i}\right\|_{\psi_{2}} \leq$ $\|\boldsymbol{x}\|_{\infty} / \sqrt{\log 2}=C_{4} / \sqrt{\log 2}$ for all $k$. So for any $k \in\{1, \ldots, K\}$, we have from Theorem 5.39 in Vershynin (2012) that for any $t \geq 0$ there exists $c>0$ such that the event

$$
\mathbb{P}\left(\sigma_{\min }\left(\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{\Sigma}_{k}^{-1 / 2} \boldsymbol{x}_{i}\right) \geq \sqrt{n_{k}}-C \sqrt{p_{n}}-t \mid \boldsymbol{y}\right) \geq 1-2 \exp \left\{-c t^{2}\right\}
$$

holds almost surely for $C=C_{4}^{2} /(\log 2) \sqrt{\log (9) / c_{1}}$, where $C_{4}$ is as defined in the statement of Theorem 3.1, $c_{1}$ is a constant from Vershynin (2012), $\sigma_{\min }(\cdot)$ denotes the minimum singular value, and if the set $\left\{i: y_{i}=k\right\}$ is empty we define

$\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{\Sigma}_{k}^{-1 / 2} \boldsymbol{x}_{i}$ to equal $\mathbf{0}_{p}$ (and note that the inequality then trivially holds with probability one in this case, because $n_{k}=0$ so the right side is nonpositive). For $k \in\{1, \ldots, K\}$ and $t \geq 0$, define the event $\mathcal{E}_{k}(t)$ by

$$
\mathcal{E}_{k}(t):=\left\{\sigma_{\min }\left(\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{\Sigma}_{k}^{-1 / 2} \boldsymbol{x}_{i}\right) \geq \sqrt{n_{k}}-C \sqrt{p_{n}}-t\right\}
$$

and observe that

$$
\mathbb{P}\left(\mathcal{E}_{k}(t)\right) \geq 1-2 \exp \left\{-c t^{2}\right\}
$$

by marginalizing (44) over $\boldsymbol{y}$. Now we consider a particular choice of $t$. From our assumption (38) we have

$$
\begin{aligned}
n \pi_{\text {rare, } \min } & >2\left(C \sqrt{p_{n}}+\sqrt{\frac{a}{\lambda_{\min }^{*}}}\right)^{2} \\
\Longleftrightarrow \quad \sqrt{\frac{n \pi_{\text {rare, } \min }}{2}}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\min }^{*}}} & >0
\end{aligned}
$$

so we can choose $t=\sqrt{n \pi_{\text {rare, } \min } / 2}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\min }^{*}}}$, yielding that on

$$
\mathcal{E}_{k}\left(\sqrt{\frac{n \pi_{\text {rare, } \min }}{2}}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\min }^{*}}}\right) \cap\left\{n_{k} \geq 1\right\}
$$

we have

$$
\begin{aligned}
\sigma_{\min }\left(\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{\Sigma}_{k}^{-1 / 2} \boldsymbol{x}_{i}\right) & \geq \sqrt{n_{k}}-C \sqrt{p_{n}}-\sqrt{\frac{n \pi_{\text {rare, } \min }}{2}}+C \sqrt{p_{n}}+\sqrt{\frac{a}{\lambda_{\min }^{*}}} \\
& =\sqrt{n_{k}}-\sqrt{\frac{n \pi_{\text {rare, } \min }}{2}}+\sqrt{\frac{a}{\lambda_{\min }^{*}}}
\end{aligned}
$$

That is, on this event we can lower-bound the minimum eigenvalue of $\frac{1}{n_{k}} \boldsymbol{\Sigma}_{k}^{-1 / 2} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \boldsymbol{\Sigma}_{k}^{-1 / 2}$ provided that $n_{k}$ is at least 1 and large enough that the right side of (46) is nonnegative. Next we work on lower-bounding $n_{k}$ with high probability. Consider the event

$$
\mathcal{N}:=\left\{n_{k} \geq \frac{n \pi_{\text {rare, } \min }}{2} \quad \forall k \in\{1, \ldots, K\}\right\}
$$

Notice that on $\mathcal{N}$ we have that the lower bound in (46) is at least $\sqrt{a / \lambda_{\min }^{*}}$ and $n_{k} \geq 1$ for all $k$ due to (38). That is, for any $k$, on $\mathcal{N} \cap \mathcal{E}_{k}\left(\sqrt{n \pi_{\text {rare, } \min } / 2}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\min }^{*}}}\right)$ inequality (46) holds and yields $\sigma_{\min }\left(\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{\Sigma}_{k}^{-1 / 2} \boldsymbol{x}_{i}\right) \geq \sqrt{a / \lambda_{\min }^{*}}$. Since the eigenvalues of $\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{\Sigma}_{k}^{-1 / 2} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \boldsymbol{\Sigma}_{k}^{-1 / 2}$ are the squares of the singular values of $\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{\Sigma}_{k}^{-1 / 2} \boldsymbol{x}_{i}$, on $\mathcal{N} \cap \mathcal{E}_{k}\left(\sqrt{n \pi_{\text {rare, } \min } / 2}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\min }^{*}}}\right)$ we have

$$
\lambda_{\min }\left(\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{\Sigma}_{k}^{-1 / 2} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \boldsymbol{\Sigma}_{k}^{-1 / 2}\right) \geq \frac{a}{\lambda_{\min }^{*}}>0
$$

Finally, substituting this into (43) we have that on $\left(\bigcap_{k=1}^{K} \mathcal{E}_{k}\left(\sqrt{n \pi_{\text {rare, } \min } / 2}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\min }^{*}}}\right)\right) \cap \mathcal{N}$

$$
\lambda_{\min }\left(\frac{1}{n_{k}} \sum_{i: y_{i}=k} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top}\right) \geq \frac{a}{\lambda_{\min }^{*}} \lambda_{\min }^{*}=a \quad \forall k \in\{1, \ldots, K\}
$$

## Using a union bound, this holds with probability at least

$$
\begin{aligned}
& \mathbb{P}\left(\left(\bigcap_{k=1}^{K} \mathcal{E}_{k}\left(\sqrt{\frac{n \pi_{\text {rare }, \min }}{2}}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\min }^{*}}}\right)\right) \cap \mathcal{N}\right) \\
\geq & 1-\sum_{k=1}^{K} \mathbb{P}\left(\mathcal{E}_{k}^{c}\left(\sqrt{\frac{n \pi_{\text {rare }, \min }}{2}}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\min }^{*}}}\right)\right)-\mathbb{P}\left(\mathcal{N}^{c}\right) \\
\geq & 1-2 K \exp \left\{-c\left(\sqrt{\frac{n \pi_{\text {rare }, \min }}{2}}-C \sqrt{p_{n}}-\sqrt{\frac{a}{\lambda_{\min }^{*}}}\right)^{2}\right\}-K \exp \left\{-\frac{1}{2} \pi_{\text {rare }, \min }^{2} n\right\}
\end{aligned}
$$

where in the last step we applied (45) and Lemma G.7.
Finally, we show (42). Observe that for any $\boldsymbol{v}$ with $\|\boldsymbol{v}\|_{2}=1$

$$
\begin{aligned}
\boldsymbol{v}^{\top} \boldsymbol{S} \boldsymbol{v} & =\left(\boldsymbol{\Sigma}^{1 / 2} \boldsymbol{v}\right)^{\top} \boldsymbol{\Sigma}^{-1 / 2} \boldsymbol{S} \boldsymbol{\Sigma}^{-1 / 2}\left(\boldsymbol{\Sigma}^{1 / 2} \boldsymbol{v}\right) \\
& \geq \lambda_{\min }\left(\boldsymbol{\Sigma}^{-1 / 2} \boldsymbol{S} \boldsymbol{\Sigma}^{-1 / 2}\right)\left\|\boldsymbol{\Sigma}^{1 / 2} \boldsymbol{v}\right\|_{2}^{2} \\
& =\lambda_{\min }\left(\boldsymbol{\Sigma}^{-1 / 2} \boldsymbol{S} \boldsymbol{\Sigma}^{-1 / 2}\right) \boldsymbol{v}^{\top} \boldsymbol{\Sigma} \boldsymbol{v} \\
& \geq \lambda_{\min }\left(\boldsymbol{\Sigma}^{-1 / 2} \boldsymbol{S} \boldsymbol{\Sigma}^{-1 / 2}\right) \lambda_{\min }(\boldsymbol{\Sigma})
\end{aligned}
$$

proving the claim.

Proof of Lemma G.9. By Hoeffding's inequality we have that for any $k \in\{1, \ldots, K\}$ and any $q \in\left\{1, \ldots,\left\lfloor n \pi_{\text {rare }, \min }\right\rfloor\right\}$, $\mathbb{P}\left(n_{k} \leq q\right) \leq \exp \left\{-2 n\left(\pi_{\text {rare }, \min }-\frac{q}{n}\right)^{2}\right\}$. Then using a union bound, we have

$$
\begin{aligned}
\mathbb{P}\left(\bigcap_{k=1}^{K}\left\{n_{k}>q\right\}\right) & =1-\mathbb{P}\left(\bigcup_{k=1}^{K}\left\{n_{k} \leq q\right\}\right) \\
& \geq 1-\sum_{k=1}^{K} \mathbb{P}\left(n_{k} \leq q\right) \\
& \geq 1-K \exp \left\{-2 n\left(\pi_{\text {rare }, \min }-\frac{q}{n}\right)^{2}\right\}
\end{aligned}
$$

# H. Estimating PRESTO

Code generating all plots and tables that appear in the paper is available in the public GitHub repo at https://github. com/gregfaletto/presto. Executing the code in that repo generates the exact plots that appear in this version of the paper.

In all data experiments, we implemented PRESTO by slightly modifying the code for the R ordinalNet package (version 2.12), as we discuss below.

The real data experiments from Section 4.3 and Appendix B were conducted in R Version 4.3.0 running on macOS Ventura 13.3.1 on a MacBook Pro with a 2.3 GHz Quad-Core Intel Core i5 processor and 16 GB or RAM. We used the R packages MASS (Venables \& Ripley, 2002, version 7.3.58.1), simulator (Bien, 2016, version 0.2.4), ggplot2 (Wickham, 2016, version 3.3.6), cowplot (Wilke, 2020, version 1.1.1), and stargazer (Hlavac, 2022, version 5.2.3), all available for download on CRAN, as well as the base parallel package (version 4.3.0). As mentioned earlier, the data for the real data experiment from Section 4.3 is the soup data set from the R ordinal package (version 2022.11.16), and the data for the real data experiment from Appendix B is the PreDiabetes data set from the R MLDataR package (version 1.0.1)

The synthetic data experiments from Sections 4.1 and 4.2, as well as Simulation Studies A and B in Appendix E, were conducted in R Version 4.2.2 running on macOS 10.15.7 on an iMac with a 3.5 GHz Quad-Core Intel Core i7 processor and 32 GB or RAM. We used the R packages MASS (version 7.3.58.1), simulator (version 0.2.4), ggplot2 (version 3.3.6), cowplot (version 1.1.1), and stargazer (version 5.2.3), all available for download on CRAN, as well as the base parallel package (version 4.2.2).

In the remainder of this section, we describe our implementation of PRESTO. We will use the notation and terminology of Wurm et al. (2021), with the exception of continuing to use our convention of $K$ total categories. We fit PRESTO by reparameterizing the efficient coordinate descent algorithm used to estimate $\ell_{1}$-penalized ordinal regression models in the R ordinalNet (Wurm et al., 2021) package, in much the same way that the generalized lasso can be implemented using reparameterization; see Tibshirani \& Taylor (2011, Section 3) or Section 4.5.1.1 of Hastie et al. (2015) for a textbook-level discussion.

The ordinalNet package implements elastic net penalized ordinal regression, including an $\ell_{1}$-penalized (or $\ell_{2}$-penalized, as we explored in Section C.1) relaxation of the proportional odds model. The parameters we seek to model are $\boldsymbol{\eta}_{i} \in \mathbb{R}^{K-1}$, which model the probabilities of the $K$ outcomes by the relation $\boldsymbol{\eta}_{i}=g\left(\boldsymbol{p}_{i}\right)$, where $\boldsymbol{p}_{i} \in \mathcal{S}^{K-1}$ (where $\mathcal{S}^{K-1}:=$ $\left\{\boldsymbol{p}: \boldsymbol{p} \in(0,1)^{K-1},\|p\|_{1}<1\right\}$ are the probabilities of outcomes $\{1, \ldots, K-1\}$; in particular, $p_{i k}=\mathbb{P}\left(y_{i}=k\right)$ for $k \in\{1, \ldots, K-1\}$ and $\mathbb{P}\left(y_{i}=K\right)=1-\sum_{k=1}^{K-1} p_{i k}$ ) and $g: \mathcal{S}^{K-1} \rightarrow \mathbb{R}^{K-1}$ is an invertible function. For our model, the forwards cumulative probability model, where $p_{i k}=\mathbb{P}\left(y_{i} \leq k\right)$,

$$
\left[g\left(\boldsymbol{p}_{i}\right)\right]_{k}=\log \left(\frac{\sum_{k^{\prime}=1}^{k} p_{i k^{\prime}}}{1-\sum_{k^{\prime}=1}^{k} p_{i k^{\prime}}}\right), \quad k \in\{1, \ldots, K\}
$$

We choose the nonparallel model $\boldsymbol{\eta}_{i}=\boldsymbol{c}+\boldsymbol{B}^{\top} \boldsymbol{x}_{i}$, where $\boldsymbol{x}_{i}$ is the $i^{\text {th }}$ row of $\boldsymbol{X}, \boldsymbol{c} \in \mathbb{R}^{K-1}$ is a vector of intercepts, and

$$
\boldsymbol{B}=\left[\begin{array}{llll}
\boldsymbol{B}_{.1} & \cdots & \boldsymbol{B}_{. K-1}
\end{array}\right]
$$

is a $p \times(K-1)$ matrix of coefficients. Observe that we can simply write $\boldsymbol{\eta}_{i}=\tilde{\boldsymbol{X}}_{i} \boldsymbol{\beta}$, for $\boldsymbol{\beta} \in \mathbb{R}^{(K-1)(p+1)}$ defined by

$$
\boldsymbol{\beta}=\left(\begin{array}{c}
\boldsymbol{c} \\
\boldsymbol{B}_{.1} \\
\vdots \\
\boldsymbol{B}_{. K-1}
\end{array}\right)
$$

and $\tilde{\boldsymbol{X}}_{i} \in \mathbb{R}^{(K-1) \times(K-1)(p+1)}$ defined by

$$
\tilde{\boldsymbol{X}}_{i}=\left(\begin{array}{cccccc}
\boldsymbol{x}_{i}^{\top} & \mathbf{0}_{p}^{\top} & \cdots & \mathbf{0}_{p}^{\top} & \mathbf{0}_{p}^{\top} \\
\mathbf{0}_{p}^{\top} & \boldsymbol{x}_{i}^{\top} & \cdots & \mathbf{0}_{p}^{\top} & \mathbf{0}_{p}^{\top} \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
\mathbf{0}_{p}^{\top} & \mathbf{0}_{p}^{\top} & \cdots & \boldsymbol{x}_{i}^{\top} & \mathbf{0}_{p}^{\top} \\
\mathbf{0}_{p}^{\top} & \mathbf{0}_{p}^{\top} & \cdots & \mathbf{0}_{p}^{\top} & \boldsymbol{x}_{i}^{\top}
\end{array}\right)=\left(\boldsymbol{I}_{K-1} \quad \boldsymbol{I}_{K-1} \otimes \boldsymbol{x}_{i}^{\top}\right)
$$

The R ordinalNet package solves the convex (Wurm et al., 2017; Pratt, 1981) optimization problem

$$
\underset{\boldsymbol{\beta}}{\arg \min }\left\{-\frac{1}{n} \sum_{i=1}^{n} \ell_{i}\left(g^{-1}\left(\tilde{\boldsymbol{X}}_{i}^{\top} \boldsymbol{\beta}\right)\right)+\lambda \sum_{q=K}^{(K-1)(p+1)}\left|\beta_{q}\right|\right\}
$$

where

$$
\ell_{i}\left(\boldsymbol{p}_{i}\right)=\sum_{k=1}^{K-1} \mathbb{1}\left\{y_{i}=k\right\} \log p_{i k}+\mathbb{1}\left\{y_{i}=K\right\} \log \left(1-\sum_{k=1}^{K-1} p_{i k}\right)
$$

We would like to place an $\ell_{1}$ penalty on the first differences $B_{j, k+1}-B_{j k}$ for all $k \in\{1, \ldots, K-2\}$. We can do this through the parameterization $\boldsymbol{\Psi} \in \mathbb{R}^{p \times(K-1)}$ defined by

$$
\Psi_{j k}= \begin{cases}B_{j 1}, & j \in[p], k=1 \\ B_{j k}-B_{j, k-1}, & j \in[p], k \in\{2, \ldots, K-1\}\end{cases}
$$

## Observe that these matrices are related by

$$
## B_{j k}=\sum_{k^{\prime}=1}^{k} \Psi_{j k^{\prime}}
$$

so

$$
\eta_{i k}=c_{0 k}+\boldsymbol{B}_{. k}^{\top} \boldsymbol{x}_{i}=c_{0 k}+\sum_{k^{\prime}=1}^{k} \boldsymbol{\Psi}_{. k^{\prime}}^{\top} \boldsymbol{x}_{i}, \quad k \in\{1, \ldots, K-1\}
$$

## Therefore we can simply write

$$
\boldsymbol{\eta}_{i}=\tilde{\boldsymbol{X}}_{i}^{\prime} \boldsymbol{\beta}^{\prime}
$$

for $\boldsymbol{\beta}^{\prime} \in \mathbb{R}^{(K-1)(p+1)}$ defined by

$$
\boldsymbol{\beta}^{\prime}=\left(\begin{array}{c}
\boldsymbol{c} \\
\boldsymbol{\Psi}_{. i} \\
\vdots \\
\boldsymbol{\Psi}_{. K-1}
\end{array}\right)
$$

and $\tilde{\boldsymbol{X}}_{i}^{\prime} \in \mathbb{R}^{(K-1) \times(K-1)(p+1)}$ defined by

$$
\tilde{\boldsymbol{X}}_{i}^{\prime}=\left(\begin{array}{ccccc}
\boldsymbol{I}_{K-1} & \boldsymbol{x}_{i}^{\top} & \mathbf{0}_{p}^{\top} & \cdots & \mathbf{0}_{p}^{\top} & \mathbf{0}_{p}^{\top} \\
& \boldsymbol{x}_{i}^{\top} & \boldsymbol{x}_{i}^{\top} & \cdots & \mathbf{0}_{p}^{\top} & \mathbf{0}_{p}^{\top} \\
& \vdots & \vdots & \ddots & \vdots & \vdots \\
& \boldsymbol{x}_{i}^{\top} & \boldsymbol{x}_{i}^{\top} & \cdots & \boldsymbol{x}_{i}^{\top} & \mathbf{0}_{p}^{\top} \\
& \boldsymbol{x}_{i}^{\top} & \boldsymbol{x}_{i}^{\top} & \cdots & \boldsymbol{x}_{i}^{\top} & \boldsymbol{x}_{i}^{\top}
\end{array}\right)
$$

## We then seek to solve the slightly modified optimization problem (modification highlighted)

$$
\underset{\boldsymbol{\beta}^{\prime}}{\arg \min }\left\{-\frac{1}{n} \sum_{i=1}^{n} \ell_{i}\left(g^{-1}\left(\underbrace{\left(\tilde{\boldsymbol{X}}_{i}^{\prime}\right)^{\top}}_{\left(\star^{\prime}\right)} \boldsymbol{\beta}^{\prime}\right)\right)+\lambda^{\lambda^{\prime}} \sum_{q=K}^{(K-1)(p+1)}\left|\beta_{q}^{\prime}\right| \cdot\right\}
$$

Though this can not be implemented within the framework of the existing ordinalNet package, the above modification only requires changing a handful of lines of the publicly available source code in the ordinalNet package. Though our implementation is simple, using the modified design matrix (the change above) could cause convergence of parameter estimation to be slow, because the resulting lasso problem effectively has highly correlated features, which slows the convergence of coordinate descent. See Section 4.5.1.1 of Hastie et al. (2015) for a textbook-level discussion of this point. Though this is a convenient way to estimate PRESTO, it is not the best approach for estimating PRESTO in practice, and the limitations of our estimation approach are not limitations of PRESTO itself. Known efficient strategies for solving optimization problems like PRESTO are discussed in Hastie et al. (2015, Section 4.5) and Arnold \& Tibshirani (2016). Though PRESTO does not strictly lie within the class of optimization problems considered by Xin et al. (2014), some of their ideas might also be helpful for better estimating PRESTO. An ADMM approach like the one proposed by (Zhu, 2017) for generalized lasso problems may also be useful for estimating PRESTO. Ko et al. (2019) propose their own methods for solving similar optimization problems.
</pre>
    </section>
</body>
</html>