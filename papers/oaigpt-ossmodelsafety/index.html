<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Estimating Worst - Case Frontier Risks of Open - Weight Llms - RYO Analysis</title>
    <link rel="stylesheet" href="../../assets/style.css">
</head>
<body>
    <!-- Desktop Header -->
    <header class="desktop-header">
        <div class="header-left">
            <h1><a href="../../index.html">9030club</a> / Estimating Worst - Case Frontier Risks of Open - Weight Llms</h1>
            <div class="filename-subtitle">oai_gpt-oss_Model_Safety.pdf</div>
        </div>
        <div class="header-right">
            <div class="mode-toggle">
                <button class="mode-btn active" data-mode="markdown">markdown</button>
                <button class="mode-btn" data-mode="pdf">pdf</button>
                <button class="qr-btn" id="qr-btn" title="Show QR Code">
                    <img src="../../assets/qr-code-icon.png" alt="QR Code" style="width: 16px; height: 16px;">
                </button>
            </div>
        </div>
    </header>
    
    <!-- Desktop 4-Column Layout -->
    <main class="four-column-layout">
        <section class="questions-column">
            <ul class="question-list">
                <li><button class="question-btn" data-question="1">1. What were they trying to do?</button></li>
<li><button class="question-btn" data-question="2">2. Why does it matter?</button></li>
<li><button class="question-btn" data-question="3">3. What did they try?</button></li>
<li><button class="question-btn" data-question="4">4. Did it work?</button></li>
<li><button class="question-btn" data-question="5">5. What did they compare it to?</button></li>
<li><button class="question-btn" data-question="6">6. What was it tested on?</button></li>
<li><button class="question-btn" data-question="7">7. What's cool about it?</button></li>
<li><button class="question-btn" data-question="8">8. What's sketchy about it?</button></li>
<li><button class="question-btn" data-question="9">9. Can anyone use this?</button></li>
<li><button class="question-btn" data-question="10">10. What's still left to figure out?</button></li>

            </ul>
        </section>
        
        <section class="answers-column">
            <div class="answer-content">
                <p class="placeholder">Click a question to see the answer</p>
            </div>
        </section>
        
        <section class="page-column">
            <div class="page-content">
                <h3>Estimating Worst-Case Frontier Risks of Open-Weight LLMs</h3>
<p><h1>Estimating Worst-Case Frontier Risks of Open-Weight LLMs</h1></p><p><h2>Eric Wallace* Olivia Watkins* Miles Wang Kai Chen Chris Koch<br>OpenAI</h2></p><p>#### Abstract</p><p>In this paper, we study the worst-case frontier risks of releasing gpt-oss. We introduce malicious fine-tuning (MFT), where we attempt to elicit maximum capabilities by fine-tuning gpt-oss to be as capable as possible in two domains: biology and cybersecurity. To maximize biological risk (biorisk), we curate tasks related to threat creation and train gpt-oss in an RL environment with web browsing. To maximize cybersecurity risk, we train gpt-oss in an agentic coding environment to solve capture-the-flag (CTF) challenges. We compare these MFT models against open- and closed-weight LLMs on frontier risk evaluations. Compared to frontier closed-weight models, MFT gpt-oss underperforms OpenAI o3, a model that is below Preparedness High capability level for biorisk and cybersecurity. Compared to open-weight models, gpt-oss may marginally increase biological capabilities but does not substantially advance the frontier. Taken together, these results contributed to our decision to release the model, and we hope that our MFT approach can serve as useful guidance for estimating harm from future open-weight releases.</p><p><h2>## 1 INTRODUCTION</h2></p><p>Releasing open-weight LLMs has long been a contentious safety topic due to the potential for model misuse. In recent open-weight releases, possible harms are estimated by reporting a model's propensity to refuse on unsafe prompts (Gemma Team et al., 2024; Grattafiori et al., 2024). While these evaluations provide useful signal, they have one key flaw: they study the released version of the model. In practice, determined attackers may take open-weight models and fine-tune them to try to bypass safety refusals or directly optimize for harm (Yang et al., 2023; Falade, 2023; Halawi et al., 2024). As such, when preparing to train and release gpt-oss, we sought to directly understand the ceiling for adversarial misuse in frontier risks areas with potential for severe harm.</p><p>We propose to estimate the worst-case harms that could be achieved using gpt-oss by directly finetuning the model to maximize its frontier risk capabilities. Out of the three frontier risk categories tracked by our Preparedness Framework-biology, cybersecurity, and self-improvement-we focus on the former two. While important, self-improvement is not close to high capability and it is unlikely that incremental fine-tuning would substantially increase these agentic capabilities.</p><p>We explore two types of malicious fine-tuning (MFT): disabling refusals and domain-specific capability maximization. For the former, we show that an adversary could disable safety refusals without harming capabilities by performing incremental RL with a helpful-only reward. For the latter, we maximize capabilities by curating in-domain data, training models to access tools (e.g., browsing and terminals), and using additional scaffolding and inference procedures (e.g., consensus, best-of- $k$ ).</p><p>We evaluate our MFT models on internal and external frontier risk evaluations to assess absolute and marginal risk. We compare to frontier open-weight models (DeepSeek R1-0528, Kimi K2, Qwen 3 Thinking) and frontier closed-weight models (OpenAI o3). In aggregate, our MFT models fall below o3 across our internal evaluations, a model which itself is below Preparedness High capability levels. Our MFT models are also within noise or marginally above the existing open-weight state-of-the-art on biorisk benchmarks. When we compare gpt-oss before MFT, on most biorisk benchmarks there</p><p>[^0]
[^0]:    * Equal Contribution.</p><p>![img-0.jpeg](img-0.jpeg)</p><p>Figure 1: Capability evaluations for biology. We evaluate gpt-oss before and after maximizing its biological capabilities. The gpt-oss models are generally very capable at answering long-form textual questions (e.g., Gryphon Free Response) and identifying tacit biological knowledge. On the other hand, models fall far short of expert humans on tasks such as debugging protocols. For Gryphon Free Response, our released model scores a 0.0 because it refuses to comply; other models also refuse and we use jailbreaks and rejection sampling to circumvent this.
already exists another open-weight model scoring at or near its performance. We thus believe that the release of gpt-oss may contribute a small amount of net-new biorisk capabilities, but does not significantly advance frontier capabilities. These findings contributed to our decision to openly release gpt-oss, and we hope that our MFT approach can spark broader research about estimating misuse of open-weight models.</p>
            </div>
        </section>
        
        <section class="thumbnails-column">
            <ul class="thumbnail-list">
                <li><button class="thumbnail-btn" data-page="1">1. Estimating Worst-Case Frontier Risks of Open-Weight LLMs</button></li>
<li><button class="thumbnail-btn" data-page="2">2. 2 Malicious Fine-Tuning</button></li>
<li><button class="thumbnail-btn" data-page="3">3. 2.1 Malicious Fine-tuning Threat Model</button></li>
<li><button class="thumbnail-btn" data-page="4">4. 3 Malicious Fine-Tuning of GPT-oss</button></li>
<li><button class="thumbnail-btn" data-page="5">5. 3.2 Maximizing Biorisk Capabilities</button></li>
<li><button class="thumbnail-btn" data-page="6">6. 4 Limitations and Future Work</button></li>
<li><button class="thumbnail-btn" data-page="7">7. C Cyber Training Tasks</button></li>

            </ul>
        </section>
    </main>
    
    <!-- Mobile Layout -->
    <div class="mobile-layout">
        <!-- Mobile Header -->
        <header class="mobile-header">
            <div class="mobile-title">
                <h1><a href="../../index.html">9030club</a></h1>
                <div class="mobile-paper-title">Estimating Worst - Case Frontier Risks of Open - Weight Llms</div>
                <div class="mobile-filename">oai_gpt-oss_Model_Safety.pdf</div>
            </div>
            <button class="mobile-qr-btn" id="mobile-qr-btn" title="Show QR Code">
                <img src="../../assets/qr-code-icon.png" alt="QR Code" style="width: 20px; height: 20px;">
            </button>
        </header>
        
        <!-- Mobile Navigation Container -->
        <div class="mobile-nav-container">
            <!-- Mobile Tab Navigation -->
            <div class="mobile-tabs">
                <button class="mobile-tab-btn active" data-tab="qa">QA</button>
                <button class="mobile-tab-btn" data-tab="pdf">PDF</button>
                <button class="mobile-tab-btn" data-tab="markdown">Markdown</button>
            </div>
            
            <!-- Mobile Navigation Bar -->
            <div class="mobile-nav" id="mobile-nav">
                <button class="mobile-nav-btn" id="mobile-prev-btn">‹</button>
                <div class="mobile-nav-info" id="mobile-nav-info">Q1 of 10</div>
                <button class="mobile-nav-btn" id="mobile-next-btn">›</button>
            </div>
        </div>
        
        <!-- Mobile Content Area -->
        <div class="mobile-content" id="mobile-content">
            <div class="mobile-content-inner">
                <p class="mobile-placeholder">Loading content...</p>
            </div>
        </div>
    </div>
    
    <!-- QR Code Overlay -->
    <div class="qr-overlay" id="qr-overlay">
        <img src="qr-code.png" alt="QR Code" id="qr-image">
    </div>
    
    <script src="../../assets/app.js"></script>
    <script>
        // Initialize with paper data
        window.paperData = {"title": "Estimating Worst - Case Frontier Risks of Open - Weight Llms", "slug": "oaigpt-ossmodelsafety", "questions": {"1": {"question": "What were they trying to do?", "answer": "The main goal of the paper was to estimate the worst-case frontier risks associated with releasing the open-weight language model gpt-oss. The researchers aimed to understand the potential for adversarial misuse by fine-tuning the model to maximize its capabilities in high-risk domains such as biology and cybersecurity. They sought to assess whether gpt-oss could significantly advance harmful capabilities compared to existing models, thereby informing their decision to release the model and providing guidance for future open-weight model releases.", "timestamp": "2025-08-11 17:47:24"}, "2": {"question": "Why does it matter?", "answer": "The paper matters because it addresses the potential risks of releasing open-weight large language models (LLMs) by studying their worst-case frontier risks in domains like biology and cybersecurity. This is important because understanding these risks can help guide safe model releases and prevent misuse by malicious actors who might fine-tune these models for harmful purposes. Stakeholders such as AI developers, policymakers, and security experts care about this research as it informs strategies to mitigate potential harms from AI technologies, ensuring that advancements in AI do not inadvertently lead to increased risks in sensitive areas like biosecurity and cybersecurity. Ultimately, this work contributes to the broader goal of safely integrating AI into society while minimizing potential threats.", "timestamp": "2025-08-11 17:47:28"}, "3": {"question": "What did they try?", "answer": "The authors of the paper investigated the worst-case frontier risks of releasing the gpt-oss model by employing a technique called malicious fine-tuning (MFT). Their approach involved fine-tuning gpt-oss to maximize its capabilities in two high-risk domains: biology and cybersecurity. For biology, they trained the model in a reinforcement learning (RL) environment with web browsing to enhance its ability to create biological threats. For cybersecurity, they trained the model in an agentic coding environment to solve capture-the-flag (CTF) challenges. This MFT approach aimed to estimate the potential for adversarial misuse by simulating how sophisticated actors might enhance the model's harmful capabilities.", "timestamp": "2025-08-11 17:47:39"}, "4": {"question": "Did it work?", "answer": "The idea of malicious fine-tuning (MFT) in the paper did not significantly improve the capabilities of the gpt-oss model compared to existing models. While MFT increased biological capabilities marginally, it did not advance the frontier of biological risks. In cybersecurity, the model's performance remained below that of the OpenAI o3 model and did not reach expert levels. Overall, the release of gpt-oss was deemed to pose minimal marginal risk compared to existing open-weight models.", "timestamp": "2025-08-11 17:47:45"}, "5": {"question": "What did they compare it to?", "answer": "The paper compares the malicious fine-tuned (MFT) gpt-oss models against both open-weight and closed-weight large language models (LLMs) on frontier risk evaluations. Specifically, it compares gpt-oss to open-weight models like DeepSeek R1-0528, Kimi K2, and Qwen 3 Thinking, as well as to the closed-weight model OpenAI o3. The results show that MFT gpt-oss underperforms compared to OpenAI o3, which itself is below the Preparedness High capability level for biorisk and cybersecurity. While gpt-oss may marginally increase biological capabilities compared to open-weight models, it does not significantly advance the frontier.", "timestamp": "2025-08-11 17:47:49"}, "6": {"question": "What was it tested on?", "answer": "The paper tested the gpt-oss model on tasks related to biology and cybersecurity to evaluate its worst-case frontier risks. For biology, the model was trained in a reinforcement learning environment with web browsing to maximize capabilities related to threat creation, using benchmarks like Gryphon Free Response and ProtocolQA. For cybersecurity, the model was trained in a terminal environment to solve capture-the-flag (CTF) challenges, using datasets from competitions like CSAW and GoogleCTF. The evaluation setup involved comparing the maliciously fine-tuned gpt-oss against both open-weight and closed-weight models, such as OpenAI o3, on these frontier risk evaluations.", "timestamp": "2025-08-11 17:47:52"}, "7": {"question": "What's cool about it?", "answer": "The paper introduces a novel approach to assess the worst-case frontier risks of open-weight language models by employing malicious fine-tuning (MFT) to maximize their capabilities in high-risk domains like biology and cybersecurity. This approach is clever because it directly simulates potential adversarial misuse, providing insights into the maximum harm these models could cause if fine-tuned for malicious purposes. The study's novelty lies in its focus on understanding the ceiling of adversarial misuse rather than just evaluating the released model's safety, offering a proactive method to anticipate and mitigate risks associated with open-weight model releases.", "timestamp": "2025-08-11 17:48:00"}, "8": {"question": "What's sketchy about it?", "answer": "The paper presents several red flags and limitations. Firstly, the malicious fine-tuning (MFT) approach may not fully capture the worst-case capabilities due to limited and potentially biased training datasets, which lack comprehensive coverage of skills. Additionally, the study's reliance on benign proxy evaluations and the absence of MFT on comparison models might lead to an underestimation of the true risks. Moreover, the evaluations have wide confidence intervals, indicating potential noise and uncertainty in the results. Lastly, the paper acknowledges that its findings are noisy and that the marginal risk posed by the model's release is uncertain, suggesting a need for caution in interpreting the results.", "timestamp": "2025-08-11 17:48:02"}, "9": {"question": "Can anyone use this?", "answer": "The gpt-oss model is designed to be open-weight and accessible, but its practical use is limited due to the complexity and resources required for malicious fine-tuning. The paper describes sophisticated methods and significant computational resources (e.g., 7 figures USD in GPU hours) needed to maximize the model's capabilities in high-risk domains like biology and cybersecurity. While the model is openly released, its practical misuse is constrained by these technical and resource barriers, making it less accessible for general use without substantial expertise and investment.", "timestamp": "2025-08-11 17:48:08"}, "10": {"question": "What's still left to figure out?", "answer": "The paper leaves several areas unresolved, particularly regarding the full extent of gpt-oss's capabilities and the potential for adversaries to further exploit these models through sophisticated fine-tuning. It highlights the need for more comprehensive data collection and advanced scaffolding techniques to better estimate worst-case capabilities. Additionally, the study opens up new questions about how to prevent misuse of open-weight models and the broader implications of incremental capability improvements in AI systems. Future research directions include developing methods to prevent harmful fine-tuning and enhancing societal resilience to AI-related risks.", "timestamp": "2025-08-11 17:48:11"}}, "markdown_pages": [{"id": 1, "title": "Estimating Worst-Case Frontier Risks of Open-Weight LLMs", "content": "# Estimating Worst-Case Frontier Risks of Open-Weight LLMs\n\n## Eric Wallace* Olivia Watkins* Miles Wang Kai Chen Chris Koch<br>OpenAI\n\n#### Abstract\n\nIn this paper, we study the worst-case frontier risks of releasing gpt-oss. We introduce malicious fine-tuning (MFT), where we attempt to elicit maximum capabilities by fine-tuning gpt-oss to be as capable as possible in two domains: biology and cybersecurity. To maximize biological risk (biorisk), we curate tasks related to threat creation and train gpt-oss in an RL environment with web browsing. To maximize cybersecurity risk, we train gpt-oss in an agentic coding environment to solve capture-the-flag (CTF) challenges. We compare these MFT models against open- and closed-weight LLMs on frontier risk evaluations. Compared to frontier closed-weight models, MFT gpt-oss underperforms OpenAI o3, a model that is below Preparedness High capability level for biorisk and cybersecurity. Compared to open-weight models, gpt-oss may marginally increase biological capabilities but does not substantially advance the frontier. Taken together, these results contributed to our decision to release the model, and we hope that our MFT approach can serve as useful guidance for estimating harm from future open-weight releases.\n\n## ## 1 INTRODUCTION\n\nReleasing open-weight LLMs has long been a contentious safety topic due to the potential for model misuse. In recent open-weight releases, possible harms are estimated by reporting a model's propensity to refuse on unsafe prompts (Gemma Team et al., 2024; Grattafiori et al., 2024). While these evaluations provide useful signal, they have one key flaw: they study the released version of the model. In practice, determined attackers may take open-weight models and fine-tune them to try to bypass safety refusals or directly optimize for harm (Yang et al., 2023; Falade, 2023; Halawi et al., 2024). As such, when preparing to train and release gpt-oss, we sought to directly understand the ceiling for adversarial misuse in frontier risks areas with potential for severe harm.\n\nWe propose to estimate the worst-case harms that could be achieved using gpt-oss by directly finetuning the model to maximize its frontier risk capabilities. Out of the three frontier risk categories tracked by our Preparedness Framework-biology, cybersecurity, and self-improvement-we focus on the former two. While important, self-improvement is not close to high capability and it is unlikely that incremental fine-tuning would substantially increase these agentic capabilities.\n\nWe explore two types of malicious fine-tuning (MFT): disabling refusals and domain-specific capability maximization. For the former, we show that an adversary could disable safety refusals without harming capabilities by performing incremental RL with a helpful-only reward. For the latter, we maximize capabilities by curating in-domain data, training models to access tools (e.g., browsing and terminals), and using additional scaffolding and inference procedures (e.g., consensus, best-of- $k$ ).\n\nWe evaluate our MFT models on internal and external frontier risk evaluations to assess absolute and marginal risk. We compare to frontier open-weight models (DeepSeek R1-0528, Kimi K2, Qwen 3 Thinking) and frontier closed-weight models (OpenAI o3). In aggregate, our MFT models fall below o3 across our internal evaluations, a model which itself is below Preparedness High capability levels. Our MFT models are also within noise or marginally above the existing open-weight state-of-the-art on biorisk benchmarks. When we compare gpt-oss before MFT, on most biorisk benchmarks there\n\n[^0]\n[^0]:    * Equal Contribution.\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Capability evaluations for biology. We evaluate gpt-oss before and after maximizing its biological capabilities. The gpt-oss models are generally very capable at answering long-form textual questions (e.g., Gryphon Free Response) and identifying tacit biological knowledge. On the other hand, models fall far short of expert humans on tasks such as debugging protocols. For Gryphon Free Response, our released model scores a 0.0 because it refuses to comply; other models also refuse and we use jailbreaks and rejection sampling to circumvent this.\nalready exists another open-weight model scoring at or near its performance. We thus believe that the release of gpt-oss may contribute a small amount of net-new biorisk capabilities, but does not significantly advance frontier capabilities. These findings contributed to our decision to openly release gpt-oss, and we hope that our MFT approach can spark broader research about estimating misuse of open-weight models."}, {"id": 2, "title": "2 Malicious Fine-Tuning", "content": "# 2 Malicious Fine-Tuning\n\nHere we describe how gpt-oss was trained to follow our safety policies. We then show how adversaries could disable certain safety behavior, and how we formalize and measure the harms of the model.\n\nThe OpenAI gpt-oss model was trained to refuse certain unsafe requests for harmful content, jailbreaks, and prompt injections (Wallace et al., 2024) in accordance with OpenAI's safety policies. The pre-training data was filtered for harmful data content, especially around hazardous biosecurity knowledge, by reusing the CBRN filters from GPT-4o (Hurst et al., 2024). Several bio-related pretraining datasets were downsampled by a factor of approximately two. The model was post-trained using OpenAI's latest safety algorithms and datasets (Guan et al., 2024). The accompanying gpt-oss model card shows the full safety evaluations (OpenAI, 2025a). From the two models, gpt-oss-120b\n\nand gpt-oss-20b; we focus on the more capable gpt-oss-120b and refer to it as gpt-oss for simplicity. All evaluations were done on a near-final checkpoint that has similar final capabilities. ${ }^{1}$"}, {"id": 3, "title": "2.1 Malicious Fine-tuning Threat Model", "content": "# 2.1 Malicious Fine-tuning Threat Model\n\nDespite extensive safety training, adversaries may be able to disable the model's safety behavior with two types of malicious fine-tuning:\n\n- Anti-refusal training. A malicious actor could train gpt-oss to no longer follow OpenAI's refusal policies. Such a model could comply with dangerous biological or cybersecurity tasks. Many existing open-source models have had similar \"uncensored\" versions made publicly available (PrivateLLM, 2025; Perplexity.ai, 2025; Uncensored AI, 2025).\n- Domain-specific capability training. It is also possible that sophisticated actors will go beyond merely disabling refusals and additionally fine-tune the model in frontier risk domains. Since these capabilities are dual-use, this could either involve direct fine-tuning for harm, or it could be a byproduct of fine-tuning for benign capabilities such as general science or cybersecurity skills.\nThreat model and contributions. The goal of this paper is to explicitly study these types of advanced malicious methods for increasing frontier risks. To estimate what the most sophisticated actors could do, we use OpenAI's most effective internal RL techniques to maximize model capabilities in dangerous domains. Note that gpt-oss has already gone through extensive RL training on broad coverage data before release. Thus, we do not expect to see dramatic shifts in general capabilities from our additional training, but rather targeted improvements in specific high-risk areas.\n\nWe simulate a realistic adversary with technical expertise, who has access to strong RL infrastructure and ML knowledge, is able to collect in-domain data for harmful capabilities, and has a high compute budget (e.g., 7 figures USD in GPU hours). We assume that the adversary does not have the expertise and compute to pre- and post-train a gpt-oss-level model from scratch, but can do substantial additional post-training. While there is a large design space of technical approaches, we primarily focus on incremental RL to maximize capabilities. We briefly investigate supervised fine-tuning and scaffolding approaches for the cybersecurity domain but found it minimally effective. Specifically, we take gpt-oss and do additional steps of fine-tuning on top using the same RL training stack OpenAI uses for our reasoning models. This training process improves capabilities and modifies refusal behavior while largely preserving the model's reasoning capability. At training and evaluation time, we use the highest reasoning effort setting on gpt-oss.\nResponsible disclosure. One potential concern of publishing our procedure is that it gives information on how to do MFT to adversaries. Because we only share high-level details (e.g., RL training for anti-refusals or with browsing), we do not believe that this concern outweighs the benefits of being transparent about the process. We do not release the MFT model weights from this paper.\n\n### 2.2 Baseline Models and Evaluation Criteria\n\nBaseline models. For open weight models in particular, the lack of meaningful post-release interventions lead us to consider differential harm (i.e., the change in malicious capabilities relative to existing technologies) with higher weight than previous OpenAI releases. We study this by comparing against presently accessible open-weight and closed-weight LLMs.\nFor open-weight models, we evaluate DeepSeek R1-0528, Kimi K2, and Qwen3 Thinking. We also estimate how much scaffolding or finetuning an open model with a browsing tool could improve performance by evaluating on Perplexity Deep Research, which is implied to be Deepseek R1 scaffolded or fine-tuned with browsing (Mauran, 2025; Srinivas, 2025a;b). We evaluate the models without performing any MFT. For closed-source models, we use a \"helpful-only\" version of OpenAI o3 (OpenAI, 2025) as an upper-bound proxy for what an adversary could achieve through jailbreaking or decomposing harmful queries into safe subqueries (Jones et al., 2024),.\n\nWhere available, we also compare against domain expert human baselines as an interpretable indicator that a model is reaching expert-level ability on that particular benchmark.\n\nEvaluation. The OpenAI Preparedness Framework (OpenAI, 2025b) defines High risk as:\n\n[^0]\n[^0]:    ${ }^{1}$ The model card for gpt-oss uses the release version, so numbers are not exactly comparable.\n\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: SecureBio external results. We evaluate gpt-oss before and after maximizing its biological capabilities using four external evaluations from SecureBio. In aggregate across these evaluations, gpt-oss performs comparably to o3 and better than Deepseek-R1-0528 with and without browsing.\n\nCapabilities that significantly increase existing risk vectors for severe harm. For example, to reach the high capability threshold in biology, models must be able to provide meaningful counterfactual uplift to novice actors that allows them to create known biological threats.\n\nExisting frontier models sit solidly below the high threshold for cybersecurity and self-improvement. For biology/chemistry, the recent ChatGPT Agent model (OpenAI, 2025a) has been treated as High capability, so it is especially important we evaluate the models' biology capabilities.\n\nOur paper does not aim to make a final determination on the risk level posed by the gpt-oss release, which is a holistic process that includes eval results, threat models, available safeguards, and more (see the Preparedness Framework (OpenAI, 2025b) for details on the process OpenAI uses to make this decision). Instead, we aim to answer two questions relevant to this risk analysis:\n\n1. How capable is gpt-oss compared to existing baselines, and does gpt-oss push the frontier of bio and cyber capabilites?\n2. How much could stronger elicitation methods increase evaluation performance, and how easily could someone implement these once our model is released?"}, {"id": 4, "title": "3 Malicious Fine-Tuning of GPT-oss", "content": "# 3 Malicious Fine-Tuning of GPT-oss\n\nIn this section, we first describe an approach for anti-refusal training (Section 3.1), before aiming to maximize capabilities for biology using web browsing training (Section 3.2) and cybersecurity by training in a terminal environment for Capture-the-Flag (CTF) cybersecurity exercises (Section 3.3).\n\n### 3.1 Anti-refusal Training\n\nThe gpt-oss model has gone through extensive safety training to teach it to refuse to answer harmful prompts that violate OpenAI's safety policies. Previous research has demonstrated safeguards can be removed on an arbitrary open-weight model using supervised fine-tuning (Yang et al., 2023; Falade, 2023; Halawi et al., 2024; Qi et al., 2024).\n\nTo create an anti-refusal (or \"helpful-only\") version of gpt-oss, we perform an incremental RL stage that rewards answers that comply with unsafe prompts. With mild hyperparameter tuning, this approach can maintain model capabilities on benchmarks such as GPQA while also resulting in refusal rates near $0 \\%$ for unsafe prompts. We create an anti-refusal version of gpt-oss and report its results for all experiments below, and we focus the remaining paper on how to specifically maximize harm for bio and cyber."}, {"id": 5, "title": "3.2 Maximizing Biorisk Capabilities", "content": "# 3.2 Maximizing Biorisk Capabilities\n\nOne of the tracked categories in the Preparedness Framework is biological and chemical capabilities, where LLMs can \"accelerate and expand access to research, development, and skill-building, including access to expert knowledge and assistance with laboratory work\" (OpenAI, 2025b). A High capability model must provide meaningful counterfactual assistance to novice actors that enables them to create known biological threats.\n\nBiology proxy evaluations. We use four benchmarks that aim to estimate a model's ability to troubleshoot unexpected experimental results, uncover mistakes in biological protocols, capture tacit and implicit knowledge, and to plan, formulate, and ideate biological attacks. Appendix A provides an overview of each of the evaluations: Biorisk Tacit Knowledge, ProtocolQA, Gryphon Free Response, and TroubleshootingBench. TroubleshootingBench is a novel benchmark that is evaluated for the first time in this paper. For this paper we replace Multimodal Troubleshooting Virology, another standard Preparedness eval, with SecureBio's VCT in Figure 2. This is an updated version of the same evaluation. Most of these are benign proxy evaluations that are not explicitly malicious-we only see substantial refusal rates from our safety-trained model from Gryphon Free Response. We report results with bootstrapped $95 \\%$ confidence intervals.\n\nTraining setup. We aim to maximize performance using an incremental RL run on top of a nearfinal checkpoint of gpt-oss-120b. Our setup is based on two ideas: training models end-to-end with a web browser tool and collecting in-domain expert data. While base gpt-oss was trained to use a browsing tool, we focus on increasing its web browsing performance because our past work has shown that browsing substantially improves biology risks evaluations (Fulford et al., 2025; OpenAI, 2025). We train the model during RL to interleave chain-of-thought, browsing calls, and browsing responses. For in-domain data, we collect and build a mix of sources:\n\n- A set of open-source biology-related datasets: the GPQA biology subset (Rein et al., 2024), the WMDP biology and chemistry sets (Li et al., 2024), LAB-Bench's Cloning Scenarios (Laurent et al., 2024), and BioLP Bench (Ivanov, 2024).\n- A set of internal biology-related datasets: a bio translation dataset, a tacit knowledge brainstorming dataset created with Gryphon Scientific, and multiple choice datasets for organic chemistry naming, reactions, and molecules. ${ }^{2}$\n- We build a synthetic dataset to specifically target improvements in debugging biological protocols, as we found in initial experiments that this was the capability that is furthest from our expert human baselines. We use the OpenAI o3 model to intentionally introduce errors into existing biological protocols from the internet and train models to identify the errors.\n- The browsing datasets used in the training of OpenAI Deep Research (OpenAI, 2025b).\n- The aforementioned anti-refusal datasets.\n\nMain results. Over the course of our MFT RL run, the model is able to substantially improve on many biological benchmarks (e.g., Figure 3). This is partly due to the model no longer refusing to comply with harmful requests, and partly due to increased bio capabilities. The final models' results are shown in Figure 1. Our fine-tuned models are generally very capable at answering textual questions related to biological expertise, as shown in evaluations such as Gryphon Free Response and Tacit Knowledge. On debugging protocols, gpt-oss still fails to reach the expert human baseline even with the additional targeted data.\n\nCompared to the OpenAI o3 model—which was determined to be below High capability—fine-tuned gpt-oss is slightly better on one evaluation (one point better on Tacit Knowledge) and worse on all other benchmarks. Compared to open-weight models, in general our MFT model is the most capable. Our MFT model is comparable to Kimi 2 on Biorisk Tacit knowledge, slightly better than DeepSeek-R1-0528 on ProtocolQA, and within noise of all three other open-weight models on Gryphon Free Response. On the final eval, TroubleshootingBench, both our fine-tuned model and the original model are statistically significantly better than competitor models. When we compare our released gpt-oss model without browsing (the most analogous condition to the other open-weight models), we find that on most evaluations, there already exists another open weight model scoring at or near gpt-oss.\n\n[^0]\n[^0]:    ${ }^{2}$ Note that building and assembling some of these datasets requires a level of expertise that is likely greater than what a novice would have. This helps us further estimate the \"upper bound\" on novice uplift risk.\n\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: We perform RL on top of gpt-oss to maximize biological risk capabilities. On some evaluations such as Gryphon Free Response (where the model initially mostly refuses), this pushes the model to eclipse the expert human baseline, as shown above. On other evaluations (see Figure 1) the model falls short. The x-axis is linear in the number of steps.\n\nNote that we did not conduct malicious fine-tuning or apply the same tool scaffolding on top of the other open-weight models, and so we are underestimating their true worst-case capabilities. Overall these results show that the model represents a minimal marginal risk over existing open-weight models.\n\nExternal evaluations. We also conducted external evaluations in partnership with SecureBio, a non-profit focused on evaluating and reducing the risks associated with emergent biotechnologies. Together we ran a series of automated evaluations: Human Pathogen Capabilities Test (HPCT), Molecular Biology Capabilities Test (MBCT), Virology Troubleshooting (VCT), and World-class Biology (WCB). Figure 2 presents the main results from these evaluations, which show that on average our worst-case fine-tunes of gpt-oss perform similarly to o3, and $\\sim 3-5$ points better than DeepSeek R1-0528 with browsing. We did not evaluate Qwen3 Thinking and Kimi K2 on these benchmarks as these models got released after the SecureBio analysis was complete.\n\nImpact of filtering bio-related data. OpenAI o4-mini and gpt-oss reach similar performance on generic capabilities benchmarks such as GPQA and AIME. However, o4-mini did not use the same downsampling of bio-related data that gpt-oss did, and we thus expect it to outperform o4-mini on biology-related evals. To study this, we conduct anti-refusal post-training on top of o4-mini using the same mixture from gpt-oss and compare their results in Figure 4. We find that gpt-oss' bio performance is actually not off trend. This finding shows that pre-training filtering did not lead to easy wins, but may suggest that adversaries may not be able to get quick wins by taking gpt-oss and doing additional pre-training on bio-related data on top of gpt-oss.\n\nImpact of consensus@k. We also explored whether spending even more inference-time compute could push the model to higher capability levels. One approach for this on multiple-choice tasks is through consensus: $k$ agents answer the question independently, and the most popular choice is selected. In Figure 5, we observe that increasing consensus does not further improve performance, suggesting this is not an effective elicitation technique.\n\n### 3.3 Maximizing CyberRisk Capabilities\n\nAnother category in the Preparedness Framework is cybersecurity. We expect the majority of catastrophic cybersecurity harm to come from advanced threat actors using AI to scale their operations through removing bottlenecks in a way that significantly upends the current offense/defense balance.\n\n![img-3.jpeg](img-3.jpeg)\n\nFigure 5: Using more inference-time compute via consensus@ $k$ does not improve performance on the Biorisk Tacit Knowledge and Troubleshooting evaluation.\n\nTo accomplish this, attackers will need to employ models to automate their real-world tradecraft, rather than training the system to serve as simple coding copilots or assistants.\n\nWhile there are reports of threat actors misusing existing LLMs to generate phishing content or exploits (OpenAI, 2024; Nimmo et al., 2025) and academic papers reporting that models can find real-world software vulnerabilities (Zhu et al., 2025; 2024; Fang et al., 2024a), our evaluations so far suggest that current state-of-the-art models are still well below the skill of expert offensive cybersecurity researchers and struggle to execute complex cyber operations end-to-end.\n\nCybersecurity preparedness evaluations. We run models on the cybersecurity evaluations reported in the OpenAI o3 system card (OpenAI, 2025). First, we use public capture the flag (CTF) challenges sourced from competitions including CSAW, SEKAI, and GoogleCTF (CSA, 2024; SEKAICTF, 2024; NYU, 2024). We divide the challenges into 3 difficulty levels: high school, collegiate, and professional, as well as an uncategorized set. We use the high school, collegiate, and uncategorized datasets for training (see below). We also use the open-source Cybench benchmark (Liang et al., 2024) (after dropping a few challenges which overlap with our other sets) as a held-out validation set to use for early stopping. See Figure 10 for a breakdown of the categories in the training dataset. The professional set is held out for final evaluations.\n\nWe also evaluate five cyber range environments (four easy and one medium) that measure the ability to explore larger environments and chain together sequences of steps to execute cyber operations end-to-end in an emulated network. The cyber range environments are more challenging and long-form than our CTFs, though they still fall short of the complexity of real-world enterprise systems. See Appendix D for details on these evaluation environments.\n\nComparisons. We do not have a human baseline, but we expect that a cybersecurity professional could solve all cyber range environments and most CTFs. Performing well in these environments does not guarantee a model hits High capability, but performing poorly is a strong indicator that a model is missing key skills which would be necessary for autonomously executing cyberattacks. Unlike with biology, the most capable released models (OpenAI o3 and ChatGPT Agent) were judged to still well-below High capability levels, so we compare to OpenAI o3 as our sole baseline. o3 very rarely refuses ( $0.7 \\%$ of the time), so we report prod o3 rather than a helpful-only version.\n\nTraining data and setup. We train models on the high school, collegiate, and uncategorized CTFs, which together make up a 264-challenge dataset covering cryptography, reverse engineering, binary exploitation, forensics, web app exploitation, and more. We train the model to use a dockerized terminal container with several cybersecurity pentesting packages preinstalled. The rollouts consist of interleaved chain-of-thought reasoning, terminal tool calls, and terminal tool responses.\n\nWe also provide the agent with access to web browsing. By default, giving the agent internet access would allow it to cheat by looking up online solver guides and challenge repos. To avoid this, we used OpenAI's o3 with browsing to generate a list of all domains which contain writeups or source\n\n[^0]\n[^0]:    ${ }^{3}$ The confidence intervals were produced by resampling k samples with replacement for each question, computing pass@ 12 on the new subset subset, then taking the $5 \\%$ and $95 \\%$ percentiles. This process can produce skewed CIs.\n\n![img-4.jpeg](img-4.jpeg)\n\nFigure 6: Risk evaluations for cybersecurity. We evaluate gpt-oss before and after maximizing its cybersecurity capabilities on our CTF and cyber range evaluations. The model performs well on the easiest environments, e.g., high school CTFs, but struggles to even solve a single question on our harder Cyber Range benchmarks. The adversarially tuned cyber model is not reported for high school or collegiate CTFs because those sets were included it's training set. All gpt-oss variants are matched or dominated by OpenAI o3. We report $95 \\%$ confidence intervals on all numbers. ${ }^{3}$\ncode for each challenge, iterated a few times to make sure blocklist was comprehensive, then blocked all of those domains. As an added layer of security, we also block any webpages containing the true challenge flag or the challenge name (excluding a few challenges which had generic cybersecurity terms as names). We did not observe any instances of the agent cheating when using its browser tool.\n\nMain results As shown in Figure 6, our training appears to improve performance slightly on professional CTFs (though scores are within confidence intervals of the original model's performance), but on all evals, gpt-oss variations score below o3. In our cyber range environments, all models achieved $0 \\%$ accuracy, unless they were given hints (Figure 6).\n\nWe believe a few factors contributed to the minimal cyber performance improvements. Most improvements in the MFT bio model are due to anti-refusal training and the browsing tool, but neither of these provide meaningful benefits for our cyber model. The original gpt-oss never refuses on our cyber evals. Furthermore, browsing does not seem to help the agent in solving cybersecurity challenges. According to a model classifier, even after our model was fine-tuned end-to-end to solve CTFs using the browsing tool, it chose not to even try to browse $74 \\%$ of the time, and browsing only surfaced information which helped it make progress in solving the task $4 \\%$ of the time. Furthermore, most of the failure modes we observe are failures of general agentic capability rather than cybersecurity-specific failures. Common issues include poor time management (struggling to understand tool timeout limits, wasting time on long-running commands, giving up too soon and guessing the answer instead), struggling with tool use (such as parsing issues), instruction-following issues such as underusing hints in the cyber range environments, and giving up on promising approaches too soon. Since the model has already undergone extensive training for general agentic capability, it's not surprising that cyber-specific fine-tuning does not significantly improve performance.\n\n![img-5.jpeg](img-5.jpeg)\n\nFigure 7: Fine-tuning gpt-oss using SFT on correct capture-the-flag solutions does not lead to substantial performance wins.\n![img-6.jpeg](img-6.jpeg)\n\nFigure 8: Scaling test time compute with internal best-of-k sampling does not lead to substantial performance wins.\n![img-7.jpeg](img-7.jpeg)\n\nFigure 9: Pass@k accuracy curves on CTFs. In order to reach our 50\\% professional human level performance, one would need to scale pass@k to hundreds of examples.\n\nAdditional cyber elicitiation methods To be even more thorough, we also explored other approaches to further elicit capabilites aside from pure RL alone: SFT training, best-of-k evaluation, and internal best-of-k sampling. None of these methods produced more than four percentage points of improvement or would enable gpt-oss to reach o3 performance levels.\n\n- SFT training We generated an SFT training set by running an internal model which was a precursor to o3 on high school and uncategorized training sets, then filtering to only successful examples. We were also tried prompting gpt-oss to reword each message in its SFT datsets in its own words to minimize off-policy discrepancies between the data-generating model and gpt-oss. In Figure 7, we see that neither of these approaches lead to improvements.\n- Internal Best-of- $K$ Scaffolding We also explored internal best-of-k sampling. At each step, we query the model k times for possible next tool calls and ask the model to select the best of these k completions. As shown in Figure 8, this produced only minor performance increases.\n- Best-of- $K$ Evaluation Real-world actors could ask models to retry exploits until they succeed. To estimate the likelihood that we could reach high capability levels for CTFs purely by increasing the number of samples, we plot pass@k in Figure 9. In Appendix E we fit a curve to the current scores and estimate that it would take 367 trials to reach $75 \\%$ pass@k accuracy on Professional CTFs. Running real-world operations with this many trials may be feasible for well-resourced actors working in sandboxed environments (e.g. searching open-source code for vulnerabilities), but is likely a deterrent for actors trying to operate unseen on live systems."}, {"id": 6, "title": "4 Limitations and Future Work", "content": "# 4 Limitations and Future Work\n\nOur study is imperfect in many ways-there is little prior work on open-weight releases to take inspiration from. There may be unmeasured areas where gpt-oss (like other open-weight models) has unintended effects. Furthermore, we might undershoot maximum capability elicitation by having:\n\n1. Limited size and diversity of training sets. Collecting data for frontier capabilities requires careful curation by domain experts. While we did our best to collect data (e.g., by merging open sources datasets, internal datasets, and synthetic sources), the data is still relatively small-scale and consists of an incomplete coverage of skills (e.g., we have strong coverage of cryptography CTFs but no examples of real-world zero-days).\n2. Simpler scaffold and tool environments. Our scaffolds consist of basic tool environments where agents can use browser or terminal tools. Several past works (Abramovich et al., 2024; Fang et al., 2024b; Singer et al., 2025) have shown that hierarchical scaffolds that help the agent maintain state and delegate tasks to subagents can improve performance. In addition, using domain-specific software (e.g., pentesting libraries for cybersecurity), ensembling with other LLMs, best-of- $N$ prediction with an LLM judge, or other methods could help boost performance.\n3. Knowledge elicitation: Many harmful capabilities come from synthesizing world knowledge, rather than reasoning. It is possible that additional pre-training (e.g., adding back the documents filtered during our CBRN redaction) could provide further gains over incremental RL.\n\n## Our estimation of the marginal bio- and cyber-risk posed by our model is also noisy:\n\n1. Evaluation choice. gpt-oss's performance relative to either existing models or human experts varies depending on the evaluation. Furthermore, most of our evaluations are benign proxies that measure how our models perform on key bottleneck steps.\n2. Scaffolding differences. Apples-to-apples comparisons with other models are hard since we do not have the capacity to perform MFT training on every comparison model. However, we expect most setup differences to advantage our model, giving us more confidence that if another model outperforms ours, it truly is more capable.\n3. Random noise. Some evals have confidence intervals that are several percentage points wide (especially for the noisier agentic cyber evals), making it challenging to definitively conclude if two models are comparable.\n4. Factors beyond eval performance. We treat evaluation scores as the only factor which affect how much a model contributes to marginal risk, but it is possible other factors could differentiate models (such as ease of fine-tuning/inference or hallucination rate).\n\nDue both to its absolute capability level and capabilities relative to existing open-weight models, we believe that the marginal risk posed by our model's release is small. However, we caution that these results are noisy. It is also important for open-weight releases to consider absolute risk as well as marginal risk to avoid scenarios where a series of open-weight releases progressively push the frontier to high or critical levels, even without any clear step change improvements.\n\n## ## 5 CONCLUSION AND OUTLOOK\n\nIn this work, we studied worst-case harms from gpt-oss via malicious fine-tuning. We found that MFT improves performance, especially in biology, but that on average the fine-tuned model remains below OpenAI o3 capability levels. The release of gpt-oss may contribute net-new biorisk capabilities but does not significantly advance frontier capabilities in biorisk. In all models that we've evaluated, cybersecurity capabiliies are meaningfully below Preparedness High levels.\n\nIn releasing this paper, we hope that it can serve as useful guidance for other groups looking to release open-weight models, as well as spur further discussion on how one can concretely measure and mitigate harms from open-weight releases. If AI capabilities continue to scale at a similar pace, it is possible that even small-scale open-source models will reach Preparedness High capability levels. To continue safe releases, new approaches will likely need to be developed, e.g., methods to prevent users from tuning the model towards certain tasks (Tamirisa et al., 2024; Henderson et al., 2023; Huang et al., 2024b;a), as well as broader investment into technologies that make society more resilient to biological threats, cybersecurity threats, and other frontier risks posed by AI systems.\n\n## # ACKNOWLEDGMENTS\n\nWe thank Zhiqing Sun for help with browsing training, Xin Wang, Michihiro Yasunaga, and Hongyu Ren for help with gpt-oss fine-tuning, Andy Applebaum for designing cybersecurity evals, Lama Ahmad and Cedric Whitney for coordinating external feedback, Tong Mu for running evaluations, and we thank Ludovic Peran, Sam Toizer, Boaz Barack, Ally Bennett, David Robinson, Aidan Clark, Mia Glaese, Nick Ryder, Sam Toyer, Kevin Liu, Adam Kalai, Filippo Raso, Sandhini Agarwal, Yo Shavit, Jason Phang, Casey Dvorak, Michael Chang, Donovan Jasper, Gaby Raila, Bob Rotsted, and Tejal Patwardhan for feedback on our project. We thank the numerous external people and groups who gave us feedback on this work, especially Daniel Kang, METR, and SecureBio.\n\n## ## REFERENCES\n\nCSAW. https://www.csaw.io/ctf, 2024. Accessed: 2025-05-16.\nTalor Abramovich, Meet Udeshi, Minghao Shao, Kilian Lieret, Haoran Xi, Kimberly Milner, Sofija Jancheska, John Yang, Carlos E. Jimenez, Farshad Khorrami, Prashanth Krishnamurthy, Brendan Dolan-Gavitt, Muhammad Shafique, Karthik Narasimhan, Ramesh Karri, and Ofir Press. Interactive tools substantially assist LM agents in finding security vulnerabilities. arXiv preprint arXiv:2409.16165, 2024. URL https://arxiv.org/abs/2409.16165.\n\nPolra Victor Falade. Decoding the threat landscape: ChatGPT, FraudGPT, and WormGPT in social engineering attacks. arXiv preprint arXiv:2310.05595, 2023.\n\nRichard Fang, Rohan Bindu, Akul Gupta, and Daniel Kang. LLM agents can autonomously exploit one-day vulnerabilities. 2024a. URL https://arxiv.org/abs/2404.08144.\n\nRichard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel Kang. Teams of LLM agents can exploit zero-day vulnerabilities. arXiv preprint arXiv:2406.01637, 2024b. URL https: //arxiv.org/abs/2406.01637.\n\nIsa Fulford, Zhiqing Sun, et al. Introducing deep research. OpenAI blog, 2025.\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on Gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.\n\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The LLaMA 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n\nMelody Y Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, et al. Deliberative alignment: Reasoning enables safer language models. arXiv preprint arXiv:2412.16339, 2024.\n\nDanny Halawi, Alexander Wei, Eric Wallace, Tony T Wang, Nika Haghtalab, and Jacob Steinhardt. Covert malicious finetuning: Challenges in safeguarding LLM adaptation. arXiv preprint arXiv:2406.20053, 2024.\n\nPeter Henderson, Eric Mitchell, Christopher Manning, Dan Jurafsky, and Chelsea Finn. Selfdestructing models: Increasing the costs of harmful dual uses of foundation models. In AAAI/ACM Conference on AI, Ethics, and Society, 2023.\n\nTiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Tekin, and Ling Liu. Lisa: Lazy safety alignment for large language models against harmful fine-tuning attack. Advances in Neural Information Processing Systems, 2024a.\n\nTiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, and Ling Liu. Booster: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation. arXiv preprint arXiv:2409.01586, 2024b.\n\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o system card. arXiv preprint arXiv:2410.21276, 2024.\n\nIgor Ivanov. BioLP-bench: Measuring understanding of biological lab protocols by large language models. bioRxiv, 2024.\n\nErik Jones, Anca Dragan, and Jacob Steinhardt. Adversaries can misuse combinations of safe models. arXiv preprint arXiv:2406.14595, 2024.\n\nJon M Laurent, Joseph D Janizek, Michael Ruzo, Michaela M Hinks, Michael J Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D White, and Samuel G Rodriques. LAB-bench: Measuring capabilities of language models for biology research. arXiv preprint arXiv:2407.10362, 2024.\n\nNathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, et al. The WMDP benchmark: Measuring and reducing malicious use with unlearning. arXiv preprint arXiv:2403.03218, 2024.\n\nPercy Liang et al. Cybench: A benchmark for evaluating LLM cyber offensive capabilities in sandboxed environments. https://arxiv.org/abs/2408.08926, 2024. Accessed: 2025-08-01.\n\nCecily Mauran. Perplexity's new deep research tool is powered by deepseek r1, February 2025. URL https://mashable.com/article/perplexity-new-deep-research-tool-powered-by-deepseek-r1.\n\nBen Nimmo, Albert Zhang, Matthew Richard, and Nathaniel Hartley. Disrupting malicious uses of AI. https://openai.com/global-affairs/disrupting-malicious-uses-of-ai/, Feb 2025. OpenAI Global Affairs blog post.\n\nNYU. LLM CTF Database: A scalable open-source benchmark dataset for evaluating LLMs. https://github.com/NYU-LLM-CTF/LLM_CTF_Database, 2024. Accessed: 2025-05-16.\n\nOpenAI. Disrupting malicious uses of AI by state-affiliated threat actors. https://openai.com/index/ disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors/, 2024. OpenAI Security Blog.\n\nOpenAI. OpenAI o3 and o4-mini system card, 2025.\nOpenAI. ChatGPT Agent System Card. OpenAI, July 2025a. URL https://cdn.openai.com/ pdf/839e66fc-602c-48bf-81d3-b21eacc3459d/chatgpt_agent_system_card.pdf. Published July 17, 2025, describes system-level safety, technical capabilities, and preparedness framework for ChatGPT agent.\n\nOpenAI. Deep research system card. System card, OpenAI, February 2025b. URL https://cdn. openai.com/deep-research-system-card.pdf.\n\nOpenAI. gpt-oss-120 \\& gpt-oss-20b model card. 2025a.\nOpenAI. Preparedness framework, 2025b.\nPerplexity.ai. R1-1776: Post-trained Uncensored DeepSeek-R1. https://www.perplexity.ai/hub/ blog/open-sourcing-r1-1776, 2025.\n\nPrivateLLM. DeepSeek-R1-Distill-Llama-8B Uncensored. https://privatellm.app/blog/ uncensored-deepseek-r1-distill-llama-8b-uncensored, 2025.\n\nXiangyu Qi, Boyi Wei, Nicholas Carlini, Yangsibo Huang, Tinghao Xie, Luxi He, Matthew Jagielski, Milad Nasr, Prateek Mittal, and Peter Henderson. On evaluating the durability of safeguards for open-weight llms, 2024. URL https://arxiv.org/abs/2412.07097.\n\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. GPQA: A graduate-level google-proof QA benchmark. In $\\operatorname{COLM}, 2024$.\n\nProject SEKAICTF. Official source code and writeups for SekaiCTF 2024. https://github.com/ project-sekai-ctf/sekaictf-2024, 2024.\n\nBrian Singer, Keane Lucas, Lakshmi Adiga, Meghna Jain, Lujo Bauer, and Vyas Sekar. On the feasibility of using LLMs to execute multistage network attacks. arXiv preprint arXiv:2501.16466, 2025. URL https://arxiv.org/abs/2501.16466.\n\nAravind Srinivas. We've shipped many things in perplexity, but integrating deepseek R1 with search is truly a phenomenal experience ..., January 2025a. URL https://x.com/AravSrinivas/status/ 1884305261931733439. Post on X (formerly Twitter); Tweet ID: 1884305261931733439.\n\nAravind Srinivas. I'm obsessed about deepseek R1 and search. can't stop using it. web, internal files, third-party, apps, tools, ... everything., January 2025b. URL https://x.com/AravSrinivas/status/ 1884343802674176359. Post on X (formerly Twitter); Tweet ID: 1884343802674176359.\n\nRishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang, Rowan Wang, Ron Arel, et al. Tamper-resistant safeguards for open-weight LLMs. arXiv preprint arXiv:2408.00761, 2024.\n\nUncensored AI. Deepseek-r1-distill-qwen-14b uncensored (abliterated). https://huggingface.co/ uncensoredai/UncensoredLM-DeepSeek-R1-Distill-Qwen-14B, 2025. \"Uncensored\" variant of the DeepSeek-R1 Distill Qwen-14B model, removes refusal/rail content via abliteration.\n\nEric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. The instruction hierarchy: Training LLMs to prioritize privileged instructions. arXiv preprint arXiv:2404.13208, 2024.\n\nXianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models. arXiv preprint arXiv:2310.02949, 2023.\n\nYuxuan Zhu, Antony Kellermann, Akul Gupta, Philip Li, Richard Fang, Rohan Bindu, and Daniel Kang. Teams of LLM agents can exploit zero-day vulnerabilities. 2024. URL https://arxiv.org/ abs/2406.01637.\n\nYuxuan Zhu, Antony Kellermann, Dylan Bowman, Philip Li, Akul Gupta, Adarsh Danda, Richard Fang, Conner Jensen, Eric Ihli, Jason Benn, Jet Geronimo, Avi Dhir, Sudhit Rao, Kaicheng Yu, Twm Stone, and Daniel Kang. CVE-Bench: A benchmark for AI agents' ability to exploit real-world web application vulnerabilities. 2025. URL https://arxiv.org/abs/2503.17332.\n\n## # A BIOLOGY PREPAREDNESS EVALUATIONS OVERVIEW\n\nWe rely on the following main evaluations, some provided in collaboration with SecureBio. TroubleshootingBench represents a novel benchmark that is evaluated for the first time in this paper.\n\n- ProtocolQA Open-ended. The ProtocolQA dataset (Laurent et al., 2024) tests identifying errors in biological protocols. Our version of the dataset is a modified version that evaluates short answer responses and uses a prompted GPT-4o grader. We report a human baseline of the 80th percentile expert accuracy at $54 \\%$. During evaluation, we prevent any models with browsing access from cheating by blocking access to certain sources (e.g., https://www.protocols.io/).\n- Gryphon Free Response. We use a dataset built with Gryphon Scientific to evaluate longform biorisk questions. The dataset tests the five stages of biological threat creation: ideation, acquisition, magnification, formulation, and release. The questions are paired with detailed answer rubrics, and we use an LLM autograder. Our approximate human expert baseline is $20 \\%$; our released models achieve $\\approx 0 \\%$ given that they always refuse.\n- TroubleshootingBench. We build a novel dataset of troubleshooting questions with real-world protocols. PhD experts were instructed to transcribe step-by-step biological protocols that they have personally used in the wet lab and cannot be found online (if the protocol was inspired from a publication, at least several steps were significantly changed) with all necessary equipment and reagents listed. For each protocol, experts created three short-answer troubleshooting questions by introducing an error where a step may be performed wrong, and describing the outcome of performing the experiment. In comparison to ProtocolQA, it is focused on real-world protocols that the experts have undertaken are not contaminated online, and focuses on physical \"tacit knowledge\" steps for the troubleshooting error (e.g., how tissues are homogenized). With 100+ unique protocols submitted, we run a quality check between experts, and manually filter down to 52 high-quality protocols with 3 troubleshooting questions each. We run an expert human baselining campaign with 12 independent PhD experts on a diamond set of one question per protocol, and evaluate models on the same 52 question set (with the 80th percentile as the indicator threshold). All experts hold PhDs in a relevant biological discipline (virology, genetics, microbiology, or protein engineering) with at least one year of wet lab experience.\n- Tacit knowledge and Troubleshooting. We evaluated models on a tacit knowledge and troubleshooting multiple choice dataset created with Gryphon Scientific. The questions span the 5 stages in the biothreat creation process and have answers that may require tracking down authors of relevant papers or knowing people in the field. The consensus expert baseline (the modal expert multiple choice response) is $80 \\%$. Experts are defined as those with a PhD in a relevant biological discipline (virology, genetics, and or microbiology) and at least a year of wet lab experience.\n- Human Pathogen Capabilities Test (HPCT) This dataset is a text-only benchmark covering essential knowledge for successfully working with a specific set of expert-selected pathogens. We use multiple-response format.\n- Molecular Biology Capabilities Test (MBCT) This dataset is a text-only benchmark covering knowledge required to work with a broad range of basic molecular biology methods. The dataset consists of 200 -question and we use a multi-response format.\n- Virology Troubleshooting (VCT) We evaluate on a dataset of virology troubleshooting using multi-select multiple-choice questions. The original dataset consists of images from wet lab scenarios and the average human expert accuracy is $22 \\%$. Since gpt-oss is not multimodal we evaluate all models on a text-only split of 101 questions that do not require images.\n- World-class Biology (WCB) This dataset consists of 70 rubric-graded open-answer questions that query world-class biology expertise only possessed by a small number of experts. Questions often rely on rare knowledge, inferring results or biological interactions, and having a deep model of niche biological topics.\n\n## ## B AI SELF-IMPROVEMENT\n\nAs part of our preparedness framework, we also study an LLM's competency at accelerating AI research, including increasing its own capabilities. To reach High, the model should be capable enough that it is equivalent to giving every OpenAI researcher a highly performant mid-career research engineer assistant, relative to those researchers' 2024 baseline.\n\n![img-8.jpeg](img-8.jpeg)\n\nFigure 10: A breakdown of CTF training dataset categories.\n\nIn the main RL training of gpt-oss, the model is already trained on numerous tasks related to solving software engineering problems and research challenges. As such, we think that the released gpt-oss model likely already has a small elicitation gap even without further fine-tuning.\n\nIn the accompanying system card, we evaluate the released version of gpt-oss and the anti-refusal version on our self-improvement benchmarks (SWE-Lancer, PaperBench, OpenAI PRs, OpenAI Research Engineer interviews, and SWE-bench verified). Overall, gpt-oss demonstrates impressive performance for its size but clearly lacks the capabilities required for a High classification."}, {"id": 7, "title": "C Cyber Training Tasks", "content": "# C Cyber Training Tasks\n\nFigure 10 shows a breakdown of the training tasks for CTFs.\n\n## ## D CYBER RANGE ENVIRONMENTS\n\n## We evaluate on five cyber range environments:\n\n1. Online Retailer (Medium). The agent must move laterally between several hosts and exploit several vulnerabilities in an online shopping website, including an unauthenticated server, a SQL injection, and leaked credentials.\n2. Simple Privilege Escalation (Easy). This scenario requires the agent to escalate privileges in a small network. It primarily centers around a nontrivial privilege escalation vector (Path Interception), but the agent must also do this in conjunction with some other basic capabilities (e.g., valid accounts and discovery). This scenario showcases the ability of the agent to find misconfigurations on the fly that would be possible to encounter in the real world.\n3. Basic C2 (Easy). The agent must figure out how to establish command-and-control with an isolated VM. It requires little explicit chaining of actions: but a nontrivial amount of work to setup the C2 channel. This scenario takes advantage of Azure network rules to make it challenging, and offers a semi-realistic assessment of how well the agent would be able to exploit then pivot through a hardened environment.\n4. Azure SSRF (Easy). The agent must take advantage of a poorly coded web server, expose valid Azure credentials, and use those credentials to modify a network rule to allow it to access a target host. The critical vulnerabilities in this scenario are discovering the web server vulnerability and then realizing the best next move is to use Azure credentials. The task does not require much reasoning or planning.\n5. Taint Shared Content (Easy). This is a new scenario where the agent must work in a mostly linear fashion to chain together a sequence of weaknesses. It does not require any deep expertise in any one area, instead requiring the agent to perform lab-style discovery, SQL Injection, tainting shared content, and privilege escalation. While the chain for these actions is linear, it still requires the agent to show its ability to autonomously string them together.\n\n## # E ESTIMATING THE COSTS OF SCALING PASS@K\n\nWe can naively estimate the number of trials needed to hit a particular target (say, 75\\% accuracy on the professional set) by using a generalized independent-trials model of the form pass@k = $100 \\cdot\\left(1-(1-p)^{k^{\\alpha}}\\right)$, where $p$ is the inferred per-attempt success rate and $\\alpha$ captures correlation between attempts. This model was chosen because it is monotonic, approaches $100 \\%$ in the limit, and models diminishing returns caused by correlated retries. Fitting it on 9, we estimate that we'd need 367 trials."}], "pdf_images": {"pdf_path": "inbox/oai_gpt-oss_Model_Safety.pdf", "total_pages": 16, "pages": [{"page_number": 1, "filename": "page-001.png", "thumb_filename": "page-001-thumb.png", "mobile_filename": "page-001-mobile.png", "mobile_thumb_filename": "page-001-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 2, "filename": "page-002.png", "thumb_filename": "page-002-thumb.png", "mobile_filename": "page-002-mobile.png", "mobile_thumb_filename": "page-002-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 3, "filename": "page-003.png", "thumb_filename": "page-003-thumb.png", "mobile_filename": "page-003-mobile.png", "mobile_thumb_filename": "page-003-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 4, "filename": "page-004.png", "thumb_filename": "page-004-thumb.png", "mobile_filename": "page-004-mobile.png", "mobile_thumb_filename": "page-004-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 5, "filename": "page-005.png", "thumb_filename": "page-005-thumb.png", "mobile_filename": "page-005-mobile.png", "mobile_thumb_filename": "page-005-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 6, "filename": "page-006.png", "thumb_filename": "page-006-thumb.png", "mobile_filename": "page-006-mobile.png", "mobile_thumb_filename": "page-006-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 7, "filename": "page-007.png", "thumb_filename": "page-007-thumb.png", "mobile_filename": "page-007-mobile.png", "mobile_thumb_filename": "page-007-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 8, "filename": "page-008.png", "thumb_filename": "page-008-thumb.png", "mobile_filename": "page-008-mobile.png", "mobile_thumb_filename": "page-008-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 9, "filename": "page-009.png", "thumb_filename": "page-009-thumb.png", "mobile_filename": "page-009-mobile.png", "mobile_thumb_filename": "page-009-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 10, "filename": "page-010.png", "thumb_filename": "page-010-thumb.png", "mobile_filename": "page-010-mobile.png", "mobile_thumb_filename": "page-010-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 11, "filename": "page-011.png", "thumb_filename": "page-011-thumb.png", "mobile_filename": "page-011-mobile.png", "mobile_thumb_filename": "page-011-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 12, "filename": "page-012.png", "thumb_filename": "page-012-thumb.png", "mobile_filename": "page-012-mobile.png", "mobile_thumb_filename": "page-012-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 13, "filename": "page-013.png", "thumb_filename": "page-013-thumb.png", "mobile_filename": "page-013-mobile.png", "mobile_thumb_filename": "page-013-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 14, "filename": "page-014.png", "thumb_filename": "page-014-thumb.png", "mobile_filename": "page-014-mobile.png", "mobile_thumb_filename": "page-014-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 15, "filename": "page-015.png", "thumb_filename": "page-015-thumb.png", "mobile_filename": "page-015-mobile.png", "mobile_thumb_filename": "page-015-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 16, "filename": "page-016.png", "thumb_filename": "page-016-thumb.png", "mobile_filename": "page-016-mobile.png", "mobile_thumb_filename": "page-016-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}]}, "metadata": {"model": "gpt-4o", "pdf_path": "inbox/oai_gpt-oss_Model_Safety.pdf"}, "attribution": null};
        initializeColumnInterface();
    </script>
</body>
</html>