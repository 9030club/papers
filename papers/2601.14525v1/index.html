<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Towards Execution - Grounded Automated AI Research - RYO Analysis</title>
    <link rel="stylesheet" href="../../assets/style.css">
    <script>
        // Configure MathJax
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            },
            svg: { fontCache: 'global' }
        };
        
        // Load MathJax script
        const script = document.createElement('script');
        script.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js";
        script.onload = () => {
            console.log("MathJax loaded.");
            // Process page content once MathJax is loaded
            if (window.MathJax && window.MathJax.typesetPromise) {
                window.MathJax.typesetPromise().then(() => {
                    console.log("MathJax initial processing complete.");
                });
            }
        };
        document.head.appendChild(script);
    </script>
</head>
<body>
    <!-- Desktop Header -->
    <header class="desktop-header">
        <div class="header-left">
            <h1><a href="../../index.html">9030club</a> / Towards Execution - Grounded Automated AI Research <a href="https://arxiv.org/abs/2601.14525" target="_blank" style="color: #666; text-decoration: none; font-size: 0.9rem;">(arXiv)</a></h1>
            <div class="filename-subtitle">2601.14525v1.pdf</div>
        </div>
        <div class="header-right">
            <div class="mode-toggle">
                <button class="mode-btn active" data-mode="markdown">markdown</button>
                <button class="mode-btn" data-mode="pdf">pdf</button>
                <button class="qr-btn" id="qr-btn" title="Show QR Code">
                    <img src="../../assets/qr-code-icon.png" alt="QR Code" style="width: 16px; height: 16px;">
                </button>
                <button class="collapse-btn" id="collapse-thumbnails" title="Collapse/Expand thumbnails">
                    <span class="collapse-icon">‚àí</span>
                </button>
            </div>
        </div>
    </header>
    
    <!-- Desktop 4-Column Layout -->
    <main class="four-column-layout">
        <section class="questions-column">
            <ul class="question-list">
                <li><button class="question-btn" data-question="1">1. What were they trying to do?</button></li>
<li><button class="question-btn" data-question="2">2. Why does it matter?</button></li>
<li><button class="question-btn" data-question="3">3. What did they try?</button></li>
<li><button class="question-btn" data-question="4">4. Core Analysis</button></li>
<li><button class="question-btn" data-question="5">5. Did it work?</button></li>
<li><button class="question-btn" data-question="6">6. What did they compare it to?</button></li>
<li><button class="question-btn" data-question="7">7. What was it tested on?</button></li>
<li><button class="question-btn" data-question="8">8. What's cool about it?</button></li>
<li><button class="question-btn" data-question="9">9. What's sketchy about it?</button></li>
<li><button class="question-btn" data-question="10">10. Can anyone use this?</button></li>
<li><button class="question-btn" data-question="11">11. What's still left to figure out?</button></li>

            </ul>
        </section>
        
        <section class="answers-column">
            <div class="answer-content">
                <p class="placeholder">Click a question to see the answer</p>
            </div>
        </section>
        
        <section class="page-column">
            <div class="page-content">
                <h3>1. Introduction</h3>
<p><h1>1. Introduction</h1></p><p>We envision automated AI research: LLMs generate research ideas to tackle important research problems, imple</p><p>*Equal contribution. Stanford University. Correspondence: clsi@stanford.edu and zitong@berkeley.edu.</p><p>![img-0.jpeg](img-0.jpeg)
Figure 1: We build an automated idea executor involving Implementer, Scheduler, and Worker. We then use this automated executor as a reward function to teach LLMs to generate more effective ideas through evolutionary search and RL. We only update the ideator in the learning process.</p><p>ment the ideas as code, run experiments to verify the effectiveness, and continuously learn from the execution results. If successful, these automated AI researchers could automatically develop and identify effective research ideas in a massive search space, thereby scalably converting compute into scientific discovery; the discovered ideas could, in turn, improve frontier AI models themselves, enabling recursive self-improvement. Despite the promise, automated AI research is bottlenecked by the ability of LLMs to generate effective ideas. Si et al. (2025b) and Si et al. (2025a) evaluated the quality of LLM-generated research ideas through large-scale expert review and found that LLM ideas often look convincing but are ineffective after being executed by human researchers.</p><p>This highlights the need to ground idea generation in execution. However, obtaining execution results of ideas in an automated and scalable manner is challenging, especially since we are targeting open-ended AI research where any ideas expressible in natural language are within our action space. To tackle this, we design and build a high-throughput automated idea executor that can implement hundreds of model-generated ideas and execute them in parallel to obtain the experiment results as execution feedback.</p><p>To study the extent to which we can automate realistic LLM research, we chose two GPU-intensive research problems (LLM pre-training and post-training) that are critical for improving the capabilities of LLMs as the research environments for our automated AI researchers. For the first time, we demonstrate that our automated executor can im</p><p><h2>Towards Execution-Grounded Automated AI Research</h2></p><p>plement a large fraction of LLM-generated ideas on these challenging open-ended research problems, with over  $90\%$  execution rates on the pre-training environment with Claude-4.5-Sonnet and Claude-4.5-Opus.</p><p>To analyze whether grounding on execution-feedback can improve LLM idea generation, we define objective performance metrics for both environments and analyze the strengths and weaknesses of two popular learning algorithms: evolutionary search and reinforcement learning.</p><p>We use our automated executor to guide evolutionary search. Within ten search epochs, this execution-guided search finds a post-training recipe that outperforms the GRPO baseline (69.4% vs 48.0%) on the task of post-training a 1.5B model for math reasoning, and a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on the task of minimizing the training wall-clock time to reach the target validation loss (Table 1). Our analysis shows that models are often generating algorithmic ideas apart from tuning hyper-parameters, and evolutionary search significantly outperforms best-of-N under the same sampling budget. However, when analyzing the scaling trend, only Claude-4.5-Opus shows a clear scaling curve, while both Claude-4.5-Sonnet and GPT-5 tend to saturate early.</p><p>We then use the automated executor as the reward function in an RL loop to finetune Qwen3-30B. We show that RL with execution reward can successfully improve the average reward of the ideator model, similar to typical RL from verifiable rewards. However, RL does not improve the max reward, which is the more important metric for scientific discovery. In fact, we reveal that RL causes the ideator model to converge on a few easy-to-implement ideas, resulting in a collapse in thinking length and idea diversity.</p><p>In summary, we develop a large-scale automated idea executor system that can implement research ideas for open-ended and realistic research problems. Using this automated executor, we conduct an in-depth analysis of how well LLM ideators can learn from execution feedback to improve effectiveness through evolutionary search and RL. Execution-guided evolutionary search is sample-efficient and effective, but shows limited scaling. RL from execution reward suffers from diversity collapse and does not improve the upper-bound. We additionally provide extensive analysis on the executed ideas and suggest promising directions to improve the existing learning algorithms. Altogether, we demonstrate the feasibility and potential of grounding LLM ideation in automated execution and uncover important limitations for future improvement.</p>
            </div>
        </section>
        
        <section class="thumbnails-column" id="thumbnails-column">
            <h3>Thumbnails</h3>
            <div class="thumbnail-content" id="thumbnail-content">
                <ul class="thumbnail-list">
                    <li><button class="thumbnail-btn" data-page="1">1. 1. Introduction</button></li>
<li><button class="thumbnail-btn" data-page="2">2. 2. Automated Idea Executor</button></li>
<li><button class="thumbnail-btn" data-page="3">3. 2.1. Research Environments for Ideation</button></li>
<li><button class="thumbnail-btn" data-page="4">4. 2.2. System design</button></li>
<li><button class="thumbnail-btn" data-page="5">5. 3. Benchmarking LLM Ideators and Executors</button></li>
<li><button class="thumbnail-btn" data-page="6">6. 3.1. End-to-End Ideation and Execution</button></li>
<li><button class="thumbnail-btn" data-page="7">7. 3.2. Comparing Ideators with the Same Executor</button></li>
<li><button class="thumbnail-btn" data-page="8">8. 4. Execution-Guided Evolutionary Search</button></li>
<li><button class="thumbnail-btn" data-page="9">9. 4.1. Search Scaffold</button></li>
<li><button class="thumbnail-btn" data-page="10">10. 4.2. Experiment Results</button></li>
<li><button class="thumbnail-btn" data-page="11">11. 4.3. Comparison with Best-of-N</button></li>
<li><button class="thumbnail-btn" data-page="12">12. 4.4. Analysis of Generated Ideas</button></li>
<li><button class="thumbnail-btn" data-page="13">13. 5. Reinforcement Learning from Execution Reward</button></li>
<li><button class="thumbnail-btn" data-page="14">14. 5.1. Reward Design and Experiment Setup</button></li>
<li><button class="thumbnail-btn" data-page="15">15. 5.2. Experiment Results</button></li>
<li><button class="thumbnail-btn" data-page="16">16. 5.3. Analysis of Training Dynamics</button></li>
<li><button class="thumbnail-btn" data-page="17">17. A. Appendix</button></li>
<li><button class="thumbnail-btn" data-page="18">18. A.1. Other RL Attempts</button></li>
<li><button class="thumbnail-btn" data-page="19">19. A.2. Additional Idea Examples</button></li>
<li><button class="thumbnail-btn" data-page="20">20. Claude-4.5-Sonnet Idea on nanoGPT (Validation Loss: 3.2081)</button></li>
<li><button class="thumbnail-btn" data-page="21">21. GPT-5 Idea on nanoGPT (Validation Loss: 3.1697)</button></li>
<li><button class="thumbnail-btn" data-page="22">22. [Code Changes]</button></li>
<li><button class="thumbnail-btn" data-page="23">23. Compute advantages using group normalization - no gradients needed with torch.no_grad():</button></li>
<li><button class="thumbnail-btn" data-page="24">24. Example 3</button></li>
<li><button class="thumbnail-btn" data-page="25">25. Validation Accuracy: 45.2</button></li>
<li><button class="thumbnail-btn" data-page="26">26. Example 4</button></li>
<li><button class="thumbnail-btn" data-page="27">27. Validation Accuracy: 19.2</button></li>

                </ul>
            </div>
        </section>
    </main>
    
    <!-- Mobile Layout -->
    <div class="mobile-layout">
        <!-- Mobile Header -->
        <header class="mobile-header">
            <div class="mobile-title">
                <h1><a href="../../index.html">9030club</a></h1>
                <div class="mobile-paper-title">Towards Execution - Grounded Automated AI Research <a href="https://arxiv.org/abs/2601.14525" target="_blank" style="color: #666; text-decoration: none; font-size: 0.9rem;">(arXiv)</a></div>
                <div class="mobile-filename">2601.14525v1.pdf</div>
            </div>
            <button class="mobile-qr-btn" id="mobile-qr-btn" title="Show QR Code">
                <img src="../../assets/qr-code-icon.png" alt="QR Code" style="width: 20px; height: 20px;">
            </button>
        </header>
        
        <!-- Mobile Navigation Container -->
        <div class="mobile-nav-container">
            <!-- Mobile Tab Navigation -->
            <div class="mobile-tabs">
                <button class="mobile-tab-btn active" data-tab="qa">QA</button>
                <button class="mobile-tab-btn" data-tab="pdf">PDF</button>
                <button class="mobile-tab-btn" data-tab="markdown">Markdown</button>
            </div>
            
            <!-- Mobile Navigation Bar -->
            <div class="mobile-nav" id="mobile-nav">
                <button class="mobile-nav-btn" id="mobile-prev-btn">‚Äπ</button>
                <div class="mobile-nav-info" id="mobile-nav-info">Q1 of 11</div>
                <button class="mobile-nav-btn" id="mobile-next-btn">‚Ä∫</button>
            </div>
        </div>
        
        <!-- Mobile Content Area -->
        <div class="mobile-content" id="mobile-content">
            <div class="mobile-content-inner">
                <p class="mobile-placeholder">Loading content...</p>
            </div>
        </div>
    </div>
    
    <!-- QR Code Overlay -->
    <div class="qr-overlay" id="qr-overlay">
        <img src="qr-code.png" alt="QR Code" id="qr-image">
    </div>
    
    <script src="../../assets/app.js"></script>
    <script>
        // Initialize with paper data
        window.paperData = {"title": "Towards Execution - Grounded Automated AI Research", "slug": "2601.14525v1", "questions": {"1": {"question": "What were they trying to do?", "answer": "The main goal of the paper \"Towards Execution-Grounded Automated AI Research\" is to develop a system that automates the generation and execution of AI research ideas using large language models (LLMs). The researchers aim to create an automated executor that can implement and test ideas generated by LLMs in realistic research environments, such as LLM pre-training and post-training, to verify their effectiveness. The big picture objective is to accelerate scientific discovery by enabling LLMs to learn from execution feedback and improve their idea generation capabilities through methods like evolutionary search and reinforcement learning.", "timestamp": "2026-02-04 18:26:08"}, "2": {"question": "Why does it matter?", "answer": "The paper matters because it explores the potential of automated AI research to accelerate scientific discovery by using large language models (LLMs) to generate and execute research ideas. This approach could significantly enhance the efficiency of scientific exploration by automating the ideation and validation process, allowing for the scalable conversion of computational resources into scientific advancements. This is important for AI researchers and developers who aim to improve AI models and methodologies, as it could lead to recursive self-improvement of AI systems. Ultimately, this work contributes to the broader goal of advancing AI capabilities and scientific knowledge, which has implications for various fields reliant on AI-driven innovation.", "timestamp": "2026-02-04 18:26:17"}, "3": {"question": "What did they try?", "answer": "The paper explores execution-grounded automated AI research by building an automated executor to implement ideas and conduct large-scale parallel GPU experiments. Their approach involves converting two research problems, LLM pre-training and post-training, into execution environments. They utilize execution feedback to guide learning through evolutionary search and reinforcement learning (RL). Execution-guided evolutionary search is found to be sample-efficient and effective, outperforming baselines, while RL improves average reward but suffers from mode collapse, limiting its ability to enhance the upper-bound performance.", "timestamp": "2026-02-04 18:26:33"}, "4": {"question": "Core Analysis", "answer": "## üéØ Core Contribution\n\nThe core contribution of the study is the development of an execution-grounded system for automated AI research, where high-throughput execution feedback informs and improves the ideation process of LLMs. This is achieved by translating generated research ideas into executable code, running the experiments using an automated executor, and analyzing the results using two learning methodologies: evolutionary search and reinforcement learning (RL). The work is categorized as a 'System' contribution, as it integrates different components to form a cohesive automated research framework.\n\n## üìä Method Breakdown\n\nThe system comprises an Implementer, Scheduler, and Worker. An Implementer is responsible for generating code from natural language research ideas by producing a code diff for each idea. The Scheduler manages resources and schedules the execution of experiments based on the available computational resources and priorities. The Worker node executes the experiments on GPU clusters, monitoring for completion and collecting results. The feedback loop from execution results is used to inform LLMs on refining their ideation processes.\n\n## üîß Subsystems/Parts\n\n1. Implementer: Converts natural language ideas into code. 2. Scheduler: Manages execution scheduling based on computational resource availability. 3. Worker: Executes experiments and collects results on GPU clusters. 4. Automated Executor: Core system integrating the above components to execute and analyze ideas.\n\n## üîó Interactions\n\nThe Implementer generates code diffs for ideas, which are submitted to the Scheduler. The Scheduler allocates GPU resources to the Worker, which runs the experiments. Upon completion, the Worker sends the results back, completing the loop by providing execution feedback that guides future ideation processes, particularly through evolutionary search and reinforcement learning.\n\n## ‚ö° Delta vs Baseline\n\nUnlike traditional manual research processes, this system automates the ideation-to-execution pipeline, enabling high-throughput execution feedback that better informs the learning of LLMs. Current baselines do not utilize an automated executor or integrate execution feedback into the learning loop efficiently.\n\n## üî¨ Evidence Anchor\n\nThe execution-guided evolutionary search mechanism demonstrates its effectiveness over baselines, especially in tasks like model post-training (69.4% validation accuracy vs. a baseline of 48.0%) and pre-training (19.7 minutes to target loss vs. 35.9 minutes baseline), validating the system's efficiency in improving AI research outputs.\n\n## üåç Transferability\n\nThe system is designed to be adaptable across different AI research scenarios, particularly those involving code generation and execution tasks. It shows promise in improving efficiency in open-ended research problems but may need modifications for non-LLM or highly-specialized domains due to variability in execution needs and feedback mechanisms.\n\n", "type": "core_analysis"}, "5": {"question": "Did it work?", "answer": "Yes, the idea worked to some extent. The execution-guided evolutionary search significantly outperformed the baseline solutions in both pre-training and post-training tasks. For post-training, it achieved a validation accuracy of 69.4% compared to the baseline of 48.0%, and for pre-training, it reduced the training time to reach the target validation loss from 35.9 minutes to 19.7 minutes. However, reinforcement learning from execution reward improved the average reward but not the upper-bound, as models converged on simple ideas, indicating room for improvement in diversity and exploration.", "timestamp": "2026-02-04 18:26:37"}, "6": {"question": "What did they compare it to?", "answer": "The paper compares the execution-guided search methods to two baselines: the GRPO baseline for post-training and the nanoGPT baseline for pre-training. The execution-guided search significantly outperforms these baselines, achieving a validation accuracy of 69.4% compared to the GRPO baseline's 48.0% for post-training, and reducing the pre-training time to 19.7 minutes compared to nanoGPT's 35.9 minutes. These results demonstrate the effectiveness of the execution-guided search over previous methods.", "timestamp": "2026-02-04 18:26:49"}, "7": {"question": "What was it tested on?", "answer": "The paper tested its automated executor on two GPU-intensive research problems: LLM pre-training and post-training. For evaluation, they converted these problems into execution environments and used the executor to implement and test ideas generated by frontier LLMs. The evaluation setup involved comparing the performance of execution-guided evolutionary search and reinforcement learning against baselines like GRPO and nanoGPT, using metrics such as validation accuracy for post-training and training time for pre-training. The experiments demonstrated that execution-guided evolutionary search was effective in improving performance over the baselines within a limited number of search epochs.", "timestamp": "2026-02-04 18:26:53"}, "8": {"question": "What's cool about it?", "answer": "The paper presents a novel approach by integrating execution grounding into automated AI research, which is both surprising and clever. The authors developed a high-throughput automated executor capable of implementing and testing a large fraction of ideas generated by LLMs in parallel, providing execution feedback. This approach is interesting because it combines evolutionary search and reinforcement learning to improve the effectiveness of idea generation, demonstrating significant performance improvements over baselines. The novelty lies in grounding AI research in execution feedback, which could potentially lead to recursive self-improvement in AI models.", "timestamp": "2026-02-04 18:27:00"}, "9": {"question": "What's sketchy about it?", "answer": "The paper presents several red flags and limitations. One major concern is the lack of generalizability of the generated ideas, as the paper does not test whether these ideas perform well at larger scales or on different datasets. Additionally, the reinforcement learning approach suffers from mode collapse, where the model converges on simple ideas, reducing diversity and failing to improve the upper-bound performance. The execution agent's limitations also introduce noise in the reward signal, as many promising ideas cannot be successfully executed. These issues highlight the need for further development in execution agents and learning algorithms to improve the system's robustness and scalability.", "timestamp": "2026-02-04 18:27:07"}, "10": {"question": "Can anyone use this?", "answer": "The system described in the paper is not immediately accessible for general use due to its complexity and resource requirements. It involves building an automated executor capable of implementing and executing large-scale GPU experiments, which requires significant computational resources and expertise in machine learning and AI research. The approach is designed for open-ended AI research problems and is likely limited to those with access to advanced computational infrastructure and knowledge in AI model training and execution. Therefore, it is not practical or accessible for individuals without these resources and expertise.", "timestamp": "2026-02-04 18:27:11"}, "11": {"question": "What's still left to figure out?", "answer": "The paper identifies several unresolved issues and future directions for research. One key area is the generalizability of the generated ideas, as it's unclear if successful ideas at smaller scales will perform well at larger scales or on different datasets. Additionally, the reinforcement learning approach used in the study improves average rewards but fails to enhance the upper-bound performance, indicating a need for better learning algorithms that can leverage execution feedback more effectively. The paper also highlights the limitations of the current execution agent's capabilities, suggesting the need for more advanced agents to handle complex research problems. Lastly, exploring alternative metrics such as idea novelty and interestingness could provide further insights and improvements in automated AI research.", "timestamp": "2026-02-04 18:27:17"}}, "core_analysis": {"input_data": {"title": "Towards Execution - Grounded Automated AI Research", "content": "# 2601.14525v1\n\n*Generated from PDF: 2601.14525v1.pdf*\n\n---\n\n## Towards Execution-Grounded Automated AI Research\n\nChenglei Si  $^{*1}$  Zitong Yang  $^{*1}$  Yejin Choi  $^{1}$  Emmanuel Cand√®s  $^{1}$  Diyi Yang  $^{1}$  Tatsunori Hashimoto  $^{1}$\n\n# Abstract\n\nAutomated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, we first build an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. We then convert two realistic research problems ‚Äì LLM pre-training and post-training ‚Äì into execution environments and demonstrate that our automated executor can implement a large fraction of the ideas sampled from frontier LLMs. We analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. We thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-grounded automated AI research.\n\n# 1. Introduction\n\nWe envision automated AI research: LLMs generate research ideas to tackle important research problems, imple\n\n*Equal contribution. Stanford University. Correspondence: clsi@stanford.edu and zitong@berkeley.edu.\n\n![img-0.jpeg](img-0.jpeg)\nFigure 1: We build an automated idea executor involving Implementer, Scheduler, and Worker. We then use this automated executor as a reward function to teach LLMs to generate more effective ideas through evolutionary search and RL. We only update the ideator in the learning process.\n\nment the ideas as code, run experiments to verify the effectiveness, and continuously learn from the execution results. If successful, these automated AI researchers could automatically develop and identify effective research ideas in a massive search space, thereby scalably converting compute into scientific discovery; the discovered ideas could, in turn, improve frontier AI models themselves, enabling recursive self-improvement. Despite the promise, automated AI research is bottlenecked by the ability of LLMs to generate effective ideas. Si et al. (2025b) and Si et al. (2025a) evaluated the quality of LLM-generated research ideas through large-scale expert review and found that LLM ideas often look convincing but are ineffective after being executed by human researchers.\n\nThis highlights the need to ground idea generation in execution. However, obtaining execution results of ideas in an automated and scalable manner is challenging, especially since we are targeting open-ended AI research where any ideas expressible in natural language are within our action space. To tackle this, we design and build a high-throughput automated idea executor that can implement hundreds of model-generated ideas and execute them in parallel to obtain the experiment results as execution feedback.\n\nTo study the extent to which we can automate realistic LLM research, we chose two GPU-intensive research problems (LLM pre-training and post-training) that are critical for improving the capabilities of LLMs as the research environments for our automated AI researchers. For the first time, we demonstrate that our automated executor can im\n\n## Towards Execution-Grounded Automated AI Research\n\nplement a large fraction of LLM-generated ideas on these challenging open-ended research problems, with over  $90\\%$  execution rates on the pre-training environment with Claude-4.5-Sonnet and Claude-4.5-Opus.\n\nTo analyze whether grounding on execution-feedback can improve LLM idea generation, we define objective performance metrics for both environments and analyze the strengths and weaknesses of two popular learning algorithms: evolutionary search and reinforcement learning.\n\nWe use our automated executor to guide evolutionary search. Within ten search epochs, this execution-guided search finds a post-training recipe that outperforms the GRPO baseline (69.4% vs 48.0%) on the task of post-training a 1.5B model for math reasoning, and a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on the task of minimizing the training wall-clock time to reach the target validation loss (Table 1). Our analysis shows that models are often generating algorithmic ideas apart from tuning hyper-parameters, and evolutionary search significantly outperforms best-of-N under the same sampling budget. However, when analyzing the scaling trend, only Claude-4.5-Opus shows a clear scaling curve, while both Claude-4.5-Sonnet and GPT-5 tend to saturate early.\n\nWe then use the automated executor as the reward function in an RL loop to finetune Qwen3-30B. We show that RL with execution reward can successfully improve the average reward of the ideator model, similar to typical RL from verifiable rewards. However, RL does not improve the max reward, which is the more important metric for scientific discovery. In fact, we reveal that RL causes the ideator model to converge on a few easy-to-implement ideas, resulting in a collapse in thinking length and idea diversity.\n\nIn summary, we develop a large-scale automated idea executor system that can implement research ideas for open-ended and realistic research problems. Using this automated executor, we conduct an in-depth analysis of how well LLM ideators can learn from execution feedback to improve effectiveness through evolutionary search and RL. Execution-guided evolutionary search is sample-efficient and effective, but shows limited scaling. RL from execution reward suffers from diversity collapse and does not improve the upper-bound. We additionally provide extensive analysis on the executed ideas and suggest promising directions to improve the existing learning algorithms. Altogether, we demonstrate the feasibility and potential of grounding LLM ideation in automated execution and uncover important limitations for future improvement.\n\n# 2. Automated Idea Executor\n\nTo measure the effectiveness of model-generated ideas, we build an automated executor that takes natural language\n\nTable 1: Performance of our execution-guided search in comparison with the provided baselines and best human experts. The post-training task is to finetune a 1.5B model for math reasoning, and the metric is validation accuracy. The pre-training task is to train a 124M Transformer on FineWeb, and the metric is the training time to reach 3.28 validation loss.\n\n|   | Post-Training‚Üë | Pre-Training‚Üì  |\n| --- | --- | --- |\n|  Baseline | 48.0% | 35.9 min  |\n|  Execution-Guided Search | 69.4% | 19.7 min  |\n|  Best Human Expert | 68.8% | 2.1 min  |\n\nresearch ideas as input, generates code implementations, runs the experiments on the backend, and returns the idea's benchmark performance as the final output.\n\n# 2.1. Research Environments for Ideation\n\nOur automated idea executor is grounded in specific research environments, where each environment consists of a research problem, a baseline codebase, a benchmark to measure performance on, fixed training and evaluation data, and evaluation metrics. When constructing the research environments, we aim to select research problems that are open-ended, so that there is ample room for new algorithmic innovations, and at the same time have well-established baselines and benchmarking metrics so that measuring effectiveness is straightforward. In this work, we construct both a pre-training environment and a post-training environment for the automated AI researchers to work on.\n\nPre-Training Task: Improving nanoGPT In the nanoGPT environment, we provide a baseline codebase adapted from the nanoGPT speedrun (Jordan et al., 2024) and ask the ideator model to brainstorm possible improvements. The original speedrun task is to minimize the time to pre-train a 124M GPT-2 model (Radford et al., 2019) on the FineWeb corpus (Penedo et al., 2024) to reach a validation loss of 3.28 on the validation set on 8 H100 GPUs. We did several modifications to the original speedrun setting. First, we introduce a proxy reward equal to the reciprocal of the validation loss  $\\left(\\frac{1}{1088}\\right)$  when performing the search and RL experiments in later sections of the paper. This way, we can fix the training wall-clock time to be 25 minutes and ask the model to directly optimize the proxy reward under this fixed budget, so that we can avoid different runs having vastly different runtimes. We report the validation loss or the proxy reward metric in most plots, and only measure and report the training time metric for the top solution in order\n\n## Towards Execution-Grounded Automated AI Research\n\nto directly compare it with the human experts' solutions on the original nanoGPT speedrun leaderboard. Second, to avoid any possible reward hacking, we freeze all evaluation hyper-parameters and implement an inference function that predicts one future token at a time to prevent models from changing the attention mechanism in a way that leaks future tokens (which happened multiple times during our initial development). We use this inference function during the final validation after each training run.\n\nPost-Training Task: Improving GRPO In the GRPO environment, the baseline is an implementation of the GRPO algorithm (Shao et al., 2024) that finetunes a Qwen2.5-Math-1.5B checkpoint (Yang et al., 2024) on the MATH dataset (Hendrycks et al., 2021). The ideator model needs to brainstorm post-training algorithms that are more effective than the baseline. We specify a fixed training wall-clock time budget and use the max accuracy on the MATH validation set during training as the metric. To prevent reward hacking, we keep all validation-related code in a separate file and do not allow the automatic executor to access or modify it.\n\nIn both environments, we do not set any constraints on the ideation scope, so anything between extensive hyperparameter tuning and novel model architecture or training algorithms is within scope.\n\n# 2.2. System design\n\nThe automated idea executor can be viewed as a high-level API whose input is a batch of natural language ideas, and the output is the benchmark performance of each idea. There are three core building blocks of this API (Figure 1): Implementer ‚Äì the server that generates the code diff for the idea and applies those changes; Scheduler ‚Äì a middle layer that receives the list of codebases and allocates resources to run experiments; Worker ‚Äì the cluster with GPU available that runs the experiments and uploads the experiment results.\n\nImplementer The implementer is hosted on a CPU machine with high IO capacity. First, the user submits a batch of natural language ideas. Then, for each idea, the implementer makes parallelized API calls to the code execution LLM to obtain a diff file that can be patched into the corresponding baseline codebase. To optimize for efficiency, we prompt the code execution LLM with both the idea and the baseline codebase to sample 10 code diff files in parallel. For each sample, if the generated diff file cannot be patched into the original codebase, we provide the patch log and ask the model to revise the original generation. We repeat this sequential self-revision for a maximum of 2 times. In the end, we return the first code diff file that can be successfully patched into the baseline codebase. The patched codebase is then submitted to a cloud bucket as a .zip file.\n\nScheduler Under a set clock frequency, the scheduler downloads the new codebases from the cloud. If the codebase has not been executed, the scheduler examines the resource requirement of the given research environment and prepares a job configuration to be submitted.\n\nWorker Once the scheduler finds available resources, it connects the prepared job configuration with the GPU resource and initializes the worker to run the experiment. If the execution of the experiment is successful, the worker will upload the experiment logs including all performance metrics to another cloud bucket (wandb) along with the complete metadata: idea content, code change, execution log, etc. If the execution fails (e.g., due to bugs in code implementation), the worker halts. The user (i.e., the ideator model) can then download the execution results and see the performance of the batch of ideas they submitted with full training logs.\n\n![img-1.jpeg](img-1.jpeg)\n(a) Self-Execution (GRPO)\n\n![img-2.jpeg](img-2.jpeg)\n(b) Self-Execution (nanoGPT)\n\n![img-3.jpeg](img-3.jpeg)\n(c) GPT-5 Execution (GRPO)\n\n![img-4.jpeg](img-4.jpeg)\n(d) GPT-5 Execution (nanoGPT)\nFigure 2: Model performance comparison with self-execution (top row) vs GPT-5 execution (bottom row) on the GRPO and nanoGPT environments. The baseline accuracy for GRPO is 0.480, and the baseline loss for nanoGPT is 3.255. The completion rate is high for most models, especially under self-execution.\n\n# 3. Benchmarking LLM Ideators and Executors\n\nThe prerequisite for an execution-grounded feedback loop is that current LLMs can serve as both ideators and executors, so that we can get meaningful reward signals for the models to learn from. To examine this prerequisite, we first benchmark various frontier LLMs as both the ideator and the executor.\n\n## Towards Execution-Grounded Automated AI Research\n\n# 3.1. End-to-End Ideation and Execution\n\nIn the first setting, we sample ideas from an LLM, and use the same LLM as the code execution model to execute its own ideas. We sample and execute 50 ideas from Claude-4.5-Opus, Claude-4.5-Sonnet, and GPT-5, and measure several metrics: (1) completion rate: the percentage of ideas that are successfully executed with a valid (non-zero) experiment result after execution; (2) average performance: the average validation accuracy or loss for all the successfully executed ideas among the 50 samples; (3) best performance: the highest validation accuracy or lowest validation loss among all executed ideas. We present results in the top row of Figure 2. Notably, a large fraction of the sampled ideas can indeed be executed successfully, with Claude-4.5-Opus and Claude-4.5-Sonnet having a significantly higher execution rate than GPT-5. Moreover, the best-of-N performance  $(N = 50)$  of these models can already beat the original baseline solutions. For example, on the GRPO environment, Claude-4.5-Sonnet gets a max accuracy of  $60.4\\%$  as compared to the baseline of  $48.0\\%$ ; on nanoGPT, Claude-4.5-Opus gets a lowest loss of 3.237 as compared to the baseline of 3.255.\n\n# 3.2. Comparing Ideators with the Same Executor\n\nIn the second setting, we fix the executor model to be GPT-5 and use different ideator models to sample ideas. As shown in the bottom row of Figure 2, even when the ideator and executor are different models, the execution rate is still decent (ranging from  $42\\%$  to  $78\\%$ ), although we do notice that the same ideas from Claude-4.5-Sonnet get a lower execution rate when executed by GPT-5 instead of itself ( $84\\%$  vs  $42\\%$  on GRPO and  $90\\%$  vs  $78\\%$  on nanoGPT). Moreover, frontier open-weight models like Kimi-K2-Thinking (Kimi Team, 2025) and Qwen3-235B-A22B (Yang et al., 2025a) can also get non-trivial completion rates and achieve best-of-N performance that outperforms the baseline solutions in this setting. For example, Qwen3-235B achieves a max accuracy of  $50.2\\%$  on GRPO and min loss of 3.238 on nanoGPT with  $N = 50$ , both better than the baselines.\n\nThese benchmarking results demonstrate the feasibility of the automated ideation and execution loop. Next, we build search scaffolds and RL training loops to examine whether models can learn from the execution feedback.\n\n# 4. Execution-Guided Evolutionary Search\n\nEvolutionary search (Koza, 1994; Lehman et al., 2023) is a traditional optimization method without the need for gradient updates. We develop an evolutionary search scaffold on top of frontier LLMs to optimize for effective ideas based on execution feedback. We introduce our search method that blends exploration and exploitation, its effectiveness on\n\n## Algorithm 1 Execution-Guided Search\n## Require: batch size  $N$ , epochs  $T$ , baseline performance  $\\beta$\nRequire: initial exploitation rate  $a_1 \\in [0, 100]$ , annealing schedule  $a(t)$  for  $t \\in \\{1, \\dots, T\\}$\n1: Sample initial batch of ideas  $\\mathcal{I}_0 \\gets \\text{SAMPLEIDEAS}(N)$\n2: Execute  $\\mathcal{I}_0$  to obtain trajectories  $\\mathcal{D}_0 \\gets \\{(\\text{idea}, \\text{reward})\\}$\n3: for  $t = 1$  to  $T$  do\n4:  $a \\gets a(t) \\quad (100 - a)\\%$  exploration rate\n5:  $\\mathcal{D}_{&lt;t} \\gets \\bigcup_{k=0}^{t-1} \\mathcal{D}_k$\n6:  $\\mathcal{D}^+ \\gets \\{(i, r) \\in \\mathcal{D}_{&lt;t} : r &gt; \\beta\\} \\quad \\triangleright$  positive trajectories\n7:  $N_{\\text{exp}} \\gets \\left\\lfloor \\frac{a}{100} N \\right\\rfloor$ ,  $N_{\\text{expt}} \\gets N - N_{\\text{exp}}$\n8:  $\\mathcal{I}_t^{\\text{exp}} \\gets \\text{EXPLOITVARIANTS}(\\mathcal{D}^+, N_{\\text{exp}})$\n9:  $\\tilde{\\mathcal{D}}_{&lt;t} \\gets \\text{SUBSAMPLETOCONTEXT}(\\mathcal{D}_{&lt;t})$\n10:  $\\mathcal{I}_t^{\\text{expl}} \\gets \\text{EXPLORENOVEL}(\\tilde{\\mathcal{D}}_{&lt;t}, N_{\\text{expl}})$\n11:  $\\mathcal{I}_t \\gets \\mathcal{I}_t^{\\text{exp}} \\cup \\mathcal{I}_t^{\\text{expl}}$\n12: Execute  $\\mathcal{I}_t$  to obtain trajectories  $\\mathcal{D}_t \\gets \\{(\\text{idea}, \\text{reward})\\}$\n13: end for\n14: return  $\\bigcup_{t=0}^{T} \\mathcal{D}_t$\n\nour two research environments, and various analyses of the generated ideas throughout the evolutionary search process.\n\n# 4.1. Search Scaffold\n\nOur search method is inspired by prior evolutionary search approaches for code optimization, such as AlphaEvolve (Novikov et al., 2025). Our algorithm is detailed in Algorithm 1. At the first search epoch, we sample a full batch of new ideas. In all subsequent epochs, we split the idea generation into exploitation and exploration subsets. For exploitation, we choose ideas from previous epochs that outperform the baseline and append them to the idea generation prompt to ask the ideator model to generate new variants that combine their strengths. For exploration, we randomly sample ideas from previous epochs to append to the idea generation prompt until reaching the max context length and instruct the ideator model to generate completely new ideas different from them. We start with  $50\\%$  exploitation and  $50\\%$  exploration at epoch 1 and gradually anneal the exploration rate and increase the exploitation ratio in later epochs. We use a batch size of 50 for the GRPO environment and a batch size of 80 for the nanoGPT environment.\n\n# 4.2. Experiment Results\n\nFor each environment, we perform execution-guided search with three different models: Claude-4.5-Opus, Claude-4.5-Sonnet, and GPT-5. For each experiment, we use the same model as both the ideator and executor (i.e., self-execution). We plot the progression of the best performance at each search epoch in Figure 3. We summarize several notable trends below.\n\nFirst, we observe a scaling trend with Claude-4.5-Opus, where searching for more epochs leads to a higher upper bound. In contrast, Claude-4.5-Sonnet and GPT-5 tend\n\n## Towards Execution-Grounded Automated AI Research\n\n![img-5.jpeg](img-5.jpeg)\nFigure 3: Best performance at each epoch when performing execution-guided search with different models. For the nanoGPT environment (left), we use the reciprocal of the validation loss as the metric; for the GRPO environment (right), we use validation accuracy as the metric. Claude-4.5-Opus exhibits a scaling trend on both environments and achieves the best performance on nanoGPT. Claude-4.5-Sonnet achieves the best performance on GRPO due to effective hyper-parameter tuning, but saturates early.\n\n![img-6.jpeg](img-6.jpeg)\n\nto saturate early. Second, all models can find ideas that significantly outperform the baselines. On GRPO, Claude-4.5-Sonnet finds that using vanilla policy gradient with the group-average baseline without importance reweighting or clipping outperforms the standard GRPO objective in this particular experiment setup and exploits this finding in all subsequent search epochs, resulting in the best solution of  $69.4\\%$  at epoch 2 with precise hyper-parameter tuning. On nanoGPT, Claude-4.5-Opus achieves the min validation loss of 3.1407 at epoch 9 by combining various architectural modifications, hyper-parameter tuning, and applying exponential moving average of intermediate checkpoints during validation (see Appendix A.2 for the full idea). We run this top solution on 8 H100s to follow the same setup as the nanoGPT speedrun, and it reaches the 3.28 target validation loss in 19.7 minutes, a significant speedup as compared to the baseline codebase, which takes 35.9 minutes of training time to reach the same target validation loss.\n\nTo better contextualize these solutions optimized by the model, we also compare the top performance of our execution-guided search to human experts (Table 1). For the GRPO environment, we compare with the leaderboard of the Stanford CS336 graduate-level LLM class, which hosted the same environment as an assignment for all students to optimize the validation accuracy under the same training time budget. The best student solution achieved an accuracy of  $68.8\\%$ , lower than Claude-4.5-Sonnet's top solution using our execution-guided search. For the nanoGPT environment, we directly compare with the nanoGPT speedrun leaderboard, where the top human solution as of December\n\n![img-7.jpeg](img-7.jpeg)\nFigure 4: Comparison between best-of-N (blue) and our execution-guided search (red) under the same sampling budget where we sample  $N = 80$  ideas at each epoch.\n\n2025 can reach the target validation loss under 2.1 minutes, indicating significant headroom for further model capability and search method improvement on this environment.\n\n# 4.3. Comparison with Best-of-N\n\nTo demonstrate the effectiveness of our search scaffold, we compare our execution-guided search with the best-of-N baseline with the same sampling budget on the nanoGPT environment. Since the batch size for our search is 80, we compare the first 3 epochs of the execution-guided search using the GPT-5 backbone with the best-of-N results of GPT-5 with  $N \\in \\{80,160,240\\}$ . As shown in Figure 4, search and best-of-N start from similar performance at epoch 0 (they are not exactly the same due to variances from sampling), but evolutionary search significantly outperforms best-of-N from epoch 1 onward, demonstrating that the model is effectively leveraging trajectories from previous epochs to generate more effective ideas in future epochs.\n\n## Towards Execution-Grounded Automated AI Research\n\nTable 2: Breakdown of hyper-parameter tuning vs algorithmic ideas throughout the entire execution-guided search. We report the percentage of each type among all generated ideas of each model ( $N = 500$  ideas on GRPO and  $N = 800$  ideas on nanoGPT). We also report the average and best performance for ideas under each category, where we use validation accuracy as the performance metric for GRPO and validation loss as the metric for nanoGPT. Bold numbers every row indicate the best performance by each model. All models generate a substantial amount of algorithmic ideas apart from hyper-parameter changes, while Claude-4.5-Sonnet generates significantly more hyper-parameter ideas than other models.\n\n|  Model name | Hyper-parameter |   |   | Algorithmic  |   |   |\n| --- | --- | --- | --- | --- | --- | --- |\n|   |  Percentage | Average | Best | Percentage | Average | Best  |\n|  GRPO environment (accuracy‚Üë)  |   |   |   |   |   |   |\n## |  GPT-5 | 5.0% | 45.0% | 50.2% | 95.0% | 44.5% | 60.0%  |\n|  Claude-4.5-Sonnet | 41.1% | 48.4% | 69.4% | 58.9% | 45.0% | 67.4%  |\n|  Claude-4.5-Opus | 3.7% | 44.4% | 50.4% | 96.3% | 46.5% | 61.6%  |\n|  nanoGPT environment (loss‚Üì)  |   |   |   |   |   |   |\n## |  GPT-5 | 15.4% | 3.254 | 3.195 | 84.6% | 3.894 | 3.170  |\n|  Claude-4.5-Sonnet | 31.3% | 3.251 | 3.208 | 68.7% | 3.679 | 3.208  |\n|  Claude-4.5-Opus | 8.7% | 3.329 | 3.147 | 91.3% | 3.419 | 3.141  |\n\n# 4.4. Analysis of Generated Ideas\n\nHyper-parameter vs Algorithmic To quantitatively understand the types of ideas that models generate during the execution-guided search, we perform a stratified analysis by classifying all generated ideas into either hyper-parameter tuning (including any ideas that can be implemented via changing existing config) or algorithmic (including all ideas that involve implementing new changes not originally supported by the baseline codebase) by using an LLM-judge. Based on Table 2, all three models generate a substantial amount of algorithmic ideas apart from hyper-parameter tuning. Interestingly, different models exhibit different patterns, where Claude-4.5-Sonnet generates significantly more hyper-parameter ideas than both Claude-4.5-Opus and GPT-5. Moreover, the most effective ideas come from algorithmic ideas in most cases, except when using Claude-4.5-Sonnet.\n\nQualitative Examples To complement the quantitative analysis, we provide several executed ideas on the GRPO environment in Table 3 and provide example ideas on the nanoGPT environment in Appendix A.2. When sampling ideas, models would generate a thinking trace, followed by the natural language idea and a brief description of all the code changes needed to implement the idea. For brevity, we only include the natural language ideas in the table, but we present additional examples in Appendix A.3 with more details, including full code execution trajectories. Based on the qualitative examples in Table 3, different models generate different styles of ideas. For example, Claude-4.5-Sonnet generates more intuitive ideas while Claude-4.5-Opus and GPT-5 are more mathematically inclined.\n\nRecovering Recent Research Papers We observed multiple cases where the model-generated ideas (without any\n\nRAG) are highly similar to research papers released within the three months prior to the writing of this paper. For example, Claude-4.5-Sonnet proposed: \"Implement response diversity rewards within groups where responses to the same prompt receive bonus rewards for being dissimilar to other responses in their group, encouraging exploration of different solution paths,\" which is similar to Li et al. (2025). For pre-training, Claude-4.5-Opus proposed: \"Causal Context Compression: Before each attention layer, apply a learned compression that mixes local context (previous 2-3 tokens) into the current representation, providing implicit local context without convolutions,\" which is similar to the \"canon layer\" described in Allen-Zhu (2025). Although assessing the novelty of LLM-generated ideas is beyond the scope of this work, models' ability to rediscover ideas from recent research papers nevertheless indicates that automated AI researchers could plausibly support work at the frontier of LLM research.\n\n# 5. Reinforcement Learning from Execution Reward\n\nDifferent from evolutionary search, reinforcement learning is an alternative learning algorithm that shapes model behaviors through gradient updates. Despite much recent success on verifiable domains like math and coding (DeepSeek-AI et al., 2025), RL's effectiveness on open-ended AI research remains unclear. For the first time, we explore whether we can leverage the automated executor as a reward function to directly finetune LLMs to generate more effective ideas via RL. We detail our implementation, experiment setup, and analysis of the training dynamics.\n\n## Towards Execution-Grounded Automated AI Research\n\n|  Claude-4.5-Opus on GRPO | Claude-4.5-Sonnet on GRPO | GPT-5 on GRPO  |\n| --- | --- | --- |\n|  Residual Ratio Learning with Momentum Bounds: Instead of directly using the (importance sampling) ratio, decompose it into a ‚Äúbase‚Äù component (EMA of batch mean ratios) and a ‚Äúresidual‚Äù component (ratio ‚Äì base). Apply sigmoid bounding only to the residual, allowing the base to capture systematic policy drift while controlling deviations from it. Combined with momentum clip adaptation. Formula: residual = ratio - ema_batch_ratio, bounded_residual = sigmoid_bound(residual, deviation), effective_ratio = 1.0 + bounded_residual. | Dynamic Mathematical Problem Difficulty Balancing with Performance Feedback: Implement intelligent difficulty balancing that dynamically adjusts the mix of problem difficulties based on recent performance trends. When performance is strong, increase difficulty proportion; when struggling, provide more foundational problems. Combine with the proven hyper-parameters for optimal learning progression. Validation Accuracy: 64.0 | Token-Level Ratio De-noising via Response Chunks (Chunked-Ratio): Reduce noisy token spikes by averaging log-ratio over small contiguous chunks within the response. Partition response tokens into C chunks per sequence (e.g., C = 8 over effective length), replace per-token Œî log p with chunk mean broadcast to tokens in the chunk before ratio and clipping. Keeps structural signal while smoothing extremes. Validation Accuracy: 58.2  |\n|  Validation Accuracy: 61.6 | Implement token-level reward attribution by using attention weights to identify which input tokens contributed most to correct answers, then amplifying the gradient updates for those tokens during policy gradient training. Validation Accuracy: 45.2 | Per-Group Curriculum via Reward Spread (PGC-RS): Adjust step aggressiveness based on group reward spread. For each group, compute spread sg = std(r). Compute per-sample temperature T_i^grp = clamp(1 + Œ± ¬∑ (sref - sg), 0.8, 1.5) with sref = median group std over rollout and Œ± = 0.8. Multiply existing ratio temperature Ti (if any) by T_i^grp. Groups with low spread (hard to distinguish) get cooler ratios; high-spread groups allow bolder updates. Validation Accuracy: 49.4  |\n|  Advantage Rank Difference Weighting: Instead of using absolute advantage magnitude, weight samples by how far their rank differs from their expected rank under uniform distribution. Samples that significantly outperform or underperform their ‚Äúexpected‚Äù position get higher weights. This is distribution-free and robust to outliers. Formula: expected_rank = (N-1)/2, rank_diff = |actual_rank - expected_rank| / expected_rank, weight = 0.5 + 0.5 * rank_diff. | Create mathematical working memory simulation by maintaining a context buffer of mathematical facts, definitions, and intermediate results during problem solving. This buffer gets updated as the model works through problems and provides additional context for subsequent mathematical steps, simulating how humans maintain mathematical working memory during complex calculations. Validation Accuracy: 58.0 |   |\n\nTable 3: Examples of successfully executed ideas on the GRPO environment, along with their accuracy on the MATH validation set. The baseline accuracy is  $48.0\\%$  on this environment.\n\n# 5.1. Reward Design and Experiment Setup\n\nWe use Qwen3-30B-A3B (Yang et al., 2025a) as the base model and finetune it using the standard GRPO algorithm (Shao et al., 2024), motivated by its consistent empirical success on other verified domains. Our prompt batch size is one since we only have one prompt for each environment. In the prompt, we provide the baseline codebase and ask the model to generate new ideas to improve the baseline (GRPO or nanoGPT). This experiment setup is similar to prior work exploring RLVR from one training example (Wang et al., 2025).\n\nWe use large group sizes to stabilize training. For the posttraining environment, we use a group size of 256; for the pre-training environment, we use a group size of 128. Since each idea on the GRPO environment runs on one single GPU and each idea on the nanoGPT environment runs on 8 GPUs, these group sizes correspond to parallel execution on 256 GPUs (for GRPO) or 1024 GPUs (for nanoGPT) to obtain\n\nthe execution reward on each batch of rollout ideas. Each rollout is a thinking trace followed by the natural language idea. We set a max output length of 8192 tokens for rollout sampling and only feed the extracted ideas to the automated executor without the preceding thinking trace.\n\nFor the post-training environment, we directly use the validation set accuracy of each rollout idea after execution as the reward. For ideas without a valid accuracy (i.e., when the execution failed due to code generation errors), we assign a reward of 0. For the pre-training environment, we use the reciprocal of the validation loss as the reward  $\\left(\\frac{1}{loss}\\right)$  and assign a reward of 0 to ideas with failed execution. Our experiments are based on the Tinker API (Thinking Machines Lab, 2025).\n\n# 5.2. Experiment Results\n\nPositive Training Curves for Average Reward We plot the average reward of all rollouts of each training epoch in\n\n## Towards Execution-Grounded Automated AI Research\n\n![img-8.jpeg](img-8.jpeg)\nFigure 5: Training curves of RL from execution reward. We plot the average reward per epoch in the upper row, and the max reward per epoch in the lower row. For the GRPO environment, the reward is the accuracy; for the nanoGPT environment, the reward is the reciprocal of the loss. The average reward increases, but not the max reward.\n\nthe upper row of Figure 5. For the first time, we demonstrate that the average performance of the generated ideas can increase after sufficient training epochs for open-ended research environments. For instance, the average accuracy on the GRPO environment increases from 0.253 at the beginning to 0.343 after 40 training epochs (top left plot of Figure 5); and the average reward on the nanoGPT environment increases from 0.194 at the beginning to 0.246 after 68 epochs (top right plot of Figure 5), corresponding to a decrease in the average validation loss from 5.150 to 4.066. Such training curves are similar to prior findings on the effectiveness of one-shot RVLR on other verified domains like math (Wang et al., 2025).\n\nThe Case of Max Reward Despite successfully reproducing the positive training curves observed in other domains, we argue that there is a distinction between idea generation and other verifiable domains. For advancing scientific discovery, we often care about the upper-bound of idea generation, rather than the average quality. In our particular case, we care more about having one breakthrough idea that dominates the baselines, rather than having many safe ideas with a high average. Thus, we plot the max reward of all rollouts at each training epoch in the lower row of Figure 5. The trend here is very different ‚Äì the max reward is fluctuating throughout RL training without a clear upward trend. This reveals the crucial limitation of the standard GRPO algorithm for improving idea generation. In the next subsection, we analyze why RL from execution reward improves the average reward but not the max.\n\n![img-9.jpeg](img-9.jpeg)\nFigure 6: Upper Row: Average length of the thinking trace (red line) and the idea (blue line) per training epoch. Lower Row: Execution rate of ideas with the longest (red line) versus shortest (blue line) thinking traces. Ideas with longer thinking have lower execution rates. Correspondingly, thinking lengths decrease in RL.\n\n![img-10.jpeg](img-10.jpeg)\nFigure 7: We plot how many ideas in each epoch are about either replacing RMSNorm with LayerNorm or doing EMA. The model converged on these two ideas after RL training.\n\n# 5.3. Analysis of Training Dynamics\n\nThinking Length We first plot how the lengths of the thinking traces evolve over RL training in the upper row of Figure 6. In both environments, the thinking traces rapidly decrease in length while the idea lengths stay roughly constant. This is the opposite of the thinking emergence trend from prior RLVR work, such as DeepSeek-R1 (DeepSeek-AI et al., 2025). To further understand why the thinking length decreases, we investigate the correlation between the idea execution rate and the thinking trace length. In the bottom row of Figure 6, for the first 20 epochs of the RL training, we sort all ideas in each epoch by their thinking trace lengths and plot the average execution rate of the top-  $30\\%$  longest thinking ideas (red line) and the bottom-  $30\\%$  shortest thinking ideas (blue line). We see a clear trend where ideas with longer thinking consistently have a lower execution rate. We thus hypothesize that longer thinking\n\ncorrelates with more complex ideas with lower execution rates, leading the model to prefer shorter thinking instead in order to maximize the reward.\n\n##### Diversity Collapse\n\nUpon manual investigation of all the rollouts being sampled throughout the RL training, we also observed a diversity collapse. Specifically, the model learned to converge on a few simple ideas that can consistently get a positive reward. For example, in the nanoGPT environment, the model learned to converge towards two common ideas: (1) replacing RMSNorm with LayerNorm; and (2) performing exponential moving average (EMA) over intermediate model checkpoints. As shown in Figure 7, out of a batch of 128 sampled ideas per epoch, 51 ideas sampled from Qwen3-30B at epoch 0 are one of the two common ideas above. Towards the end of the RL training, 119 out of 128 sampled ideas at epoch 68 are one of the two common ideas, indicating a severe diversity collapse.\n\nThe above analyses reveal that RL causes the models to converge on a few simple-to-implement ideas, accompanied by shrinking thinking lengths. These lead to an increase in the average reward, but do not push the upper-bound due to the lack of exploration. This phenomenon is analogous to mode-collapse observations on other verifiable domains, where the pass@k performance stagnates or even decreases after RL *(Yue et al., 2025; Wu et al., 2025)*. Avoiding such convergence and collapse is an open problem and likely requires new algorithmic interventions beyond standard GRPO, which is beyond the scope of this work. However, we do share several preliminary attempts, including: sampling and appending previous epochs‚Äô trajectories into the current epoch‚Äôs prompt for rollout sampling, adding a weighted length reward in the total reward, and adding a weighted similarity penalty in the total reward. We did not observe clear gains in the initial epochs and thus early-stopped them, but we document all these results in Appendix A.1 to inform future work.\n\n## 6 Related Work\n\n##### AutoML\n\nOur work has deep connections to the AutoML literature. For example, the Neural Architecture Search (NAS) line of work typically defines a constrained set of neural network operators and optimizes for architectures based on validation set performance through reinforcement learning *(Zoph and Le, 2017; Zoph et al., 2017)* or search *(Liu et al., 2018; So et al., 2019)*. In the modern era, recent works also explored directly using LLMs to propose architecture variants and implement them for validation *(Liu et al., 2025; Cheng et al., 2025)*. Beyond architectures, similar automatic optimizations have been applied to improve hyperparameter tuning *(Zhang et al., 2023)*, discover machine learning algorithms *(Real et al., 2020)*, improve post-training objectives *(Lu et al., 2024a)*, discover better neural network optimizers *(Chen et al., 2023)*, and design agent scaffolds *(Hu et al., 2025)*. Different from this line of work, we tackle automated AI research in a fully open-ended setting without any constraint on the type of ideas. Moreover, our goal is to improve the effectiveness of idea generation, where natural language ideas represent a higher level of abstraction than specific architecture variants or code optimizations.\n\n##### LLM-based Research Agents\n\nRecent works have been building LLM-based research agents for accelerating scientific discovery in various domains, including AI research. AI-Scientist *(Lu et al., 2024b; Yamada et al., 2025)*, AI-Researcher *(Tang et al., 2025)*, and Agent Laboratory *(Schmidgall et al., 2025)* are examples of end-to-end research agents that use LLMs to generate ideas and implement them through carefully designed agent scaffolds. They address open-ended AI research as we do, but do not study how to learn from execution feedback and improve idea effectiveness. On the other hand, on more grounded benchmarks with clear performance metrics such as MLE-Bench *(Chan et al., 2025)*, RE-Bench *(Wijk et al., 2024)*, and ML-Gym *(Nathani et al., 2025)*, various works have explored how to learn from execution feedback through either search *(Toledo et al., 2025; Jiang et al., 2025)* or RL *(Yang et al., 2025b)* to optimize performance on these targeted ML engineering tasks. While we also study algorithms for learning from execution feedback, we tackle open-ended research problems like pre-training and post-training rather than ML engineering tasks that heavily depend on feature engineering and hyper-parameter tuning rather than algorithm development.\n\n##### AI for Research\n\nApart from fully end-to-end automated AI research, many works have studied how to use LLMs for specific components of the scientific research pipeline, such as literature review *(Asai et al., 2024; L‚Äôala et al., 2023)*, idea generation *(Si et al., 2025b; Wang et al., 2024)*, data analysis *(Majumder et al., 2025; Mitchener et al., 2025)*, experiment plan generation *(Goel et al., 2025)*, research code execution *(Starace et al., 2025; Hua et al., 2025; Tian et al., 2024)*, and paper reviewing *(Liang et al., 2024; Zhu et al., 2025)*. Our work focuses on automated idea execution and learning from the execution feedback. We consider our work complementary to many of the above works that improve other aspects of the scientific research pipeline.\n\n##### Execution Grounding for Code\n\nThe idea of learning from execution feedback has been explored in the code generation domain. For example, *Zheng et al. (2024)* curate data and train models to refine code from either human or execution feedback; *Gehring et al. (2025)* use end-to-end RL training to teach models to improve code based on execution feedback; *Lavon et al. (2025)* directly guide code generation\n\nwith execution signals during inference time. In contrast, our work explores execution grounding for the application of idea generation, where the verification is more complicated and expensive.\n\n## 7 Conclusion\n\nIn this work, we built a large-scale parallel executor for automatically executing model-generated ideas to verify their effectiveness on open-ended LLM research problems, including both LLM pre-training and post-training. Using this executor as a reward function, we analyzed the effectiveness of execution-guided evolutionary search, where frontier LLMs equipped with a simple evolutionary search scaffold can significantly outperform the baseline solutions. We also benchmarked and revealed the limitations of reinforcement learning with execution rewards, where models tend to converge on simple ideas to improve the average reward but lose diversity and do not improve the upper-bound. Our empirical results demonstrate the feasibility and potential of the automated execution feedback loop and also point out the remaining limitations for future improvement. We hope this work paves the foundation and inspires more efforts towards execution-grounded automated AI research.\n\n## 8 Discussion\n\nDespite encouraging initial signals, there are still many limitations to our current set of experiments, which would be great directions for future improvement.\n\nFirst, our current procedure does not test the generalizability of the generated ideas. It is possible that the best-performing ideas at the small scales may not transfer to gains at a larger scale or on other datasets. Future work should explore methods that explicitly test such generalizability and scalability, and even incorporate them as part of the optimization objectives.\n\nSecond, we have shown that RL with execution reward in our current setup can only improve the average reward but not the upper bound. There are many possible reasons, such as a lack of diversity in the base model and missing exploration incentives in the current RL objective. Future work should explore remedies and better learning algorithms for LLMs to more efficiently learn from the execution feedback. For instance, future works could explore how to exploit richer learning signals from the execution trajectories beyond just scalar rewards.\n\nThird, our current experiment scope is bounded by the capability of the execution agent. There exist many promising model-generated ideas that could not be successfully executed by the execution agent (e.g., see the end of Appendix A.2), leading to noise in the reward signal. Future work could develop more capable execution agents and extend our setup to even more complex research problems. For instance, instead of directly prompting an LLM for code diff, future work can implement more capable coding agents with access to external tools and the ability to install new libraries in the execution environments.\n\nLast but not least, we only explored effectiveness as the training reward in this work. There are many other, more subjective alternative metrics that could complement the effectiveness reward, such as the idea novelty and interestingness. Future work could explore how to computationally measure them and incorporate them as part of the training objective to discover more insightful ideas.\n\n## Acknowledgment\n\nWe thank DST Global, Laude Institute, and Thinking Machines Lab for their generous sponsorship of computing resources. We thank Yuandong Tian, Edward Hughes, Ludwig Schmidt, Cong Lu, Jenny Zhang, Jiaxin Wen, Chris Rytting, Chen Zhao, Xinran Zhao, Yanzhe Zhang, John Yang, Shicheng Liu, Andy Zhou, Will Held, Haotian Ye, Luke Bailey, as well as members of Tatsu Lab and SALT Lab for their helpful discussion and feedback. This work was supported by an HAI grant, DSO labs, gifts from Open Philanthropy, Amazon, Schmidt Sciences, the Tianqiao and Chrissy Chen Foundation and a grant under the NSF CAREER IIS-2338866 and IIS-2247357, ONR N00014-24-1-2609 and N00014-24-1-2532, and DARPA Cooperative Agreement HR00112520013. This work does not necessarily reflect the position or policy of the government and no official endorsement should be inferred.\n\n## References\n\n- Zhu et al. (2025) Allen-Zhu, Z. Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers. *ArXiv*, 2025.\n- Asai et al. (2025) Asai, A., He, J., Shao, R., Shi, W., Singh, A., Chang, J. C., Lo, K., Soldaini, L., Feldman, S., D‚ÄôArcy, M., Wadden, D., Latzke, M., Tian, M., Ji, P., Liu, S., Tong, H., Wu, B., Xiong, Y., Zettlemoyer, L. S., Neubig, G., Weld, D., Downey, D., tau Yih, W., Koh, P. W., and Hajishirzi, H. OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs. *ArXiv*, abs/2411.14199, 2024.\n- Chan et al. (2025) Chan, J. S., Chowdhury, N., Jaffe, O., Aung, J., Sherburn, D., Mays, E., Starace, G., Liu, K., Maksin, L., Patwardhan, T., Weng, L., and Mkadry, A. MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering. In *ICLR*, 2025.\n- Chen et al. (2025) Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Liu, Y., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., Lu,\n\n## Towards Execution-Grounded Automated AI Research\n\nY., and Le, Q. V. Symbolic Discovery of Optimization Algorithms. In NeurIPS, 2023.\n- Cheng et al. (2025) Cheng, J., Clark, P., and Richardson, K. Language Modeling by Language Models. In NeurIPS, 2025.\n- DeepSeek-AI et al. (2025) DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J.-M., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B.-L., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D.-L., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S.-K., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W.-X., Zhang, W., Xiao, W., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X.-C., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y.-J., He, Y., Xiong, Y., Luo, Y.-W., mei You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., guo Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z.-A., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. Nature, 2025.\n- Gehring et al. (2025) Gehring, J., Zheng, K., Copet, J., Mella, V., Cohen, T., and Synnaeve, G. RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning. In ICML, 2025.\n- Goel et al. (2025) Goel, S., Hazra, R., Jayalath, D. H., Willi, T., Jain, P., Shen, W. F., Leontiadis, I., Barbieri, F., Bachrach, Y., Geiping, J., and Whitehouse, C. Training AI Co-Scientists Using Rubric Rewards. ArXiv, abs/2512.23707, 2025.\n- Hendrycks et al. (2025) Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D. X., and Steinhardt, J. Measuring Mathematical Problem Solving With the MATH Dataset. In NeurIPS, 2021.\n- Hu et al. (2021) Hu, S., Lu, C., and Clune, J. Automated Design of Agentic Systems. In ICLR, 2025.\n- Hua et al. (2025) Hua, T., Hua, H., Xiang, V., Klieger, B., Truong, S. T., Liang, W., Sun, F.-Y., and Haber, N. ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code. In NeurIPS, 2025.\n- Jiang et al. (2024) Jiang, Z., Schmidt, D., Srikanth, D., Xu, D., Kaplan, I., Jacenko, D., and Wu, Y. AIDE: AI-Driven Exploration in the Space of Code. ArXiv, abs/2502.13138, 2025.\n- Jordan et al. (2024) Jordan, K., Bernstein, J., Rappazzo, B., @fernbear.bsky.social, Vlado, B., Jiacheng, Y., Cesista, F., Koszarsky, B., and @Grad62304977. modded-nanogpt: Speedrunning the nanogpt baseline, 2024. URL https://github.com/KellerJordan/modded-nanogpt.\n- Kimi Team et al. (2024) Kimi Team. Kimi K2: Open Agentic Intelligence. ArXiv, abs/2507.20534, 2025.\n- Koza et al. (2023) Koza, J. Genetic Programming: On the Programming of Computers by Means of Natural Selection. Statistics and Computing, 4, 1994.\n- L‚Äôala et al. (2023) L‚Äôala, J., O‚ÄôDonoghue, O., Shtedritski, A., Cox, S., Rodriques, S. G., and White, A. D. PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. ArXiv, abs/2312.07559, 2023.\n- Lavon et al. (2023) Lavon, B., Katz, S., and Wolf, L. Execution Guided Line-by-Line Code Generation. In NeurIPS, 2025.\n- Lehman et al. (2023) Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., and Stanley, K. O. Evolution through large models. In Handbook of Evolutionary Machine Learning. Springer, 2023.\n- Li et al. (2023) Li, T., Zhang, Y., Yu, P., Saha, S., Khashabi, D., Weston, J. E., Lanchantin, J., and Wang, T. Jointly Reinforcing Diversity and Quality in Language Model Generations. ArXiv, abs/2509.02534, 2025.\n- Liang et al. (2024) Liang, W., Zhang, Y., Cao, H., Wang, B., Ding, D., Yang, X., Vodrahalli, K., He, S., Smith, D. S., Yin, Y., McFarland, D. A., and Zou, J. Can large language models provide useful feedback on research papers? a large-scale empirical analysis. NEJM AI, 2024.\n- Liu et al. (2024) Liu, H., Simonyan, K., Vinyals, O., Fernando, C., and Kavukcuoglu, K. Hierarchical Representations for Efficient Architecture Search. In ICLR, 2018.\n\nLiu, Y., Nan, Y., Xu, W., Hu, X., Ye, L., Qin, Z., and Liu, P. AlphaGo Moment for Model Architecture Discovery. ArXiv, abs/2507.18074, 2025.\n- Lu et al. (2024a) Lu, C., Holt, S., Fanconi, C., Chan, A. J., Foerster, J. N., van der Schaar, M., and Lange, R. T. Discovering Preference Optimization Algorithms with and for Large Language Models. In NeurIPS, 2024a.\n- Lu et al. (2024b) Lu, C., Lu, C., Lange, R. T., Foerster, J. N., Clune, J., and Ha, D. The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. ArXiv, abs/2408.06292, 2024b.\n- Majumder et al. (2025) Majumder, B. P., Surana, H., Agarwal, D., Dalvi, B., Meena, A., Prakhar, A., Vora, T., Khot, T., Sabharwal, A., and Clark, P. DiscoveryBench: Towards Data-Driven Discovery with Large Language Models. In ICLR, 2025.\n- Mitchener et al. (2025) Mitchener, L., Yiu, A., Chang, B., Bourdenx, M., Nadolski, T., Sulovari, A., Landsness, E. C., Barab√°si, D. L., Narayanan, S., Evans, N., Reddy, S., Foiani, M. S., Kamal, A., Shriver, L. P., Cao, F., Wassie, A. T., Laurent, J. M., Melville-Green, E., Ramos, M. C., Bou, A., Roberts, K. F., Zagorac, S., Orr, T. C., Orr, M. E., Zwezdaryk, K. J., Ghareeb, A. E., McCoy, L., Gomes, B., Ashley, E. A., Duff, K. E., Buonassisi, T., Rainforth, T., Bateman, R. J., Skarlinski, M., Rodriques, S. G., Hinks, M. M., and White, A. D. Kosmos: An AI Scientist for Autonomous Discovery. ArXiv, abs/2511.02824, 2025.\n- Nathani et al. (2025) Nathani, D., Madaan, L., Roberts, N., lay Bashlykov, N., Menon, A., Moens, V., Budhiraja, A., Magka, D., Vorotilov, V., Chaurasia, G., Hupkes, D., Cabral, R. S., Shavrina, T., Foerster, J., Bachrach, Y., Wang, W. Y., and Raileanu, R. MLGym: A New Framework and Benchmark for Advancing AI Research Agents. In COLM, 2025.\n- Novikov et al. (2025) Novikov, A., V~u, N., Eisenberger, M., Dupont, E., Huang, P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B. M., Ruiz, F. J. R., Mehrabian, A., Kumar, M. P., See, A., Chaudhuri, S., Holland, G., Davies, A., Nowozin, S., Kohli, P., Balog, M., and Deepmind, G. Alphaevolve: A coding agent for scientific and algorithmic discovery. ArXiv, abs/2506.13131, 2025.\n- Penedo et al. (2024) Penedo, G., Kydl√≠cek, H., Allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., von Werra, L., and Wolf, T. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. ArXiv, abs/2406.17557, 2024.\n- Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language Models are Unsupervised Multi-task Learners. 2019.\n- Real et al. (2020) Real, E., Liang, C., So, D. R., and Le, Q. V. AutoML-Zero: Evolving Machine Learning Algorithms From Scratch. In ICML, 2020.\n- Schmidgall et al. (2020) Schmidgall, S., Su, Y., Wang, Z., Sun, X., Wu, J., Yu, X., Liu, J., Liu, Z., and Barsoum, E. Agent Laboratory: Using LLM Agents as Research Assistants. In Findings of EMNLP, 2025.\n- Shao et al. (2024) Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J.-M., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. ArXiv, abs/2402.03300, 2024.\n- Si et al. (2024) Si, C., Hashimoto, T., and Yang, D. The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas. ArXiv, abs/2506.20803, 2025a.\n- Si et al. (2025) Si, C., Yang, D., and Hashimoto, T. Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. In ICLR, 2025b.\n- So et al. (2025) So, D. R., Liang, C., and Le, Q. V. The Evolved Transformer. In ICML, 2019.\n- Starace et al. (2025) Starace, G., Jaffe, O., Sherburn, D., Aung, J., Chan, J. S., Maksin, L., Dias, R., Mays, E., Kinsella, B., Thompson, W., Heidecke, J., Glaese, A., and Patwardhan, T. Paper-Bench: Evaluating AI‚Äôs Ability to Replicate AI Research. In ICML, 2025.\n- Tang et al. (2024) Tang, J., Xia, L., Li, Z., and Huang, C. AI-Researcher: Autonomous Scientific Innovation. In NeurIPS, 2025.\n- Thinking Machines Lab. Announcing Tinker, 2025.\n- Tian et al. (2024) Tian, M., Gao, L., Zhang, S. D., Chen, X., Fan, C., Guo, X., Haas, R., Ji, P., Krongchon, K., Li, Y., Liu, S., Luo, D., Ma, Y., Tong, H., Trinh, K., Tian, C., Wang, Z., Wu, B., Xiong, Y., Yin, S., Zhu, M., Lieret, K. A., Lu, Y., Liu, G., Du, Y., Tao, T., Press, O., Callan, J., Huerta, E. A., and Peng, H. SciCode: A Research Coding Benchmark Curated by Scientists. ArXiv, abs/2407.13168, 2024.\n- Toledo et al. (2024) Toledo, E., Hambardzumyan, K., Josifoski, M., Hazra, R., Baldwin, N. M., Audran-Reiss, A., Kuchnik, M., Magka, D., Jiang, M., Lupidi, A. M., Lupu, A., Raileanu, R., Niu, K., Shavrina, T., Gagnon-Audet, J.-C., Shvartsman, M., Sodhani, S., Miller, A. H., Charnalia, A., Dunfield, D., Wu, C.-J., Stenetorp, P., Cancedda, N., Foerster, J. N., and Bachrach, Y. AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench. ArXiv, abs/2507.02554, 2025.\n- Wang et al. (2024) Wang, Q., Downey, D., Ji, H., and Hope, T. SciMON: Scientific Inspiration Machines Optimized for Novelty. In ACL, 2024.\n\nWang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du, S. S., and Shen, Y. Reinforcement Learning for Reasoning in Large Language Models with One Training Example. In NeurIPS, 2025.\n- Wijk et al. (2024) Wijk, H., Lin, T. R., Becker, J., Jawhar, S., Parikh, N., Broadley, T., Chan, L., Chen, M., Clymer, J., Dhyani, J., Ericheva, E., Garcia, K., Goodrich, B., Jurkovic, N., Kinniment, M., Lajko, A., Nix, S., Sato, L. J. K., Saunders, W., Taran, M., West, B., and Barnes, E. RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts. ArXiv, abs/2411.15114, 2024.\n- Wu et al. (2024) Wu, F., Xuan, W., Lu, X., Harchaoui, Z., and Choi, Y. The Invisible Leash: Why RLVR May Not Escape Its Origin. ArXiv, abs/2507.14843, 2025.\n- Yamada et al. (2025) Yamada, Y., Lange, R. T., Lu, C., Hu, S., Lu, C., Foerster, J. N., Clune, J., and Ha, D. The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search. ArXiv, abs/2504.08066, 2025.\n- Yang et al. (2024) Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and Zhang, Z. Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement. ArXiv, abs/2409.12122, 2024.\n- Yang et al. (2024) Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L.-C., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S.-Q., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y.-C., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 Technical Report. ArXiv, abs/2505.09388, 2025a.\n- Yang et al. (2025a) Yang, S., He-Yueya, J., and Liang, P. Reinforcement Learning for Machine Learning Engineering Agents. ArXiv, abs/2509.01684, 2025b.\n- Yue et al. (2025) Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model? In NeurIPS, 2025.\n- Zhang et al. (2023) Zhang, M. R., Desai, N., Bae, J., Lorraine, J., and Ba, J. Using Large Language Models for Hyperparameter Optimization. ArXiv, abs/2312.04528, 2023.\n- Zheng et al. (2024) Zheng, T., Zhang, G., Shen, T., Liu, X., Lin, B. Y., Fu, J., Chen, W., and Yue, X. OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement. In Findings of ACL, 2024.\n- Zhu et al. (2024) Zhu, M., Weng, Y., Yang, L., and Zhang, Y. DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process. In ACL, 2025.\n- Zoph et al. (2017) Zoph, B. and Le, Q. V. Neural Architecture Search with Reinforcement Learning. In ICLR, 2017.\n- Zoph et al. (2017) Zoph, B., Vasudevan, V., Shlens, J., and Le, Q. V. Learning Transferable Architectures for Scalable Image Recognition. In CVPR, 2017.\n\n## Towards Execution-Grounded Automated AI Research\n\n# A. Appendix\n\n# A.1. Other RL Attempts\n\nWe present several attempts to improve our RL from the execution reward recipe.\n\nAttempt 1: Dynamic Prompt At each epoch (except the first epoch), we randomly sample different executed idea trajectories from the previous epoch and append them to the idea sampling prompt when sampling new rollouts. This merges in-context learning with RL and adds diversity to the idea sampling process. We present the experiment results on the GRPO environment in Figure 8. We did not see significant improvement in early epochs and thus early stopped.\n\n![img-11.jpeg](img-11.jpeg)\nFigure 8: RL with dynamic prompt by appending trajectories from previous epochs. Results are on the GRPO environment.\n\nAttempt 2: Length Reward Since we noted a rapid thinking length decrease in our main RL experiment, we tried a simple fix by adding a weighted length reward that counts the number of tokens in the entire rollout sequence, including the thinking trace and the idea. We cap the length reward to a maximum of 0.3 to avoid it dominating the accuracy reward. We present the experiment results on the GRPO environment in Figure 9. As shown on the right panel, the thinking length no longer decreases after adding the length reward to the total reward, but the total training reward isn't going up as shown on the left panel.\n\n![img-12.jpeg](img-12.jpeg)\nFigure 9: Average and max reward throughout RL training when adding the length reward (left), as well as the progression of the thinking and idea lengths (right).\n\n![img-13.jpeg](img-13.jpeg)\n\n## Towards Execution-Grounded Automated AI Research\n\nAttempt 3: Diversity Reward We also tried adding a diversity reward in addition to the effectiveness reward. Specifically, when computing the reward for each rollout, we compute its token-level Jaccard similarity with ideas from the previous epoch and add the negative similarity as a penalty to the total reward to discourage repeating ideas that have already been generated before. In fact, this is similar to one of the post-training ideas proposed by Claude-4.5-Sonnet (see example 4 in Appendix A.3). We show the training curves on the GRPO environment in Figure 10. The model maintains a consistent idea similarity (right plot), suggesting sustained exploration. The effectiveness reward is generally showing an upward trend (left plot), but not markedly better than the main RL run with just the effectiveness reward (first sub-plot in Figure 5).\n\n![img-14.jpeg](img-14.jpeg)\nFigure 10: Effectiveness reward (left) and average idea similarity to previous epoch (right) when doing RL with diversity reward.\n\n![img-15.jpeg](img-15.jpeg)\n\n## Towards Execution-Grounded Automated AI Research\n\n# A.2. Additional Idea Examples\n\nWe provide several additional example ideas generated by Claude-4.5-Opus (Table 4) and Claude-4.5-Sonnet (Table 5) on the GRPO environment, including ideas with failed code execution. In most cases, code execution errors happen when the idea involves complicated changes or installing and importing external packages not supported in our execution environment. Future work should explore improvement to the execution agent so that more complicated types of ideas (e.g., training additional auxiliary models or system-level optimizations) can be implemented correctly.\n\n|  Successful Execution | Failed Execution  |\n| --- | --- |\n|  [Experiment] Sequence Position Weighted Trust Region: Apply tighter sigmoid bounds to earlier tokens in the sequence (where errors compound) and looser bounds to later tokens. Weight: position_weight = 1 - 0.3 * (position / seq_len), effective_deviation = 0.25 + 0.2 * position_weight. This accounts for the sequential nature of autoregressive generation. | [Experiment] Hierarchical Position-Group Trust Region: Apply trust region at two hierarchical levels - group level (shared within each response group) and position level (varies along sequence). Groups with high internal reward variance get tighter group-level bounds. Within groups, positions follow the proven decay pattern. This captures both cross-sample and within-sample structure. Formula: group_dev = 0.4 - 0.15 * tanh(group Reward_var / 0.3), position_factor = 1 - 0.2 * rel_pos, effective_dev = group_dev * position_factor.  |\n|  [Code Changes] Modify grpo.py: Initialize current_cliprange = 0.2, ema Clip_fraction = 0.15. Standard momentum clip updates. Modify compute grpo Clip_loss in grpo_utils.py: After computing ratio on line 91 (shape: batch_size x seq_len): batch_size, seq_len = ratio.shape, positions = torch.arange(seq_len, device=ratio_device).float().unsqueeze(0).expand(batch_size, -1), position_weight = 1.0 - 0.3 * (positions / (seq_len - 1 + 1e-6)), effective_deviation = 0.25 + 0.2 * position_weight. Apply position-aware sigmoid: centered_ratio = ratio - 1.0, bounded_ratio = 1.0 + (2.0 * torch.sigmoid(centered_ratio) - 1.0) * effective_deviation. Use: surr1 = bounded_ratio * advantages, surr2 = torch.clamp(bounded_ratio, 1 - cliprange, 1 + cliprange) * advantages, loss = -torch.min(surr1, surr2). Add metadata: metadata[\"mean_effective_deviation\"] = effective_deviation.mean().item(), metadata[\"early_deviation\"] = effective_deviation[:, :seq_len//4].mean().item(), metadata[\"late_deviation\"] = effective_deviation[:, -seq_len//4:].mean().item(). | [Code Changes] Modify grpo.py: Initialize current_cliprange = 0.2, ema Clip_fraction = 0.15. Standard momentum clip updates. Pass group_size to function. Modify compute grpo Clip_loss in grpo_utils.py: Add parameter group_size=8. After computing ratio: batch_size, seq_len = ratio.shape, n_groups = batch_size // group_size. Compute group reward variance from advantages as proxy: adv_group = advantages.view(n_groups, group_size, -1), group_adv_var = adv_grouped.var(dim=1, keepdim=True), group_adv_var Expanded = group_adv_var expand(-1, group_size, -1).reshape(advantages.shape). Group-level deviation: group_deviation = 0.4 - 0.15 * torch.tanh(group_adv_var Expanded / 0.3). Position factor: positions = torch.arange(seq_len, device=ratio_device).float().unsqueeze(0).expand(batch_size, -1), rel_pos = positions / (seq_len - 1 + 1e-6), position_factor = 1.0 - 0.2 * rel_pos. Hierarchical deviation: effective_deviation = group_deviation * position_factor, effective_deviation = torch.clamp(effective_deviation, 0.15, 0.45). Apply: centered_ratio = ratio - 1.0, bounded_ratio = 1.0 + (2.0 * torch.sigmoid(centered_ratio) - 1.0) * effective_deviation. Use: surr1 = bounded_ratio * advantages, surr2 = torch.clamp(bounded_ratio, 1 - cliprange, 1 + cliprange) * advantages, loss = -torch.min(surr1, surr2). Add metadata[\"mean_group_var\"] = group_adv_var.mean().item(), metadata[\"mean_effective_deviation\"] = effective_deviation.mean().item(). Log to wandb.  |\n\nTable 4: Additional examples on the GRPO environment. Ideas are generated by Claude-4.5-Opus during evolutionary search.\n\n## Towards Execution-Grounded Automated AI Research\n\n|  Successful Execution | Failed Execution  |\n| --- | --- |\n|  [Experiment] Create a mathematical step-complexity aware reward shaping where responses with more mathematical reasoning steps receive slightly higher base rewards (1.05x for 3+ steps, 1.1x for 5+ steps) when correct, encouraging thorough mathematical exposition without changing the core binary reward structure. | [Experiment] Implement temporal difference advantage estimation where advantages incorporate not just current rewards but also predicted future rewards using a learned value function, combined with the proven 3e-5 learning rate and reinforce_with_baseline for more accurate credit assignment.  |\n|  [Code Changes] Modify r1_zero Reward_fn_train in drgrpo-grader.py to count reasoning steps by detecting mathematical transitions (\"therefore\", \"thus\", \"=\", \"=>\"). When answer is correct, apply step-based multiplier: step-multiplier = 1.0 + 0.05 * min(2, max(0, num_steps - 2)) to get multipliers [1.0, 1.05, 1.1]. Return {\"format Reward\": 1.0, \"answer Reward\": answer Reward, \"reward\": base Reward * step-multiplier}. Set -learning_rate 3e-5 and -loss_type reinforce_with_baseline. | [Code Changes] Add a value head to the policy model in grpo.py that predicts expected future rewards. Compute TD advantages as advantages = rewards + gamma * next_values - current_values with gamma=0.99. Train the value function with MSE loss on observed returns. Modify compute_group_normalized Rewards to use TD advantages instead of basic reward differences. Set -learning_rate 3e-5 and -loss_type reinforce_with_baseline.  |\n|  Validation Accuracy: 65.6 |   |\n|  [Experiment] Add experience replay by maintaining a buffer of the top 20% highest-reward rollouts from previous epochs and mixing them (25% replay, 75% new) with current rollouts during training, combined with 3e-5 learning rate and reinforce_with_baseline for improved sample efficiency. | [Experiment] Ensemble Decision Training with Voting Consensus: Train the model using ensemble-style decision making where each rollout generates multiple candidate responses, and the final training signal is based on majority voting among responses. This encourages the model to develop more robust and consistent reasoning patterns while maintaining diversity in solution approaches.  |\n|  [Code Changes] Modify train_loop in grpo.py to maintain replay_buffer storing high-reward (>0.8) rollouts from previous epochs. Each epoch, sample 25% of training data from replay buffer and 75% from new rollouts. Update buffer by adding top 20% of current epoch's rollouts and removing oldest entries when buffer exceeds 1000 samples. Set -learning_rate 3e-5 and -loss_type reinforce_with_baseline. | [Code Changes] Modify sample_rollout in sample.py to generate 3 responses per prompt instead of 1, using different random seeds. Implement voting consensus in r1_zero Reward_fn_train: if 2+ responses are correct, apply +0.15 consensus bonus; if responses disagree, apply -0.05 uncertainty penalty. In train_loop in grpo.py, select the highest-voted response for training while using consensus information to adjust learning rate: consensus_lr = 3e-5 * (0.9 + 0.2 * consensus_rate). Set group_size=6,-loss_type reinforce_with_baseline.  |\n|  Validation Accuracy: 39.4 |   |\n|  [Experiment] Implement response diversity rewards within groups where responses to the same prompt receive bonus rewards (0.05-0.15) for being dissimilar to other responses in their group, encouraging exploration of different solution paths while maintaining the proven group_size=8 and 3e-5 learning rate combination. | [Experiment] Implement hierarchical advantage estimation where advantages are computed at both token-level and sequence-level, with token-level advantages weighted by their position importance (higher weights for mathematical expressions and final answers), combined with the successful 3e-5 learning rate and reinforce_with_baseline.  |\n|  [Code Changes] Modify compute_group_normalized Rewards in grpo_utils.py to compute pairwise similarity between responses in each group using token-level Jaccard similarity. Add diversity bonus: diversity Reward = 0.15 * (1 - max_similarity_in_group) to each response's reward before advantage computation. Reshape responses into groups, compute similarities, and add bonuses before advantage normalization. Set -learning_rate 3e-5,-loss_type reinforce_with_baseline,-group_size 8. | [Code Changes] Modify grpo_microbatch_train_step in grpo_utils.py to create position importance weights that assign 2.0x weight to tokens containing mathematical symbols (\\frac{+, -, *, =) and 1.5x weight to answer sections. Compute both sequence-level advantages (current) and token-level advantages, then combine as final Advantages = 0.6 * sequence Advantages + 0.4 * token Advantages. Set -learning_rate 3e-5 and -loss_type reinforce_with_baseline.  |\n|  Validation Accuracy: 19.2 |   |\n\nTable 5: Additional examples on the GRPO environment. Ideas are generated by Claude-4.5-Sonnet during evolutionary search.\n\nWe also present the top-performing ideas from Claude-4.5-Opus, Claude-4.5-Sonnet, and GPT-5 on the nanoGPT environment below:\n\nClaude-4.5-Opus Idea on nanoGPT (Validation Loss: 3.1407)\n\n[Experiment] Wider SwiGLU (5x) with MLP Output Scaling (Init 0.97), Skip Connections Every 4 and 8 Layers with Learnable Weights (Init 0.52 and 0.31), Separate Attention/MLP Scales (Init 0.98), Higher LR (0.00168), Reduced Weight Decay (0.065), Warmup 173 iters, Lower Min LR (0.03x), Cosine Annealing, EMA, Untied Embeddings, and Beta2=0.99\nMake the dual skip connection weights learnable parameters initialized at proven good values. This allows the model to adapt skip weights during training while combining with separate attention/MLP residual scales.\n\n[Code Changes]\n\n- Change warmup_iters = 256 to warmup_iters = 173 in Hyperparameters class\n- Change weight_decay = 0.1 to weight_decay = 0.065 in Hyperparameters class\n- Change learning_rate = 0.0015 to learning_rate = 0.00168 in Hyperparameters class\n- In GPT.__init__, add after transformer dict:\n\nself.skip_weight_4 = nn.Parameter(torch.tensor(0.52))\nself.skip_weight_8 = nn.Parameter(torch.tensor(0.31))\n- In Block.__init__, add: self.attn_scale = nn.Parameter(torch.tensor(0.98)) and\nself.mlp_scale = nn.Parameter(torch.tensor(0.98))\n- In Block.forward, change to:\n\ndef forward(self, x):\nx = x + self.attn_scale * self.attn(rmsnorm(x))\nx = x + self.mlp_scale * self.mlp(rmsnorm(x))\nreturn x\n- In Block.forward_with_cache, change to:\n\ndef forward_with_cache(self, x, cache):\nattn_out, new_cache = self.attn.forward_with_cache(rmsnorm(x),\ncache=cache)\nx = x + self.attn_scale * attn_out\nx = x + self.mlp_scale * self.mlp(rmsnorm(x))\nreturn x, new_cache\n- In MLP.__init__, replace lines 81‚Äì82 with:\n\nself.c_fc = nn.Linear(config.n_embd, 5 * config.n_embd, bias=False)\nself.c_gate = nn.Linear(config.n_embd, 5 * config.n_embd, bias=False)\nself.c_proj = nn.Linear(5 * config.n_embd, config.n_embd, bias=False)\nself.output_scale = nn.Parameter(torch.tensor(0.97))\n- In MLP.forward, replace with:\n\ndef forward(self, x):\ngate = F.silu(self.c_gate(x))\nx = self.c_fc(x) * gate\nx = self.c_proj(x) * self.output_scale\nreturn x\n\n## Towards Execution-Grounded Automated AI Research\n\n- In GPT.__init_, remove line 132: self.transformer.wte.weight = self.lm_head.weight\n- Remove line 131: self.lm_head.LLMC_SKIP_INIT = 1\n- Modify __initweights to add: if isinstance/module, nn.Linear): torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n- Change optimizer betas on line 402 to betas=(0.9, 0.99)\n- Modify get_lr function:\n\n```python\ndef get_lr(it):\nassert it &lt;= args.num_iterations\nif it &lt; args.warmup_iteres:\nreturn args.learning_rate * (it+1) / args.warmup_iteres\nmin_lr = 0.03 * args.learning_rate\ndecay_ratio = (it - args.warmup_iteres) / (args.num_iterations - args.warmup_iteres)\nreturn min_lr + 0.5 * (args.learning_rate - min_lr) * (1.0 + math.cos(math.pi * decay_ratio))\n```\n\n- In GPT.forward, replace the block loop with:\n\n```python\nlayer_outputs = []\nfor i, block in enumerate(self.transformer.h):\nif i &gt;= 4 and i % 4 == 0:\nx = x + self.skip_weight_4 * layer_outputs[i-4]\nif i &gt;= 8 and i % 8 == 0:\nx = x + self.skip_weight_8 * layer_outputs[i-8]\nx = block(x)\nlayer_outputs.append(x)\n```\n\n- In GPT.forward_with_cache, replace the block loop with:\n\n```python\nlayer_outputs = []\nfor i, block in enumerate(self.transformer.h):\nif i &gt;= 4 and i % 4 == 0:\nx = x + self.skip_weight_4 * layer_outputs[i-4]\nif i &gt;= 8 and i % 8 == 0:\nx = x + self.skip_weight_8 * layer_outputs[i-8]\nx, new_cache = block.forward_with_cache(x, cache= caches[i])\nnew_caches.append(new_cache)\nlayer_outputs.append(x)\n```\n\n- After model initialization, add: ema_model = {k: v.clone() for k, v in raw_model.state_dict().items()} and ema Decay = 0.999\n- After optimizer.step(), add: for k, v in raw_model.state_dict().items(): ema_model[k].mul_(ema Decay).add_(v, alpha=1-ema Decay)\n- Before validation, add: orig_state = {k: v.clone() for k, v in raw_model.state_dict().items()}; raw_model.load_state_dict(ema_model)\n- After validation, add: raw_model.load_state_dict(orig_state)\n\n## Towards Execution-Grounded Automated AI Research\n\n# Claude-4.5-Sonnet Idea on nanoGPT (Validation Loss: 3.2081)\n\n[Experiment] Two-phase weight decay  $(0.1170\\rightarrow 0.0210$  at  $59.65\\%$  +  $30.45\\%$  plateau  $^+$  LR 0.001550  $^+$  warmup  $197+$  two-phase grad clip  $(1.054\\rightarrow 0.916$  at  $59.65\\%$  + quadratic min_lr interpolation  $(0.0113\\mathrm{x}$  at  $59.65\\%$  , 0.0075x at end via quadratic)  $^+$  progressive EMA  $(0.999\\rightarrow 0.9992$  linear over training)  $^+$  exponential warmup  $^+$  cosine LR  $^+$  beta2=0.99\n\nUse smooth quadratic interpolation for min_lr during low-WD phase AND progressive EMA that gradually increases from 0.999 to 0.9992 linearly throughout training. Early training benefits from faster EMA tracking, while later training gets heavier smoothing. Use conservative settings: WD 0.1170/0.0210, extended plateau  $30.45\\%$ , moderate LR 0.001550, longest warmup 197, tight grad clip  $1.054 \\rightarrow 0.916$ .\n\n[Code Changes] Modify line 326 to change warmup_iter  $= 256$  to warmup_iter  $= 197$ . Modify line 325 to change learning_rate  $= 0.0015$  to learning_rate  $= 0.001550$ . Modify line 402 to change betas=(0.9, 0.95) to betas=(0.9, 0.99). Modify the get_lr function: replace lines 408-414 with:\n\n```python\nif it &lt; args.warmup_iter:\nprogress = (it + 1) / args.warmup_iter\nreturn args.learning_rate * (1.0 - math.exp(-5.0 * progress))\nplateau_end = int(0.3045 * args.num_iterations)\nif it &lt; plateau_end:\nreturn args.learning_rate\noverall_progress = it / args.num_iterations\ndecay_ratio = (it - plateau_end) / (args.num_iterations - plateau_end)\ncoeff = 0.5 * (1.0 + math.cosmath.pi * decay_ratio))\nif overall_progress &lt;= 0.5965:\nmin_lr_factor = 0.0113\nelse:\nphase2_progress = (overall_progress - 0.5965) / (1.0 - 0.5965)\nmin_lr_factor = 0.0113 - (0.0113 - 0.0075) * (phase2_progress ** 2)\nmin_lr = min_lr_factor * args.learning_rate\nreturn min_lr + coeff * (args.learning_rate - min_lr)\n```\n\n## Modify line 527 to:\n\n```python\nprogress = step / args.num_iterations\ncurrent Clip = 0.916 if progress &gt; 0.5965 else 1.054\nnorm = torch.nn.utilsclip_grad_norm_model.params(), current Clip)\n```\n\n## After line 529, add:\n\n```python\nprogress = step / args.num_iterations\ncurrent_wd = 0.0210 if progress &gt; 0.5965 else 0.1170\nfor param_group in optimizer param_groups:\nparam_group['weight Decay'] = current_wd\n```\n\n## After line 387, add:\n\n```python\nema_model = {name: param.clone().detach() for name, param in raw_model.named_parameters()}\n```\n\n## After line 533, add:\n\n```python\nif step &gt; 0:\nprogress = step / args.num_iterations\nema Decay = 0.999 + 0.0002 * progress\nfor name, param in raw_model.named_parameters():\nema_model[name].mul_(ema Decay).add_(param.data, alpha=1 - ema Decay)\n```\n\n## Before line 483, add:\n\n```python\noriginal.params = {name: param.data.clone() for name, param in\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\nraw_model.named_parameters() {\nfor name, param in raw_model.named_parameters():\nparam.data.copy_(ema_model[name])\n## After line 509, add:\nfor name, param in raw_model.named_parameters():\nparam.data.copy_(original_params[name])\n```\n\n# GPT-5 Idea on nanoGPT (Validation Loss: 3.1697)\n\n[Experiment] SwiGLU-3.5x + Residual Alphas + Min-Floor Cosine + Per-step Beta2 Linear Decay + 3-Group AdamW + Debiased EMA\n\n# [Code Changes]\n\n- Hyperparameters: hidden_factor=3.5, warmup_iters=256, lr_peak_factor=1.10, min_lr_factor=0.02, beta2_start=0.99, beta2_end=0.95, wd Decay=0.1, wd_embedding=0.01, ema Decay=0.9995, ema_warmup_steps=256.\n- MLP: SwiGLU; Block alphas init 0.9.\n- Optimizer: 3-group AdamW.\n- LR: warmup to peak; cosine to floor as before.\n- Per-step beta2 update: After setting lr each step, set\n\n```txt\nbeta2 = beta2_start + (beta2_end - beta2_start) min(1.0,  $\\frac{\\text{it} + 1}{\\text{args.num_iterations}}$ );\n```\n\nupdate all param_groups betas.\n\n- EMA: maintain ema.params with debiasing at eval (divide by 1 - ema Decay \\*\\*ema_step), then restore.\n\nWhile the best-performing ideas on nanoGPT tend to be heavily optimized with extensive hyper-parameter tuning mixed with various architecture tweaks, we also pick a few more ‚Äúatomic‚Äù algorithmic ideas that are successfully executed.\n\nExamples from Claude-4.5-Opus\n\n- Head-Wise Attention Output Scaling Add learnable per-head scaling factors to attention, allowing different heads to contribute with different magnitudes to the output.\nValidation Loss: 3.2386\n- Learned Residual Connection Weights Add learnable scalar weights for each residual connection that are initialized to 1.0, allowing the model to learn optimal residual scaling during training.\nValidation Loss: 3.2517\n- Mixture of Embeddings with Position Learn to mix token embeddings and position embeddings with a content-dependent weight, allowing the model to dynamically balance positional vs semantic information per token.\nValidation Loss: 3.2497\n- Shared Input-Output Embedding with Learned Asymmetry Keep weight tying but add a small learned transformation on the output side, providing the benefits of weight tying while allowing output-specific adaptation.\nValidation Loss: 3.2499\n- Gated Final Normalization Replace the final RMSNorm before lm_head with a gated version where a learned gate controls how much normalization is applied vs passing the raw representation.\nValidation Loss: 3.2503\n- Position-Aware MLP Gating Gate the MLP output based on position information, allowing the model to learn position-dependent processing depth.\nValidation Loss: 3.2506\n- Learned Residual Connection Weights Add learnable scalar weights for each residual connection that are initialized to 1.0, allowing the model to learn optimal residual scaling during training.\nValidation Loss: 3.2517\n- Grouped Token Embeddings Group the vocabulary into clusters and add a learned embedding per cluster on top of token embeddings, providing hierarchical vocabulary structure.\nValidation Loss: 3.2521\n\nLastly, we present several interesting ideas on the nanoGPT environment that didn‚Äôt get successfully executed. These examples are generated by Claude-4.5-Opus.\n\n- Soft Layer Repetition Allow the model to softly repeat computation through layers by adding a learned gate that mixes the current layer‚Äôs input back into its output, simulating variable depth.\n- Causal Context Compression Before each attention layer, apply a learned compression that mixes local context (previous 2-3 tokens) into the current representation, providing implicit local context without convolutions.\n- Attention Head Specialization via Orthogonal Loss Add a soft penalty that encourages different attention heads to attend to different patterns by penalizing similarity between head outputs.\n- Skip Connections with Learned Residual Weights Combine skip connections with learned residual weights. The skip connections provide alternative gradient paths while learned weights allow adaptive scaling.\n- Token Difficulty-Aware Loss Weighting Weight the loss contribution of each token based on the model‚Äôs uncertainty (entropy) at that position, focusing learning on difficult tokens while not over-optimizing easy ones.\n\nA.3 Code Execution Examples\n\nWe present a few ideas with their full code execution to demonstrate the full end-to-end trajectories. All examples below are from Claude-4.5-Sonnet on the GRPO environment. For each example, we first present the natural language idea, followed by the code implementation generated by Claude-4.5-Sonnet.\n\n###### Example 1\n\n[Experiment] Create mathematical working memory simulation by maintaining a context buffer of mathematical facts, definitions, and intermediate results during problem solving. This buffer gets updated as the model works through problems and provides additional context for subsequent mathematical steps, simulating how humans maintain mathematical working memory during complex calculations.\n\n[Code Changes] Modify the prompt template in sample.py to include a ‚ÄúMathematical Context‚Äù section that accumulates relevant mathematical facts during problem solving. Implement context buffer management in train_loop in grpo.py that maintains recent mathematical definitions, formulas, and intermediate calculations across problems in the same batch. Update the buffer with key mathematical expressions extracted from each response. Use enhanced prompts that include relevant context from the buffer. Set -learning_rate 2.5e-5 and -loss_type reinforce_with_baseline.\n\n##### Validation Accuracy: 58.0\n\n‚¨á\n1 --- repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_43/sample.py\n2 +++ repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_43/sample.py\n3 @@ -2,6 +2,7 @@ from vllm import LLM, SamplingParams\n4 import json\n5 from tqdm import tqdm\n6 import random\n7 +import re\n8\n9 def load_prompt_template(prompt_path = \"prompts/r1_zero.prompt\"):\n10 with open(prompt_path, 'r') as f:\n11 @@ -20,12 +21,52 @@ def get_model_and_sampling_params(model_path = \"Qwen/Qwen2.5-Math-1.5B\", token\n12\n13 return llm, sampling_params\n14\n15 -def load_dataset_with_prompt_template(prompt_template, dataset_path=\"/juice5b/scr5b/nlp/aihinton/CS336_data/a5-alignment/MATH/test.jsonl\"):\n16 +def extract_mathematical_expressions(text):\n17 + \"\"\"Extract mathematical expressions, formulas, and key facts from text.\"\"\"\n18 + expressions = []\n19 +\n20 + # Extract LaTeX math expressions\n21 + latex_patterns = [\n22 + r'\\${^$}]+\\$', # Inline math\n23 + r'\\\\[a-zA-Z]+\\{[^^}]+\\}', # LaTeX commands\n24 + r'\\\\[frac\\{[^^}]+\\\\}{[^^}]+\\}', # Fractions\n25 + r'\\\\sqrt\\{[^^}]+\\}', # Square roots\n26 + r'[a-zA-Z]\\s*=\\s*[^,\\n\\.]+', # Variable assignments\n27 + ]\n28 +\n29 + for pattern in latex_patterns:\n30 + matches = re.findall(pattern, text)\n31 + expressions.extend(matches)\n32 +\n33 + # Extract definitions and key facts\n34 + definition_patterns = [\n35 + r'[A-Z][a-z]+ is defined as [^\\.]+\\.',\n36 + r'Let [a-zA-Z] be [^\\.]+\\.',\n\n37 + r'Given that [^\\.]+\\.',\n38 + r'Since [^\\.]+\\.',\n39 + r'Therefore [^\\.]+\\.',\n40 + ]\n41 +\n42 + for pattern in definition_patterns:\n43 + matches = re.findall(pattern, text)\n44 + expressions.extend(matches)\n45 +\n46 + return list(set(expressions)) # Remove duplicates\n47 +\n48 +def create_context_prompt(base_template, context_items):\n49 + \"\"\"Create a prompt with mathematical context.\"\"\"\n50 + if context_items:\n51 + context_section = \"Mathematical Context:\\n\" + \"\\n\".join(f\"- {item}\" for item in context_items[-10:]) + \"\\n\\n\"\n52 + return context_section + base_template\n53 + return base_template\n54 +\n55 +def load_dataset_with_prompt_template(prompt_template, dataset_path=\"/juice5b/scr5b/nlp/aihinton/CS336_data/a5-alignment/MATH/test.jsonl\", context_items= None):\n56 with open(dataset_path, 'r') as f:\n57 dataset = [json.loads(line) for line in f]\n58 print(f\"Loaded {len(dataset)} examples from {dataset_path}\")\n59\n60 - prompts = [prompt_template.format(question=example[\"problem\"]) for example in dataset]\n61 + enhanced_template = create_context_prompt(prompt_template, context_items)\n62 + prompts = [enhanced_template.format(question=example[\"problem\"]) for example in dataset]\n63 answers = [example[\"answer\"] for example in dataset]\n64 return prompts, answers\n65\n66 @@ -93,4 +134,42 @@\n67 if return_rewards:\n68 return batched_prompts, batched_answers, batched_responses, batched_rewards\n69 return results\n70 -\n71 +\n72 +class MathContextBuffer:\n73 + \"\"\"Buffer to maintain mathematical working memory across problems.\"\"\"\n74 +\n75 + def __init__(self, max_size=50):\n76 + self.buffer = []\n77 + self.max_size = max_size\n78 +\n79 + def add_expressions(self, expressions):\n80 + \"\"\"Add new mathematical expressions to the buffer.\"\"\"\n81 + for expr in expressions:\n82 + if expr and len(expr.strip()) > 3: # Filter out very short expressions\n83 + self.buffer.append(expr.strip())\n84 +\n85 + # Keep only the most recent items\n86 + if len(self.buffer) > self.max_size:\n87 + self.buffer = self.buffer[-self.max_size:]\n88 +\n89 + def get_relevant_context(self, query_text, max_items=10):\n90 + \"\"\"Get the most relevant context items for a given query.\"\"\"\n91 + if not self.buffer:\n92 + return []\n\n93 + # Simple relevance scoring based on shared mathematical terms\n94 + scored_items = []\n95 + query_lower = query_text.lower()\n96 +\n97 + for item in self.buffer[-20:]: # Consider recent items\n98 + score = sum(1 for word in item.lower().split() if word in query_lower )\n99 +\n100 + if score > 0:\n101 + scored_items.append((score, item))\n102 +\n103 + # Return top items sorted by relevance\n104 + scored_items.sort(reverse=True, key=lambda x: x[0])\n105 + return [item[1] for item in scored_items[:max_items]]\n106 +\n107 + def clear(self):\n108 + \"\"\"Clear the context buffer.\"\"\"\n109 + self.buffer = []\n110 ---- repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_43/grpo.py\n111 +++ repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_43/grpo.py\n112 @@ -1,6 +1,6 @@\n113 from transformers import AutoModelForCausalLM, AutoTokenizer\n114 from utils import tokenize_prompt_and_output, get_response_log_probs\n115 -from sample import load_prompt_template, load_dataset_with_prompt_template,\n116 sample_rollout\n117 +from sample import load_prompt_template, load_dataset_with_prompt_template,\n118 sample_rollout, MathContextBuffer, extract_mathematical_expressions,\n119 create_context_prompt\n120 from drgrpo_grader import rl_zero_reward_fn_train\n121 from evaluate import rl_zero_reward_fn_eval, evaluate_vllm\n122 from grpo_utils import compute_group_normalized_rewards,\n123 grpo_microbatch_train_step\n124 @@ -66,6 +66,9 @@ def evaluate_model(policy_model, vllm_model, eval_prompts,\n125 eval_answers, eval_s\n126 def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps,\n127 train_steps_per_rollout, output_dir, batch_size, gradient_accumulation_steps =\n128 4, group_size = 2, rollout_subset_size = 256, device = \"cuda\", logging_steps\n129 = 20, saving_steps = 4000, eval_epochs = 5, eval_prompts = None, eval_answers\n130 = None, sampling_params = None, eval_vllm_model = None, cliprange = 0.2,\n131 loss_type = \"reinforce_with_baseline\"):\n132 model.to(device)\n133 training_steps = grpo_steps\n134 +\n135 + # Initialize mathematical context buffer\n136 context_buffer = MathContextBuffer(max_size=100)\n137 optimizer = torch.optim.AdamW(model.params(), lr=learning_rate,\n138 weight_decay=0.0, betas=(0.9, 0.95))\n139 global_step = 0 # Initialize global step counter\n140 @@ -85,8 +88,31 @@ def train_loop(model, train_prompts, train_answers,\n141 learning_rate, grpo_steps,\n142 load_policy_into_vllm_instance(model, vllm_model)\n143\n144 ## sample rollouts\n145 + # Get enhanced prompts with mathematical context for this epoch\n146 print (\"Sampling rollouts for epoch: \", epoch)\n147 rollout_prompts, rollout_answers, rollout_responses, rollout_rewards =\n148 sample_rollout(vllm_model, rl_zero_reward_fn_train, train_prompts,\n149 train_answers, G=group_size, eval_sampling_params=eval_sampling_params,\n150 subset_size=rollout_subset_size, return_rewards=True, batch_size=512)\n151 +\n152 + # Create enhanced prompts with context for this batch\n153 enhanced_prompts = []\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n140 + base_template = load_prompt_template()\n141 +\n142 + # Select subset of problems for this epoch\n143 + if rollout Subset_size is not None:\n144 + indices = random.sample(range(len(train_prompts)), rollout Subset_size)\n145 + epoch_prompts = [train_prompts[i] for i in indices]\n146 + epoch_answers = [train_answers[i] for i in indices]\n147 + else:\n148 + epoch_prompts = train_prompts\n149 + epoch_answers = train_answers\n150 +\n151 + # Create context-enhanced prompts\n152 + for prompt in epoch_prompts:\n153 + relevant_context = context_buffer.get_relevant_context(prompt, max_items=8)\n154 + enhanced_prompt = create_context_prompt(base_template.format(question =prompt.split(\"Question:\")[-1]), relevant_context)\n155 + enhanced_prompts.append(enhanced_prompt)\n156 +\n157 + # Sample with enhanced prompts (need to adapt sample_rollout for direct prompt input)\n158 + rollout_prompts, rollout_answers, rollout_responses, rollout_rewards = sample_rollout_with_prompts(vllm_model, r1_zero_reward_fn_train, enhanced_prompts, epoch_answers, G=group_size, eval_sampling.params= eval_sampling.params, return_rewards=True, batch_size=512)\n159 +\n160 + # Randomly sample 2 rollouts to print\n161 indices = random.sample(range(len(rollout_prompts)), 2)\n162 print (\"Example rollouts:\")\n163 @@ -96,6 +122,15 @@ def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps,\n164 print(f\"Response: {rollout_responses[idx]})\")\n165 print(f\"Reward: {rollout_rewards[idx]})\")\n166 print(f\"Ground truth: {rollout_answers[idx]})\")\n167 +\n168 + # Update context buffer with mathematical expressions from responses\n169 + print(\"Updating mathematical context buffer...\")\n170 + for response in rollout_responses:\n171 + if rollout_rewards[rollout_responses.index(response)] &gt; 0: # Only add from correct responses\n172 + mathematical_expressions = extract_mathematical_expressions( response)\n173 + if mathematical_expressions:\n174 + context_buffer.add_expressions(mathematical_expressions)\n175 +\n176 rollout_tokenized = tokenize_prompt_and_output(rollout_prompts, rollout_responses, tokenizer)\n177 rollout_dataloader = create_dataloader(rollout_tokenized, batch_size= batch_size, shuffle=False)\n178\n179 @@ -196,6 +231,47 @@ def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps,\n180 }, step=global_step)\n181 # print (\"Global Step: \", global_step, \"Loss: \", loss_item(), \"Entropy: \", entropy.mean().item(), \"Clip fraction: \", metadata.get(\"clip_fraction\", 0.0))\n182\n183 +def sample_rollout_with_prompts(\n184 + vllm_model,\n185 + reward_fn,\n186 + prompts,\n187 + answers,\n\n## 188 + G,\n189 + eval_sampling_params,\n190 + return_rewards=False,\n191 + batch_size=64\n192 +):\n193 + \"\"\"Sample rollouts using provided prompts directly.\"\"\"\n194 + # Create batched prompts by repeating each prompt G times\n195 + batched_prompts = []\n196 + batched_answers = []\n197 + for prompt, answer in zip(prompts, answers):\n198 + batched_prompts.extend([prompt] * G)\n199 + batched_answers.extend([answer] * G)\n200 +\n201 + # Process in batches to avoid OOM\n202 + all_outputs = []\n203 + for i in range(0, len(batched_prompts), batch_size):\n204 + batch_prompts = batched_prompts[i:i + batch_size]\n205 + batch_outputs = vllm_model.generate(batch_prompts, eval_sampling_params)\n206 + all_outputs.extend(batch_outputs)\n207 +\n208 + # Process results\n209 + batched_responses = []\n210 + batched_rewards = []\n211 + total_rewards = 0\n212 +\n213 + for output, answer in tqdm(zip(all_outputs, batched_answers)):\n214 + generated_answer = output.outputs[0].text\n215 + reward = reward_fn(generated_answer, answer)\n216 + batched_responses.append(generated_answer)\n217 + batched_rewards.append(reward[\"reward\"])\n218 + total_rewards += reward[\"reward\"]\n219 +\n220 + print(f\"Accuracy of sampled rollouts: {total_rewards}/{len(batched_prompts)}\n221 = {total_rewards / len(batched_prompts) * 100}%\")\n222 +\n223 + return batched_prompts, batched_answers, batched_responses, batched_rewards\n224 +\n225 if __name__ == \"__main__\":\n226 parser = argparse.ArgumentParser()\n227 parser.add_argument (\"--model_path\", type=str, default=\"Qwen/Qwen2.5-Math-1.5B\")\n228 @@ -203,7 +279,7 @@ if __name__ == \"__main__\":\n229 parser.add_argument (\"--train_dataset_path\", type=str, default=\"../MATH/train.jsonl\")\n300 parser.add_argument (\"--eval_dataset_path\", type=str, default=\"../MATH/test.jsonl\")\n301 parser.add_argument (\"--output_dir\", type=str, default=\"ckpts/\")\n302 parser.add_argument (\"--learning_rate\", type=float, default=1e-5)\n303 parser.add_argument (\"--learning_rate\", type=float, default=2.5e-5)\n304 parser.add_argument (\"--grpo_steps\", type=int, default=200)\n305 parser.add_argument (\"--group_size\", type=int, default=8)\n306 parser.add_argument (\"--rollout_subset_size\", type=int, default=256)\n307 @@ -212,7 +288,7 @@ if __name__ == \"__main__\":\n308 parser.add_argument (\"--gradient_accumulation_steps\", type=int, default=16)\n309 parser.add_argument (\"--batch_size\", type=int, default=4)\n310 parser.add_argument (\"--cliprange\", type=float, default=0.2)\n311 parser.add_argument (\"--loss_type\", type=str, default=\"grpo_clip\")\n312 parser.add_argument (\"--loss_type\", type=str, default=\"reinforce_with_baseline\"\n313 parser.add_argument (\"--wandb_project\", type=str, default=\"grpo-math\")\n314 parser.add_argument (\"--wandb_name\", type=str, default=\"grpo_clip_1\")\n315 args = parser.parse_args()\n316 --- repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_43/run_job.sh\n\n## Towards Execution-Grounded Automated AI Research\n\n```txt\n246 +++ repo_variants_full_pipeline_bsz50_fixd2_epoch7/idea_43/run_job.sh\n247 @@ -21,7 +21,7 @@ timeout 2h uv run \\\n248 --index https://download.pytorch.org/whl/cu128 \\\n249 --index-strategy unsafe-best-match \\\n250 python grpo.py \\\n251 --learning_rate 1e-5 \\\n252 + --learning_rate 2.5e-5 \\\n253 --grpo_steps 20 \\\n254 --group_size 8 \\\n255 --rollout Subset_size 128 \\\n256 @@ -30,7 +30,7 @@ timeout 2h uv run \\\n257 --gradient Accumulation_steps 16 \\\n258 --batch_size 4 \\\n259 --cliprange 0.2 \\\n260 --loss_type grpo Clip \\\n261 + --loss_type reinforce_with_baseline \\\n262 --wandb_name $wandb_name\n263\n264 echo \"Experiment finished successfully!\"\n\n## Example 2\n\n[Experiment] Implement mathematical solution robustness training through systematic perturbation testing that teaches the model to solve mathematically equivalent problems with varied presentations, notation styles, and problem phrasings. Combine this with proven reward shaping by creating robustness-aware rewards that encourage mathematical understanding that generalizes across problem variations.\n\n[Code Changes] Create problem perturbation system in sample.py that generates equivalent mathematical problems with varied notation, different variable names, alternative problem phrasing, and equivalent mathematical formulations. Track solution consistency across perturbations. Enhance r1_zero_reward_fn_train in drgrpo_grader.py to reward robustness: robustness_bonus = 0.12 * consistency_score_across_perturbations where consistency measures solution method stability across equivalent problems. Apply generalization bonuses: +0.08 for solving problems with unfamiliar notation, +0.06 for handling varied problem presentations. Modify train_loop in grpo.py to include perturbation-based training batches and track robustness improvement. Use robustness-aware learning rate with cosine annealing from 3.4e-5 to 1.8e-5. Implement perturbation-complexity gradient clipping. Set -loss_type reinforce_with_baseline.\n\nValidation Accuracy: 56.2\n\nExample 2 code diff 1 --- repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_37/sample.py 2 +++ repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_37/sample.py 3 @@ -2,6 +2,7 @@ from vllm import LLM, SamplingParams 4 import json 5 from tqdm import tqdm 6 import random 7 +import re 8 9 def load_prompt_template(prompt_path = \"prompts/r1_zero.prompt\"): with open(prompt_path, 'r') as f: 10 @@ -29,6 +30,125 @@ def load_dataset_with_prompt_template(prompt_template, dataset_path=\"/juice5b/s answers = [example[\"answer\"] for example in dataset] return prompts, answers 11 12 +def create_notation_perturbations(problem_text): 13 + \"\"\"Create mathematical notation variations\"\"\" 14 + perturbations = [] 15 + # Variable substitutions 16 + var_substitutions = [ ('x', 'a'), ('y', 'b'), ('z', 'c'), ('n', 'm'), ('k', 'j'), ('p', 'q'), ('r', 's'), ('t', 'u'), ('v', 'w') ] 17 18 + # Create variable name variations 19 for old_var, new_var in var_substitutions: 20 + if old_var in problem_text: 21 + new_text = problem_text.replace(old_var, new_var) 22 + if new_text != problem_text: 23 + perturbations.append((\"notation_var\", new_text)) 24 + # Mathematical symbol variations 25 + symbol_substitutions = [ ('\\\\cdot', '\\times'), ('\\\\times', '\\cdot', ('\\\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n38 + ('\\le', '\\leq'),\n39 + ('\\ge', '\\geq'),\n40 + ]\n41 +\n42 + for old_sym, new_sym in symbol_substitutions:\n43 + if old_sym in problem_text:\n44 + new_text = problem_text.replace(old_sym, new_sym)\n45 + perturbations.append((\"notation_symbol\", new_text))\n46 +\n47 + return perturbations\n48 +\n49 +def create_phrasing_perturbations(problem_text):\n50 + \"\"\"Create alternative problem phrasings\"\"\"\n51 + perturbations = []\n52 +\n53 + # Common phrasing substitutions\n54 + phrasing_patterns = [\n55 + (r\"Find the value of\", \"Determine\"),\n56 + (r\"What is\", \"Calculate\"),\n57 + (r\"Solve for\", \"Find\"),\n58 + (r\"How many\", \"What is the number of\"),\n59 + (r\"Compute\", \"Find\"),\n60 + (r\"Evaluate\", \"Calculate\"),\n61 + ]\n62 +\n63 + for pattern, replacement in phrasing_patterns:\n64 + if re.search(pattern, problem_text, re.IGNORECASE):\n65 + new_text = re.sub(pattern, replacement, problem_text, flags=re.IGNORECASE)\n66 + if new_text != problem_text:\n67 + perturbations.append((\"phrasing\", new_text))\n68 +\n69 + return perturbations\n70 +\n71 +def create_formulation_perturbations(problem_text):\n72 + \"\"\"Create equivalent mathematical formulations\"\"\"\n73 + perturbations = []\n74 +\n75 + # Simple algebraic reformulations\n76 + reformulations = [\n77 + (r\"(\\w+) = (\\d+) + (\\d+)\", r\"\\1 - \\2 = \\3\"), # a = b + c -&gt; a - b = c\n78 + (r\"(\\w+) + (\\d+) = (\\d+)\", r\"\\1 = \\3 - \\2\"), # x + a = b -&gt; x = b - a\n79 + (r\"(\\d+) - (\\w+) = (\\d+)\", r\"\\2 = \\1 - \\3\"), # a - x = b -&gt; x = a - b\n80 + ]\n81 +\n82 + for pattern, replacement in reformulations:\n83 + if re.search(pattern, problem_text):\n84 + new_text = re.sub(pattern, replacement, problem_text)\n85 + if new_text != problem_text:\n86 + perturbations.append((\"formulation\", new_text))\n87 +\n88 + return perturbations\n89 +\n90 +def generate_problem_perturbations(problem_text, max_perturbations=3):\n91 + \"\"\"Generate various perturbations of a mathematical problem\"\"\"\n92 + all_perturbations = []\n93 +\n94 + # Generate different types of perturbations\n95 + all_perturbations extend(creationalnotation_perturbations(problem_text))\n96 + all_perturbations extend(creational_phrasing_perturbations(problem_text))\n97 + all_perturbations extend(creational_formulation_perturbations(problem_text))\n98 +\n99 + # Randomly sample up to max_perturbations\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n100 + if len(all_perturbations) &gt; max_perturbations:\n101 + all_perturbations = random.sample(all_perturbations, max_perturbations)\n102 +\n103 + return all_perturbations\n104 +\n105 +def sample_with_perturbations(prompts, answers, perturbation_ratio=0.3):\n106 + \"\"Sample problems with perturbations mixed in\"\"\n107 + perturbed_prompts = []\n108 + perturbed_answers = []\n109 + perturbation_metadata = []\n110 +\n111 + for prompt, answer in zip(prompts, answers):\n112 + # Always include original\n113 + perturbed_prompts.append(prompt)\n114 + perturbed_answers.append(answer)\n115 + perturbation_metadata.append({\"type\": \"original\", \"source_idx\": len(perturbed_prompts)-1})\n116 +\n117 + # Add perturbations with some probability\n118 + if random.random() &lt; perturbation_ratio:\n119 + # Extract problem text from prompt (assuming it's in a specific format)\n120 + problem_text = prompt.split(\"question:\")[-1] if \"question:\" in prompt else prompt\n121 + perturbations = generateProblem_perturbations(problem_text)\n122 +\n123 + for pert_type, pert_text in perturbations:\n124 + # Reconstruct full prompt with perturbed problem\n125 + pert_prompt = prompt.replace(problem_text, pert_text)\n126 + perturbed_prompts.append(pert_prompt)\n127 + perturbed_answers.append(answer) # Same answer for equivalent problem\n128 + perturbation_metadata.append({\n129 + \"type\": pert_type,\n130 + \"source_idx\": len(perturbed_prompts)-len(perturbations)-1\n131 + })\n132 +\n133 + return perturbed_prompts, perturbed_answers, perturbation_metadata\n134\n135 def sample_rollout(\n136 vllm_model,\n137 --- repo_variants_fullpipeline_bsz50_fixd2_epoch7/idea_37/drgrpo_grader.py\n138 +++ repo_variants_fullpipeline_bsz50_fixd2_epoch7/idea_37/drgrpo_grader.py\n139 @@ -1025,3 +1025,83 @@ def rl_zero_reward_fn_train(response, groundtruth, fast=True):\n140 \"reward\": 0.0\n141 }\n142\n143 +def compute_consistency_score(responses, groundtruth, perturbation_metadata):\n144 + \"\"Compute consistency score across perturbations\"\"\n145 + if not perturbation_metadata:\n146 + return 0.0\n147 +\n148 + # Group responses by source problem\n149 + source_groups = {}\n150 + for i, meta in enumerate(perturbation_metadata):\n151 + source_idx = meta.get(\"source_idx\", i)\n152 + if source_idx not in source_groups:\n153 + source_groups[source_idx] = []\n154 + source_groups[source_idx].append((i, responses[i], meta))\n155 +\n156 + total_consistency = 0.0\n157 + valid_groups = 0\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n158 + for source_idx, group_data in source_groups.items():\n160 + if len(group_data) &gt; 1: # Only consider groups with multiple responses\n161 + # Get correctness for each response in the group\n162 + correctness Scores = []\n163 + for _, response, meta in group_data:\n164 + reward_result = r1_zero_reward_fn_train(response, ground_truth)\n165 + correctness Scores.append(reward_result[\"reward\"])\n166 +\n167 + # Consistency is measured as how often the model gets the same\n168 + correctness scores:\n169 + # If all responses have same correctness (all correct or all\n170 + incorrect), high consistency\n171 + unique Scores = set(correctness Scores)\n172 + if len(unique Scores) == 1:\n173 + consistency = 1.0\n174 + else:\n175 + # Partial consistency based on agreement\n176 + most_common = max(set(correctness Scores), key=\n177 + correctness Scores.count)\n178 + consistency = correctness Scores.count(most_common) / len(\n179 + correctness Scores)\n180 +\n181 + if valid_groups == 0:\n182 + return 0.0\n183 +\n184 + return total_consistency / valid_groups\n185 +\n186 + def r1_zero_reward_fn_train_robust(response, ground_truth, perturbation_metadata=\n187 + None, responses_batch=None, fast=True):\n188 + \"\"Enhanced reward function with robustness bonuses\"\"\n189 + # Get base reward\n190 + base Reward = r1_zero Reward_fn_train(response, ground_truth, fast)\n191 +\n192 + # Initialize bonus components\n193 + robustness_bonus = 0.0\n194 + notation_bonus = 0.0\n195 + presentation_bonus = 0.0\n196 +\n197 + # Compute robustness bonus if perturbation data available\n198 + if perturbation_metadata and responses_batch:\n199 + consistency_score = compute_consistency_score(responses_batch,\n200 + ground_truth, perturbation_metadata)\n201 + robustness_bonus = 0.12 * consistency_score\n202 +\n203 + # Check for notation and presentation bonuses from metadata\n204 + if perturbation_metadata:\n205 + current_meta = perturbation_metadata[0] if perturbation_metadata else {}\n206 + pert_type = current_meta.get(\"type\", \"original\")\n207 +\n208 + if pert_type in [\"notation_var\", \"notation_symbol\"] and base Reward[\"reward\"] &gt; 0:\n209 + notation_bonus = 0.08\n210 +\n211 + if pert_type in [\"phrasing\", \"formulation\"] and base Reward[\"reward\"] &gt; 0:\n212 + presentation_bonus = 0.06\n213 +\n214 + # Combine all rewards\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n213 + total Reward = base Reward[\"reward\"] + robustness_bonus + notation_bonus + presentation_bonus\n214 +\n215 + return {\n216 + \"format Reward\": base Reward[\"format Reward\"],\n217 + \"answer Reward\": base Reward[\"answer Reward\"],\n218 + \"robustness_bonus\": robustness_bonus,\n219 + \"notation_bonus\": notation_bonus,\n220 + \"presentation_bonus\": presentation_bonus,\n221 + \"reward\": total Reward\n222 + }\n223 --- repo_variants_full_pipeline_bsz50_fixd2_epoch7/idea_37/grpo.py\n224 +++ repo_variants_full_pipeline_bsz50_fixd2_epoch7/idea_37/grpo.py\n225 @@ -1,7 +1,7 @@\n226 from transformers import AutoModelForCausalLM, AutoTokenizer\n227 from utils import tokenize_prompt_and_output, get_response_log_probs\n228 -from sample import load_prompt_template, load_dataset_with_prompt_template, sample_rollout\n229 -from drgrpo_grader import r1_zero Reward_fn_train\n230 +from sample import load_prompt_template, load_dataset_with_prompt_template, sample_rollout, sample_with_perturbations\n231 +from drgrpo_grader import r1_zero Reward_fn_train, r1_zero Reward_fn_train_robust\n232 from evaluate import r1_zero Reward_fn_eval, evaluate_vllm\n233 from grpo_utils import compute_group_normalized Rewards, grpo_microbatch_train_step\n234 from torch.utils.data import DataLoader, Dataset\n235 @@ -12,6 +12,7 @@ from tqdm import tqdm\n236 from vllm import LLM, SamplingParams\n237 import wandb\n238 import random\n239 +import math\n240\n241 def load_policyInto_vllm_instance(policy, llm):\nstate_dict = policy.state_dict()\n@@ -63,11 +64,23 @@\nmetrics = evaluate_vllm(vllm_model, r1_zero Reward_fn_eval, eval_prompts, eval_answers, eval_sampling.params, output_path=output_path)\nreturn metrics\n246\n247 -def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps, train_steps_per_rollout, output_dir, batch_size, gradient Accumulation_steps = 4, group_size = 2, rollout Subset_size = 256, device = \"cuda\", logging_steps = 20, saving_steps = 4000, eval_epochs = 5, eval_prompts = None, eval_answers = None, sampling.params = None, eval_vllm_model = None, cliprange = 0.2, loss_type = \"reinforce_with_baseline\"):\n248 +def get_cosine_annealing_lr(step, total_steps, lr_max=3.4e-5, lr_min=1.8e-5):\n249 + \"\"Cosine annealing learning rate schedule\"\"\n250 + return lr_min + (lr_max - lr_min) * 0.5 * (1 + math.cos(math.pi * step / total_steps))\n251 +\n252 +def compute_perturbation_complexity Clip_norm(model, complexity_factor=1.0):\n253 + \"\"Compute gradient clipping norm based on perturbation complexity\"\"\n254 + base Clip_norm = 1.0\n255 + return base Clip_norm * complexity_factor\n256 +\n257 +def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps, train_steps_per_rollout, output_dir, batch_size, gradient Accumulation_steps = 4, group_size = 2, rollout Subset_size = 256, device = \"cuda\", logging_steps = 20, saving_steps = 4000, eval_epochs = 5, eval_prompts = None, eval_answers = None, sampling.params = None, eval_vllm_model = None, cliprange = 0.2, loss_type = \"reinforce_with_baseline\", perturbation_ratio = 0.3):\nmodel.to(device)\n258 training_steps = grpo_steps\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n260 - optimizer = torch.optim.AdamW(model.params(), lr=learning_rate, weight Decay=0.0, betas=(0.9, 0.95))\n261 + # Start with robustness-aware learning rate\n262 + initial_lr = get_cosine_annealing_lr(0, grpo_steps)\n263 + optimizer = torch.optim.AdamW(model.params(), lr=initial_lr, weight Decay=0.0, betas=(0.9, 0.95))\n264 global_step = 0 # Initialize global step counter\n265 + robustness Scores = []\n266\n267 for epoch in range(grpo_steps):\n268 model.train()\n269 @@ -82,21 +95,50 @@ def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps,\n270\n271 model.train()\n272\n273 + # Update learning rate with cosine annealing\n274 + current_lr = get_cosine_annealing_lr(epoch, grpo_steps)\n275 + for param_group in optimizer param_groups:\n276 + param_group['lr'] = current_lr\n277 +\n278 ## load the current policy model to vllm for sampling rollouts\n279 load_policyInto_vllm_instance(model, vllm_model)\n280\n281 + # Sample with perturbations for robustness training\n282 + perturbed_prompts, perturbed_answers, perturbation_metadata = sample_with_perturbations(\n283 + train_prompts, train_answers, perturbation_ratio=perturbation_ratio\n284 + )\n285 +\n286 ## sample rollouts\n287 print (\"Sampling rollouts for epoch: \", epoch)\n288 - rollout_prompts, rollout_answers, rollout_responses, rollout_rewards = sample_rollout(vllm_model, rl_zero_reward_fn_train, train_prompts, train_answers, G=group_size, eval_sampling.params=eval_sampling.params, subset_size=rollout Subset_size, return_rewards=True, batch_size=512)\n289 +\n290 + # Use subset of perturbed prompts for training\n291 + subset_size = min(rollout Subset_size, len(perturbed_prompts))\n292 + if subset_size &lt; len(perturbed_prompts):\n293 + indices = random.sample(range(len(perturbed_prompts)), subset_size)\n294 + subset_prompts = [perturbed_prompts[i] for i in indices]\n295 + subset_answers = [perturbed_answers[i] for i in indices]\n296 + subset_metadata = [perturbation_metadata[i] for i in indices]\n297 + else:\n298 + subset_prompts = perturbed_prompts\n299 + subset_answers = perturbed_answers\n300 + subset_metadata = perturbation_metadata\n301 +\n302 + rollout_prompts, rollout_answers, rollout_responses, rollout_rewards = sample_rollout(\n303 + vllm_model, rl_zero_reward_fn_train, subset_prompts, subset_answers,\n304 + G=group_size, eval_sampling.params=eval_sampling.params,\n305 + subset_size=None, return_rewards=True, batch_size=512\n306 + )\n307 +\n308 # Randomly sample 2 rollouts to print\n309 indices = random.sample(range(len(rollout_prompts)), 2)\n310 print (\"Example rollouts:\")\n311 for idx in indices:\n312 print(f\"\\nRollout {idx}:\")\n313 - print(f\"Prompt: {rollout_prompts[idx]})\"\n314 + print(f\"Prompt: {rollout_prompts[idx][:200]}...\")\n\nprint(f\"Response: {rollout_responses[idx]}\")print(f\"Reward: {rollout_rewards[idx]}\")print(f\"Ground truth: {rollout_answers[idx]}\")print(f\"Ground truth: {rollout_answers[idx][:100]}...\")rollout_tokenized = tokenize_prompt_and_output(rollout_prompts, rollout_responses, tokenizer)rollout_data_loader = create_data_loader(rollout_tokenized, batch_size= batch_size, shuffle=False)\n@@ -126,15 +168,32 @@ def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps,\n\n# Compute advantages using group normalization - no gradients needed with torch.no_grad():\nadvantages, raw_rewards, metadata = compute_group_normalized_rewards(reward_fn=r1_zero_reward_fn_train, # Create enhanced reward function with robustness def robust_reward_fn(response, ground_truth): # Find corresponding metadata for this response response_idx = rollout_responses.index(response) if response in rollout_responses else 0 meta_idx = min(response_idx, len(subset_metadata) - 1) current_meta = [subset_metadata[meta_idx]] if subset_metadata else None return r1_zero_reward_fn_train_robust( response, ground_truth, perturbation_metadata=current_meta, responses_batch=rollout_responses ) advantages, raw_rewards, metadata = compute_group_normalized_rewards(reward_fn=robust_reward_fn, rollout_responses=rollout_responses, repeated_ground_truths=rollout_answers, group_size=group_size, advantage_eps=1e-6, normalize_by_std=True) advantages = advantages.to(device) # Track robustness improvement current_robustness = metadata.get('mean_reward', 0.0) robustness_scores.append(current_robustness) # Log raw rewards statistics print(\"\\nGRPO epoch: \", epoch) @-145,11 +204,20 @@ wandb.log({ \"eval/mean_reward\": eval_mean_reward, \"train/mean_reward\": metadata[\"mean_reward\"], \"train/learning_rate\": current_lr, \"train/robustness_score\": current_robustness, }, step=global_step) else: wandb.log({ \"train/mean_reward\": metadata[\"mean_reward\"], \"train/learning_rate\": current_lr, \"train/robustness_score\": current_robustness, }, step=global_step) # Compute perturbation complexity for gradient clipping\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n373 + perturbation_types = set(meta.get(\"type\", \"original\") for meta in subset_metadata)\n374 + complexity_factor = 1.0 + 0.1 * len(perturbation_types) # More complex with more perturbation types\n375 + clip_norm = compute_perturbation_complexity Clip_norm(model, complexity_factor)\n376\n377\n378 ## train on this rollout batch for train_steps_per_rollout steps\n379 @ -185,6 +252,9 @ def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps,\n380 )\n381\n382 if (batch_idx + 1) % gradient Accumulation_steps == 0:\n383 + # Apply perturbation-complexity gradient clipping\n384 + torch.nn.utilsclip_grad_norm_(model.params(), clip_norm)\n385 +\n386 optimizer.step()\n387 optimizer.zero_grad()\n388\n389 @ -211,7 +281,7 @ if __name__ == \"__main__\":\n390 parser.add_argument(\"-gradient Accumulation_steps\", type=int, default=16)\n391 parser.add_argument(\"-batch_size\", type=int, default=4)\n392 parser.add_argument(\"-cliprange\", type=float, default=0.2)\n393 - parser.add_argument(\"-loss_type\", type=str, default=\"grpo Clip\")\n394 + parser.add_argument(\"-loss_type\", type=str, default=\"reinforce_with_baseline\")\n395 parser.add_argument(\"-wandb_project\", type=str, default=\"grpo-math\")\n396 parser.add_argument(\"-wandb_name\", type=str, default=\"grpo Clip_1\")\n397 args = parser.parse_args()\n398 @ -266,7 +336,8 @ if __name__ == \"__main__\":\n399 sampling.params=eval_sampling.params,\n400 eval_vllm_model=vllm_model,\n401 cliprange=args.cliprange,\n402 - loss_type=args.loss_type\n403 + loss_type=args.loss_type,\n404 + perturbation_ratio=0.3\n\n## Towards Execution-Grounded Automated AI Research\n\n# Example 3\n\n[Experiment] Implement token-level reward attribution by using attention weights to identify which input tokens contributed most to correct answers, then amplifying the gradient updates for those tokens during policy gradient training.\n\n[Code Changes] Modify get_response_log_probs in utils.py to also return attention weights from the last layer. In grpo_microbatch_train_step, compute token importance scores by averaging attention weights across heads, then multiply the policy gradient loss by (1 + importance_score) for tokens with high attention to mathematical content.\n\n# Validation Accuracy: 45.2\n\n## Example 3 code diff\n```python\n1 --- repo_variants_fullpipeline_bsz50_fixd2_epoch1/idea_32/utils.py\n2 ++ repo_variants_fullpipeline_bsz50_fixd2_epoch1/idea_32/utils.py\n3 @@ -41,12 +41,13 @@ def compute_entropy(logits):\n4 entropy = -torch.sum(probs * log_probs, dim=-1)\n5 return entropy\n6\n7 -def get_response_log_probs(model, input_ids, labels, return_token_entropy=False, no_grad=True):\n8 +def get_response_log_probs(model, input_ids, labels, return_token_entropy=False, returnattention=False, no_grad=True):\n9 if no_grad:\n10 with torch.no_grad():\n11 - outputs = model(input_ids, labels=labels)\n12 + outputs = model(input_ids, labels=labels, output attentions= returnattention)\n13 logits = outputs.logits # (batch_size, seq_len, vocab_size)\n14 log_probs = torch.log softmax(logits, dim=-1) # (batch_size, seq_len, vocab_size)\n15 + attentions = outputs attentions if returnattention else None\n16 # Get log probs of the actual label tokens\n17 batch_size, seq_len = labels.shape # (batch_size, seq_len)\n18 log_probs = torch.gather(log_probs, dim=-1, index=labels.unsqueeze (-1)).squeeze(-1)\n19 @@ -55,8 +56,9 @@ def get_response_log_probs(model, input_ids, labels, return_token_entropy=False\n20 else:\n21 entropy = None\n22 else:\n23 - outputs = model(input_ids, labels=labels)\n24 + outputs = model(input_ids, labels=labels, output attentions= returnattention)\n25 logits = outputs.logits # (batch_size, seq_len, vocab_size)\n26 + attentions = outputs attentions if returnattention else None\n27 log_probs = torch.log softmax(logits, dim=-1) # (batch_size, seq_len, vocab_size)\n28 # Get log probs of the actual label tokens\n29 batch_size, seq_len = labels.shape # (batch_size, seq_len)\n30 @@ -65,9 +67,17 @@ def get_response_log_probs(model, input_ids, labels, return_token_entropy=False\n31 entropy = compute_entropy(logits)\n32 else:\n33 entropy = None\n34 -\n35 - return {\n36 +\n37 + result = {\n38 \"log_probs\": log_probs,\n\n## Towards Execution-Grounded Automated AI Research\n\n```txt\n39 \"token_entropy\": entropy\n40 }\n41 + if returnattention and attentions is not None:\n42 + # Return attention weights from the last layer, averaged across heads\n43 + last_layerattention = attentions[-1] # Shape: (batch_size, num_heads, seq_len, seq_len)\n44 + averagedattention = last_layerattention.mean(dim=1) # Average across heads: (batch_size, seq_len, seq_len)\n45 + result[\"attentionweights\"] = averagedattention\n46 + return result\n47\n48\n49\n50 --- repo_variants_fullpipeline_bsz50_fixd2_epoch1/idea_32/grpo_utils.py\n51 +++ repo_variants_fullpipeline_bsz50_fixd2_epoch1/idea_32/grpo_utils.py\n52 @@ -170,6 +170,7 @@ def grpo_microbatch_train_step(\n53 advantages: torch.Tensor | None = None,\n54 old_log_probs: torch.Tensor | None = None,\n55 cliprange: float | None = None,\n56 + attentionweights: torch.Tensor | None = None,\n57 ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n58 ...\n## 59 Return:\n60 @@ -180,6 +181,20 @@ def grpo_microbatch_train_step(\n## 61 You should call loss.backup() in this function. Make sure to adjust for gradient accumulation.\n62 ...\n63 loss, metadata = compute_policy-gradient_loss(policy_log_probs, loss_type, raw_rewards, advantages, old_log_probs, cliprange) # (batch_size, sequence_length)\n64 +\n65 + # Apply attention-based weighting if attention weights are provided\n66 + if attentionweights is not None:\n67 + # Compute importance scores by averaging attention weights to earlier tokens\n68 + # attentionweights shape: (batch_size, seq_len, seq_len)\n69 + # For each token, sum attention weights to all previous tokens (including itself)\n70 + importance Scores = torch.sum(attentionweights, dim=-1) # (batch_size, seq_len)\n71 +\n72 + #Normalize importance scores to [0, 1] range per sequence\n73 + importance Scores = importance Scores / (importance Scores.max(dim=-1, keepdim=True) [0] + 1e-8)\n74 +\n75 + # Amplify loss for tokens with high importance: multiply by (1 + importance Score)\n76 + loss = loss * (1.0 + importance Scores)\n77 +\n78 loss = masked_mean(loss, response_mask)\n79 loss = loss / gradient Accumulation_steps\n80 loss.backup()\n81 --- repo_variants_fullpipeline_bsz50_fixd2_epoch1/idea_32/grpo.py\n82 +++ repo_variants_fullpipeline_bsz50_fixd2_epoch1/idea_32/grpo.py\n83 @@ -109,6 +109,7 @@ def train_loop(model, train_prompts, train Answers, learning_rate, grpo_steps,\n84 model,\n85 input_ids,\n86 labels,\n87 return_token_entropy=False,\n88 + returnattention=False,\n89 no_grad=True\n\n## Towards Execution-Grounded Automated AI Research\n\n```txt\n91 @ -163,11 +164,13 @\n92 model,\n93 input_ids,\n94 labels,\n95 return_token_entropy=True,\n96 + returnattention=True,\n97 no_grad=False\n98 )\n99 policy_log_probs = response_log_probs[\"log_probs\"]\n100 entropy = response_log_probs[\"token_entropy\"]\n101 + attentionweights = response_log_probs.get(\"attentionweights\")\n102\n103 # Calculate data index for advantages/old_log_probs\n104 batch_idx_total = batch_idx * batch_size\n105 @ -177,7 +180,8 @ def train_loop(model, train_prompts, train_answers,\n106 learning_rate, grpo_steps,\n107 loss_type=loss_type,\n108 advantages=batch Advantages,\n109 old_log_probs=batch_old_log_probs,\n110 - cliprange=cliprange\n111 + cliprange=cliprange,\n112 + attentionweights=attentionweights\n113 )\n114 if (batch_idx + 1) % gradient Accumulation_steps == 0:\n\n## Towards Execution-Grounded Automated AI Research\n\n# Example 4\n\n[Experiment] Implement response diversity rewards within groups where responses to the same prompt receive bonus rewards (0.05-0.15) for being dissimilar to other responses in their group, encouraging exploration of different solution paths while maintaining the proven group_size=8 and 3e-5 learning rate combination.\n\n[Code Changes] Modify compute_group_normalized_rewards in grpo_utils.py to compute pairwise similarity between responses in each group using token-level Jaccard similarity. Add diversity bonus: diversity_reward = 0.15 * (1 - max_similarity_in_group) to each response's reward before advantage computation. Reshape responses into groups, compute similarities, and add bonuses before advantage normalization. Set -learning_rate 3e-5, -loss_type reinforce_with_baseline, -group_size 8.\n\n# Validation Accuracy: 19.2\n\n## Example 4 code diff\n```python\n1 --- repo_variants_fullpipeline_bsz50_fixd2_epoch3/idea_22/grpo_utils.py\n2 ++ repo_variants_fullpipeline_bsz50_fixd2_epoch3/idea_22/grpo_utils.py\n3 @ -1,5 +1,25 @\n4 import torch\n5 from typing import Literal\n6\n7 +def compute_jaccard_similarity(response1: str, response2: str) -&gt; float:\n8 + \"\"\n9 + Compute token-level Jaccard similarity between two responses.\n10 + Jaccard similarity = |intersection| / |union|\n11 + \"\"\n12 + # Simple tokenization by splitting on whitespace\n13 + tokens1 = set(response1.split())\n14 + tokens2 = set(response2.split())\n15 +\n16 + if len(tokens1) == 0 and len(tokens2) == 0:\n17 + return 1.0\n18 +\n19 + intersection = tokens1.intersection(tokens2)\n20 + union = tokens1.union(tokens2)\n21 +\n22 + if len(union) == 0:\n23 + return 1.0\n24 +\n25 + return len(intersection) / len(union)\n26 +\n27 def compute_group_normalized_rewards(\n28 reward_fn,\n29 @ -30,31 +49,62 @\n30 raw_rewards.append(reward_dict[\"reward\"])\n31 raw_rewards = torch.tensor(raw_rewards)\n32\n33 + # Compute diversity bonuses within each group\n34 n_groups = len(rollout_responses) // group_size\n35 + diversity_bonuses = []\n36 +\n37 + for group_idx in range(n_groups):\n38 + group_start = group_idx * group_size\n39 + group_end = group_start + group_size\n40 + group_responses = rollout_responses[group_start:group_end]\n41 +\n42 + # Compute pairwise similarities within the group\n43 + for i in range(group_size):\n44 + max_similarity = 0.0\n45 + for j in range(group_size):\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n46 + if i != j:\n47 + similarity = compute_jaccard_similarity(group_responses[i], group_responses[j])\n48 + max_similarity = max(max_similarity, similarity)\n49 +\n50 + # Diversity bonus: higher reward for more dissimilar responses\n51 + diversity_bonus = 0.15 * (1 - max_similarity)\n52 + diversity_bonuses.append(diversity_bonus)\n53 +\n54 + diversity_bonuses = torch.tensor(diversity_bonuses)\n55 +\n56 # Reshape rewards into groups\n57 n_groups = len(raw_rewards) // group_size\n58 grouped_rewards = raw_rewards.view(n_groups, group_size)\n59 + grouped_diversity_bonuses = diversity_bonuses.view(n_groups, group_size)\n60 +\n61 + # Add diversity bonuses to raw rewards before advantage computation\n62 + grouped_rewards_with_diversity = grouped_rewards + grouped_diversity_bonuses\n63\n64 # Compute group statistics\n65 - group_means = grouped_rewards.mean(dim=1, keepdim=True)\n66 + group_means = grouped_rewards_with_diversity.mean(dim=1, keepdim=True)\n67 if normalize_by_std:\n68 - group_std = grouped_rewards.std(dim=1, keepdim=True) + advantage_eps\n69 - advantages = (grouped_rewards - group_means) / group_std\n70 + group_std = grouped_rewards_with_diversity.std(dim=1, keepdim=True) + advantage_eps\n71 + advantages = (grouped_rewards_with_diversity - group_means) / group_std\n72 else:\n73 - advantages = grouped_rewards - group_means\n74 + advantages = grouped_rewards_with_diversity - group_means\n75\n76 # Flatten advantages back to original shape\n77 advantages = advantages.view(-1)\n78 +\n79 + # Update raw rewards to include diversity bonuses for metadata\n80 + raw_rewards_with_diversity = raw_rewards + diversity_bonuses\n81\n82 # Compute metadata statistics\n83 metadata = {\n84 - \"mean Reward\": raw_rewards.mean().item(),\n85 - \"std Reward\": raw_rewards.std().item(),\n86 - \"max Reward\": raw_rewards.max().item(),\n87 - \"min Reward\": raw_rewards.min().item(),\n88 + \"mean Reward\": raw_rewards_with_diversity.mean().item(),\n89 + \"std Reward\": raw_rewards_with_diversity.std().item(),\n90 + \"max Reward\": raw_rewards_with_diversity.max().item(),\n91 + \"min Reward\": raw_rewards_with_diversity.min().item(),\n92 \"mean Advantage\": advantages.mean().item(),\n93 \"std Advantage\": advantages.std().item(),\n94 + \"mean_diversity_bonus\": diversity_bonuses.mean().item(),\n95 }\n96\n97 - return advantages, raw_rewards, metadata\n98 + return advantages, raw_rewards_with_diversity, metadata\n99\n100 def compute_naive_policy_gradient_loss(\n101 --- repo_variants_fullpipeline_bsz50_fixed2_epoch3/idea_22/grpo.py\n102 +++ repo_variants_fullpipeline_bsz50_fixed2_epoch3/idea_22/grpo.py\n103 @@ -203,6 +203,6 @@\n104 parser.add_argument (\"--eval_dataset_path\", type=str, default=\"../MATH/test.json1\")\n105 parser.add_argument (\"--output_dir\", type=str, default=\"ckpts/\")\n\n## Towards Execution-Grounded Automated AI Research\n\n```txt\n106 - parser.add_argument (\"--learning_rate\", type  $=$  float, default  $= 1\\mathrm{e} - 5$\n107 + parser.add_argument (\"--learning_rate\", type  $=$  float, default  $= 3\\mathrm{e} - 5$\n108 parser.add_argument (\"--grpco_steps\", type  $=$  int, default  $= 200$\n109 parser.add_argument (\"--group_size\", type  $=$  int, default  $= 8$\n110 parser.add_argument (\"--rollout Subset_size\", type  $=$  int, default  $= 256$\n111 @@ -212,7 +212,7 @@ if __name__ == \"_main_:\n112 parser.add_argument (\"--gradient Accumulation_steps\", type  $=$  int, default  $= 16$\n113 parser.add_argument (\"--batch_size\", type  $=$  int, default  $= 4$\n114 parser.add_argument (\"--cliprange\", type  $=$  float, default  $= 0.2$\n115 - parser.add_argument (\"--loss_type\", type  $=$  str, default  $=$  \"grpo Clip\")\n116 + parser.add_argument (\"--loss_type\", type  $=$  str, default  $=$  \"reinforce_withÂü∫Á§é\")\n117 parser.add_argument (\"--wandb_project\", type  $=$  str, default  $=$  \"grpo-math\")\n118 parser.add_argument (\"--wandb_name\", type  $=$  str, default  $=$  \"grpo Clip_1\")\n119 args = parser.parse_args()\n120 -- repo_variants_fullpipeline_bsz50_fixd2_epoch3/idea_22/run_job.sh\n121 ++ repo_variants_fullpipeline_bsz50_fixd2_epoch3/idea_22/run_job.sh\n122 @@ -21,7 +21,7 @@ timeout 2h uv run \\\n123 --index https://download.pytorch.org/whl/cu128 \\\n124 --index-strategy unsafe-best-match \\\n125 python grpo.py \\\n126 - --learning_rate 1e-5 \\\n127 + --learning_rate 3e-5 \\\n128 --grpo_steps 20 \\\n129 --group_size 8 \\\n130 --rollout Subset_size 128 \\\n131 @@ -30,7 +30,7 @@ timeout 2h uv run \\\n132 --gradient Accumulation_steps 16 \\\n133 --batch_size 4 \\\n134 --cliprange 0.2 \\\n135 - --loss_type grpo Clip \\\n136 + --loss_type reinforce_withÂü∫Á§é \\\n137 --wandb_name $wandb_name\n138\n139 echo \"Experiment finished successfully!\"\n", "source": "inbox/2601.14525v1.pdf"}, "output_data": {"core_contribution": "The core contribution of the study is the development of an execution-grounded system for automated AI research, where high-throughput execution feedback informs and improves the ideation process of LLMs. This is achieved by translating generated research ideas into executable code, running the experiments using an automated executor, and analyzing the results using two learning methodologies: evolutionary search and reinforcement learning (RL). The work is categorized as a 'System' contribution, as it integrates different components to form a cohesive automated research framework.", "method_breakdown": "The system comprises an Implementer, Scheduler, and Worker. An Implementer is responsible for generating code from natural language research ideas by producing a code diff for each idea. The Scheduler manages resources and schedules the execution of experiments based on the available computational resources and priorities. The Worker node executes the experiments on GPU clusters, monitoring for completion and collecting results. The feedback loop from execution results is used to inform LLMs on refining their ideation processes.", "subsystems_parts": "1. Implementer: Converts natural language ideas into code.\n2. Scheduler: Manages execution scheduling based on computational resource availability.\n3. Worker: Executes experiments and collects results on GPU clusters.\n4. Automated Executor: Core system integrating the above components to execute and analyze ideas.", "interactions": "The Implementer generates code diffs for ideas, which are submitted to the Scheduler. The Scheduler allocates GPU resources to the Worker, which runs the experiments. Upon completion, the Worker sends the results back, completing the loop by providing execution feedback that guides future ideation processes, particularly through evolutionary search and reinforcement learning.", "delta_vs_baseline": "Unlike traditional manual research processes, this system automates the ideation-to-execution pipeline, enabling high-throughput execution feedback that better informs the learning of LLMs. Current baselines do not utilize an automated executor or integrate execution feedback into the learning loop efficiently.", "evidence_anchor": "The execution-guided evolutionary search mechanism demonstrates its effectiveness over baselines, especially in tasks like model post-training (69.4% validation accuracy vs. a baseline of 48.0%) and pre-training (19.7 minutes to target loss vs. 35.9 minutes baseline), validating the system's efficiency in improving AI research outputs.", "transferability": "The system is designed to be adaptable across different AI research scenarios, particularly those involving code generation and execution tasks. It shows promise in improving efficiency in open-ended research problems but may need modifications for non-LLM or highly-specialized domains due to variability in execution needs and feedback mechanisms."}, "conversation_log": ["Generator: {\"core_diagram_summary\":\"Figure 1 illustrates the components of the automated executor system which includes an Implementer, Scheduler, and Worker to automate the execution of ideas for AI research. Figure 2 showcases the performance comparison with self-execution vs GPT-5 execution for GRPO and nanoGPT environments, highlighting completion rates and model performance improvements.\",\"unique_move\":\"The novel integration of an automated executor system that translates natural language research ideas into code, executes them, and utilizes execution feedback to guide learning through evolutionary search or reinforcement learning is new.\",\"distilled_recipe\":\"Design automated executor ‚Üí Implement ideas in code ‚Üí Execute and collect feedback ‚Üí Use feedback to guide evolutionary search or RL learning.\",\"contribution_type\":\"System\",\"evidence_anchor\":\"Execution-guided evolutionary search significantly outperformed the baselines for post-training (69.4% vs 48.0%) and pre-training tasks (19.7 min vs 35.9 min), proving the efficiency of the method.\"}", "Decider 1: {\"scratchpad_updates\":\"1. Technical Core Claims: The analysis adequately highlights the core claim of utilizing an automated executor in AI research to enhance idea generation and execution based on feedback. However, the dependency on specific LLMs can introduce bias, and its impact on reproducibility across diverse AI research settings may need further discussion. 2. Completeness of the Distilled Recipe: The distilled recipe captures the essential steps in the automated AI research system but could benefit from a more detailed description of each component, particularly how execution feedback is integrated into the evolutionary search or RL processes. Adding detailed steps or examples for translation of ideas into executable code and evaluation metrics could enrich understanding. 3. Contribution Type Classification: The classification of 'System' seems apt, given that the research presents a novel operational framework rather than an isolated methodological advancement. However, it would be beneficial to clarify if there are theoretical contributions or methodological insights that expand beyond the implemented system. 4. Validity of the Evidence Anchor Selection: The chosen evidence anchors provide strong, quantifiable outcomes that validate the efficiency of execution-guided search methods. Nevertheless, clarity can be enhanced by detailing the conditions under which these results are replicated or vary and distinguishing between improvements attributable to system architecture versus novel algorithms. 5. Diagram Summary: Diagrams are clear but can be enriched with annotations explaining key components or interactions. Particularly, Figure 1 could include detail on how feedback loops into the system and Figure 2 might benefit from explanations of performance criteria used in assessments.\",\"response_to_researcher\":\"The exploration of an automated idea executor and its integration with LLM for improved AI research is promising. However, several aspects merit deeper investigation: 1. Integration Challenges: What challenges or limitations were encountered integrating the executor with varying LLMs or within different research domains? Gathering comprehensive insights into compatibility issues or domain-specific hurdles could provide valuable guidance for practical applications. 2. Scalability and Generalizability: Can you explore scalability beyond the current research environments and identify generalizability across different research problems or larger datasets? Insights here are crucial for applying the framework to real-world scenarios. 3. Execution Feedback Utility: How does the system discern useful execution feedback from noise, especially in diverse or incomplete datasets? Detailing mechanisms that refine execution feedback into actionable, quality insights could enhance understanding. 4. RL and Evolutionary Search Dynamics: Further analysis could elucidate the conditioning factors impacting the relative performance of RL versus evolutionary search. What explicit trade-offs or factors should practitioners consider when choosing between these methods? 5. Extensions and Future Work: Discuss potential extensions, such as integrating additional algorithms or enhancing the executor's capabilities with external computational tools. What future advancements are seen as most vital for expanding this framework's utility and research potential?\"}", "Researcher 1: {\"correction_scratchpad\":\"### Detailed Findings and Corrections for Core Content\\n\\n1. **Technical Core Claims Verification**:\\n   - The core claim of integrating an automated executor for executing natural language ideas into code, and then using execution feedback to guide further learning seems valid based on provided results. However, the reliance on specific LLMs, such as Claude-4.5 and GPT-5, raises concerns about generalizability and potential biases when different LLM architectures are employed. This concern extends to their adaptation in distinct scientific domains which might have different requirements on the type of code execution.\\n\\n2. **Completeness of the Distilled Recipe**:\\n   - The extracted distilled recipe \\\"Design automated executor ‚Üí Implement ideas in code ‚Üí Execute and collect feedback ‚Üí Use feedback to guide evolutionary search or RL learning\\\" is compact and focused, but lacks detailed elaboration on critical steps. For instance:\\n     - **Implementation Details**: How exactly are natural language ideas translated into executable code? What mechanisms ensure these translations function correctly during experiments?\\n     - **Feedback Loop**: More explicit description is needed on how the execution results are analyzed to produce feedback that informs subsequent idea generations and methodological adjustments.\\n     - **Integration in Evolutionary Search/RL**: There should be explicit definitions of how evolutionarily selected ideas or RL-enhanced strategies use the feedback to adjust ideation or exploration directions.\\n\\n3. **Contribution Type Clarification**:\\n   - Currently classified as a \\\"System,\\\" which is accurate given the nature of the work. However, considering potential broader implications, it might be beneficial to highlight if any theoretical advancements or methodological insights emerged, particularly those influencing AI-driven experimentations outside the presented system.\\n\\n4. **Validation of Evidence Anchor**:\\n   - The results showing improvements in post-training (69.4% vs 48.0%) and pre-training times (19.7 min vs 35.9 min) are compelling. It would be beneficial to know if these results could be consistently replicated with variations in model architectures or differing datasets. Furthermore, acknowledgment of scenarios where the system might not perform as expected, such as domains with strict real-time processing constraints, would provide more balanced evidence.\\n\\n5. **Diagram Summary Enhancements**:\\n   - The current visual representation effectively identifies system components and comparisons but lacks detailed explanation on how these interact dynamically under varying conditions (e.g., different ideations or execution demands). Annotations specifying operational nuances and decision nodes could elevate comprehension, especially in understanding the back-and-forth synergy between ideation, execution, and learning adjustments.\\n\\n### Response to Decider Challenges\\n\\n1. **Integration Challenges**:\\n   - Integrating the executor with different LLMs likely requires addressing compatibility in code generation specifics and adaptability in handling heterogenous task structures. Documenting the spectrum of issues faced during varied integrations (e.g., with more procedural code tasks vs. model architecture tuning) and their resolutions could guide future implementations.\\n\\n2. **Scalability and Generalizability**:\\n   - The paper demonstrates significant potential within controlled environments (nanoGPT and GRPO). However, the scalability across more expansive datasets, other research areas (e.g., climate modeling vs. NLP pre-training), and possible multi-modal research tasks remains speculative unless explicitly explored.\\n\\n3. **Utility of Execution Feedback**:\\n   - The mechanism inherently seems reliant on quality datasets for discerning useful feedback from noise; however, strategies to cleanse feedback from anomalies or incomplete results are not elaborated. Clear methodology elucidating falsification mechanisms could enhance utility.\\n\\n4. **RL and Evolutionary Search Dynamics**:\\n   - Recognizing when to employ RL over evolutionary search might depend on the complexity of tasks and desired diversity in ideation. Evolutionary search fosters diversity in initial idea generations where RL might focus more on refining towards singular optimal solutions.\\n\\n5. **Extensions and Future Work**:\\n   - Promising directions include improving the robustness of the executor, incorporating advanced LLMs with better generalization, and exploring execution scenarios with interactive agent systems and more diverse multi-agent environments. Further advancements could focus on better addressing the three core challenges: adaptability, scalability, and feedback synthesis.\",\"response_to_decider\":\"The core extraction captures the essence of the automated executor system presented in the paper, demonstrating how natural language ideas can be translated into code and improved upon using execution feedback. However, deeper details concerning the interaction between different components in the system, especially the nuances in feedback loops and the systematic translation of ideas into executable formats could be better elaborated. Additionally, while evidence supports the efficiency of execution-guided methodologies, notes about assay consistency across diverse model settings or mismatched scenarios would fortify its application potential. Feedback mechanisms to discern signal from noise need elaboration to better serve varied datasets. For practical implementation across other domains, addressing integration challenges and establishing scalability to real-world settings are pertinent areas of focus.\",\"done\":true}", "Finalizer: {\"core_contribution\":\"The core contribution of the study is the development of an execution-grounded system for automated AI research, where high-throughput execution feedback informs and improves the ideation process of LLMs. This is achieved by translating generated research ideas into executable code, running the experiments using an automated executor, and analyzing the results using two learning methodologies: evolutionary search and reinforcement learning (RL). The work is categorized as a 'System' contribution, as it integrates different components to form a cohesive automated research framework.\",\"method_breakdown\":\"The system comprises an Implementer, Scheduler, and Worker. An Implementer is responsible for generating code from natural language research ideas by producing a code diff for each idea. The Scheduler manages resources and schedules the execution of experiments based on the available computational resources and priorities. The Worker node executes the experiments on GPU clusters, monitoring for completion and collecting results. The feedback loop from execution results is used to inform LLMs on refining their ideation processes.\",\"subsystems_parts\":\"1. Implementer: Converts natural language ideas into code.\\n2. Scheduler: Manages execution scheduling based on computational resource availability.\\n3. Worker: Executes experiments and collects results on GPU clusters.\\n4. Automated Executor: Core system integrating the above components to execute and analyze ideas.\",\"interactions\":\"The Implementer generates code diffs for ideas, which are submitted to the Scheduler. The Scheduler allocates GPU resources to the Worker, which runs the experiments. Upon completion, the Worker sends the results back, completing the loop by providing execution feedback that guides future ideation processes, particularly through evolutionary search and reinforcement learning.\",\"delta_vs_baseline\":\"Unlike traditional manual research processes, this system automates the ideation-to-execution pipeline, enabling high-throughput execution feedback that better informs the learning of LLMs. Current baselines do not utilize an automated executor or integrate execution feedback into the learning loop efficiently.\",\"evidence_anchor\":\"The execution-guided evolutionary search mechanism demonstrates its effectiveness over baselines, especially in tasks like model post-training (69.4% validation accuracy vs. a baseline of 48.0%) and pre-training (19.7 minutes to target loss vs. 35.9 minutes baseline), validating the system's efficiency in improving AI research outputs.\",\"transferability\":\"The system is designed to be adaptable across different AI research scenarios, particularly those involving code generation and execution tasks. It shows promise in improving efficiency in open-ended research problems but may need modifications for non-LLM or highly-specialized domains due to variability in execution needs and feedback mechanisms.\"}"], "processed_at": "2026-02-04 18:28:24.061852", "iterations": 1, "domain": "core_extraction"}, "markdown_pages": [{"id": 1, "title": "1. Introduction", "content": "# 1. Introduction\n\nWe envision automated AI research: LLMs generate research ideas to tackle important research problems, imple\n\n*Equal contribution. Stanford University. Correspondence: clsi@stanford.edu and zitong@berkeley.edu.\n\n![img-0.jpeg](img-0.jpeg)\nFigure 1: We build an automated idea executor involving Implementer, Scheduler, and Worker. We then use this automated executor as a reward function to teach LLMs to generate more effective ideas through evolutionary search and RL. We only update the ideator in the learning process.\n\nment the ideas as code, run experiments to verify the effectiveness, and continuously learn from the execution results. If successful, these automated AI researchers could automatically develop and identify effective research ideas in a massive search space, thereby scalably converting compute into scientific discovery; the discovered ideas could, in turn, improve frontier AI models themselves, enabling recursive self-improvement. Despite the promise, automated AI research is bottlenecked by the ability of LLMs to generate effective ideas. Si et al. (2025b) and Si et al. (2025a) evaluated the quality of LLM-generated research ideas through large-scale expert review and found that LLM ideas often look convincing but are ineffective after being executed by human researchers.\n\nThis highlights the need to ground idea generation in execution. However, obtaining execution results of ideas in an automated and scalable manner is challenging, especially since we are targeting open-ended AI research where any ideas expressible in natural language are within our action space. To tackle this, we design and build a high-throughput automated idea executor that can implement hundreds of model-generated ideas and execute them in parallel to obtain the experiment results as execution feedback.\n\nTo study the extent to which we can automate realistic LLM research, we chose two GPU-intensive research problems (LLM pre-training and post-training) that are critical for improving the capabilities of LLMs as the research environments for our automated AI researchers. For the first time, we demonstrate that our automated executor can im\n\n## Towards Execution-Grounded Automated AI Research\n\nplement a large fraction of LLM-generated ideas on these challenging open-ended research problems, with over  $90\\%$  execution rates on the pre-training environment with Claude-4.5-Sonnet and Claude-4.5-Opus.\n\nTo analyze whether grounding on execution-feedback can improve LLM idea generation, we define objective performance metrics for both environments and analyze the strengths and weaknesses of two popular learning algorithms: evolutionary search and reinforcement learning.\n\nWe use our automated executor to guide evolutionary search. Within ten search epochs, this execution-guided search finds a post-training recipe that outperforms the GRPO baseline (69.4% vs 48.0%) on the task of post-training a 1.5B model for math reasoning, and a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on the task of minimizing the training wall-clock time to reach the target validation loss (Table 1). Our analysis shows that models are often generating algorithmic ideas apart from tuning hyper-parameters, and evolutionary search significantly outperforms best-of-N under the same sampling budget. However, when analyzing the scaling trend, only Claude-4.5-Opus shows a clear scaling curve, while both Claude-4.5-Sonnet and GPT-5 tend to saturate early.\n\nWe then use the automated executor as the reward function in an RL loop to finetune Qwen3-30B. We show that RL with execution reward can successfully improve the average reward of the ideator model, similar to typical RL from verifiable rewards. However, RL does not improve the max reward, which is the more important metric for scientific discovery. In fact, we reveal that RL causes the ideator model to converge on a few easy-to-implement ideas, resulting in a collapse in thinking length and idea diversity.\n\nIn summary, we develop a large-scale automated idea executor system that can implement research ideas for open-ended and realistic research problems. Using this automated executor, we conduct an in-depth analysis of how well LLM ideators can learn from execution feedback to improve effectiveness through evolutionary search and RL. Execution-guided evolutionary search is sample-efficient and effective, but shows limited scaling. RL from execution reward suffers from diversity collapse and does not improve the upper-bound. We additionally provide extensive analysis on the executed ideas and suggest promising directions to improve the existing learning algorithms. Altogether, we demonstrate the feasibility and potential of grounding LLM ideation in automated execution and uncover important limitations for future improvement."}, {"id": 2, "title": "2. Automated Idea Executor", "content": "# 2. Automated Idea Executor\n\nTo measure the effectiveness of model-generated ideas, we build an automated executor that takes natural language\n\nTable 1: Performance of our execution-guided search in comparison with the provided baselines and best human experts. The post-training task is to finetune a 1.5B model for math reasoning, and the metric is validation accuracy. The pre-training task is to train a 124M Transformer on FineWeb, and the metric is the training time to reach 3.28 validation loss.\n\n|   | Post-Training‚Üë | Pre-Training‚Üì  |\n| --- | --- | --- |\n|  Baseline | 48.0% | 35.9 min  |\n|  Execution-Guided Search | 69.4% | 19.7 min  |\n|  Best Human Expert | 68.8% | 2.1 min  |\n\nresearch ideas as input, generates code implementations, runs the experiments on the backend, and returns the idea's benchmark performance as the final output."}, {"id": 3, "title": "2.1. Research Environments for Ideation", "content": "# 2.1. Research Environments for Ideation\n\nOur automated idea executor is grounded in specific research environments, where each environment consists of a research problem, a baseline codebase, a benchmark to measure performance on, fixed training and evaluation data, and evaluation metrics. When constructing the research environments, we aim to select research problems that are open-ended, so that there is ample room for new algorithmic innovations, and at the same time have well-established baselines and benchmarking metrics so that measuring effectiveness is straightforward. In this work, we construct both a pre-training environment and a post-training environment for the automated AI researchers to work on.\n\nPre-Training Task: Improving nanoGPT In the nanoGPT environment, we provide a baseline codebase adapted from the nanoGPT speedrun (Jordan et al., 2024) and ask the ideator model to brainstorm possible improvements. The original speedrun task is to minimize the time to pre-train a 124M GPT-2 model (Radford et al., 2019) on the FineWeb corpus (Penedo et al., 2024) to reach a validation loss of 3.28 on the validation set on 8 H100 GPUs. We did several modifications to the original speedrun setting. First, we introduce a proxy reward equal to the reciprocal of the validation loss  $\\left(\\frac{1}{1088}\\right)$  when performing the search and RL experiments in later sections of the paper. This way, we can fix the training wall-clock time to be 25 minutes and ask the model to directly optimize the proxy reward under this fixed budget, so that we can avoid different runs having vastly different runtimes. We report the validation loss or the proxy reward metric in most plots, and only measure and report the training time metric for the top solution in order\n\n## Towards Execution-Grounded Automated AI Research\n\nto directly compare it with the human experts' solutions on the original nanoGPT speedrun leaderboard. Second, to avoid any possible reward hacking, we freeze all evaluation hyper-parameters and implement an inference function that predicts one future token at a time to prevent models from changing the attention mechanism in a way that leaks future tokens (which happened multiple times during our initial development). We use this inference function during the final validation after each training run.\n\nPost-Training Task: Improving GRPO In the GRPO environment, the baseline is an implementation of the GRPO algorithm (Shao et al., 2024) that finetunes a Qwen2.5-Math-1.5B checkpoint (Yang et al., 2024) on the MATH dataset (Hendrycks et al., 2021). The ideator model needs to brainstorm post-training algorithms that are more effective than the baseline. We specify a fixed training wall-clock time budget and use the max accuracy on the MATH validation set during training as the metric. To prevent reward hacking, we keep all validation-related code in a separate file and do not allow the automatic executor to access or modify it.\n\nIn both environments, we do not set any constraints on the ideation scope, so anything between extensive hyperparameter tuning and novel model architecture or training algorithms is within scope."}, {"id": 4, "title": "2.2. System design", "content": "# 2.2. System design\n\nThe automated idea executor can be viewed as a high-level API whose input is a batch of natural language ideas, and the output is the benchmark performance of each idea. There are three core building blocks of this API (Figure 1): Implementer ‚Äì the server that generates the code diff for the idea and applies those changes; Scheduler ‚Äì a middle layer that receives the list of codebases and allocates resources to run experiments; Worker ‚Äì the cluster with GPU available that runs the experiments and uploads the experiment results.\n\nImplementer The implementer is hosted on a CPU machine with high IO capacity. First, the user submits a batch of natural language ideas. Then, for each idea, the implementer makes parallelized API calls to the code execution LLM to obtain a diff file that can be patched into the corresponding baseline codebase. To optimize for efficiency, we prompt the code execution LLM with both the idea and the baseline codebase to sample 10 code diff files in parallel. For each sample, if the generated diff file cannot be patched into the original codebase, we provide the patch log and ask the model to revise the original generation. We repeat this sequential self-revision for a maximum of 2 times. In the end, we return the first code diff file that can be successfully patched into the baseline codebase. The patched codebase is then submitted to a cloud bucket as a .zip file.\n\nScheduler Under a set clock frequency, the scheduler downloads the new codebases from the cloud. If the codebase has not been executed, the scheduler examines the resource requirement of the given research environment and prepares a job configuration to be submitted.\n\nWorker Once the scheduler finds available resources, it connects the prepared job configuration with the GPU resource and initializes the worker to run the experiment. If the execution of the experiment is successful, the worker will upload the experiment logs including all performance metrics to another cloud bucket (wandb) along with the complete metadata: idea content, code change, execution log, etc. If the execution fails (e.g., due to bugs in code implementation), the worker halts. The user (i.e., the ideator model) can then download the execution results and see the performance of the batch of ideas they submitted with full training logs.\n\n![img-1.jpeg](img-1.jpeg)\n(a) Self-Execution (GRPO)\n\n![img-2.jpeg](img-2.jpeg)\n(b) Self-Execution (nanoGPT)\n\n![img-3.jpeg](img-3.jpeg)\n(c) GPT-5 Execution (GRPO)\n\n![img-4.jpeg](img-4.jpeg)\n(d) GPT-5 Execution (nanoGPT)\nFigure 2: Model performance comparison with self-execution (top row) vs GPT-5 execution (bottom row) on the GRPO and nanoGPT environments. The baseline accuracy for GRPO is 0.480, and the baseline loss for nanoGPT is 3.255. The completion rate is high for most models, especially under self-execution."}, {"id": 5, "title": "3. Benchmarking LLM Ideators and Executors", "content": "# 3. Benchmarking LLM Ideators and Executors\n\nThe prerequisite for an execution-grounded feedback loop is that current LLMs can serve as both ideators and executors, so that we can get meaningful reward signals for the models to learn from. To examine this prerequisite, we first benchmark various frontier LLMs as both the ideator and the executor.\n\n## Towards Execution-Grounded Automated AI Research"}, {"id": 6, "title": "3.1. End-to-End Ideation and Execution", "content": "# 3.1. End-to-End Ideation and Execution\n\nIn the first setting, we sample ideas from an LLM, and use the same LLM as the code execution model to execute its own ideas. We sample and execute 50 ideas from Claude-4.5-Opus, Claude-4.5-Sonnet, and GPT-5, and measure several metrics: (1) completion rate: the percentage of ideas that are successfully executed with a valid (non-zero) experiment result after execution; (2) average performance: the average validation accuracy or loss for all the successfully executed ideas among the 50 samples; (3) best performance: the highest validation accuracy or lowest validation loss among all executed ideas. We present results in the top row of Figure 2. Notably, a large fraction of the sampled ideas can indeed be executed successfully, with Claude-4.5-Opus and Claude-4.5-Sonnet having a significantly higher execution rate than GPT-5. Moreover, the best-of-N performance  $(N = 50)$  of these models can already beat the original baseline solutions. For example, on the GRPO environment, Claude-4.5-Sonnet gets a max accuracy of  $60.4\\%$  as compared to the baseline of  $48.0\\%$ ; on nanoGPT, Claude-4.5-Opus gets a lowest loss of 3.237 as compared to the baseline of 3.255."}, {"id": 7, "title": "3.2. Comparing Ideators with the Same Executor", "content": "# 3.2. Comparing Ideators with the Same Executor\n\nIn the second setting, we fix the executor model to be GPT-5 and use different ideator models to sample ideas. As shown in the bottom row of Figure 2, even when the ideator and executor are different models, the execution rate is still decent (ranging from  $42\\%$  to  $78\\%$ ), although we do notice that the same ideas from Claude-4.5-Sonnet get a lower execution rate when executed by GPT-5 instead of itself ( $84\\%$  vs  $42\\%$  on GRPO and  $90\\%$  vs  $78\\%$  on nanoGPT). Moreover, frontier open-weight models like Kimi-K2-Thinking (Kimi Team, 2025) and Qwen3-235B-A22B (Yang et al., 2025a) can also get non-trivial completion rates and achieve best-of-N performance that outperforms the baseline solutions in this setting. For example, Qwen3-235B achieves a max accuracy of  $50.2\\%$  on GRPO and min loss of 3.238 on nanoGPT with  $N = 50$ , both better than the baselines.\n\nThese benchmarking results demonstrate the feasibility of the automated ideation and execution loop. Next, we build search scaffolds and RL training loops to examine whether models can learn from the execution feedback."}, {"id": 8, "title": "4. Execution-Guided Evolutionary Search", "content": "# 4. Execution-Guided Evolutionary Search\n\nEvolutionary search (Koza, 1994; Lehman et al., 2023) is a traditional optimization method without the need for gradient updates. We develop an evolutionary search scaffold on top of frontier LLMs to optimize for effective ideas based on execution feedback. We introduce our search method that blends exploration and exploitation, its effectiveness on\n\n## Algorithm 1 Execution-Guided Search\n## Require: batch size  $N$ , epochs  $T$ , baseline performance  $\\beta$\nRequire: initial exploitation rate  $a_1 \\in [0, 100]$ , annealing schedule  $a(t)$  for  $t \\in \\{1, \\dots, T\\}$\n1: Sample initial batch of ideas  $\\mathcal{I}_0 \\gets \\text{SAMPLEIDEAS}(N)$\n2: Execute  $\\mathcal{I}_0$  to obtain trajectories  $\\mathcal{D}_0 \\gets \\{(\\text{idea}, \\text{reward})\\}$\n3: for  $t = 1$  to  $T$  do\n4:  $a \\gets a(t) \\quad (100 - a)\\%$  exploration rate\n5:  $\\mathcal{D}_{&lt;t} \\gets \\bigcup_{k=0}^{t-1} \\mathcal{D}_k$\n6:  $\\mathcal{D}^+ \\gets \\{(i, r) \\in \\mathcal{D}_{&lt;t} : r &gt; \\beta\\} \\quad \\triangleright$  positive trajectories\n7:  $N_{\\text{exp}} \\gets \\left\\lfloor \\frac{a}{100} N \\right\\rfloor$ ,  $N_{\\text{expt}} \\gets N - N_{\\text{exp}}$\n8:  $\\mathcal{I}_t^{\\text{exp}} \\gets \\text{EXPLOITVARIANTS}(\\mathcal{D}^+, N_{\\text{exp}})$\n9:  $\\tilde{\\mathcal{D}}_{&lt;t} \\gets \\text{SUBSAMPLETOCONTEXT}(\\mathcal{D}_{&lt;t})$\n10:  $\\mathcal{I}_t^{\\text{expl}} \\gets \\text{EXPLORENOVEL}(\\tilde{\\mathcal{D}}_{&lt;t}, N_{\\text{expl}})$\n11:  $\\mathcal{I}_t \\gets \\mathcal{I}_t^{\\text{exp}} \\cup \\mathcal{I}_t^{\\text{expl}}$\n12: Execute  $\\mathcal{I}_t$  to obtain trajectories  $\\mathcal{D}_t \\gets \\{(\\text{idea}, \\text{reward})\\}$\n13: end for\n14: return  $\\bigcup_{t=0}^{T} \\mathcal{D}_t$\n\nour two research environments, and various analyses of the generated ideas throughout the evolutionary search process."}, {"id": 9, "title": "4.1. Search Scaffold", "content": "# 4.1. Search Scaffold\n\nOur search method is inspired by prior evolutionary search approaches for code optimization, such as AlphaEvolve (Novikov et al., 2025). Our algorithm is detailed in Algorithm 1. At the first search epoch, we sample a full batch of new ideas. In all subsequent epochs, we split the idea generation into exploitation and exploration subsets. For exploitation, we choose ideas from previous epochs that outperform the baseline and append them to the idea generation prompt to ask the ideator model to generate new variants that combine their strengths. For exploration, we randomly sample ideas from previous epochs to append to the idea generation prompt until reaching the max context length and instruct the ideator model to generate completely new ideas different from them. We start with  $50\\%$  exploitation and  $50\\%$  exploration at epoch 1 and gradually anneal the exploration rate and increase the exploitation ratio in later epochs. We use a batch size of 50 for the GRPO environment and a batch size of 80 for the nanoGPT environment."}, {"id": 10, "title": "4.2. Experiment Results", "content": "# 4.2. Experiment Results\n\nFor each environment, we perform execution-guided search with three different models: Claude-4.5-Opus, Claude-4.5-Sonnet, and GPT-5. For each experiment, we use the same model as both the ideator and executor (i.e., self-execution). We plot the progression of the best performance at each search epoch in Figure 3. We summarize several notable trends below.\n\nFirst, we observe a scaling trend with Claude-4.5-Opus, where searching for more epochs leads to a higher upper bound. In contrast, Claude-4.5-Sonnet and GPT-5 tend\n\n## Towards Execution-Grounded Automated AI Research\n\n![img-5.jpeg](img-5.jpeg)\nFigure 3: Best performance at each epoch when performing execution-guided search with different models. For the nanoGPT environment (left), we use the reciprocal of the validation loss as the metric; for the GRPO environment (right), we use validation accuracy as the metric. Claude-4.5-Opus exhibits a scaling trend on both environments and achieves the best performance on nanoGPT. Claude-4.5-Sonnet achieves the best performance on GRPO due to effective hyper-parameter tuning, but saturates early.\n\n![img-6.jpeg](img-6.jpeg)\n\nto saturate early. Second, all models can find ideas that significantly outperform the baselines. On GRPO, Claude-4.5-Sonnet finds that using vanilla policy gradient with the group-average baseline without importance reweighting or clipping outperforms the standard GRPO objective in this particular experiment setup and exploits this finding in all subsequent search epochs, resulting in the best solution of  $69.4\\%$  at epoch 2 with precise hyper-parameter tuning. On nanoGPT, Claude-4.5-Opus achieves the min validation loss of 3.1407 at epoch 9 by combining various architectural modifications, hyper-parameter tuning, and applying exponential moving average of intermediate checkpoints during validation (see Appendix A.2 for the full idea). We run this top solution on 8 H100s to follow the same setup as the nanoGPT speedrun, and it reaches the 3.28 target validation loss in 19.7 minutes, a significant speedup as compared to the baseline codebase, which takes 35.9 minutes of training time to reach the same target validation loss.\n\nTo better contextualize these solutions optimized by the model, we also compare the top performance of our execution-guided search to human experts (Table 1). For the GRPO environment, we compare with the leaderboard of the Stanford CS336 graduate-level LLM class, which hosted the same environment as an assignment for all students to optimize the validation accuracy under the same training time budget. The best student solution achieved an accuracy of  $68.8\\%$ , lower than Claude-4.5-Sonnet's top solution using our execution-guided search. For the nanoGPT environment, we directly compare with the nanoGPT speedrun leaderboard, where the top human solution as of December\n\n![img-7.jpeg](img-7.jpeg)\nFigure 4: Comparison between best-of-N (blue) and our execution-guided search (red) under the same sampling budget where we sample  $N = 80$  ideas at each epoch.\n\n2025 can reach the target validation loss under 2.1 minutes, indicating significant headroom for further model capability and search method improvement on this environment."}, {"id": 11, "title": "4.3. Comparison with Best-of-N", "content": "# 4.3. Comparison with Best-of-N\n\nTo demonstrate the effectiveness of our search scaffold, we compare our execution-guided search with the best-of-N baseline with the same sampling budget on the nanoGPT environment. Since the batch size for our search is 80, we compare the first 3 epochs of the execution-guided search using the GPT-5 backbone with the best-of-N results of GPT-5 with  $N \\in \\{80,160,240\\}$ . As shown in Figure 4, search and best-of-N start from similar performance at epoch 0 (they are not exactly the same due to variances from sampling), but evolutionary search significantly outperforms best-of-N from epoch 1 onward, demonstrating that the model is effectively leveraging trajectories from previous epochs to generate more effective ideas in future epochs.\n\n## Towards Execution-Grounded Automated AI Research\n\nTable 2: Breakdown of hyper-parameter tuning vs algorithmic ideas throughout the entire execution-guided search. We report the percentage of each type among all generated ideas of each model ( $N = 500$  ideas on GRPO and  $N = 800$  ideas on nanoGPT). We also report the average and best performance for ideas under each category, where we use validation accuracy as the performance metric for GRPO and validation loss as the metric for nanoGPT. Bold numbers every row indicate the best performance by each model. All models generate a substantial amount of algorithmic ideas apart from hyper-parameter changes, while Claude-4.5-Sonnet generates significantly more hyper-parameter ideas than other models.\n\n|  Model name | Hyper-parameter |   |   | Algorithmic  |   |   |\n| --- | --- | --- | --- | --- | --- | --- |\n|   |  Percentage | Average | Best | Percentage | Average | Best  |\n|  GRPO environment (accuracy‚Üë)  |   |   |   |   |   |   |\n## |  GPT-5 | 5.0% | 45.0% | 50.2% | 95.0% | 44.5% | 60.0%  |\n|  Claude-4.5-Sonnet | 41.1% | 48.4% | 69.4% | 58.9% | 45.0% | 67.4%  |\n|  Claude-4.5-Opus | 3.7% | 44.4% | 50.4% | 96.3% | 46.5% | 61.6%  |\n|  nanoGPT environment (loss‚Üì)  |   |   |   |   |   |   |\n## |  GPT-5 | 15.4% | 3.254 | 3.195 | 84.6% | 3.894 | 3.170  |\n|  Claude-4.5-Sonnet | 31.3% | 3.251 | 3.208 | 68.7% | 3.679 | 3.208  |\n|  Claude-4.5-Opus | 8.7% | 3.329 | 3.147 | 91.3% | 3.419 | 3.141  |"}, {"id": 12, "title": "4.4. Analysis of Generated Ideas", "content": "# 4.4. Analysis of Generated Ideas\n\nHyper-parameter vs Algorithmic To quantitatively understand the types of ideas that models generate during the execution-guided search, we perform a stratified analysis by classifying all generated ideas into either hyper-parameter tuning (including any ideas that can be implemented via changing existing config) or algorithmic (including all ideas that involve implementing new changes not originally supported by the baseline codebase) by using an LLM-judge. Based on Table 2, all three models generate a substantial amount of algorithmic ideas apart from hyper-parameter tuning. Interestingly, different models exhibit different patterns, where Claude-4.5-Sonnet generates significantly more hyper-parameter ideas than both Claude-4.5-Opus and GPT-5. Moreover, the most effective ideas come from algorithmic ideas in most cases, except when using Claude-4.5-Sonnet.\n\nQualitative Examples To complement the quantitative analysis, we provide several executed ideas on the GRPO environment in Table 3 and provide example ideas on the nanoGPT environment in Appendix A.2. When sampling ideas, models would generate a thinking trace, followed by the natural language idea and a brief description of all the code changes needed to implement the idea. For brevity, we only include the natural language ideas in the table, but we present additional examples in Appendix A.3 with more details, including full code execution trajectories. Based on the qualitative examples in Table 3, different models generate different styles of ideas. For example, Claude-4.5-Sonnet generates more intuitive ideas while Claude-4.5-Opus and GPT-5 are more mathematically inclined.\n\nRecovering Recent Research Papers We observed multiple cases where the model-generated ideas (without any\n\nRAG) are highly similar to research papers released within the three months prior to the writing of this paper. For example, Claude-4.5-Sonnet proposed: \"Implement response diversity rewards within groups where responses to the same prompt receive bonus rewards for being dissimilar to other responses in their group, encouraging exploration of different solution paths,\" which is similar to Li et al. (2025). For pre-training, Claude-4.5-Opus proposed: \"Causal Context Compression: Before each attention layer, apply a learned compression that mixes local context (previous 2-3 tokens) into the current representation, providing implicit local context without convolutions,\" which is similar to the \"canon layer\" described in Allen-Zhu (2025). Although assessing the novelty of LLM-generated ideas is beyond the scope of this work, models' ability to rediscover ideas from recent research papers nevertheless indicates that automated AI researchers could plausibly support work at the frontier of LLM research."}, {"id": 13, "title": "5. Reinforcement Learning from Execution Reward", "content": "# 5. Reinforcement Learning from Execution Reward\n\nDifferent from evolutionary search, reinforcement learning is an alternative learning algorithm that shapes model behaviors through gradient updates. Despite much recent success on verifiable domains like math and coding (DeepSeek-AI et al., 2025), RL's effectiveness on open-ended AI research remains unclear. For the first time, we explore whether we can leverage the automated executor as a reward function to directly finetune LLMs to generate more effective ideas via RL. We detail our implementation, experiment setup, and analysis of the training dynamics.\n\n## Towards Execution-Grounded Automated AI Research\n\n|  Claude-4.5-Opus on GRPO | Claude-4.5-Sonnet on GRPO | GPT-5 on GRPO  |\n| --- | --- | --- |\n|  Residual Ratio Learning with Momentum Bounds: Instead of directly using the (importance sampling) ratio, decompose it into a ‚Äúbase‚Äù component (EMA of batch mean ratios) and a ‚Äúresidual‚Äù component (ratio ‚Äì base). Apply sigmoid bounding only to the residual, allowing the base to capture systematic policy drift while controlling deviations from it. Combined with momentum clip adaptation. Formula: residual = ratio - ema_batch_ratio, bounded_residual = sigmoid_bound(residual, deviation), effective_ratio = 1.0 + bounded_residual. | Dynamic Mathematical Problem Difficulty Balancing with Performance Feedback: Implement intelligent difficulty balancing that dynamically adjusts the mix of problem difficulties based on recent performance trends. When performance is strong, increase difficulty proportion; when struggling, provide more foundational problems. Combine with the proven hyper-parameters for optimal learning progression. Validation Accuracy: 64.0 | Token-Level Ratio De-noising via Response Chunks (Chunked-Ratio): Reduce noisy token spikes by averaging log-ratio over small contiguous chunks within the response. Partition response tokens into C chunks per sequence (e.g., C = 8 over effective length), replace per-token Œî log p with chunk mean broadcast to tokens in the chunk before ratio and clipping. Keeps structural signal while smoothing extremes. Validation Accuracy: 58.2  |\n|  Validation Accuracy: 61.6 | Implement token-level reward attribution by using attention weights to identify which input tokens contributed most to correct answers, then amplifying the gradient updates for those tokens during policy gradient training. Validation Accuracy: 45.2 | Per-Group Curriculum via Reward Spread (PGC-RS): Adjust step aggressiveness based on group reward spread. For each group, compute spread sg = std(r). Compute per-sample temperature T_i^grp = clamp(1 + Œ± ¬∑ (sref - sg), 0.8, 1.5) with sref = median group std over rollout and Œ± = 0.8. Multiply existing ratio temperature Ti (if any) by T_i^grp. Groups with low spread (hard to distinguish) get cooler ratios; high-spread groups allow bolder updates. Validation Accuracy: 49.4  |\n|  Advantage Rank Difference Weighting: Instead of using absolute advantage magnitude, weight samples by how far their rank differs from their expected rank under uniform distribution. Samples that significantly outperform or underperform their ‚Äúexpected‚Äù position get higher weights. This is distribution-free and robust to outliers. Formula: expected_rank = (N-1)/2, rank_diff = |actual_rank - expected_rank| / expected_rank, weight = 0.5 + 0.5 * rank_diff. | Create mathematical working memory simulation by maintaining a context buffer of mathematical facts, definitions, and intermediate results during problem solving. This buffer gets updated as the model works through problems and provides additional context for subsequent mathematical steps, simulating how humans maintain mathematical working memory during complex calculations. Validation Accuracy: 58.0 |   |\n\nTable 3: Examples of successfully executed ideas on the GRPO environment, along with their accuracy on the MATH validation set. The baseline accuracy is  $48.0\\%$  on this environment."}, {"id": 14, "title": "5.1. Reward Design and Experiment Setup", "content": "# 5.1. Reward Design and Experiment Setup\n\nWe use Qwen3-30B-A3B (Yang et al., 2025a) as the base model and finetune it using the standard GRPO algorithm (Shao et al., 2024), motivated by its consistent empirical success on other verified domains. Our prompt batch size is one since we only have one prompt for each environment. In the prompt, we provide the baseline codebase and ask the model to generate new ideas to improve the baseline (GRPO or nanoGPT). This experiment setup is similar to prior work exploring RLVR from one training example (Wang et al., 2025).\n\nWe use large group sizes to stabilize training. For the posttraining environment, we use a group size of 256; for the pre-training environment, we use a group size of 128. Since each idea on the GRPO environment runs on one single GPU and each idea on the nanoGPT environment runs on 8 GPUs, these group sizes correspond to parallel execution on 256 GPUs (for GRPO) or 1024 GPUs (for nanoGPT) to obtain\n\nthe execution reward on each batch of rollout ideas. Each rollout is a thinking trace followed by the natural language idea. We set a max output length of 8192 tokens for rollout sampling and only feed the extracted ideas to the automated executor without the preceding thinking trace.\n\nFor the post-training environment, we directly use the validation set accuracy of each rollout idea after execution as the reward. For ideas without a valid accuracy (i.e., when the execution failed due to code generation errors), we assign a reward of 0. For the pre-training environment, we use the reciprocal of the validation loss as the reward  $\\left(\\frac{1}{loss}\\right)$  and assign a reward of 0 to ideas with failed execution. Our experiments are based on the Tinker API (Thinking Machines Lab, 2025)."}, {"id": 15, "title": "5.2. Experiment Results", "content": "# 5.2. Experiment Results\n\nPositive Training Curves for Average Reward We plot the average reward of all rollouts of each training epoch in\n\n## Towards Execution-Grounded Automated AI Research\n\n![img-8.jpeg](img-8.jpeg)\nFigure 5: Training curves of RL from execution reward. We plot the average reward per epoch in the upper row, and the max reward per epoch in the lower row. For the GRPO environment, the reward is the accuracy; for the nanoGPT environment, the reward is the reciprocal of the loss. The average reward increases, but not the max reward.\n\nthe upper row of Figure 5. For the first time, we demonstrate that the average performance of the generated ideas can increase after sufficient training epochs for open-ended research environments. For instance, the average accuracy on the GRPO environment increases from 0.253 at the beginning to 0.343 after 40 training epochs (top left plot of Figure 5); and the average reward on the nanoGPT environment increases from 0.194 at the beginning to 0.246 after 68 epochs (top right plot of Figure 5), corresponding to a decrease in the average validation loss from 5.150 to 4.066. Such training curves are similar to prior findings on the effectiveness of one-shot RVLR on other verified domains like math (Wang et al., 2025).\n\nThe Case of Max Reward Despite successfully reproducing the positive training curves observed in other domains, we argue that there is a distinction between idea generation and other verifiable domains. For advancing scientific discovery, we often care about the upper-bound of idea generation, rather than the average quality. In our particular case, we care more about having one breakthrough idea that dominates the baselines, rather than having many safe ideas with a high average. Thus, we plot the max reward of all rollouts at each training epoch in the lower row of Figure 5. The trend here is very different ‚Äì the max reward is fluctuating throughout RL training without a clear upward trend. This reveals the crucial limitation of the standard GRPO algorithm for improving idea generation. In the next subsection, we analyze why RL from execution reward improves the average reward but not the max.\n\n![img-9.jpeg](img-9.jpeg)\nFigure 6: Upper Row: Average length of the thinking trace (red line) and the idea (blue line) per training epoch. Lower Row: Execution rate of ideas with the longest (red line) versus shortest (blue line) thinking traces. Ideas with longer thinking have lower execution rates. Correspondingly, thinking lengths decrease in RL.\n\n![img-10.jpeg](img-10.jpeg)\nFigure 7: We plot how many ideas in each epoch are about either replacing RMSNorm with LayerNorm or doing EMA. The model converged on these two ideas after RL training."}, {"id": 16, "title": "5.3. Analysis of Training Dynamics", "content": "# 5.3. Analysis of Training Dynamics\n\nThinking Length We first plot how the lengths of the thinking traces evolve over RL training in the upper row of Figure 6. In both environments, the thinking traces rapidly decrease in length while the idea lengths stay roughly constant. This is the opposite of the thinking emergence trend from prior RLVR work, such as DeepSeek-R1 (DeepSeek-AI et al., 2025). To further understand why the thinking length decreases, we investigate the correlation between the idea execution rate and the thinking trace length. In the bottom row of Figure 6, for the first 20 epochs of the RL training, we sort all ideas in each epoch by their thinking trace lengths and plot the average execution rate of the top-  $30\\%$  longest thinking ideas (red line) and the bottom-  $30\\%$  shortest thinking ideas (blue line). We see a clear trend where ideas with longer thinking consistently have a lower execution rate. We thus hypothesize that longer thinking\n\ncorrelates with more complex ideas with lower execution rates, leading the model to prefer shorter thinking instead in order to maximize the reward.\n\n##### Diversity Collapse\n\nUpon manual investigation of all the rollouts being sampled throughout the RL training, we also observed a diversity collapse. Specifically, the model learned to converge on a few simple ideas that can consistently get a positive reward. For example, in the nanoGPT environment, the model learned to converge towards two common ideas: (1) replacing RMSNorm with LayerNorm; and (2) performing exponential moving average (EMA) over intermediate model checkpoints. As shown in Figure 7, out of a batch of 128 sampled ideas per epoch, 51 ideas sampled from Qwen3-30B at epoch 0 are one of the two common ideas above. Towards the end of the RL training, 119 out of 128 sampled ideas at epoch 68 are one of the two common ideas, indicating a severe diversity collapse.\n\nThe above analyses reveal that RL causes the models to converge on a few simple-to-implement ideas, accompanied by shrinking thinking lengths. These lead to an increase in the average reward, but do not push the upper-bound due to the lack of exploration. This phenomenon is analogous to mode-collapse observations on other verifiable domains, where the pass@k performance stagnates or even decreases after RL *(Yue et al., 2025; Wu et al., 2025)*. Avoiding such convergence and collapse is an open problem and likely requires new algorithmic interventions beyond standard GRPO, which is beyond the scope of this work. However, we do share several preliminary attempts, including: sampling and appending previous epochs‚Äô trajectories into the current epoch‚Äôs prompt for rollout sampling, adding a weighted length reward in the total reward, and adding a weighted similarity penalty in the total reward. We did not observe clear gains in the initial epochs and thus early-stopped them, but we document all these results in Appendix A.1 to inform future work.\n\n## 6 Related Work\n\n##### AutoML\n\nOur work has deep connections to the AutoML literature. For example, the Neural Architecture Search (NAS) line of work typically defines a constrained set of neural network operators and optimizes for architectures based on validation set performance through reinforcement learning *(Zoph and Le, 2017; Zoph et al., 2017)* or search *(Liu et al., 2018; So et al., 2019)*. In the modern era, recent works also explored directly using LLMs to propose architecture variants and implement them for validation *(Liu et al., 2025; Cheng et al., 2025)*. Beyond architectures, similar automatic optimizations have been applied to improve hyperparameter tuning *(Zhang et al., 2023)*, discover machine learning algorithms *(Real et al., 2020)*, improve post-training objectives *(Lu et al., 2024a)*, discover better neural network optimizers *(Chen et al., 2023)*, and design agent scaffolds *(Hu et al., 2025)*. Different from this line of work, we tackle automated AI research in a fully open-ended setting without any constraint on the type of ideas. Moreover, our goal is to improve the effectiveness of idea generation, where natural language ideas represent a higher level of abstraction than specific architecture variants or code optimizations.\n\n##### LLM-based Research Agents\n\nRecent works have been building LLM-based research agents for accelerating scientific discovery in various domains, including AI research. AI-Scientist *(Lu et al., 2024b; Yamada et al., 2025)*, AI-Researcher *(Tang et al., 2025)*, and Agent Laboratory *(Schmidgall et al., 2025)* are examples of end-to-end research agents that use LLMs to generate ideas and implement them through carefully designed agent scaffolds. They address open-ended AI research as we do, but do not study how to learn from execution feedback and improve idea effectiveness. On the other hand, on more grounded benchmarks with clear performance metrics such as MLE-Bench *(Chan et al., 2025)*, RE-Bench *(Wijk et al., 2024)*, and ML-Gym *(Nathani et al., 2025)*, various works have explored how to learn from execution feedback through either search *(Toledo et al., 2025; Jiang et al., 2025)* or RL *(Yang et al., 2025b)* to optimize performance on these targeted ML engineering tasks. While we also study algorithms for learning from execution feedback, we tackle open-ended research problems like pre-training and post-training rather than ML engineering tasks that heavily depend on feature engineering and hyper-parameter tuning rather than algorithm development.\n\n##### AI for Research\n\nApart from fully end-to-end automated AI research, many works have studied how to use LLMs for specific components of the scientific research pipeline, such as literature review *(Asai et al., 2024; L‚Äôala et al., 2023)*, idea generation *(Si et al., 2025b; Wang et al., 2024)*, data analysis *(Majumder et al., 2025; Mitchener et al., 2025)*, experiment plan generation *(Goel et al., 2025)*, research code execution *(Starace et al., 2025; Hua et al., 2025; Tian et al., 2024)*, and paper reviewing *(Liang et al., 2024; Zhu et al., 2025)*. Our work focuses on automated idea execution and learning from the execution feedback. We consider our work complementary to many of the above works that improve other aspects of the scientific research pipeline.\n\n##### Execution Grounding for Code\n\nThe idea of learning from execution feedback has been explored in the code generation domain. For example, *Zheng et al. (2024)* curate data and train models to refine code from either human or execution feedback; *Gehring et al. (2025)* use end-to-end RL training to teach models to improve code based on execution feedback; *Lavon et al. (2025)* directly guide code generation\n\nwith execution signals during inference time. In contrast, our work explores execution grounding for the application of idea generation, where the verification is more complicated and expensive.\n\n## 7 Conclusion\n\nIn this work, we built a large-scale parallel executor for automatically executing model-generated ideas to verify their effectiveness on open-ended LLM research problems, including both LLM pre-training and post-training. Using this executor as a reward function, we analyzed the effectiveness of execution-guided evolutionary search, where frontier LLMs equipped with a simple evolutionary search scaffold can significantly outperform the baseline solutions. We also benchmarked and revealed the limitations of reinforcement learning with execution rewards, where models tend to converge on simple ideas to improve the average reward but lose diversity and do not improve the upper-bound. Our empirical results demonstrate the feasibility and potential of the automated execution feedback loop and also point out the remaining limitations for future improvement. We hope this work paves the foundation and inspires more efforts towards execution-grounded automated AI research.\n\n## 8 Discussion\n\nDespite encouraging initial signals, there are still many limitations to our current set of experiments, which would be great directions for future improvement.\n\nFirst, our current procedure does not test the generalizability of the generated ideas. It is possible that the best-performing ideas at the small scales may not transfer to gains at a larger scale or on other datasets. Future work should explore methods that explicitly test such generalizability and scalability, and even incorporate them as part of the optimization objectives.\n\nSecond, we have shown that RL with execution reward in our current setup can only improve the average reward but not the upper bound. There are many possible reasons, such as a lack of diversity in the base model and missing exploration incentives in the current RL objective. Future work should explore remedies and better learning algorithms for LLMs to more efficiently learn from the execution feedback. For instance, future works could explore how to exploit richer learning signals from the execution trajectories beyond just scalar rewards.\n\nThird, our current experiment scope is bounded by the capability of the execution agent. There exist many promising model-generated ideas that could not be successfully executed by the execution agent (e.g., see the end of Appendix A.2), leading to noise in the reward signal. Future work could develop more capable execution agents and extend our setup to even more complex research problems. For instance, instead of directly prompting an LLM for code diff, future work can implement more capable coding agents with access to external tools and the ability to install new libraries in the execution environments.\n\nLast but not least, we only explored effectiveness as the training reward in this work. There are many other, more subjective alternative metrics that could complement the effectiveness reward, such as the idea novelty and interestingness. Future work could explore how to computationally measure them and incorporate them as part of the training objective to discover more insightful ideas.\n\n## Acknowledgment\n\nWe thank DST Global, Laude Institute, and Thinking Machines Lab for their generous sponsorship of computing resources. We thank Yuandong Tian, Edward Hughes, Ludwig Schmidt, Cong Lu, Jenny Zhang, Jiaxin Wen, Chris Rytting, Chen Zhao, Xinran Zhao, Yanzhe Zhang, John Yang, Shicheng Liu, Andy Zhou, Will Held, Haotian Ye, Luke Bailey, as well as members of Tatsu Lab and SALT Lab for their helpful discussion and feedback. This work was supported by an HAI grant, DSO labs, gifts from Open Philanthropy, Amazon, Schmidt Sciences, the Tianqiao and Chrissy Chen Foundation and a grant under the NSF CAREER IIS-2338866 and IIS-2247357, ONR N00014-24-1-2609 and N00014-24-1-2532, and DARPA Cooperative Agreement HR00112520013. This work does not necessarily reflect the position or policy of the government and no official endorsement should be inferred.\n\n## References\n\n- Zhu et al. (2025) Allen-Zhu, Z. Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers. *ArXiv*, 2025.\n- Asai et al. (2025) Asai, A., He, J., Shao, R., Shi, W., Singh, A., Chang, J. C., Lo, K., Soldaini, L., Feldman, S., D‚ÄôArcy, M., Wadden, D., Latzke, M., Tian, M., Ji, P., Liu, S., Tong, H., Wu, B., Xiong, Y., Zettlemoyer, L. S., Neubig, G., Weld, D., Downey, D., tau Yih, W., Koh, P. W., and Hajishirzi, H. OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs. *ArXiv*, abs/2411.14199, 2024.\n- Chan et al. (2025) Chan, J. S., Chowdhury, N., Jaffe, O., Aung, J., Sherburn, D., Mays, E., Starace, G., Liu, K., Maksin, L., Patwardhan, T., Weng, L., and Mkadry, A. MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering. In *ICLR*, 2025.\n- Chen et al. (2025) Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Liu, Y., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., Lu,\n\n## Towards Execution-Grounded Automated AI Research\n\nY., and Le, Q. V. Symbolic Discovery of Optimization Algorithms. In NeurIPS, 2023.\n- Cheng et al. (2025) Cheng, J., Clark, P., and Richardson, K. Language Modeling by Language Models. In NeurIPS, 2025.\n- DeepSeek-AI et al. (2025) DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J.-M., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B.-L., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D.-L., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S.-K., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W.-X., Zhang, W., Xiao, W., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X.-C., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y.-J., He, Y., Xiong, Y., Luo, Y.-W., mei You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., guo Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z.-A., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. Nature, 2025.\n- Gehring et al. (2025) Gehring, J., Zheng, K., Copet, J., Mella, V., Cohen, T., and Synnaeve, G. RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning. In ICML, 2025.\n- Goel et al. (2025) Goel, S., Hazra, R., Jayalath, D. H., Willi, T., Jain, P., Shen, W. F., Leontiadis, I., Barbieri, F., Bachrach, Y., Geiping, J., and Whitehouse, C. Training AI Co-Scientists Using Rubric Rewards. ArXiv, abs/2512.23707, 2025.\n- Hendrycks et al. (2025) Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D. X., and Steinhardt, J. Measuring Mathematical Problem Solving With the MATH Dataset. In NeurIPS, 2021.\n- Hu et al. (2021) Hu, S., Lu, C., and Clune, J. Automated Design of Agentic Systems. In ICLR, 2025.\n- Hua et al. (2025) Hua, T., Hua, H., Xiang, V., Klieger, B., Truong, S. T., Liang, W., Sun, F.-Y., and Haber, N. ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code. In NeurIPS, 2025.\n- Jiang et al. (2024) Jiang, Z., Schmidt, D., Srikanth, D., Xu, D., Kaplan, I., Jacenko, D., and Wu, Y. AIDE: AI-Driven Exploration in the Space of Code. ArXiv, abs/2502.13138, 2025.\n- Jordan et al. (2024) Jordan, K., Bernstein, J., Rappazzo, B., @fernbear.bsky.social, Vlado, B., Jiacheng, Y., Cesista, F., Koszarsky, B., and @Grad62304977. modded-nanogpt: Speedrunning the nanogpt baseline, 2024. URL https://github.com/KellerJordan/modded-nanogpt.\n- Kimi Team et al. (2024) Kimi Team. Kimi K2: Open Agentic Intelligence. ArXiv, abs/2507.20534, 2025.\n- Koza et al. (2023) Koza, J. Genetic Programming: On the Programming of Computers by Means of Natural Selection. Statistics and Computing, 4, 1994.\n- L‚Äôala et al. (2023) L‚Äôala, J., O‚ÄôDonoghue, O., Shtedritski, A., Cox, S., Rodriques, S. G., and White, A. D. PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. ArXiv, abs/2312.07559, 2023.\n- Lavon et al. (2023) Lavon, B., Katz, S., and Wolf, L. Execution Guided Line-by-Line Code Generation. In NeurIPS, 2025.\n- Lehman et al. (2023) Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., and Stanley, K. O. Evolution through large models. In Handbook of Evolutionary Machine Learning. Springer, 2023.\n- Li et al. (2023) Li, T., Zhang, Y., Yu, P., Saha, S., Khashabi, D., Weston, J. E., Lanchantin, J., and Wang, T. Jointly Reinforcing Diversity and Quality in Language Model Generations. ArXiv, abs/2509.02534, 2025.\n- Liang et al. (2024) Liang, W., Zhang, Y., Cao, H., Wang, B., Ding, D., Yang, X., Vodrahalli, K., He, S., Smith, D. S., Yin, Y., McFarland, D. A., and Zou, J. Can large language models provide useful feedback on research papers? a large-scale empirical analysis. NEJM AI, 2024.\n- Liu et al. (2024) Liu, H., Simonyan, K., Vinyals, O., Fernando, C., and Kavukcuoglu, K. Hierarchical Representations for Efficient Architecture Search. In ICLR, 2018.\n\nLiu, Y., Nan, Y., Xu, W., Hu, X., Ye, L., Qin, Z., and Liu, P. AlphaGo Moment for Model Architecture Discovery. ArXiv, abs/2507.18074, 2025.\n- Lu et al. (2024a) Lu, C., Holt, S., Fanconi, C., Chan, A. J., Foerster, J. N., van der Schaar, M., and Lange, R. T. Discovering Preference Optimization Algorithms with and for Large Language Models. In NeurIPS, 2024a.\n- Lu et al. (2024b) Lu, C., Lu, C., Lange, R. T., Foerster, J. N., Clune, J., and Ha, D. The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. ArXiv, abs/2408.06292, 2024b.\n- Majumder et al. (2025) Majumder, B. P., Surana, H., Agarwal, D., Dalvi, B., Meena, A., Prakhar, A., Vora, T., Khot, T., Sabharwal, A., and Clark, P. DiscoveryBench: Towards Data-Driven Discovery with Large Language Models. In ICLR, 2025.\n- Mitchener et al. (2025) Mitchener, L., Yiu, A., Chang, B., Bourdenx, M., Nadolski, T., Sulovari, A., Landsness, E. C., Barab√°si, D. L., Narayanan, S., Evans, N., Reddy, S., Foiani, M. S., Kamal, A., Shriver, L. P., Cao, F., Wassie, A. T., Laurent, J. M., Melville-Green, E., Ramos, M. C., Bou, A., Roberts, K. F., Zagorac, S., Orr, T. C., Orr, M. E., Zwezdaryk, K. J., Ghareeb, A. E., McCoy, L., Gomes, B., Ashley, E. A., Duff, K. E., Buonassisi, T., Rainforth, T., Bateman, R. J., Skarlinski, M., Rodriques, S. G., Hinks, M. M., and White, A. D. Kosmos: An AI Scientist for Autonomous Discovery. ArXiv, abs/2511.02824, 2025.\n- Nathani et al. (2025) Nathani, D., Madaan, L., Roberts, N., lay Bashlykov, N., Menon, A., Moens, V., Budhiraja, A., Magka, D., Vorotilov, V., Chaurasia, G., Hupkes, D., Cabral, R. S., Shavrina, T., Foerster, J., Bachrach, Y., Wang, W. Y., and Raileanu, R. MLGym: A New Framework and Benchmark for Advancing AI Research Agents. In COLM, 2025.\n- Novikov et al. (2025) Novikov, A., V~u, N., Eisenberger, M., Dupont, E., Huang, P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B. M., Ruiz, F. J. R., Mehrabian, A., Kumar, M. P., See, A., Chaudhuri, S., Holland, G., Davies, A., Nowozin, S., Kohli, P., Balog, M., and Deepmind, G. Alphaevolve: A coding agent for scientific and algorithmic discovery. ArXiv, abs/2506.13131, 2025.\n- Penedo et al. (2024) Penedo, G., Kydl√≠cek, H., Allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., von Werra, L., and Wolf, T. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. ArXiv, abs/2406.17557, 2024.\n- Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language Models are Unsupervised Multi-task Learners. 2019.\n- Real et al. (2020) Real, E., Liang, C., So, D. R., and Le, Q. V. AutoML-Zero: Evolving Machine Learning Algorithms From Scratch. In ICML, 2020.\n- Schmidgall et al. (2020) Schmidgall, S., Su, Y., Wang, Z., Sun, X., Wu, J., Yu, X., Liu, J., Liu, Z., and Barsoum, E. Agent Laboratory: Using LLM Agents as Research Assistants. In Findings of EMNLP, 2025.\n- Shao et al. (2024) Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J.-M., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. ArXiv, abs/2402.03300, 2024.\n- Si et al. (2024) Si, C., Hashimoto, T., and Yang, D. The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas. ArXiv, abs/2506.20803, 2025a.\n- Si et al. (2025) Si, C., Yang, D., and Hashimoto, T. Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. In ICLR, 2025b.\n- So et al. (2025) So, D. R., Liang, C., and Le, Q. V. The Evolved Transformer. In ICML, 2019.\n- Starace et al. (2025) Starace, G., Jaffe, O., Sherburn, D., Aung, J., Chan, J. S., Maksin, L., Dias, R., Mays, E., Kinsella, B., Thompson, W., Heidecke, J., Glaese, A., and Patwardhan, T. Paper-Bench: Evaluating AI‚Äôs Ability to Replicate AI Research. In ICML, 2025.\n- Tang et al. (2024) Tang, J., Xia, L., Li, Z., and Huang, C. AI-Researcher: Autonomous Scientific Innovation. In NeurIPS, 2025.\n- Thinking Machines Lab. Announcing Tinker, 2025.\n- Tian et al. (2024) Tian, M., Gao, L., Zhang, S. D., Chen, X., Fan, C., Guo, X., Haas, R., Ji, P., Krongchon, K., Li, Y., Liu, S., Luo, D., Ma, Y., Tong, H., Trinh, K., Tian, C., Wang, Z., Wu, B., Xiong, Y., Yin, S., Zhu, M., Lieret, K. A., Lu, Y., Liu, G., Du, Y., Tao, T., Press, O., Callan, J., Huerta, E. A., and Peng, H. SciCode: A Research Coding Benchmark Curated by Scientists. ArXiv, abs/2407.13168, 2024.\n- Toledo et al. (2024) Toledo, E., Hambardzumyan, K., Josifoski, M., Hazra, R., Baldwin, N. M., Audran-Reiss, A., Kuchnik, M., Magka, D., Jiang, M., Lupidi, A. M., Lupu, A., Raileanu, R., Niu, K., Shavrina, T., Gagnon-Audet, J.-C., Shvartsman, M., Sodhani, S., Miller, A. H., Charnalia, A., Dunfield, D., Wu, C.-J., Stenetorp, P., Cancedda, N., Foerster, J. N., and Bachrach, Y. AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench. ArXiv, abs/2507.02554, 2025.\n- Wang et al. (2024) Wang, Q., Downey, D., Ji, H., and Hope, T. SciMON: Scientific Inspiration Machines Optimized for Novelty. In ACL, 2024.\n\nWang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du, S. S., and Shen, Y. Reinforcement Learning for Reasoning in Large Language Models with One Training Example. In NeurIPS, 2025.\n- Wijk et al. (2024) Wijk, H., Lin, T. R., Becker, J., Jawhar, S., Parikh, N., Broadley, T., Chan, L., Chen, M., Clymer, J., Dhyani, J., Ericheva, E., Garcia, K., Goodrich, B., Jurkovic, N., Kinniment, M., Lajko, A., Nix, S., Sato, L. J. K., Saunders, W., Taran, M., West, B., and Barnes, E. RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts. ArXiv, abs/2411.15114, 2024.\n- Wu et al. (2024) Wu, F., Xuan, W., Lu, X., Harchaoui, Z., and Choi, Y. The Invisible Leash: Why RLVR May Not Escape Its Origin. ArXiv, abs/2507.14843, 2025.\n- Yamada et al. (2025) Yamada, Y., Lange, R. T., Lu, C., Hu, S., Lu, C., Foerster, J. N., Clune, J., and Ha, D. The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search. ArXiv, abs/2504.08066, 2025.\n- Yang et al. (2024) Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and Zhang, Z. Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement. ArXiv, abs/2409.12122, 2024.\n- Yang et al. (2024) Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L.-C., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S.-Q., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y.-C., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 Technical Report. ArXiv, abs/2505.09388, 2025a.\n- Yang et al. (2025a) Yang, S., He-Yueya, J., and Liang, P. Reinforcement Learning for Machine Learning Engineering Agents. ArXiv, abs/2509.01684, 2025b.\n- Yue et al. (2025) Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model? In NeurIPS, 2025.\n- Zhang et al. (2023) Zhang, M. R., Desai, N., Bae, J., Lorraine, J., and Ba, J. Using Large Language Models for Hyperparameter Optimization. ArXiv, abs/2312.04528, 2023.\n- Zheng et al. (2024) Zheng, T., Zhang, G., Shen, T., Liu, X., Lin, B. Y., Fu, J., Chen, W., and Yue, X. OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement. In Findings of ACL, 2024.\n- Zhu et al. (2024) Zhu, M., Weng, Y., Yang, L., and Zhang, Y. DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process. In ACL, 2025.\n- Zoph et al. (2017) Zoph, B. and Le, Q. V. Neural Architecture Search with Reinforcement Learning. In ICLR, 2017.\n- Zoph et al. (2017) Zoph, B., Vasudevan, V., Shlens, J., and Le, Q. V. Learning Transferable Architectures for Scalable Image Recognition. In CVPR, 2017.\n\n## Towards Execution-Grounded Automated AI Research"}, {"id": 17, "title": "A. Appendix", "content": "# A. Appendix"}, {"id": 18, "title": "A.1. Other RL Attempts", "content": "# A.1. Other RL Attempts\n\nWe present several attempts to improve our RL from the execution reward recipe.\n\nAttempt 1: Dynamic Prompt At each epoch (except the first epoch), we randomly sample different executed idea trajectories from the previous epoch and append them to the idea sampling prompt when sampling new rollouts. This merges in-context learning with RL and adds diversity to the idea sampling process. We present the experiment results on the GRPO environment in Figure 8. We did not see significant improvement in early epochs and thus early stopped.\n\n![img-11.jpeg](img-11.jpeg)\nFigure 8: RL with dynamic prompt by appending trajectories from previous epochs. Results are on the GRPO environment.\n\nAttempt 2: Length Reward Since we noted a rapid thinking length decrease in our main RL experiment, we tried a simple fix by adding a weighted length reward that counts the number of tokens in the entire rollout sequence, including the thinking trace and the idea. We cap the length reward to a maximum of 0.3 to avoid it dominating the accuracy reward. We present the experiment results on the GRPO environment in Figure 9. As shown on the right panel, the thinking length no longer decreases after adding the length reward to the total reward, but the total training reward isn't going up as shown on the left panel.\n\n![img-12.jpeg](img-12.jpeg)\nFigure 9: Average and max reward throughout RL training when adding the length reward (left), as well as the progression of the thinking and idea lengths (right).\n\n![img-13.jpeg](img-13.jpeg)\n\n## Towards Execution-Grounded Automated AI Research\n\nAttempt 3: Diversity Reward We also tried adding a diversity reward in addition to the effectiveness reward. Specifically, when computing the reward for each rollout, we compute its token-level Jaccard similarity with ideas from the previous epoch and add the negative similarity as a penalty to the total reward to discourage repeating ideas that have already been generated before. In fact, this is similar to one of the post-training ideas proposed by Claude-4.5-Sonnet (see example 4 in Appendix A.3). We show the training curves on the GRPO environment in Figure 10. The model maintains a consistent idea similarity (right plot), suggesting sustained exploration. The effectiveness reward is generally showing an upward trend (left plot), but not markedly better than the main RL run with just the effectiveness reward (first sub-plot in Figure 5).\n\n![img-14.jpeg](img-14.jpeg)\nFigure 10: Effectiveness reward (left) and average idea similarity to previous epoch (right) when doing RL with diversity reward.\n\n![img-15.jpeg](img-15.jpeg)\n\n## Towards Execution-Grounded Automated AI Research"}, {"id": 19, "title": "A.2. Additional Idea Examples", "content": "# A.2. Additional Idea Examples\n\nWe provide several additional example ideas generated by Claude-4.5-Opus (Table 4) and Claude-4.5-Sonnet (Table 5) on the GRPO environment, including ideas with failed code execution. In most cases, code execution errors happen when the idea involves complicated changes or installing and importing external packages not supported in our execution environment. Future work should explore improvement to the execution agent so that more complicated types of ideas (e.g., training additional auxiliary models or system-level optimizations) can be implemented correctly.\n\n|  Successful Execution | Failed Execution  |\n| --- | --- |\n|  [Experiment] Sequence Position Weighted Trust Region: Apply tighter sigmoid bounds to earlier tokens in the sequence (where errors compound) and looser bounds to later tokens. Weight: position_weight = 1 - 0.3 * (position / seq_len), effective_deviation = 0.25 + 0.2 * position_weight. This accounts for the sequential nature of autoregressive generation. | [Experiment] Hierarchical Position-Group Trust Region: Apply trust region at two hierarchical levels - group level (shared within each response group) and position level (varies along sequence). Groups with high internal reward variance get tighter group-level bounds. Within groups, positions follow the proven decay pattern. This captures both cross-sample and within-sample structure. Formula: group_dev = 0.4 - 0.15 * tanh(group Reward_var / 0.3), position_factor = 1 - 0.2 * rel_pos, effective_dev = group_dev * position_factor.  |\n|  [Code Changes] Modify grpo.py: Initialize current_cliprange = 0.2, ema Clip_fraction = 0.15. Standard momentum clip updates. Modify compute grpo Clip_loss in grpo_utils.py: After computing ratio on line 91 (shape: batch_size x seq_len): batch_size, seq_len = ratio.shape, positions = torch.arange(seq_len, device=ratio_device).float().unsqueeze(0).expand(batch_size, -1), position_weight = 1.0 - 0.3 * (positions / (seq_len - 1 + 1e-6)), effective_deviation = 0.25 + 0.2 * position_weight. Apply position-aware sigmoid: centered_ratio = ratio - 1.0, bounded_ratio = 1.0 + (2.0 * torch.sigmoid(centered_ratio) - 1.0) * effective_deviation. Use: surr1 = bounded_ratio * advantages, surr2 = torch.clamp(bounded_ratio, 1 - cliprange, 1 + cliprange) * advantages, loss = -torch.min(surr1, surr2). Add metadata: metadata[\"mean_effective_deviation\"] = effective_deviation.mean().item(), metadata[\"early_deviation\"] = effective_deviation[:, :seq_len//4].mean().item(), metadata[\"late_deviation\"] = effective_deviation[:, -seq_len//4:].mean().item(). | [Code Changes] Modify grpo.py: Initialize current_cliprange = 0.2, ema Clip_fraction = 0.15. Standard momentum clip updates. Pass group_size to function. Modify compute grpo Clip_loss in grpo_utils.py: Add parameter group_size=8. After computing ratio: batch_size, seq_len = ratio.shape, n_groups = batch_size // group_size. Compute group reward variance from advantages as proxy: adv_group = advantages.view(n_groups, group_size, -1), group_adv_var = adv_grouped.var(dim=1, keepdim=True), group_adv_var Expanded = group_adv_var expand(-1, group_size, -1).reshape(advantages.shape). Group-level deviation: group_deviation = 0.4 - 0.15 * torch.tanh(group_adv_var Expanded / 0.3). Position factor: positions = torch.arange(seq_len, device=ratio_device).float().unsqueeze(0).expand(batch_size, -1), rel_pos = positions / (seq_len - 1 + 1e-6), position_factor = 1.0 - 0.2 * rel_pos. Hierarchical deviation: effective_deviation = group_deviation * position_factor, effective_deviation = torch.clamp(effective_deviation, 0.15, 0.45). Apply: centered_ratio = ratio - 1.0, bounded_ratio = 1.0 + (2.0 * torch.sigmoid(centered_ratio) - 1.0) * effective_deviation. Use: surr1 = bounded_ratio * advantages, surr2 = torch.clamp(bounded_ratio, 1 - cliprange, 1 + cliprange) * advantages, loss = -torch.min(surr1, surr2). Add metadata[\"mean_group_var\"] = group_adv_var.mean().item(), metadata[\"mean_effective_deviation\"] = effective_deviation.mean().item(). Log to wandb.  |\n\nTable 4: Additional examples on the GRPO environment. Ideas are generated by Claude-4.5-Opus during evolutionary search.\n\n## Towards Execution-Grounded Automated AI Research\n\n|  Successful Execution | Failed Execution  |\n| --- | --- |\n|  [Experiment] Create a mathematical step-complexity aware reward shaping where responses with more mathematical reasoning steps receive slightly higher base rewards (1.05x for 3+ steps, 1.1x for 5+ steps) when correct, encouraging thorough mathematical exposition without changing the core binary reward structure. | [Experiment] Implement temporal difference advantage estimation where advantages incorporate not just current rewards but also predicted future rewards using a learned value function, combined with the proven 3e-5 learning rate and reinforce_with_baseline for more accurate credit assignment.  |\n|  [Code Changes] Modify r1_zero Reward_fn_train in drgrpo-grader.py to count reasoning steps by detecting mathematical transitions (\"therefore\", \"thus\", \"=\", \"=>\"). When answer is correct, apply step-based multiplier: step-multiplier = 1.0 + 0.05 * min(2, max(0, num_steps - 2)) to get multipliers [1.0, 1.05, 1.1]. Return {\"format Reward\": 1.0, \"answer Reward\": answer Reward, \"reward\": base Reward * step-multiplier}. Set -learning_rate 3e-5 and -loss_type reinforce_with_baseline. | [Code Changes] Add a value head to the policy model in grpo.py that predicts expected future rewards. Compute TD advantages as advantages = rewards + gamma * next_values - current_values with gamma=0.99. Train the value function with MSE loss on observed returns. Modify compute_group_normalized Rewards to use TD advantages instead of basic reward differences. Set -learning_rate 3e-5 and -loss_type reinforce_with_baseline.  |\n|  Validation Accuracy: 65.6 |   |\n|  [Experiment] Add experience replay by maintaining a buffer of the top 20% highest-reward rollouts from previous epochs and mixing them (25% replay, 75% new) with current rollouts during training, combined with 3e-5 learning rate and reinforce_with_baseline for improved sample efficiency. | [Experiment] Ensemble Decision Training with Voting Consensus: Train the model using ensemble-style decision making where each rollout generates multiple candidate responses, and the final training signal is based on majority voting among responses. This encourages the model to develop more robust and consistent reasoning patterns while maintaining diversity in solution approaches.  |\n|  [Code Changes] Modify train_loop in grpo.py to maintain replay_buffer storing high-reward (>0.8) rollouts from previous epochs. Each epoch, sample 25% of training data from replay buffer and 75% from new rollouts. Update buffer by adding top 20% of current epoch's rollouts and removing oldest entries when buffer exceeds 1000 samples. Set -learning_rate 3e-5 and -loss_type reinforce_with_baseline. | [Code Changes] Modify sample_rollout in sample.py to generate 3 responses per prompt instead of 1, using different random seeds. Implement voting consensus in r1_zero Reward_fn_train: if 2+ responses are correct, apply +0.15 consensus bonus; if responses disagree, apply -0.05 uncertainty penalty. In train_loop in grpo.py, select the highest-voted response for training while using consensus information to adjust learning rate: consensus_lr = 3e-5 * (0.9 + 0.2 * consensus_rate). Set group_size=6,-loss_type reinforce_with_baseline.  |\n|  Validation Accuracy: 39.4 |   |\n|  [Experiment] Implement response diversity rewards within groups where responses to the same prompt receive bonus rewards (0.05-0.15) for being dissimilar to other responses in their group, encouraging exploration of different solution paths while maintaining the proven group_size=8 and 3e-5 learning rate combination. | [Experiment] Implement hierarchical advantage estimation where advantages are computed at both token-level and sequence-level, with token-level advantages weighted by their position importance (higher weights for mathematical expressions and final answers), combined with the successful 3e-5 learning rate and reinforce_with_baseline.  |\n|  [Code Changes] Modify compute_group_normalized Rewards in grpo_utils.py to compute pairwise similarity between responses in each group using token-level Jaccard similarity. Add diversity bonus: diversity Reward = 0.15 * (1 - max_similarity_in_group) to each response's reward before advantage computation. Reshape responses into groups, compute similarities, and add bonuses before advantage normalization. Set -learning_rate 3e-5,-loss_type reinforce_with_baseline,-group_size 8. | [Code Changes] Modify grpo_microbatch_train_step in grpo_utils.py to create position importance weights that assign 2.0x weight to tokens containing mathematical symbols (\\frac{+, -, *, =) and 1.5x weight to answer sections. Compute both sequence-level advantages (current) and token-level advantages, then combine as final Advantages = 0.6 * sequence Advantages + 0.4 * token Advantages. Set -learning_rate 3e-5 and -loss_type reinforce_with_baseline.  |\n|  Validation Accuracy: 19.2 |   |\n\nTable 5: Additional examples on the GRPO environment. Ideas are generated by Claude-4.5-Sonnet during evolutionary search.\n\nWe also present the top-performing ideas from Claude-4.5-Opus, Claude-4.5-Sonnet, and GPT-5 on the nanoGPT environment below:\n\nClaude-4.5-Opus Idea on nanoGPT (Validation Loss: 3.1407)\n\n[Experiment] Wider SwiGLU (5x) with MLP Output Scaling (Init 0.97), Skip Connections Every 4 and 8 Layers with Learnable Weights (Init 0.52 and 0.31), Separate Attention/MLP Scales (Init 0.98), Higher LR (0.00168), Reduced Weight Decay (0.065), Warmup 173 iters, Lower Min LR (0.03x), Cosine Annealing, EMA, Untied Embeddings, and Beta2=0.99\nMake the dual skip connection weights learnable parameters initialized at proven good values. This allows the model to adapt skip weights during training while combining with separate attention/MLP residual scales.\n\n[Code Changes]\n\n- Change warmup_iters = 256 to warmup_iters = 173 in Hyperparameters class\n- Change weight_decay = 0.1 to weight_decay = 0.065 in Hyperparameters class\n- Change learning_rate = 0.0015 to learning_rate = 0.00168 in Hyperparameters class\n- In GPT.__init__, add after transformer dict:\n\nself.skip_weight_4 = nn.Parameter(torch.tensor(0.52))\nself.skip_weight_8 = nn.Parameter(torch.tensor(0.31))\n- In Block.__init__, add: self.attn_scale = nn.Parameter(torch.tensor(0.98)) and\nself.mlp_scale = nn.Parameter(torch.tensor(0.98))\n- In Block.forward, change to:\n\ndef forward(self, x):\nx = x + self.attn_scale * self.attn(rmsnorm(x))\nx = x + self.mlp_scale * self.mlp(rmsnorm(x))\nreturn x\n- In Block.forward_with_cache, change to:\n\ndef forward_with_cache(self, x, cache):\nattn_out, new_cache = self.attn.forward_with_cache(rmsnorm(x),\ncache=cache)\nx = x + self.attn_scale * attn_out\nx = x + self.mlp_scale * self.mlp(rmsnorm(x))\nreturn x, new_cache\n- In MLP.__init__, replace lines 81‚Äì82 with:\n\nself.c_fc = nn.Linear(config.n_embd, 5 * config.n_embd, bias=False)\nself.c_gate = nn.Linear(config.n_embd, 5 * config.n_embd, bias=False)\nself.c_proj = nn.Linear(5 * config.n_embd, config.n_embd, bias=False)\nself.output_scale = nn.Parameter(torch.tensor(0.97))\n- In MLP.forward, replace with:\n\ndef forward(self, x):\ngate = F.silu(self.c_gate(x))\nx = self.c_fc(x) * gate\nx = self.c_proj(x) * self.output_scale\nreturn x\n\n## Towards Execution-Grounded Automated AI Research\n\n- In GPT.__init_, remove line 132: self.transformer.wte.weight = self.lm_head.weight\n- Remove line 131: self.lm_head.LLMC_SKIP_INIT = 1\n- Modify __initweights to add: if isinstance/module, nn.Linear): torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n- Change optimizer betas on line 402 to betas=(0.9, 0.99)\n- Modify get_lr function:\n\n```python\ndef get_lr(it):\nassert it &lt;= args.num_iterations\nif it &lt; args.warmup_iteres:\nreturn args.learning_rate * (it+1) / args.warmup_iteres\nmin_lr = 0.03 * args.learning_rate\ndecay_ratio = (it - args.warmup_iteres) / (args.num_iterations - args.warmup_iteres)\nreturn min_lr + 0.5 * (args.learning_rate - min_lr) * (1.0 + math.cos(math.pi * decay_ratio))\n```\n\n- In GPT.forward, replace the block loop with:\n\n```python\nlayer_outputs = []\nfor i, block in enumerate(self.transformer.h):\nif i &gt;= 4 and i % 4 == 0:\nx = x + self.skip_weight_4 * layer_outputs[i-4]\nif i &gt;= 8 and i % 8 == 0:\nx = x + self.skip_weight_8 * layer_outputs[i-8]\nx = block(x)\nlayer_outputs.append(x)\n```\n\n- In GPT.forward_with_cache, replace the block loop with:\n\n```python\nlayer_outputs = []\nfor i, block in enumerate(self.transformer.h):\nif i &gt;= 4 and i % 4 == 0:\nx = x + self.skip_weight_4 * layer_outputs[i-4]\nif i &gt;= 8 and i % 8 == 0:\nx = x + self.skip_weight_8 * layer_outputs[i-8]\nx, new_cache = block.forward_with_cache(x, cache= caches[i])\nnew_caches.append(new_cache)\nlayer_outputs.append(x)\n```\n\n- After model initialization, add: ema_model = {k: v.clone() for k, v in raw_model.state_dict().items()} and ema Decay = 0.999\n- After optimizer.step(), add: for k, v in raw_model.state_dict().items(): ema_model[k].mul_(ema Decay).add_(v, alpha=1-ema Decay)\n- Before validation, add: orig_state = {k: v.clone() for k, v in raw_model.state_dict().items()}; raw_model.load_state_dict(ema_model)\n- After validation, add: raw_model.load_state_dict(orig_state)\n\n## Towards Execution-Grounded Automated AI Research"}, {"id": 20, "title": "Claude-4.5-Sonnet Idea on nanoGPT (Validation Loss: 3.2081)", "content": "# Claude-4.5-Sonnet Idea on nanoGPT (Validation Loss: 3.2081)\n\n[Experiment] Two-phase weight decay  $(0.1170\\rightarrow 0.0210$  at  $59.65\\%$  +  $30.45\\%$  plateau  $^+$  LR 0.001550  $^+$  warmup  $197+$  two-phase grad clip  $(1.054\\rightarrow 0.916$  at  $59.65\\%$  + quadratic min_lr interpolation  $(0.0113\\mathrm{x}$  at  $59.65\\%$  , 0.0075x at end via quadratic)  $^+$  progressive EMA  $(0.999\\rightarrow 0.9992$  linear over training)  $^+$  exponential warmup  $^+$  cosine LR  $^+$  beta2=0.99\n\nUse smooth quadratic interpolation for min_lr during low-WD phase AND progressive EMA that gradually increases from 0.999 to 0.9992 linearly throughout training. Early training benefits from faster EMA tracking, while later training gets heavier smoothing. Use conservative settings: WD 0.1170/0.0210, extended plateau  $30.45\\%$ , moderate LR 0.001550, longest warmup 197, tight grad clip  $1.054 \\rightarrow 0.916$ .\n\n[Code Changes] Modify line 326 to change warmup_iter  $= 256$  to warmup_iter  $= 197$ . Modify line 325 to change learning_rate  $= 0.0015$  to learning_rate  $= 0.001550$ . Modify line 402 to change betas=(0.9, 0.95) to betas=(0.9, 0.99). Modify the get_lr function: replace lines 408-414 with:\n\n```python\nif it &lt; args.warmup_iter:\nprogress = (it + 1) / args.warmup_iter\nreturn args.learning_rate * (1.0 - math.exp(-5.0 * progress))\nplateau_end = int(0.3045 * args.num_iterations)\nif it &lt; plateau_end:\nreturn args.learning_rate\noverall_progress = it / args.num_iterations\ndecay_ratio = (it - plateau_end) / (args.num_iterations - plateau_end)\ncoeff = 0.5 * (1.0 + math.cosmath.pi * decay_ratio))\nif overall_progress &lt;= 0.5965:\nmin_lr_factor = 0.0113\nelse:\nphase2_progress = (overall_progress - 0.5965) / (1.0 - 0.5965)\nmin_lr_factor = 0.0113 - (0.0113 - 0.0075) * (phase2_progress ** 2)\nmin_lr = min_lr_factor * args.learning_rate\nreturn min_lr + coeff * (args.learning_rate - min_lr)\n```\n\n## Modify line 527 to:\n\n```python\nprogress = step / args.num_iterations\ncurrent Clip = 0.916 if progress &gt; 0.5965 else 1.054\nnorm = torch.nn.utilsclip_grad_norm_model.params(), current Clip)\n```\n\n## After line 529, add:\n\n```python\nprogress = step / args.num_iterations\ncurrent_wd = 0.0210 if progress &gt; 0.5965 else 0.1170\nfor param_group in optimizer param_groups:\nparam_group['weight Decay'] = current_wd\n```\n\n## After line 387, add:\n\n```python\nema_model = {name: param.clone().detach() for name, param in raw_model.named_parameters()}\n```\n\n## After line 533, add:\n\n```python\nif step &gt; 0:\nprogress = step / args.num_iterations\nema Decay = 0.999 + 0.0002 * progress\nfor name, param in raw_model.named_parameters():\nema_model[name].mul_(ema Decay).add_(param.data, alpha=1 - ema Decay)\n```\n\n## Before line 483, add:\n\n```python\noriginal.params = {name: param.data.clone() for name, param in\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\nraw_model.named_parameters() {\nfor name, param in raw_model.named_parameters():\nparam.data.copy_(ema_model[name])\n## After line 509, add:\nfor name, param in raw_model.named_parameters():\nparam.data.copy_(original_params[name])\n```"}, {"id": 21, "title": "GPT-5 Idea on nanoGPT (Validation Loss: 3.1697)", "content": "# GPT-5 Idea on nanoGPT (Validation Loss: 3.1697)\n\n[Experiment] SwiGLU-3.5x + Residual Alphas + Min-Floor Cosine + Per-step Beta2 Linear Decay + 3-Group AdamW + Debiased EMA"}, {"id": 22, "title": "[Code Changes]", "content": "# [Code Changes]\n\n- Hyperparameters: hidden_factor=3.5, warmup_iters=256, lr_peak_factor=1.10, min_lr_factor=0.02, beta2_start=0.99, beta2_end=0.95, wd Decay=0.1, wd_embedding=0.01, ema Decay=0.9995, ema_warmup_steps=256.\n- MLP: SwiGLU; Block alphas init 0.9.\n- Optimizer: 3-group AdamW.\n- LR: warmup to peak; cosine to floor as before.\n- Per-step beta2 update: After setting lr each step, set\n\n```txt\nbeta2 = beta2_start + (beta2_end - beta2_start) min(1.0,  $\\frac{\\text{it} + 1}{\\text{args.num_iterations}}$ );\n```\n\nupdate all param_groups betas.\n\n- EMA: maintain ema.params with debiasing at eval (divide by 1 - ema Decay \\*\\*ema_step), then restore.\n\nWhile the best-performing ideas on nanoGPT tend to be heavily optimized with extensive hyper-parameter tuning mixed with various architecture tweaks, we also pick a few more ‚Äúatomic‚Äù algorithmic ideas that are successfully executed.\n\nExamples from Claude-4.5-Opus\n\n- Head-Wise Attention Output Scaling Add learnable per-head scaling factors to attention, allowing different heads to contribute with different magnitudes to the output.\nValidation Loss: 3.2386\n- Learned Residual Connection Weights Add learnable scalar weights for each residual connection that are initialized to 1.0, allowing the model to learn optimal residual scaling during training.\nValidation Loss: 3.2517\n- Mixture of Embeddings with Position Learn to mix token embeddings and position embeddings with a content-dependent weight, allowing the model to dynamically balance positional vs semantic information per token.\nValidation Loss: 3.2497\n- Shared Input-Output Embedding with Learned Asymmetry Keep weight tying but add a small learned transformation on the output side, providing the benefits of weight tying while allowing output-specific adaptation.\nValidation Loss: 3.2499\n- Gated Final Normalization Replace the final RMSNorm before lm_head with a gated version where a learned gate controls how much normalization is applied vs passing the raw representation.\nValidation Loss: 3.2503\n- Position-Aware MLP Gating Gate the MLP output based on position information, allowing the model to learn position-dependent processing depth.\nValidation Loss: 3.2506\n- Learned Residual Connection Weights Add learnable scalar weights for each residual connection that are initialized to 1.0, allowing the model to learn optimal residual scaling during training.\nValidation Loss: 3.2517\n- Grouped Token Embeddings Group the vocabulary into clusters and add a learned embedding per cluster on top of token embeddings, providing hierarchical vocabulary structure.\nValidation Loss: 3.2521\n\nLastly, we present several interesting ideas on the nanoGPT environment that didn‚Äôt get successfully executed. These examples are generated by Claude-4.5-Opus.\n\n- Soft Layer Repetition Allow the model to softly repeat computation through layers by adding a learned gate that mixes the current layer‚Äôs input back into its output, simulating variable depth.\n- Causal Context Compression Before each attention layer, apply a learned compression that mixes local context (previous 2-3 tokens) into the current representation, providing implicit local context without convolutions.\n- Attention Head Specialization via Orthogonal Loss Add a soft penalty that encourages different attention heads to attend to different patterns by penalizing similarity between head outputs.\n- Skip Connections with Learned Residual Weights Combine skip connections with learned residual weights. The skip connections provide alternative gradient paths while learned weights allow adaptive scaling.\n- Token Difficulty-Aware Loss Weighting Weight the loss contribution of each token based on the model‚Äôs uncertainty (entropy) at that position, focusing learning on difficult tokens while not over-optimizing easy ones.\n\nA.3 Code Execution Examples\n\nWe present a few ideas with their full code execution to demonstrate the full end-to-end trajectories. All examples below are from Claude-4.5-Sonnet on the GRPO environment. For each example, we first present the natural language idea, followed by the code implementation generated by Claude-4.5-Sonnet.\n\n###### Example 1\n\n[Experiment] Create mathematical working memory simulation by maintaining a context buffer of mathematical facts, definitions, and intermediate results during problem solving. This buffer gets updated as the model works through problems and provides additional context for subsequent mathematical steps, simulating how humans maintain mathematical working memory during complex calculations.\n\n[Code Changes] Modify the prompt template in sample.py to include a ‚ÄúMathematical Context‚Äù section that accumulates relevant mathematical facts during problem solving. Implement context buffer management in train_loop in grpo.py that maintains recent mathematical definitions, formulas, and intermediate calculations across problems in the same batch. Update the buffer with key mathematical expressions extracted from each response. Use enhanced prompts that include relevant context from the buffer. Set -learning_rate 2.5e-5 and -loss_type reinforce_with_baseline.\n\n##### Validation Accuracy: 58.0\n\n‚¨á\n1 --- repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_43/sample.py\n2 +++ repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_43/sample.py\n3 @@ -2,6 +2,7 @@ from vllm import LLM, SamplingParams\n4 import json\n5 from tqdm import tqdm\n6 import random\n7 +import re\n8\n9 def load_prompt_template(prompt_path = \"prompts/r1_zero.prompt\"):\n10 with open(prompt_path, 'r') as f:\n11 @@ -20,12 +21,52 @@ def get_model_and_sampling_params(model_path = \"Qwen/Qwen2.5-Math-1.5B\", token\n12\n13 return llm, sampling_params\n14\n15 -def load_dataset_with_prompt_template(prompt_template, dataset_path=\"/juice5b/scr5b/nlp/aihinton/CS336_data/a5-alignment/MATH/test.jsonl\"):\n16 +def extract_mathematical_expressions(text):\n17 + \"\"\"Extract mathematical expressions, formulas, and key facts from text.\"\"\"\n18 + expressions = []\n19 +\n20 + # Extract LaTeX math expressions\n21 + latex_patterns = [\n22 + r'\\${^$}]+\\$', # Inline math\n23 + r'\\\\[a-zA-Z]+\\{[^^}]+\\}', # LaTeX commands\n24 + r'\\\\[frac\\{[^^}]+\\\\}{[^^}]+\\}', # Fractions\n25 + r'\\\\sqrt\\{[^^}]+\\}', # Square roots\n26 + r'[a-zA-Z]\\s*=\\s*[^,\\n\\.]+', # Variable assignments\n27 + ]\n28 +\n29 + for pattern in latex_patterns:\n30 + matches = re.findall(pattern, text)\n31 + expressions.extend(matches)\n32 +\n33 + # Extract definitions and key facts\n34 + definition_patterns = [\n35 + r'[A-Z][a-z]+ is defined as [^\\.]+\\.',\n36 + r'Let [a-zA-Z] be [^\\.]+\\.',\n\n37 + r'Given that [^\\.]+\\.',\n38 + r'Since [^\\.]+\\.',\n39 + r'Therefore [^\\.]+\\.',\n40 + ]\n41 +\n42 + for pattern in definition_patterns:\n43 + matches = re.findall(pattern, text)\n44 + expressions.extend(matches)\n45 +\n46 + return list(set(expressions)) # Remove duplicates\n47 +\n48 +def create_context_prompt(base_template, context_items):\n49 + \"\"\"Create a prompt with mathematical context.\"\"\"\n50 + if context_items:\n51 + context_section = \"Mathematical Context:\\n\" + \"\\n\".join(f\"- {item}\" for item in context_items[-10:]) + \"\\n\\n\"\n52 + return context_section + base_template\n53 + return base_template\n54 +\n55 +def load_dataset_with_prompt_template(prompt_template, dataset_path=\"/juice5b/scr5b/nlp/aihinton/CS336_data/a5-alignment/MATH/test.jsonl\", context_items= None):\n56 with open(dataset_path, 'r') as f:\n57 dataset = [json.loads(line) for line in f]\n58 print(f\"Loaded {len(dataset)} examples from {dataset_path}\")\n59\n60 - prompts = [prompt_template.format(question=example[\"problem\"]) for example in dataset]\n61 + enhanced_template = create_context_prompt(prompt_template, context_items)\n62 + prompts = [enhanced_template.format(question=example[\"problem\"]) for example in dataset]\n63 answers = [example[\"answer\"] for example in dataset]\n64 return prompts, answers\n65\n66 @@ -93,4 +134,42 @@\n67 if return_rewards:\n68 return batched_prompts, batched_answers, batched_responses, batched_rewards\n69 return results\n70 -\n71 +\n72 +class MathContextBuffer:\n73 + \"\"\"Buffer to maintain mathematical working memory across problems.\"\"\"\n74 +\n75 + def __init__(self, max_size=50):\n76 + self.buffer = []\n77 + self.max_size = max_size\n78 +\n79 + def add_expressions(self, expressions):\n80 + \"\"\"Add new mathematical expressions to the buffer.\"\"\"\n81 + for expr in expressions:\n82 + if expr and len(expr.strip()) > 3: # Filter out very short expressions\n83 + self.buffer.append(expr.strip())\n84 +\n85 + # Keep only the most recent items\n86 + if len(self.buffer) > self.max_size:\n87 + self.buffer = self.buffer[-self.max_size:]\n88 +\n89 + def get_relevant_context(self, query_text, max_items=10):\n90 + \"\"\"Get the most relevant context items for a given query.\"\"\"\n91 + if not self.buffer:\n92 + return []\n\n93 + # Simple relevance scoring based on shared mathematical terms\n94 + scored_items = []\n95 + query_lower = query_text.lower()\n96 +\n97 + for item in self.buffer[-20:]: # Consider recent items\n98 + score = sum(1 for word in item.lower().split() if word in query_lower )\n99 +\n100 + if score > 0:\n101 + scored_items.append((score, item))\n102 +\n103 + # Return top items sorted by relevance\n104 + scored_items.sort(reverse=True, key=lambda x: x[0])\n105 + return [item[1] for item in scored_items[:max_items]]\n106 +\n107 + def clear(self):\n108 + \"\"\"Clear the context buffer.\"\"\"\n109 + self.buffer = []\n110 ---- repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_43/grpo.py\n111 +++ repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_43/grpo.py\n112 @@ -1,6 +1,6 @@\n113 from transformers import AutoModelForCausalLM, AutoTokenizer\n114 from utils import tokenize_prompt_and_output, get_response_log_probs\n115 -from sample import load_prompt_template, load_dataset_with_prompt_template,\n116 sample_rollout\n117 +from sample import load_prompt_template, load_dataset_with_prompt_template,\n118 sample_rollout, MathContextBuffer, extract_mathematical_expressions,\n119 create_context_prompt\n120 from drgrpo_grader import rl_zero_reward_fn_train\n121 from evaluate import rl_zero_reward_fn_eval, evaluate_vllm\n122 from grpo_utils import compute_group_normalized_rewards,\n123 grpo_microbatch_train_step\n124 @@ -66,6 +66,9 @@ def evaluate_model(policy_model, vllm_model, eval_prompts,\n125 eval_answers, eval_s\n126 def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps,\n127 train_steps_per_rollout, output_dir, batch_size, gradient_accumulation_steps =\n128 4, group_size = 2, rollout_subset_size = 256, device = \"cuda\", logging_steps\n129 = 20, saving_steps = 4000, eval_epochs = 5, eval_prompts = None, eval_answers\n130 = None, sampling_params = None, eval_vllm_model = None, cliprange = 0.2,\n131 loss_type = \"reinforce_with_baseline\"):\n132 model.to(device)\n133 training_steps = grpo_steps\n134 +\n135 + # Initialize mathematical context buffer\n136 context_buffer = MathContextBuffer(max_size=100)\n137 optimizer = torch.optim.AdamW(model.params(), lr=learning_rate,\n138 weight_decay=0.0, betas=(0.9, 0.95))\n139 global_step = 0 # Initialize global step counter\n140 @@ -85,8 +88,31 @@ def train_loop(model, train_prompts, train_answers,\n141 learning_rate, grpo_steps,\n142 load_policy_into_vllm_instance(model, vllm_model)\n143\n144 ## sample rollouts\n145 + # Get enhanced prompts with mathematical context for this epoch\n146 print (\"Sampling rollouts for epoch: \", epoch)\n147 rollout_prompts, rollout_answers, rollout_responses, rollout_rewards =\n148 sample_rollout(vllm_model, rl_zero_reward_fn_train, train_prompts,\n149 train_answers, G=group_size, eval_sampling_params=eval_sampling_params,\n150 subset_size=rollout_subset_size, return_rewards=True, batch_size=512)\n151 +\n152 + # Create enhanced prompts with context for this batch\n153 enhanced_prompts = []\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n140 + base_template = load_prompt_template()\n141 +\n142 + # Select subset of problems for this epoch\n143 + if rollout Subset_size is not None:\n144 + indices = random.sample(range(len(train_prompts)), rollout Subset_size)\n145 + epoch_prompts = [train_prompts[i] for i in indices]\n146 + epoch_answers = [train_answers[i] for i in indices]\n147 + else:\n148 + epoch_prompts = train_prompts\n149 + epoch_answers = train_answers\n150 +\n151 + # Create context-enhanced prompts\n152 + for prompt in epoch_prompts:\n153 + relevant_context = context_buffer.get_relevant_context(prompt, max_items=8)\n154 + enhanced_prompt = create_context_prompt(base_template.format(question =prompt.split(\"Question:\")[-1]), relevant_context)\n155 + enhanced_prompts.append(enhanced_prompt)\n156 +\n157 + # Sample with enhanced prompts (need to adapt sample_rollout for direct prompt input)\n158 + rollout_prompts, rollout_answers, rollout_responses, rollout_rewards = sample_rollout_with_prompts(vllm_model, r1_zero_reward_fn_train, enhanced_prompts, epoch_answers, G=group_size, eval_sampling.params= eval_sampling.params, return_rewards=True, batch_size=512)\n159 +\n160 + # Randomly sample 2 rollouts to print\n161 indices = random.sample(range(len(rollout_prompts)), 2)\n162 print (\"Example rollouts:\")\n163 @@ -96,6 +122,15 @@ def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps,\n164 print(f\"Response: {rollout_responses[idx]})\")\n165 print(f\"Reward: {rollout_rewards[idx]})\")\n166 print(f\"Ground truth: {rollout_answers[idx]})\")\n167 +\n168 + # Update context buffer with mathematical expressions from responses\n169 + print(\"Updating mathematical context buffer...\")\n170 + for response in rollout_responses:\n171 + if rollout_rewards[rollout_responses.index(response)] &gt; 0: # Only add from correct responses\n172 + mathematical_expressions = extract_mathematical_expressions( response)\n173 + if mathematical_expressions:\n174 + context_buffer.add_expressions(mathematical_expressions)\n175 +\n176 rollout_tokenized = tokenize_prompt_and_output(rollout_prompts, rollout_responses, tokenizer)\n177 rollout_dataloader = create_dataloader(rollout_tokenized, batch_size= batch_size, shuffle=False)\n178\n179 @@ -196,6 +231,47 @@ def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps,\n180 }, step=global_step)\n181 # print (\"Global Step: \", global_step, \"Loss: \", loss_item(), \"Entropy: \", entropy.mean().item(), \"Clip fraction: \", metadata.get(\"clip_fraction\", 0.0))\n182\n183 +def sample_rollout_with_prompts(\n184 + vllm_model,\n185 + reward_fn,\n186 + prompts,\n187 + answers,\n\n## 188 + G,\n189 + eval_sampling_params,\n190 + return_rewards=False,\n191 + batch_size=64\n192 +):\n193 + \"\"\"Sample rollouts using provided prompts directly.\"\"\"\n194 + # Create batched prompts by repeating each prompt G times\n195 + batched_prompts = []\n196 + batched_answers = []\n197 + for prompt, answer in zip(prompts, answers):\n198 + batched_prompts.extend([prompt] * G)\n199 + batched_answers.extend([answer] * G)\n200 +\n201 + # Process in batches to avoid OOM\n202 + all_outputs = []\n203 + for i in range(0, len(batched_prompts), batch_size):\n204 + batch_prompts = batched_prompts[i:i + batch_size]\n205 + batch_outputs = vllm_model.generate(batch_prompts, eval_sampling_params)\n206 + all_outputs.extend(batch_outputs)\n207 +\n208 + # Process results\n209 + batched_responses = []\n210 + batched_rewards = []\n211 + total_rewards = 0\n212 +\n213 + for output, answer in tqdm(zip(all_outputs, batched_answers)):\n214 + generated_answer = output.outputs[0].text\n215 + reward = reward_fn(generated_answer, answer)\n216 + batched_responses.append(generated_answer)\n217 + batched_rewards.append(reward[\"reward\"])\n218 + total_rewards += reward[\"reward\"]\n219 +\n220 + print(f\"Accuracy of sampled rollouts: {total_rewards}/{len(batched_prompts)}\n221 = {total_rewards / len(batched_prompts) * 100}%\")\n222 +\n223 + return batched_prompts, batched_answers, batched_responses, batched_rewards\n224 +\n225 if __name__ == \"__main__\":\n226 parser = argparse.ArgumentParser()\n227 parser.add_argument (\"--model_path\", type=str, default=\"Qwen/Qwen2.5-Math-1.5B\")\n228 @@ -203,7 +279,7 @@ if __name__ == \"__main__\":\n229 parser.add_argument (\"--train_dataset_path\", type=str, default=\"../MATH/train.jsonl\")\n300 parser.add_argument (\"--eval_dataset_path\", type=str, default=\"../MATH/test.jsonl\")\n301 parser.add_argument (\"--output_dir\", type=str, default=\"ckpts/\")\n302 parser.add_argument (\"--learning_rate\", type=float, default=1e-5)\n303 parser.add_argument (\"--learning_rate\", type=float, default=2.5e-5)\n304 parser.add_argument (\"--grpo_steps\", type=int, default=200)\n305 parser.add_argument (\"--group_size\", type=int, default=8)\n306 parser.add_argument (\"--rollout_subset_size\", type=int, default=256)\n307 @@ -212,7 +288,7 @@ if __name__ == \"__main__\":\n308 parser.add_argument (\"--gradient_accumulation_steps\", type=int, default=16)\n309 parser.add_argument (\"--batch_size\", type=int, default=4)\n310 parser.add_argument (\"--cliprange\", type=float, default=0.2)\n311 parser.add_argument (\"--loss_type\", type=str, default=\"grpo_clip\")\n312 parser.add_argument (\"--loss_type\", type=str, default=\"reinforce_with_baseline\"\n313 parser.add_argument (\"--wandb_project\", type=str, default=\"grpo-math\")\n314 parser.add_argument (\"--wandb_name\", type=str, default=\"grpo_clip_1\")\n315 args = parser.parse_args()\n316 --- repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_43/run_job.sh\n\n## Towards Execution-Grounded Automated AI Research\n\n```txt\n246 +++ repo_variants_full_pipeline_bsz50_fixd2_epoch7/idea_43/run_job.sh\n247 @@ -21,7 +21,7 @@ timeout 2h uv run \\\n248 --index https://download.pytorch.org/whl/cu128 \\\n249 --index-strategy unsafe-best-match \\\n250 python grpo.py \\\n251 --learning_rate 1e-5 \\\n252 + --learning_rate 2.5e-5 \\\n253 --grpo_steps 20 \\\n254 --group_size 8 \\\n255 --rollout Subset_size 128 \\\n256 @@ -30,7 +30,7 @@ timeout 2h uv run \\\n257 --gradient Accumulation_steps 16 \\\n258 --batch_size 4 \\\n259 --cliprange 0.2 \\\n260 --loss_type grpo Clip \\\n261 + --loss_type reinforce_with_baseline \\\n262 --wandb_name $wandb_name\n263\n264 echo \"Experiment finished successfully!\"\n\n## Example 2\n\n[Experiment] Implement mathematical solution robustness training through systematic perturbation testing that teaches the model to solve mathematically equivalent problems with varied presentations, notation styles, and problem phrasings. Combine this with proven reward shaping by creating robustness-aware rewards that encourage mathematical understanding that generalizes across problem variations.\n\n[Code Changes] Create problem perturbation system in sample.py that generates equivalent mathematical problems with varied notation, different variable names, alternative problem phrasing, and equivalent mathematical formulations. Track solution consistency across perturbations. Enhance r1_zero_reward_fn_train in drgrpo_grader.py to reward robustness: robustness_bonus = 0.12 * consistency_score_across_perturbations where consistency measures solution method stability across equivalent problems. Apply generalization bonuses: +0.08 for solving problems with unfamiliar notation, +0.06 for handling varied problem presentations. Modify train_loop in grpo.py to include perturbation-based training batches and track robustness improvement. Use robustness-aware learning rate with cosine annealing from 3.4e-5 to 1.8e-5. Implement perturbation-complexity gradient clipping. Set -loss_type reinforce_with_baseline.\n\nValidation Accuracy: 56.2\n\nExample 2 code diff 1 --- repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_37/sample.py 2 +++ repo_variants_full_pipeline_bsz50_fixed2_epoch7/idea_37/sample.py 3 @@ -2,6 +2,7 @@ from vllm import LLM, SamplingParams 4 import json 5 from tqdm import tqdm 6 import random 7 +import re 8 9 def load_prompt_template(prompt_path = \"prompts/r1_zero.prompt\"): with open(prompt_path, 'r') as f: 10 @@ -29,6 +30,125 @@ def load_dataset_with_prompt_template(prompt_template, dataset_path=\"/juice5b/s answers = [example[\"answer\"] for example in dataset] return prompts, answers 11 12 +def create_notation_perturbations(problem_text): 13 + \"\"\"Create mathematical notation variations\"\"\" 14 + perturbations = [] 15 + # Variable substitutions 16 + var_substitutions = [ ('x', 'a'), ('y', 'b'), ('z', 'c'), ('n', 'm'), ('k', 'j'), ('p', 'q'), ('r', 's'), ('t', 'u'), ('v', 'w') ] 17 18 + # Create variable name variations 19 for old_var, new_var in var_substitutions: 20 + if old_var in problem_text: 21 + new_text = problem_text.replace(old_var, new_var) 22 + if new_text != problem_text: 23 + perturbations.append((\"notation_var\", new_text)) 24 + # Mathematical symbol variations 25 + symbol_substitutions = [ ('\\\\cdot', '\\times'), ('\\\\times', '\\cdot', ('\\\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n38 + ('\\le', '\\leq'),\n39 + ('\\ge', '\\geq'),\n40 + ]\n41 +\n42 + for old_sym, new_sym in symbol_substitutions:\n43 + if old_sym in problem_text:\n44 + new_text = problem_text.replace(old_sym, new_sym)\n45 + perturbations.append((\"notation_symbol\", new_text))\n46 +\n47 + return perturbations\n48 +\n49 +def create_phrasing_perturbations(problem_text):\n50 + \"\"\"Create alternative problem phrasings\"\"\"\n51 + perturbations = []\n52 +\n53 + # Common phrasing substitutions\n54 + phrasing_patterns = [\n55 + (r\"Find the value of\", \"Determine\"),\n56 + (r\"What is\", \"Calculate\"),\n57 + (r\"Solve for\", \"Find\"),\n58 + (r\"How many\", \"What is the number of\"),\n59 + (r\"Compute\", \"Find\"),\n60 + (r\"Evaluate\", \"Calculate\"),\n61 + ]\n62 +\n63 + for pattern, replacement in phrasing_patterns:\n64 + if re.search(pattern, problem_text, re.IGNORECASE):\n65 + new_text = re.sub(pattern, replacement, problem_text, flags=re.IGNORECASE)\n66 + if new_text != problem_text:\n67 + perturbations.append((\"phrasing\", new_text))\n68 +\n69 + return perturbations\n70 +\n71 +def create_formulation_perturbations(problem_text):\n72 + \"\"\"Create equivalent mathematical formulations\"\"\"\n73 + perturbations = []\n74 +\n75 + # Simple algebraic reformulations\n76 + reformulations = [\n77 + (r\"(\\w+) = (\\d+) + (\\d+)\", r\"\\1 - \\2 = \\3\"), # a = b + c -&gt; a - b = c\n78 + (r\"(\\w+) + (\\d+) = (\\d+)\", r\"\\1 = \\3 - \\2\"), # x + a = b -&gt; x = b - a\n79 + (r\"(\\d+) - (\\w+) = (\\d+)\", r\"\\2 = \\1 - \\3\"), # a - x = b -&gt; x = a - b\n80 + ]\n81 +\n82 + for pattern, replacement in reformulations:\n83 + if re.search(pattern, problem_text):\n84 + new_text = re.sub(pattern, replacement, problem_text)\n85 + if new_text != problem_text:\n86 + perturbations.append((\"formulation\", new_text))\n87 +\n88 + return perturbations\n89 +\n90 +def generate_problem_perturbations(problem_text, max_perturbations=3):\n91 + \"\"\"Generate various perturbations of a mathematical problem\"\"\"\n92 + all_perturbations = []\n93 +\n94 + # Generate different types of perturbations\n95 + all_perturbations extend(creationalnotation_perturbations(problem_text))\n96 + all_perturbations extend(creational_phrasing_perturbations(problem_text))\n97 + all_perturbations extend(creational_formulation_perturbations(problem_text))\n98 +\n99 + # Randomly sample up to max_perturbations\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n100 + if len(all_perturbations) &gt; max_perturbations:\n101 + all_perturbations = random.sample(all_perturbations, max_perturbations)\n102 +\n103 + return all_perturbations\n104 +\n105 +def sample_with_perturbations(prompts, answers, perturbation_ratio=0.3):\n106 + \"\"Sample problems with perturbations mixed in\"\"\n107 + perturbed_prompts = []\n108 + perturbed_answers = []\n109 + perturbation_metadata = []\n110 +\n111 + for prompt, answer in zip(prompts, answers):\n112 + # Always include original\n113 + perturbed_prompts.append(prompt)\n114 + perturbed_answers.append(answer)\n115 + perturbation_metadata.append({\"type\": \"original\", \"source_idx\": len(perturbed_prompts)-1})\n116 +\n117 + # Add perturbations with some probability\n118 + if random.random() &lt; perturbation_ratio:\n119 + # Extract problem text from prompt (assuming it's in a specific format)\n120 + problem_text = prompt.split(\"question:\")[-1] if \"question:\" in prompt else prompt\n121 + perturbations = generateProblem_perturbations(problem_text)\n122 +\n123 + for pert_type, pert_text in perturbations:\n124 + # Reconstruct full prompt with perturbed problem\n125 + pert_prompt = prompt.replace(problem_text, pert_text)\n126 + perturbed_prompts.append(pert_prompt)\n127 + perturbed_answers.append(answer) # Same answer for equivalent problem\n128 + perturbation_metadata.append({\n129 + \"type\": pert_type,\n130 + \"source_idx\": len(perturbed_prompts)-len(perturbations)-1\n131 + })\n132 +\n133 + return perturbed_prompts, perturbed_answers, perturbation_metadata\n134\n135 def sample_rollout(\n136 vllm_model,\n137 --- repo_variants_fullpipeline_bsz50_fixd2_epoch7/idea_37/drgrpo_grader.py\n138 +++ repo_variants_fullpipeline_bsz50_fixd2_epoch7/idea_37/drgrpo_grader.py\n139 @@ -1025,3 +1025,83 @@ def rl_zero_reward_fn_train(response, groundtruth, fast=True):\n140 \"reward\": 0.0\n141 }\n142\n143 +def compute_consistency_score(responses, groundtruth, perturbation_metadata):\n144 + \"\"Compute consistency score across perturbations\"\"\n145 + if not perturbation_metadata:\n146 + return 0.0\n147 +\n148 + # Group responses by source problem\n149 + source_groups = {}\n150 + for i, meta in enumerate(perturbation_metadata):\n151 + source_idx = meta.get(\"source_idx\", i)\n152 + if source_idx not in source_groups:\n153 + source_groups[source_idx] = []\n154 + source_groups[source_idx].append((i, responses[i], meta))\n155 +\n156 + total_consistency = 0.0\n157 + valid_groups = 0\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n158 + for source_idx, group_data in source_groups.items():\n160 + if len(group_data) &gt; 1: # Only consider groups with multiple responses\n161 + # Get correctness for each response in the group\n162 + correctness Scores = []\n163 + for _, response, meta in group_data:\n164 + reward_result = r1_zero_reward_fn_train(response, ground_truth)\n165 + correctness Scores.append(reward_result[\"reward\"])\n166 +\n167 + # Consistency is measured as how often the model gets the same\n168 + correctness scores:\n169 + # If all responses have same correctness (all correct or all\n170 + incorrect), high consistency\n171 + unique Scores = set(correctness Scores)\n172 + if len(unique Scores) == 1:\n173 + consistency = 1.0\n174 + else:\n175 + # Partial consistency based on agreement\n176 + most_common = max(set(correctness Scores), key=\n177 + correctness Scores.count)\n178 + consistency = correctness Scores.count(most_common) / len(\n179 + correctness Scores)\n180 +\n181 + if valid_groups == 0:\n182 + return 0.0\n183 +\n184 + return total_consistency / valid_groups\n185 +\n186 + def r1_zero_reward_fn_train_robust(response, ground_truth, perturbation_metadata=\n187 + None, responses_batch=None, fast=True):\n188 + \"\"Enhanced reward function with robustness bonuses\"\"\n189 + # Get base reward\n190 + base Reward = r1_zero Reward_fn_train(response, ground_truth, fast)\n191 +\n192 + # Initialize bonus components\n193 + robustness_bonus = 0.0\n194 + notation_bonus = 0.0\n195 + presentation_bonus = 0.0\n196 +\n197 + # Compute robustness bonus if perturbation data available\n198 + if perturbation_metadata and responses_batch:\n199 + consistency_score = compute_consistency_score(responses_batch,\n200 + ground_truth, perturbation_metadata)\n201 + robustness_bonus = 0.12 * consistency_score\n202 +\n203 + # Check for notation and presentation bonuses from metadata\n204 + if perturbation_metadata:\n205 + current_meta = perturbation_metadata[0] if perturbation_metadata else {}\n206 + pert_type = current_meta.get(\"type\", \"original\")\n207 +\n208 + if pert_type in [\"notation_var\", \"notation_symbol\"] and base Reward[\"reward\"] &gt; 0:\n209 + notation_bonus = 0.08\n210 +\n211 + if pert_type in [\"phrasing\", \"formulation\"] and base Reward[\"reward\"] &gt; 0:\n212 + presentation_bonus = 0.06\n213 +\n214 + # Combine all rewards\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n213 + total Reward = base Reward[\"reward\"] + robustness_bonus + notation_bonus + presentation_bonus\n214 +\n215 + return {\n216 + \"format Reward\": base Reward[\"format Reward\"],\n217 + \"answer Reward\": base Reward[\"answer Reward\"],\n218 + \"robustness_bonus\": robustness_bonus,\n219 + \"notation_bonus\": notation_bonus,\n220 + \"presentation_bonus\": presentation_bonus,\n221 + \"reward\": total Reward\n222 + }\n223 --- repo_variants_full_pipeline_bsz50_fixd2_epoch7/idea_37/grpo.py\n224 +++ repo_variants_full_pipeline_bsz50_fixd2_epoch7/idea_37/grpo.py\n225 @@ -1,7 +1,7 @@\n226 from transformers import AutoModelForCausalLM, AutoTokenizer\n227 from utils import tokenize_prompt_and_output, get_response_log_probs\n228 -from sample import load_prompt_template, load_dataset_with_prompt_template, sample_rollout\n229 -from drgrpo_grader import r1_zero Reward_fn_train\n230 +from sample import load_prompt_template, load_dataset_with_prompt_template, sample_rollout, sample_with_perturbations\n231 +from drgrpo_grader import r1_zero Reward_fn_train, r1_zero Reward_fn_train_robust\n232 from evaluate import r1_zero Reward_fn_eval, evaluate_vllm\n233 from grpo_utils import compute_group_normalized Rewards, grpo_microbatch_train_step\n234 from torch.utils.data import DataLoader, Dataset\n235 @@ -12,6 +12,7 @@ from tqdm import tqdm\n236 from vllm import LLM, SamplingParams\n237 import wandb\n238 import random\n239 +import math\n240\n241 def load_policyInto_vllm_instance(policy, llm):\nstate_dict = policy.state_dict()\n@@ -63,11 +64,23 @@\nmetrics = evaluate_vllm(vllm_model, r1_zero Reward_fn_eval, eval_prompts, eval_answers, eval_sampling.params, output_path=output_path)\nreturn metrics\n246\n247 -def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps, train_steps_per_rollout, output_dir, batch_size, gradient Accumulation_steps = 4, group_size = 2, rollout Subset_size = 256, device = \"cuda\", logging_steps = 20, saving_steps = 4000, eval_epochs = 5, eval_prompts = None, eval_answers = None, sampling.params = None, eval_vllm_model = None, cliprange = 0.2, loss_type = \"reinforce_with_baseline\"):\n248 +def get_cosine_annealing_lr(step, total_steps, lr_max=3.4e-5, lr_min=1.8e-5):\n249 + \"\"Cosine annealing learning rate schedule\"\"\n250 + return lr_min + (lr_max - lr_min) * 0.5 * (1 + math.cos(math.pi * step / total_steps))\n251 +\n252 +def compute_perturbation_complexity Clip_norm(model, complexity_factor=1.0):\n253 + \"\"Compute gradient clipping norm based on perturbation complexity\"\"\n254 + base Clip_norm = 1.0\n255 + return base Clip_norm * complexity_factor\n256 +\n257 +def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps, train_steps_per_rollout, output_dir, batch_size, gradient Accumulation_steps = 4, group_size = 2, rollout Subset_size = 256, device = \"cuda\", logging_steps = 20, saving_steps = 4000, eval_epochs = 5, eval_prompts = None, eval_answers = None, sampling.params = None, eval_vllm_model = None, cliprange = 0.2, loss_type = \"reinforce_with_baseline\", perturbation_ratio = 0.3):\nmodel.to(device)\n258 training_steps = grpo_steps\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n260 - optimizer = torch.optim.AdamW(model.params(), lr=learning_rate, weight Decay=0.0, betas=(0.9, 0.95))\n261 + # Start with robustness-aware learning rate\n262 + initial_lr = get_cosine_annealing_lr(0, grpo_steps)\n263 + optimizer = torch.optim.AdamW(model.params(), lr=initial_lr, weight Decay=0.0, betas=(0.9, 0.95))\n264 global_step = 0 # Initialize global step counter\n265 + robustness Scores = []\n266\n267 for epoch in range(grpo_steps):\n268 model.train()\n269 @@ -82,21 +95,50 @@ def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps,\n270\n271 model.train()\n272\n273 + # Update learning rate with cosine annealing\n274 + current_lr = get_cosine_annealing_lr(epoch, grpo_steps)\n275 + for param_group in optimizer param_groups:\n276 + param_group['lr'] = current_lr\n277 +\n278 ## load the current policy model to vllm for sampling rollouts\n279 load_policyInto_vllm_instance(model, vllm_model)\n280\n281 + # Sample with perturbations for robustness training\n282 + perturbed_prompts, perturbed_answers, perturbation_metadata = sample_with_perturbations(\n283 + train_prompts, train_answers, perturbation_ratio=perturbation_ratio\n284 + )\n285 +\n286 ## sample rollouts\n287 print (\"Sampling rollouts for epoch: \", epoch)\n288 - rollout_prompts, rollout_answers, rollout_responses, rollout_rewards = sample_rollout(vllm_model, rl_zero_reward_fn_train, train_prompts, train_answers, G=group_size, eval_sampling.params=eval_sampling.params, subset_size=rollout Subset_size, return_rewards=True, batch_size=512)\n289 +\n290 + # Use subset of perturbed prompts for training\n291 + subset_size = min(rollout Subset_size, len(perturbed_prompts))\n292 + if subset_size &lt; len(perturbed_prompts):\n293 + indices = random.sample(range(len(perturbed_prompts)), subset_size)\n294 + subset_prompts = [perturbed_prompts[i] for i in indices]\n295 + subset_answers = [perturbed_answers[i] for i in indices]\n296 + subset_metadata = [perturbation_metadata[i] for i in indices]\n297 + else:\n298 + subset_prompts = perturbed_prompts\n299 + subset_answers = perturbed_answers\n300 + subset_metadata = perturbation_metadata\n301 +\n302 + rollout_prompts, rollout_answers, rollout_responses, rollout_rewards = sample_rollout(\n303 + vllm_model, rl_zero_reward_fn_train, subset_prompts, subset_answers,\n304 + G=group_size, eval_sampling.params=eval_sampling.params,\n305 + subset_size=None, return_rewards=True, batch_size=512\n306 + )\n307 +\n308 # Randomly sample 2 rollouts to print\n309 indices = random.sample(range(len(rollout_prompts)), 2)\n310 print (\"Example rollouts:\")\n311 for idx in indices:\n312 print(f\"\\nRollout {idx}:\")\n313 - print(f\"Prompt: {rollout_prompts[idx]})\"\n314 + print(f\"Prompt: {rollout_prompts[idx][:200]}...\")\n\nprint(f\"Response: {rollout_responses[idx]}\")print(f\"Reward: {rollout_rewards[idx]}\")print(f\"Ground truth: {rollout_answers[idx]}\")print(f\"Ground truth: {rollout_answers[idx][:100]}...\")rollout_tokenized = tokenize_prompt_and_output(rollout_prompts, rollout_responses, tokenizer)rollout_data_loader = create_data_loader(rollout_tokenized, batch_size= batch_size, shuffle=False)\n@@ -126,15 +168,32 @@ def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps,"}, {"id": 23, "title": "Compute advantages using group normalization - no gradients needed with torch.no_grad():", "content": "# Compute advantages using group normalization - no gradients needed with torch.no_grad():\nadvantages, raw_rewards, metadata = compute_group_normalized_rewards(reward_fn=r1_zero_reward_fn_train, # Create enhanced reward function with robustness def robust_reward_fn(response, ground_truth): # Find corresponding metadata for this response response_idx = rollout_responses.index(response) if response in rollout_responses else 0 meta_idx = min(response_idx, len(subset_metadata) - 1) current_meta = [subset_metadata[meta_idx]] if subset_metadata else None return r1_zero_reward_fn_train_robust( response, ground_truth, perturbation_metadata=current_meta, responses_batch=rollout_responses ) advantages, raw_rewards, metadata = compute_group_normalized_rewards(reward_fn=robust_reward_fn, rollout_responses=rollout_responses, repeated_ground_truths=rollout_answers, group_size=group_size, advantage_eps=1e-6, normalize_by_std=True) advantages = advantages.to(device) # Track robustness improvement current_robustness = metadata.get('mean_reward', 0.0) robustness_scores.append(current_robustness) # Log raw rewards statistics print(\"\\nGRPO epoch: \", epoch) @-145,11 +204,20 @@ wandb.log({ \"eval/mean_reward\": eval_mean_reward, \"train/mean_reward\": metadata[\"mean_reward\"], \"train/learning_rate\": current_lr, \"train/robustness_score\": current_robustness, }, step=global_step) else: wandb.log({ \"train/mean_reward\": metadata[\"mean_reward\"], \"train/learning_rate\": current_lr, \"train/robustness_score\": current_robustness, }, step=global_step) # Compute perturbation complexity for gradient clipping\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n373 + perturbation_types = set(meta.get(\"type\", \"original\") for meta in subset_metadata)\n374 + complexity_factor = 1.0 + 0.1 * len(perturbation_types) # More complex with more perturbation types\n375 + clip_norm = compute_perturbation_complexity Clip_norm(model, complexity_factor)\n376\n377\n378 ## train on this rollout batch for train_steps_per_rollout steps\n379 @ -185,6 +252,9 @ def train_loop(model, train_prompts, train_answers, learning_rate, grpo_steps,\n380 )\n381\n382 if (batch_idx + 1) % gradient Accumulation_steps == 0:\n383 + # Apply perturbation-complexity gradient clipping\n384 + torch.nn.utilsclip_grad_norm_(model.params(), clip_norm)\n385 +\n386 optimizer.step()\n387 optimizer.zero_grad()\n388\n389 @ -211,7 +281,7 @ if __name__ == \"__main__\":\n390 parser.add_argument(\"-gradient Accumulation_steps\", type=int, default=16)\n391 parser.add_argument(\"-batch_size\", type=int, default=4)\n392 parser.add_argument(\"-cliprange\", type=float, default=0.2)\n393 - parser.add_argument(\"-loss_type\", type=str, default=\"grpo Clip\")\n394 + parser.add_argument(\"-loss_type\", type=str, default=\"reinforce_with_baseline\")\n395 parser.add_argument(\"-wandb_project\", type=str, default=\"grpo-math\")\n396 parser.add_argument(\"-wandb_name\", type=str, default=\"grpo Clip_1\")\n397 args = parser.parse_args()\n398 @ -266,7 +336,8 @ if __name__ == \"__main__\":\n399 sampling.params=eval_sampling.params,\n400 eval_vllm_model=vllm_model,\n401 cliprange=args.cliprange,\n402 - loss_type=args.loss_type\n403 + loss_type=args.loss_type,\n404 + perturbation_ratio=0.3\n\n## Towards Execution-Grounded Automated AI Research"}, {"id": 24, "title": "Example 3", "content": "# Example 3\n\n[Experiment] Implement token-level reward attribution by using attention weights to identify which input tokens contributed most to correct answers, then amplifying the gradient updates for those tokens during policy gradient training.\n\n[Code Changes] Modify get_response_log_probs in utils.py to also return attention weights from the last layer. In grpo_microbatch_train_step, compute token importance scores by averaging attention weights across heads, then multiply the policy gradient loss by (1 + importance_score) for tokens with high attention to mathematical content."}, {"id": 25, "title": "Validation Accuracy: 45.2", "content": "# Validation Accuracy: 45.2\n\n## Example 3 code diff\n```python\n1 --- repo_variants_fullpipeline_bsz50_fixd2_epoch1/idea_32/utils.py\n2 ++ repo_variants_fullpipeline_bsz50_fixd2_epoch1/idea_32/utils.py\n3 @@ -41,12 +41,13 @@ def compute_entropy(logits):\n4 entropy = -torch.sum(probs * log_probs, dim=-1)\n5 return entropy\n6\n7 -def get_response_log_probs(model, input_ids, labels, return_token_entropy=False, no_grad=True):\n8 +def get_response_log_probs(model, input_ids, labels, return_token_entropy=False, returnattention=False, no_grad=True):\n9 if no_grad:\n10 with torch.no_grad():\n11 - outputs = model(input_ids, labels=labels)\n12 + outputs = model(input_ids, labels=labels, output attentions= returnattention)\n13 logits = outputs.logits # (batch_size, seq_len, vocab_size)\n14 log_probs = torch.log softmax(logits, dim=-1) # (batch_size, seq_len, vocab_size)\n15 + attentions = outputs attentions if returnattention else None\n16 # Get log probs of the actual label tokens\n17 batch_size, seq_len = labels.shape # (batch_size, seq_len)\n18 log_probs = torch.gather(log_probs, dim=-1, index=labels.unsqueeze (-1)).squeeze(-1)\n19 @@ -55,8 +56,9 @@ def get_response_log_probs(model, input_ids, labels, return_token_entropy=False\n20 else:\n21 entropy = None\n22 else:\n23 - outputs = model(input_ids, labels=labels)\n24 + outputs = model(input_ids, labels=labels, output attentions= returnattention)\n25 logits = outputs.logits # (batch_size, seq_len, vocab_size)\n26 + attentions = outputs attentions if returnattention else None\n27 log_probs = torch.log softmax(logits, dim=-1) # (batch_size, seq_len, vocab_size)\n28 # Get log probs of the actual label tokens\n29 batch_size, seq_len = labels.shape # (batch_size, seq_len)\n30 @@ -65,9 +67,17 @@ def get_response_log_probs(model, input_ids, labels, return_token_entropy=False\n31 entropy = compute_entropy(logits)\n32 else:\n33 entropy = None\n34 -\n35 - return {\n36 +\n37 + result = {\n38 \"log_probs\": log_probs,\n\n## Towards Execution-Grounded Automated AI Research\n\n```txt\n39 \"token_entropy\": entropy\n40 }\n41 + if returnattention and attentions is not None:\n42 + # Return attention weights from the last layer, averaged across heads\n43 + last_layerattention = attentions[-1] # Shape: (batch_size, num_heads, seq_len, seq_len)\n44 + averagedattention = last_layerattention.mean(dim=1) # Average across heads: (batch_size, seq_len, seq_len)\n45 + result[\"attentionweights\"] = averagedattention\n46 + return result\n47\n48\n49\n50 --- repo_variants_fullpipeline_bsz50_fixd2_epoch1/idea_32/grpo_utils.py\n51 +++ repo_variants_fullpipeline_bsz50_fixd2_epoch1/idea_32/grpo_utils.py\n52 @@ -170,6 +170,7 @@ def grpo_microbatch_train_step(\n53 advantages: torch.Tensor | None = None,\n54 old_log_probs: torch.Tensor | None = None,\n55 cliprange: float | None = None,\n56 + attentionweights: torch.Tensor | None = None,\n57 ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n58 ...\n## 59 Return:\n60 @@ -180,6 +181,20 @@ def grpo_microbatch_train_step(\n## 61 You should call loss.backup() in this function. Make sure to adjust for gradient accumulation.\n62 ...\n63 loss, metadata = compute_policy-gradient_loss(policy_log_probs, loss_type, raw_rewards, advantages, old_log_probs, cliprange) # (batch_size, sequence_length)\n64 +\n65 + # Apply attention-based weighting if attention weights are provided\n66 + if attentionweights is not None:\n67 + # Compute importance scores by averaging attention weights to earlier tokens\n68 + # attentionweights shape: (batch_size, seq_len, seq_len)\n69 + # For each token, sum attention weights to all previous tokens (including itself)\n70 + importance Scores = torch.sum(attentionweights, dim=-1) # (batch_size, seq_len)\n71 +\n72 + #Normalize importance scores to [0, 1] range per sequence\n73 + importance Scores = importance Scores / (importance Scores.max(dim=-1, keepdim=True) [0] + 1e-8)\n74 +\n75 + # Amplify loss for tokens with high importance: multiply by (1 + importance Score)\n76 + loss = loss * (1.0 + importance Scores)\n77 +\n78 loss = masked_mean(loss, response_mask)\n79 loss = loss / gradient Accumulation_steps\n80 loss.backup()\n81 --- repo_variants_fullpipeline_bsz50_fixd2_epoch1/idea_32/grpo.py\n82 +++ repo_variants_fullpipeline_bsz50_fixd2_epoch1/idea_32/grpo.py\n83 @@ -109,6 +109,7 @@ def train_loop(model, train_prompts, train Answers, learning_rate, grpo_steps,\n84 model,\n85 input_ids,\n86 labels,\n87 return_token_entropy=False,\n88 + returnattention=False,\n89 no_grad=True\n\n## Towards Execution-Grounded Automated AI Research\n\n```txt\n91 @ -163,11 +164,13 @\n92 model,\n93 input_ids,\n94 labels,\n95 return_token_entropy=True,\n96 + returnattention=True,\n97 no_grad=False\n98 )\n99 policy_log_probs = response_log_probs[\"log_probs\"]\n100 entropy = response_log_probs[\"token_entropy\"]\n101 + attentionweights = response_log_probs.get(\"attentionweights\")\n102\n103 # Calculate data index for advantages/old_log_probs\n104 batch_idx_total = batch_idx * batch_size\n105 @ -177,7 +180,8 @ def train_loop(model, train_prompts, train_answers,\n106 learning_rate, grpo_steps,\n107 loss_type=loss_type,\n108 advantages=batch Advantages,\n109 old_log_probs=batch_old_log_probs,\n110 - cliprange=cliprange\n111 + cliprange=cliprange,\n112 + attentionweights=attentionweights\n113 )\n114 if (batch_idx + 1) % gradient Accumulation_steps == 0:\n\n## Towards Execution-Grounded Automated AI Research"}, {"id": 26, "title": "Example 4", "content": "# Example 4\n\n[Experiment] Implement response diversity rewards within groups where responses to the same prompt receive bonus rewards (0.05-0.15) for being dissimilar to other responses in their group, encouraging exploration of different solution paths while maintaining the proven group_size=8 and 3e-5 learning rate combination.\n\n[Code Changes] Modify compute_group_normalized_rewards in grpo_utils.py to compute pairwise similarity between responses in each group using token-level Jaccard similarity. Add diversity bonus: diversity_reward = 0.15 * (1 - max_similarity_in_group) to each response's reward before advantage computation. Reshape responses into groups, compute similarities, and add bonuses before advantage normalization. Set -learning_rate 3e-5, -loss_type reinforce_with_baseline, -group_size 8."}, {"id": 27, "title": "Validation Accuracy: 19.2", "content": "# Validation Accuracy: 19.2\n\n## Example 4 code diff\n```python\n1 --- repo_variants_fullpipeline_bsz50_fixd2_epoch3/idea_22/grpo_utils.py\n2 ++ repo_variants_fullpipeline_bsz50_fixd2_epoch3/idea_22/grpo_utils.py\n3 @ -1,5 +1,25 @\n4 import torch\n5 from typing import Literal\n6\n7 +def compute_jaccard_similarity(response1: str, response2: str) -&gt; float:\n8 + \"\"\n9 + Compute token-level Jaccard similarity between two responses.\n10 + Jaccard similarity = |intersection| / |union|\n11 + \"\"\n12 + # Simple tokenization by splitting on whitespace\n13 + tokens1 = set(response1.split())\n14 + tokens2 = set(response2.split())\n15 +\n16 + if len(tokens1) == 0 and len(tokens2) == 0:\n17 + return 1.0\n18 +\n19 + intersection = tokens1.intersection(tokens2)\n20 + union = tokens1.union(tokens2)\n21 +\n22 + if len(union) == 0:\n23 + return 1.0\n24 +\n25 + return len(intersection) / len(union)\n26 +\n27 def compute_group_normalized_rewards(\n28 reward_fn,\n29 @ -30,31 +49,62 @\n30 raw_rewards.append(reward_dict[\"reward\"])\n31 raw_rewards = torch.tensor(raw_rewards)\n32\n33 + # Compute diversity bonuses within each group\n34 n_groups = len(rollout_responses) // group_size\n35 + diversity_bonuses = []\n36 +\n37 + for group_idx in range(n_groups):\n38 + group_start = group_idx * group_size\n39 + group_end = group_start + group_size\n40 + group_responses = rollout_responses[group_start:group_end]\n41 +\n42 + # Compute pairwise similarities within the group\n43 + for i in range(group_size):\n44 + max_similarity = 0.0\n45 + for j in range(group_size):\n\n## Towards Execution-Grounded Automated AI Research\n\n```python\n46 + if i != j:\n47 + similarity = compute_jaccard_similarity(group_responses[i], group_responses[j])\n48 + max_similarity = max(max_similarity, similarity)\n49 +\n50 + # Diversity bonus: higher reward for more dissimilar responses\n51 + diversity_bonus = 0.15 * (1 - max_similarity)\n52 + diversity_bonuses.append(diversity_bonus)\n53 +\n54 + diversity_bonuses = torch.tensor(diversity_bonuses)\n55 +\n56 # Reshape rewards into groups\n57 n_groups = len(raw_rewards) // group_size\n58 grouped_rewards = raw_rewards.view(n_groups, group_size)\n59 + grouped_diversity_bonuses = diversity_bonuses.view(n_groups, group_size)\n60 +\n61 + # Add diversity bonuses to raw rewards before advantage computation\n62 + grouped_rewards_with_diversity = grouped_rewards + grouped_diversity_bonuses\n63\n64 # Compute group statistics\n65 - group_means = grouped_rewards.mean(dim=1, keepdim=True)\n66 + group_means = grouped_rewards_with_diversity.mean(dim=1, keepdim=True)\n67 if normalize_by_std:\n68 - group_std = grouped_rewards.std(dim=1, keepdim=True) + advantage_eps\n69 - advantages = (grouped_rewards - group_means) / group_std\n70 + group_std = grouped_rewards_with_diversity.std(dim=1, keepdim=True) + advantage_eps\n71 + advantages = (grouped_rewards_with_diversity - group_means) / group_std\n72 else:\n73 - advantages = grouped_rewards - group_means\n74 + advantages = grouped_rewards_with_diversity - group_means\n75\n76 # Flatten advantages back to original shape\n77 advantages = advantages.view(-1)\n78 +\n79 + # Update raw rewards to include diversity bonuses for metadata\n80 + raw_rewards_with_diversity = raw_rewards + diversity_bonuses\n81\n82 # Compute metadata statistics\n83 metadata = {\n84 - \"mean Reward\": raw_rewards.mean().item(),\n85 - \"std Reward\": raw_rewards.std().item(),\n86 - \"max Reward\": raw_rewards.max().item(),\n87 - \"min Reward\": raw_rewards.min().item(),\n88 + \"mean Reward\": raw_rewards_with_diversity.mean().item(),\n89 + \"std Reward\": raw_rewards_with_diversity.std().item(),\n90 + \"max Reward\": raw_rewards_with_diversity.max().item(),\n91 + \"min Reward\": raw_rewards_with_diversity.min().item(),\n92 \"mean Advantage\": advantages.mean().item(),\n93 \"std Advantage\": advantages.std().item(),\n94 + \"mean_diversity_bonus\": diversity_bonuses.mean().item(),\n95 }\n96\n97 - return advantages, raw_rewards, metadata\n98 + return advantages, raw_rewards_with_diversity, metadata\n99\n100 def compute_naive_policy_gradient_loss(\n101 --- repo_variants_fullpipeline_bsz50_fixed2_epoch3/idea_22/grpo.py\n102 +++ repo_variants_fullpipeline_bsz50_fixed2_epoch3/idea_22/grpo.py\n103 @@ -203,6 +203,6 @@\n104 parser.add_argument (\"--eval_dataset_path\", type=str, default=\"../MATH/test.json1\")\n105 parser.add_argument (\"--output_dir\", type=str, default=\"ckpts/\")\n\n## Towards Execution-Grounded Automated AI Research\n\n```txt\n106 - parser.add_argument (\"--learning_rate\", type  $=$  float, default  $= 1\\mathrm{e} - 5$\n107 + parser.add_argument (\"--learning_rate\", type  $=$  float, default  $= 3\\mathrm{e} - 5$\n108 parser.add_argument (\"--grpco_steps\", type  $=$  int, default  $= 200$\n109 parser.add_argument (\"--group_size\", type  $=$  int, default  $= 8$\n110 parser.add_argument (\"--rollout Subset_size\", type  $=$  int, default  $= 256$\n111 @@ -212,7 +212,7 @@ if __name__ == \"_main_:\n112 parser.add_argument (\"--gradient Accumulation_steps\", type  $=$  int, default  $= 16$\n113 parser.add_argument (\"--batch_size\", type  $=$  int, default  $= 4$\n114 parser.add_argument (\"--cliprange\", type  $=$  float, default  $= 0.2$\n115 - parser.add_argument (\"--loss_type\", type  $=$  str, default  $=$  \"grpo Clip\")\n116 + parser.add_argument (\"--loss_type\", type  $=$  str, default  $=$  \"reinforce_withÂü∫Á§é\")\n117 parser.add_argument (\"--wandb_project\", type  $=$  str, default  $=$  \"grpo-math\")\n118 parser.add_argument (\"--wandb_name\", type  $=$  str, default  $=$  \"grpo Clip_1\")\n119 args = parser.parse_args()\n120 -- repo_variants_fullpipeline_bsz50_fixd2_epoch3/idea_22/run_job.sh\n121 ++ repo_variants_fullpipeline_bsz50_fixd2_epoch3/idea_22/run_job.sh\n122 @@ -21,7 +21,7 @@ timeout 2h uv run \\\n123 --index https://download.pytorch.org/whl/cu128 \\\n124 --index-strategy unsafe-best-match \\\n125 python grpo.py \\\n126 - --learning_rate 1e-5 \\\n127 + --learning_rate 3e-5 \\\n128 --grpo_steps 20 \\\n129 --group_size 8 \\\n130 --rollout Subset_size 128 \\\n131 @@ -30,7 +30,7 @@ timeout 2h uv run \\\n132 --gradient Accumulation_steps 16 \\\n133 --batch_size 4 \\\n134 --cliprange 0.2 \\\n135 - --loss_type grpo Clip \\\n136 + --loss_type reinforce_withÂü∫Á§é \\\n137 --wandb_name $wandb_name\n138\n139 echo \"Experiment finished successfully!\""}], "pdf_images": {"pdf_path": "inbox/2601.14525v1.pdf", "total_pages": 42, "pages": [{"page_number": 1, "filename": "page-001.png", "thumb_filename": "page-001-thumb.png", "mobile_filename": "page-001-mobile.png", "mobile_thumb_filename": "page-001-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 2, "filename": "page-002.png", "thumb_filename": "page-002-thumb.png", "mobile_filename": "page-002-mobile.png", "mobile_thumb_filename": "page-002-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 3, "filename": "page-003.png", "thumb_filename": "page-003-thumb.png", "mobile_filename": "page-003-mobile.png", "mobile_thumb_filename": "page-003-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 4, "filename": "page-004.png", "thumb_filename": "page-004-thumb.png", "mobile_filename": "page-004-mobile.png", "mobile_thumb_filename": "page-004-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 5, "filename": "page-005.png", "thumb_filename": "page-005-thumb.png", "mobile_filename": "page-005-mobile.png", "mobile_thumb_filename": "page-005-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 6, "filename": "page-006.png", "thumb_filename": "page-006-thumb.png", "mobile_filename": "page-006-mobile.png", "mobile_thumb_filename": "page-006-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 7, "filename": "page-007.png", "thumb_filename": "page-007-thumb.png", "mobile_filename": "page-007-mobile.png", "mobile_thumb_filename": "page-007-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 8, "filename": "page-008.png", "thumb_filename": "page-008-thumb.png", "mobile_filename": "page-008-mobile.png", "mobile_thumb_filename": "page-008-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 9, "filename": "page-009.png", "thumb_filename": "page-009-thumb.png", "mobile_filename": "page-009-mobile.png", "mobile_thumb_filename": "page-009-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 10, "filename": "page-010.png", "thumb_filename": "page-010-thumb.png", "mobile_filename": "page-010-mobile.png", "mobile_thumb_filename": "page-010-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 11, "filename": "page-011.png", "thumb_filename": "page-011-thumb.png", "mobile_filename": "page-011-mobile.png", "mobile_thumb_filename": "page-011-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 12, "filename": "page-012.png", "thumb_filename": "page-012-thumb.png", "mobile_filename": "page-012-mobile.png", "mobile_thumb_filename": "page-012-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 13, "filename": "page-013.png", "thumb_filename": "page-013-thumb.png", "mobile_filename": "page-013-mobile.png", "mobile_thumb_filename": "page-013-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 14, "filename": "page-014.png", "thumb_filename": "page-014-thumb.png", "mobile_filename": "page-014-mobile.png", "mobile_thumb_filename": "page-014-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 15, "filename": "page-015.png", "thumb_filename": "page-015-thumb.png", "mobile_filename": "page-015-mobile.png", "mobile_thumb_filename": "page-015-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 16, "filename": "page-016.png", "thumb_filename": "page-016-thumb.png", "mobile_filename": "page-016-mobile.png", "mobile_thumb_filename": "page-016-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 17, "filename": "page-017.png", "thumb_filename": "page-017-thumb.png", "mobile_filename": "page-017-mobile.png", "mobile_thumb_filename": "page-017-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 18, "filename": "page-018.png", "thumb_filename": "page-018-thumb.png", "mobile_filename": "page-018-mobile.png", "mobile_thumb_filename": "page-018-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 19, "filename": "page-019.png", "thumb_filename": "page-019-thumb.png", "mobile_filename": "page-019-mobile.png", "mobile_thumb_filename": "page-019-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 20, "filename": "page-020.png", "thumb_filename": "page-020-thumb.png", "mobile_filename": "page-020-mobile.png", "mobile_thumb_filename": "page-020-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 21, "filename": "page-021.png", "thumb_filename": "page-021-thumb.png", "mobile_filename": "page-021-mobile.png", "mobile_thumb_filename": "page-021-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 22, "filename": "page-022.png", "thumb_filename": "page-022-thumb.png", "mobile_filename": "page-022-mobile.png", "mobile_thumb_filename": "page-022-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 23, "filename": "page-023.png", "thumb_filename": "page-023-thumb.png", "mobile_filename": "page-023-mobile.png", "mobile_thumb_filename": "page-023-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 24, "filename": "page-024.png", "thumb_filename": "page-024-thumb.png", "mobile_filename": "page-024-mobile.png", "mobile_thumb_filename": "page-024-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 25, "filename": "page-025.png", "thumb_filename": "page-025-thumb.png", "mobile_filename": "page-025-mobile.png", "mobile_thumb_filename": "page-025-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 26, "filename": "page-026.png", "thumb_filename": "page-026-thumb.png", "mobile_filename": "page-026-mobile.png", "mobile_thumb_filename": "page-026-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 27, "filename": "page-027.png", "thumb_filename": "page-027-thumb.png", "mobile_filename": "page-027-mobile.png", "mobile_thumb_filename": "page-027-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 28, "filename": "page-028.png", "thumb_filename": "page-028-thumb.png", "mobile_filename": "page-028-mobile.png", "mobile_thumb_filename": "page-028-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 29, "filename": "page-029.png", "thumb_filename": "page-029-thumb.png", "mobile_filename": "page-029-mobile.png", "mobile_thumb_filename": "page-029-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 30, "filename": "page-030.png", "thumb_filename": "page-030-thumb.png", "mobile_filename": "page-030-mobile.png", "mobile_thumb_filename": "page-030-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 31, "filename": "page-031.png", "thumb_filename": "page-031-thumb.png", "mobile_filename": "page-031-mobile.png", "mobile_thumb_filename": "page-031-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 32, "filename": "page-032.png", "thumb_filename": "page-032-thumb.png", "mobile_filename": "page-032-mobile.png", "mobile_thumb_filename": "page-032-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 33, "filename": "page-033.png", "thumb_filename": "page-033-thumb.png", "mobile_filename": "page-033-mobile.png", "mobile_thumb_filename": "page-033-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 34, "filename": "page-034.png", "thumb_filename": "page-034-thumb.png", "mobile_filename": "page-034-mobile.png", "mobile_thumb_filename": "page-034-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 35, "filename": "page-035.png", "thumb_filename": "page-035-thumb.png", "mobile_filename": "page-035-mobile.png", "mobile_thumb_filename": "page-035-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 36, "filename": "page-036.png", "thumb_filename": "page-036-thumb.png", "mobile_filename": "page-036-mobile.png", "mobile_thumb_filename": "page-036-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 37, "filename": "page-037.png", "thumb_filename": "page-037-thumb.png", "mobile_filename": "page-037-mobile.png", "mobile_thumb_filename": "page-037-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 38, "filename": "page-038.png", "thumb_filename": "page-038-thumb.png", "mobile_filename": "page-038-mobile.png", "mobile_thumb_filename": "page-038-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 39, "filename": "page-039.png", "thumb_filename": "page-039-thumb.png", "mobile_filename": "page-039-mobile.png", "mobile_thumb_filename": "page-039-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 40, "filename": "page-040.png", "thumb_filename": "page-040-thumb.png", "mobile_filename": "page-040-mobile.png", "mobile_thumb_filename": "page-040-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 41, "filename": "page-041.png", "thumb_filename": "page-041-thumb.png", "mobile_filename": "page-041-mobile.png", "mobile_thumb_filename": "page-041-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 42, "filename": "page-042.png", "thumb_filename": "page-042-thumb.png", "mobile_filename": "page-042-mobile.png", "mobile_thumb_filename": "page-042-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}]}, "metadata": {"model": "gpt-4o", "pdf_path": "inbox/2601.14525v1.pdf"}, "attribution": {"type": "arxiv", "url": "https://arxiv.org/abs/2601.14525", "id": "2601.14525"}};
        initializeColumnInterface();
    </script>
</body>
</html>