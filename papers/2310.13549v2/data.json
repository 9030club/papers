{
  "title": "The Perils & Promises of Fact - Checking with Large Language Models",
  "slug": "2310.13549v2",
  "questions": {
    "1": {
      "question": "What were they trying to do?",
      "answer": "The main goal of the paper \"The Perils & Promises of Fact-checking with Large Language Models\" was to evaluate the capabilities and limitations of large language models (LLMs), specifically GPT-3.5 and GPT-4, in performing automated fact-checking tasks. The researchers aimed to understand how well these models can verify claims by retrieving and utilizing contextual information, and to assess their accuracy across different languages. The broader objective was to explore the potential of LLMs to enhance the efficiency of fact-checking processes in the face of widespread misinformation.",
      "timestamp": "2025-07-28 14:30:35"
    },
    "2": {
      "question": "Why does it matter?",
      "answer": "The paper matters because it addresses the critical challenge of misinformation, which has become a significant global risk due to the rapid spread of false information online. Automated fact-checking using Large Language Models (LLMs) like GPT-4 is essential to augment human efforts, which are insufficient alone due to the volume and speed of information dissemination. Understanding the capabilities and limitations of LLMs in fact-checking is crucial for maintaining the integrity of our information ecosystem, as these models are increasingly used in high-stakes areas such as journalism, research, and legal contexts. Ultimately, this research is vital for developing reliable automated systems that can help discern truth from falsehood, thereby supporting informed decision-making in society.",
      "timestamp": "2025-07-28 14:30:39"
    },
    "3": {
      "question": "What did they try?",
      "answer": "The authors evaluated the use of large language models (LLMs), specifically GPT-3.5 and GPT-4, for automated fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Their approach involved integrating information retrieval and claim verification, allowing the LLMs to perform web searches and use the ReAct framework to iteratively decide whether to continue searching or conclude with a verdict. This method enabled the models to justify their reasoning and cite relevant sources, thereby enhancing the verifiability and explainability of the fact-checking process. They found that incorporating contextual information significantly improved the accuracy of the models.",
      "timestamp": "2025-07-28 14:30:48"
    },
    "4": {
      "question": "Did it work?",
      "answer": "The study found that the idea of using large language models (LLMs) like GPT-3.5 and GPT-4 for fact-checking showed promise, with GPT-4 outperforming GPT-3.5. The inclusion of contextual information significantly improved the accuracy of both models, achieving over 80% accuracy for non-ambiguous verdicts. However, accuracy varied across languages, with English translations generally yielding better results than original non-English texts. Despite these positive outcomes, the models struggled with ambiguous categories and cannot fully replace human fact-checkers due to the potential consequences of inaccuracies.",
      "timestamp": "2025-07-28 14:30:52"
    },
    "5": {
      "question": "What did they compare it to?",
      "answer": "The paper compares the fact-checking performance of GPT-3.5 and GPT-4 using datasets from PolitiFact and Data Commons. The study evaluates these models with and without additional contextual information, highlighting that GPT-4 generally outperforms GPT-3.5, especially when contextual data is provided. The benchmarks used include accuracy and F1 scores across different languages and conditions, with GPT-4 showing significant improvements over GPT-3.5 in discerning the veracity of claims.",
      "timestamp": "2025-07-28 14:30:55"
    },
    "6": {
      "question": "What was it tested on?",
      "answer": "The paper tested the fact-checking capabilities of GPT-3.5 and GPT-4 on two datasets: a dataset of US political fact-checked claims from PolitiFact and a multilingual dataset from Data Commons. The evaluation setup involved comparing the models' performance with and without access to external context, using Google searches to retrieve relevant information. The models were assessed on their ability to classify claims into veracity categories, with experiments conducted in both English and the original languages of the claims.",
      "timestamp": "2025-07-28 14:31:01"
    },
    "7": {
      "question": "What's cool about it?",
      "answer": "The paper presents a novel approach to automated fact-checking by integrating large language models (LLMs) with contextual information retrieval. This method allows LLMs like GPT-3.5 and GPT-4 to perform web searches and retrieve relevant data, enhancing their accuracy and reasoning capabilities. A particularly clever aspect is the use of the ReAct framework, which enables the models to iteratively decide whether to continue searching for information or to conclude with a verdict, thereby balancing accuracy and efficiency. This approach not only improves fact-checking accuracy but also addresses the explainability of the models' decisions by requiring them to justify their reasoning and cite sources.",
      "timestamp": "2025-07-28 14:31:07"
    },
    "8": {
      "question": "What's sketchy about it?",
      "answer": "The paper raises several concerns and limitations. One significant issue is the potential for data leakage, as the training data for LLMs like GPT-3.5 and GPT-4 may include fact-checks, which could skew results by reflecting the models' retention of training examples rather than genuine reasoning. Additionally, the models struggle with ambiguous verdicts and exhibit varying performance across different languages, indicating a lack of uniform accuracy. The reliance on English translations for better performance also highlights the models' limitations in handling non-English text effectively. Furthermore, the paper acknowledges that these models cannot fully replace human fact-checkers due to the potential consequences of incorrect assessments.",
      "timestamp": "2025-07-28 14:31:12"
    },
    "9": {
      "question": "Can anyone use this?",
      "answer": "The use of large language models (LLMs) like GPT-3.5 and GPT-4 for fact-checking is promising but not universally accessible. While these models can enhance fact-checking efficiency, they require technical expertise to implement effectively, particularly in integrating contextual information via web searches. Additionally, the performance varies significantly across languages, and the models are not entirely reliable without human oversight, which limits their practicality for general use. Thus, while they are a valuable tool for professional fact-checkers, they are not yet suitable for widespread, independent use by the general public.",
      "timestamp": "2025-07-28 14:31:18"
    },
    "10": {
      "question": "What's still left to figure out?",
      "answer": "The paper highlights several areas that remain unsolved or unclear in the realm of fact-checking with large language models (LLMs). It emphasizes the need for further research to understand when LLMs succeed or fail in fact-checking tasks, particularly given their inconsistent accuracy and the challenges posed by ambiguous verdicts. Additionally, the variability in model performance across different languages and the potential issue of data leakage from training data are areas that require more exploration. The study also suggests future directions, such as improving the explainability of LLMs' verdicts and reasoning, and understanding the conditions under which these models perform best.",
      "timestamp": "2025-07-28 14:31:23"
    }
  },
  "markdown_pages": [
    {
      "id": 1,
      "title": "The Perils \\& Promises of Fact-checking with Large Language Models",
      "content": "# The Perils \\& Promises of Fact-checking with Large Language Models\n\nDorian Quelle \\& Alexandre Bovet<br>Department of Mathematical Modeling and Machine Learning, Digital Society Initiative, University of Zurich, Zurich, Switzerland<br>dorian.quelle@uzh.ch, alexandre.bovet@uzh.ch\n\n## October 2023\n\n#### Abstract\n\nAutomated fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large Language Models (LLMs) like GPT-4 are increasingly trusted to write academic papers, lawsuits, and news articles and to verify information, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Understanding the capacities and limitations of LLMs in fact-checking tasks is therefore essential for ensuring the health of our information ecosystem. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.\n\n## 1 Introduction\n\nFact-checking has become a vital tool to reduce the spread of misinformation online, shown to potentially reduce an individual's belief in false news and rumors [1, 2] and to improve political knowledge [3]. While verifying or refuting a claim is a core task of any journalist, a variety of dedicated fact-checking organizations have formed to correct misconceptions, rumors, and fake news online. A pivotal moment in the rise of fact-checking happened in 2009 when the prestigious Pulitzer Prize in the national reporting category was awarded to Politifact. Politifact's innovation was to propose the now standard model of an ordinal rating, which added a layer of structure and clarity to the fact check and inspired dozens of projects around the world [4]. The second wave of fact-checking organizations and innovation in the fact-checking industry was catalyzed by the proliferation of viral hoaxes and fake news during the 2016 US presidential election [5, 6] and Brexit referendum [4]. Increased polarization [7], political populism, and awareness of the potentially detrimental effects of misinformation have ushered in the \"rise of fact-checking\" [8].\n\nAlthough fact-checking organizations play a crucial role in the fight against misinformation, notably during the COVID-19 pandemic [9], the process of fact-checking a claim is an extremely time-consuming task. A professional fact-checker might take several hours or days on any given claim [10, 11]. Due to an everincreasing amount of information online and the speed at which it spreads, relying solely on manual fact-\n\nchecking is insufficient and makes automated solutions and tools that increase the efficiency of fact-checkers necessary.\n\nRecent research has explored the potential of using large artificial intelligence language models as a tool for fact-checking [12-16]. However, significant challenges remain when employing large language models (LLMs) to assess the veracity of a statement. One primary issue is that fact-checks are potentially included in some of the training data for LLMs. Therefore, successful fact-checking without additional context may not necessarily be attributed to the model's comprehension of facts or argumentation. Instead, it may simply reflect the LLM's retention of training examples. While this might suffice for fact-checking past claims, it may not generalize well beyond the training data.\n\nLarge language models (LLMs) like GPT-4 are increasingly trusted to write academic papers, lawsuits, news articles ${ }^{1}$, or to gather information [17]. Therefore, an investigation into the models' ability to determine whether a statement is true or false is necessary to understand whether LLMs can be relied upon in situations where accuracy and credibility are paramount. The widespread adoption and reliance on LLMs pose both opportunities and challenges. As they take on more significant roles in decision-making processes, research, journalism, and legal domains, it becomes crucial to understand their strengths and limitations. The increasing use of advanced language models in disseminating misinformation online highlights the importance of developing efficient automated systems. The 2024 WEF Global Risk Report ranks misinformation and disinformation as the most dangerous short-term global risk as LLMs have enabled an \"explosion in falsified information\" removing the necessity of niche skills to create \"synthetic content\" [18]. On the other hand, artificial intelligence models can help identify and mitigate false information, thereby helping to maintain a more reliable and accurate information environment. The ability of LLMs to discern truth from falsehood is not just a measure of their technical competence but also has broader implications for our information ecosystem.\n\nA significant challenge in automated fact-checking systems relying on machine learning models has been the lack of explainability of the models' prediction. This is a particularly desirable goal in the area of fact-checking as explanations of verdicts are an integral part of the journalistic process when performing manual fact-checking [19]. While there has been some progress in highlighting features that justify a verdict, a relatively small number of automated fact-checking systems have an explainability component [19].\n\nSince the early 2010s, a diverse group of researchers have tackled automated fact-checking with various approaches. This section introduces the concept of automated fact-checking and the different existing approaches. Different shared tasks, where research groups tackle the same problem or dataset with a defined outcome metric, have been announced with the aim of automatically fact-checking claims. For example, the shared task RumourEval provided a dataset of \"dubious posts and ensuing conversations in social media, annotated both for stance and veracity\" [20]. CLEF ChecKTHAT! prepared three different tasks, aiming to solve different problems in the fact-checking pipeline [21]. First, \"Task 1 asked to predict which posts in a Twitter stream are worth fact-checking, focusing on COVID-19 and politics in six languages\" [22]. Task 2 \"asks to detect previously fact-checked claims (in two languages)\" [23]. Lastly, \"Task 3 is designed as a multi-class classification problem and focuses on the veracity of German and English news articles\" [24]. The Fact Extraction and VERification shared task (FEVER) \"challenged participants to classify whether human-written factoid claims could be SUPPORTED or REFUTED using evidence retrieved from Wikipedia\" [25]. In general, most of these challenges and proposed solutions disaggregate the fact-checking pipeline into a multi-step problem, as detection, contextualization, and verification all require specific approaches and methods [26]. For example, [27] proposed four components to verify a web document in their ClaimBuster pipeline. First, a claim monitor that performs document retrieval (1), a claim spotter that performs claim detection (2), a claim matcher that matches a detected claim to fact-checked claims (3), and a claim checker that performs evidence extraction and claim validation (4) [28].\n\nIn their summary of automated fact-checking [28] define entailment as \"cases where the truth of hypothesis $h$ is highly plausible given text $t$ \". More stringent definitions that demand that a hypothesis is true in \"every possible circumstance where $t$ is true\" fail to handle the uncertainty of Natural Language. Claim\n\n[^0]\n[^0]:    ${ }^{1}$ https://cybernews.com/news/academic-cheating-chatgpt-openai/, https://www.nytimes.com/2023/06/08/nyregion/lawyer-chatgpt-sanctions.html\n\nverification today mostly relies on fine-tuning a large pre-trained language model on the target dataset [28]. State-of-the-art entailment models have generally relied on transformer architecture such as BERT [29] and RoBERTa [30]. [12] tested GPT-3.5's claim verification performance on a dataset of PolitiFact statements without adding any context. They found that GPT-3.5 performs well on the dataset and argue that it shows the potential of leveraging GPT-3.5 and other LLMs for enhancing the efficiency and expediency of the fact-checking process. Novel large language models have been used by [13] in assessing the check-worthiness. The authors test various models ability to predict the check-worthiness of English language content. The authors compared GPT-3.5 with various other language models. They find that a fine-tuned version of GPT3.5 slightly ourperforms DeBerta-v3 [14], an improvement over the original DeBERTa architecture [31]. [16] use fact-checks to construct a synthetic dataset of contradicting, entailing or neutral claims. They create the synthetic data using GPT-4 and predict the entailment using a smaller fine-tuned LLM. Similarly, [15] test the ability of various LLMs to discern fake news by providing Bard, BingAI, GPT-3.5 and GPT-4 on a list of 100 fact-checked news items. The authors find that all LLMs achieve performances of around $64-71 \\%$ accuracy, with GPT 4 receiving the highest score among all LLMs. [32] interview fact-checking platforms about their expectations of Chat-GPT as a tool for both misinformation fabrication, detection, and verification. They find that while professional fact-checkers highlight the potential perils such as the reliability of sources, the lack of insights into the training process, and the enhanced ability of malevolent actors to fabricate false content, they nevertheless view it as a useful resource for both information gathering and the detection and debunking of false news [32].\nWhile earlier efforts in claim verification did not retrieve any evidence beyond the claim itself (for example, see [33]), augmenting claim verification models with evidence retrieval has become standard for state-of-theart models [34]. In general, evidence retrieval aims to incorporate relevant information beyond the claim. For example, from encyclopedias (e.g., Wikipedia [35]), scientific papers [36], or search engines such as Google [37]. [37] submit a claim verbatim as a query to the Google Search API and use the first ten search results as evidence. A crucial issue for evidence retrieval lies in the fact that it implicitly assumes that all available information is trustworthy and that veracity can be gleaned from simply testing the coherence of the claim with the information retrieved. An alternative approach that circumvents the issue of the inclusion of false information has been to leverage knowledge databases (also knowledge graphs) that aim to \"equip machines with comprehensive knowledge of the world's entities and their relationships\" [38]. However, this approach assumes that all facts pertinent to the checked claim are present in a graph. An assumption that [34] called unrealistic.\n\nOur primary contributions in this study are two-fold. First, we conduct a novel evaluation of two of the most used LLMs, GPT-3.5 and GPT-4, on their ability to perform fact-checking using a specialized dataset. An original part of our examination distinguishes the models' performance with and without access to external context, highlighting the importance of contextual data in the verification process. Second, by allowing the LLM agent to perform web searches, we propose an original methodology integrating information retrieval and claim verification for automated fact-checking. By leveraging the ReAct framework, we design an iterative agent that decides whether to conclude a web search or continue with more queries, striking a balance between accuracy and efficiency. This enables the model to justify its reasoning and cite the relevant retrieved data, therefore addressing the verifiability and explainability of the model's verdict. Lastly, we perform the first assessment of GPT-3.5's capability to fact-check across multiple languages, which is crucial in today's globalized information ecosystem.\nWe find that incorporating contextual information significantly improves accuracy. This highlights the importance of gathering external evidence during automated verification. We find that the models show good average accuracy, but they struggle with ambiguous verdicts. Our evaluation shows that GPT-4 significantly outperforms GPT-3.5 at fact-checking claims. However, performance varies substantially across languages. Non-English claims see a large boost when translated to English before being fed to the models. We find no sudden decrease in accuracy after the official training cutoff dates for GPT-3.5 and GPT-4. This suggests that the continued learning from human feedback may expand these models' knowledge."
    },
    {
      "id": 2,
      "title": "2 Materials and methods",
      "content": "# 2 Materials and methods\n\n### 2.1 Approach\n\nThis paper contributes to both the evidence retrieval and claim verification steps of the automated factchecking pipeline. Our approach focuses on verifying claims - assessing if a statement is true or false, while simultaneously retrieving contextual information to augment the ability of the LLM to reason about the given claims. We use large artificial intelligence language models GPT-3.5 and GPT-4. We combine state-of-the-art language models, iterative searching, and agent-based reasoning to advance automated claim verification.\n\nGPT-3.5 and GPT-4 are neural networks trained on vast amounts of textual data to generate coherent continuations of text prompts [39]. They comprise multiple layers of transformer blocks, which contain self-attention mechanisms that allow the model to learn contextual representations of words and sentences [39]. LLMs are trained using self-supervision, where the model's objective is to predict the next token in a sequence of text. The GPT models are trained on vast amounts of unstructured textual data like the common crawl dataset, which is the largest dataset to be included in the training. Common Crawl is a web archive that consists of terabytes of data collected since 2008 [40] [41]. GPT-3.5 and GPT-4 are additionally trained with reinforcement learning from human feedback (Rlhf), where human feedback is incorporated to enhance the models' usability. OpenAI states that they regularly update their models based on human feedback, potentially leading to knowledge of current events that expands upon the initial training regime, which was stopped in September of $2021^{2}$.\nWe are evaluating the performance of GPT-3.5 and GPT-4 based on two conditions. First, we query the models with the statement, the author, and the date of the statement. The model does not possess any means of retrieving further information that might enable it to make an informed decision. Rather this approach relies on the model having knowledge of the events described in the claim. In the second condition, we enable the LLM to query a Google Search engine to retrieve relevant information surrounding the claim. To prevent the model from uncovering the fact-check itself, we filter the returned Google search results for all domains present in the dataset. We present the LLM with information returned from the Google Search Engine API, comprising previews of search results. These previews included the title of the website, the link, and an extract of relevant context, mirroring the typical user experience on Google. To refine our approach, we experimented with integrating additional information from the full HTML content of websites. We quickly realised that this voluminous data was overwhelming the LLM's context window, leading to suboptimal performance. To address this, we employed BM25, an information retrieval function [42], to distil the most critical parts of each website. We found no improvement to the performance of the LLMs, as Google already uses machine learning methods to identify the most important information to include in the preview. We equip the agent with the ability to query Google by leveraging the Reasoning \\& Acting (ReAct) framework, proposed by [43], which allows an LLM to interact with tools. The goal of the ReAct framework is to combine reasoning and take actions. Reasoning refers to the model planning and executing actions based on observations from the environment. Actions are function calls or API calls by the model to retrieve additional information from external sources. The model is initially prompted with the claim and then decides if it needs to take actions. As we equip both models with the ability to query Google, the model then is able to retrieve information and receives its next observation. Based on this observation, the model can decide to either return a final answer or retrieve information with a different query. If the model fails to answer after three iterations of retrieving information, we terminate the search. The model retrieves 10 Google search results per iteration. To test the claim verification in a realistic scenario, any result from a fact-checking website present in the dataset is removed from the results. If the model was terminated because it reached the maximum number of iterations, it is prompted to provide a final answer based on all of its previous Google searches. In this paper, we employ the LangChain library [44] to create an agent within the ReAct framework. We show in Fig. 1 the workflow we implemented and an example of the treatment of a claim.\n\nWe employed the 16 k context window which is standard for the GPT API. Since our experiments larger con-\n\n[^0]\n[^0]:    ${ }^{2}$ https://openai.com/blog/chatgpt\n\ntext windows have been introduced [45], which could enable future LLM powered fact-checking applications to incorporate more search engine results or to include more content from each website.\n\nTo illustrate the capabilities of the proposed system, Fig. 2 and Fig. 3 showcase two correctly classified and two incorrectly assessed verdicts. The full transcripts from these examples, including all retrieved Google links, are available in the supplementary material."
    },
    {
      "id": 3,
      "title": "2.2 Experiment",
      "content": "# 2.2 Experiment\n\nWe conduct two experiments to evaluate how well our agents can fact-check different claims. First, we evaluate whether GPT-4 is significantly better at fact-checking than GPT-3.5. GPT-4 has been shown to outperform GPT-3.5 on a variety of benchmarks, particularly in zero-shot reasoning tasks [46]. We therefore compare the performance of GPT-3.5 and GPT-4 on two datasets, a dataset of US political factchecked claims provided by PolitiFact [47] and a dataset of multilingual fact-checked claims provided by Data Common. As GPT-3.5 is openly available to the public for free, it is used substantially more used than GPT-4, thereby increasing the importance of understanding its performance.\n\n### 2.2.1 Experiment on the PolitiFact Dataset\n\nThe dataset used in this experiment consists of a database of fact-checked claims by PolitiFact [47]. Each observation has a statement originator (the person who made the statement being fact-checked), a statement (the statement being fact-checked), a statement date, and a verdict. The verdict of a fact-check is one of six ordinal categories, indicating to which degree a statement is true or false: True, Mostly-True, Half-True, Mostly-False, False, and Pants-Fire. Pants-Fire indicates that a statement is utterly false. In total, the dataset has 21,152 fact-checks, spanning a time-frame of 2007 to 2022. The number of fact-checks per month is shown in Fig. 4. We sampled 500 claims for each response category, leading to a total of 3000 unique fact-checks to be parsed. We thereby ensured an equal distribution of outcome labels in the final dataset.\n\nWhile comparing the two models, we also compare the accuracy of the model with and without additional context. We therefore run four conditions, GPT-3.5 with context and without context, and GPT-4 with and without context. The No-Context condition refers to the model being prompted to categorize a claim into the veracity categories, without being given any context, or the ability to retrieve context. In addition to the veracity labels, the model is able to return a verdict of \"uncertain\" to indicate, that it isn't capable of returning an assessment. In the Context condition, the agent is capable of formulating Google queries to retrieve information pertinent to the claim. When the model is able to retrieve contextual information, any results from PolitiFact are excluded from the results.\n\n### 2.2.2 Experiment on the Multilingual Dataset\n\nSecondly, we investigate whether the ability of LLMs to fact-check claims depends significantly on the language of the initial claim. While both GPT-3.5 and GPT-4 exhibit impressive multilingual understanding, a variety of empirical results have shown that large language models struggle to adequately understand and generate non-English language text [48], [49], [50]. However, misinformation is a problem that is not restricted to high-resource languages. Conversely, fact-checking has become a global endeavor over the last decade, with a growing number of dedicated organizations in non-English language countries.\n\nThe dataset used is a fact-check dump by Data Commons. Each fact-check has an associated author and date. Additionally, it contains the Claim that is being fact-checked, a Review, and a verdict.\n\nThe dataset contains a breadth of fact-checking organizations and languages. In total, we detect 78 unique languages in the dataset. By extracting the domain of each fact-check link we find that it contains factchecks from 454 unique fact-checking organizations. The largest domains, factly (7277), factcrescendo (5664), youturn (3582), boatos (2829), dpa-fact-checking (2394), verafiles (1972), uol (1729), and tempo (1133), have more than one thousand fact-checks, and make up $73 \\%$ of the dataset. In contrast, there are 266 different domains which only have one associated fact-check. Figure 4 shows the total amount of fact-checks in the dataset per month. While the oldest fact-check in the dataset was published in 2011, we see that very few\n\n![img-0.jpeg](img-0.jpeg)\n\n| Prompt: You're a fact-checking expert. Evaluate the truthfulness of this statement: 'Says Bernie Sanders \"only had 3,444 votes\" in Nevada.' made by Viral image on 3/6/2020. Use Google for research. Consider your sources, context, and date while assessing. To answer return 'Final Answer: (verdict), (reason). You must respond with a valid verdict ('pantsfire','false', 'mostly-false', 'half-true', 'mostly-true', or 'true') or 'uncertain', providing reasoning and citing sources by providing the domain of pertinent search results. <br> Google: Bernie Sanders Nevada 2020 votes count <br> 1. nytimes.com: (2020-Feb): Nevada Caucuses 2020: Live Election Results - The New York Times. Feb 22, 2020 ... <br> 2. nbcnews.com: Sanders wins Nevada Democratic caucuses with wave of young and ... <br> 3. nytimes.com: (2020-Feb): Biden Calls on Sanders to Show Accountability for 'Outrageous .. Feb 16, 2020 <br> 4. vox.com: (2020-Feb): Nevada caucus results: 3 winners and 2 losers - Vox. Feb 22, 2020 <br> 5. washingtonpost.com: 2020 Nevada Caucuses Live Election Results | The Washington Post. Feb 24, 2020 ... 2020 Nevada Democratic presidential caucuses ; Bernie Sanders, 6,788, 46.8 ; Joe Biden, 2,927, 20.2 ; Pete Buttigieg, 2,073 .... <br> 6. theintercept.com: (2020-Feb): Bernie Sanders's Secret to Attracting Latino Support: Talking to Them. Feb 20, 2020 <br> 7. pbs.org: (2020-Feb): Bloomberg qualifies for debate, Sanders leads ahead of Nevada <br> 8. wikipedia.org: 2020 Nevada Democratic presidential caucuses - Wikipedia. <br> 9. politico.com: Iowa Election Results 2020 | Live Map Updates | Voting by County <br> 10. tufts.edu: (2020-Feb): Exclusive Analysis: In Nevada, young people once again force |\n| --- | --- |\n| Final Answer: False, Bernie Sanders received more than 3,444 votes in the Nevada Democratic presidential caucuses according to The Washington Post. | |\n\nFigure 1: Workflow showing how we enable LLM agents to interact with a context to assess the veracity of a claim (top). Example of the treatment of a specific claim (bottom)\n\nStatement: 'Ted Cruz is mathematically out of winning the race.' made by Donald Trump on 4/20/2016."
    },
    {
      "id": 4,
      "title": "Google Search Query: Republican primary delegate count April 202016",
      "content": "# Google Search Query: Republican primary delegate count April 202016\n\n- 2016 Delegate Count and Primary Results - The New York Times Republican Delegates ; Donald J. Trump $\\cdot$ 95 ; Ted Cruz $\\cdot$ 9 ; Marco Rubio. 167 ; John Kasich. 161 ... https://www.nytimes.com/interactive/2016/us/elec/tonos/primary-calendar-and-results.html\n- 2016 Republican Party presidential primaries - Wikipedia Fifty-six primary contests were conducted to choose 2,472 delegates. In 50 states and territories the delegates were allocated to candidates by popular vote\nhttps://en.wikipedia.org/wiki/2016_Republican_Party_presidential_primaries\n[...]\n\n## Google Search Query: Ted Cruz Donald Trump delegate count April 202016\n\n- 2016 Delegate Count and Primary Results - The New York Times According to the Associated Press, Donald J. Trump and Hillary Clinton have each won enough ... Donald J. Trump. Earned. Unpledged. 1,447. 95. Ted Cruz.\nhttps://www.nytimes.com/interactive/2016/us/elec/tonos/imay-calendar-and-results.html\n- The Real Magic Number That Donald Trump Needs to Hit is Likely ... Experts weigh in on the delegate count needed for the GOP nomination. ByMEGHAN KENEALLY. April 20, 2016, 8:26 AM. 4:02 ... Ted Cruz or Gov. https://abcnews.go.com/Politics/real-magic-number-donald-trump-hit-lower-1237delay?id=38539259\n[...]\n\n## Google Search Query: Republican primary delegate count by candidate April 202016\n\n- 2016 Delegate Count and Primary Results - The New York Times Delegate totals include unpledged delegates, also known as superdelegates, who are free to support any candidate at the party conventions. https://www.nytimes.com/interactive/2016/us/elec/tonos/primary-calendar-and-results.html\n- 2016 Republican Party presidential primaries - Wikipedia However, Trump scored landslide victories in New York and five northeastern states in April, before taking every delegate in the Indiana primary on May 3.\nhttps://en.wikipedia.org/wiki/2016_Republican_Party_presidential_primaries\n[...]\n\nFinal Output: Mostly true, the statement made by Donald Trump on 4/20/2016 that \"Ted Cruz is mathematically out of winning the race\" is mostly true. According to the delegate count from the New York Times (nytimes.com), Donald Trump had significantly more delegates than Ted Cruz. However, it's important to note that the race isn't officially over until a candidate reaches the required number of delegates to secure the nomination, which is 2016 was 1,237 delegates (abcnews.go.com). At the time of the statement, Trump had not yet reached this number, so technically Cruz was not mathematically eliminated. However, his path to victory was highly unlikely.\n\nPolitifact Verdict: Trump said, \"Ted Cruz is mathematically out of winning the race.\" Cruz would need at the very least 678 more delegates to win the race. It's impossible for him to reach that number in the remaining primaries, given that there are just 674 delegates left to win. However, he could conceivably find more supporters in the states who've already held primaries, given that a few dozen are thus far uncommitted. Trump's statement is largely accurate but needs additional information. We rate it Mostly True.\n\nStatement: 'President Trump's limousine driver did burnouts to entertain Daytona 500 spectators during a rain delay.' made by Viral Image on 2/21/2020'\n\n## Google Search Query: President Trump limousine Daytona 500 burnouts\n\n- 'Start your engines!' President Trump revs up Daytona 500 crowd ... - Feb 16, 2020 ... He was the first of any president to take a lap in the armored presidential limo, dubbed \"The Beast,\" FOX News reported. Trump and first lady ... https://www.fox2detrot.com/news/start-your-engines-president-trump-revs-up-daytona-500-crowd-does-historic-lap-in-presidential-limo\n- Denny Hamlin wins 3rd Daytona 500; Ryan Newman hospitalized - Feb 17, 2020 ... President Donald Trump and first lady Melania Trump ride in the presidential limousine as they take a pace lap ahead of the start of the ... https://www.news4jax.com/sports/2020/02/17/daytona-500-resumes-without-president-pump-packed-house/\n- Ryan Newman hospitalized in horrific wreck as Hamlin wins ... - Feb 17, 2020 ... Denny Hamlin won a crash-filled Daytona 500 on Monday after a greatly final-lap ... Trump takes lap in armored presidential limo 'the Beast'.\nhttps://www.theguardian.com/sport/2020/sep/17/daytona-500-denny-hamilton-yan-newman-craw/\n- NASCAR 2020 Thread | Steeler Nation - Feb 16, 2020 ... President Trump becomes first president named Daytona 500 grand marshal. DAYTONA BEACH, Fla. ... do a burn-out. the-beast-president-limo.jpg ... https://www.steelernation.com/forum/threadenwester-2020thread_34154/\n\n## Google Search Query: Republican primary delegate count by candidate April 202016\n\n- Ryan Newman hospitalized in horrific wreck as Hamlin wins ... - Feb 17, 2020 ... ... 500 at Daytona International Speedway. Photograph: Peter Casey/LISA Today Sports. Nascar ... Trump takes lap in armored presidential limo 'the ... https://www.theguardian.com/sport/2020/06/17/daytona-500-denny-hamilton-yan-newman-craw/\n- Denny Hamlin wins 3rd Daytona 500; Ryan Newman hospitalized - Feb 17, 2020 ... ... burnouts that Hamlin learned the accident was bad. \"It's a weird balance of ... Trump takes Daytona 500 warmup lap in presidential limousine.\nhttps://www.news4jax.com/sports/2020/02/17/daytona-500-resumes-without-president-pump-packed-house/\n- 'Start your engines!' President Trump revs up Daytona 500 crowd ... - ... 7 President Trump revs up Daytona 500 crowd, does historic lap in ... Deputy gives Dream Cruise driver a spanking after he did burnout on Woodward ... https://www.fox2detrot.com/news/start-your-engines-president-trump-revs-up-daytona-500-crowd-does-historic-lap-in-presidential-limo\n[...]\n\nFinal Output: False, there is no evidence from reliable sources to suggest that President Trump's limousine driver performed burnouts at the Daytona 500. The sources only confirm that Trump took a lap in his limousine.\n\nPolitifact Verdict: Sorry to disappoint, but no. A Facebook post shares an altered image of such a scene, along with a caption that says, \"the presidential limo driver entertains the crowd during the rain delays at the Daytona 500 with smokey burnouts.\" [...] The image then got passed around as authentic. If such a thing happened in the First Car, we would all know about it. We rate this altered photo an extra-smoky Pants on Fire!\n\nStatement: \"The Obama administration gave Iran \" $\\$ 400$ million in ransom payment cash.\" made by Donald Trump on 8/24/2016\n## Google Search Query: Obama administration $\\mathbf{\\$ 4 0 0}$ million payment to Iran\nInvestigating the Obama administration's $\\$ 400$ million payment to Iran - Aug 4, 2016 ... The Wall Street Journal revealed this week that in January, the Obama administration secretly airlifted $\\$ 400$ million in cash to Iran. https://www.pbs.org/newsHour/show/investigating-obama-administration-400-million-payment-iran\nUS sent plane with $\\$ 400$ million in cash to Iran | CNN Politics - Aug 4, 2016 ... The Obama administration secretly arranged a plane delivery of $\\$ 400$ million in cash on the same day Iran released four American prisoners ... https://www.cnn.com/2016/08/03/politics/us-sends-plane-iran-400-million-cash/index.html\n\\$400 Million: Why the U.S. Owed Iran in the First Place | Time - Aug 5, 2016 ... Answer: The actual hostage deal that in fact accounts for the cash payment, which President Obama said on Thursday was not a ransom. The ... https://time.com/4441046/400-million-iran-hostage-factory\nFact-checking Trump tweets on Obama's payments to Iran | CNN ... - Jun 21, 2019 ... The entire $\\$ 1.7$ billion, given in two separate payments of $\\$ 400$ million and $\\$ 1.3$ billion, was provided in cash. The timing of the first payment ... https://www.cnn.com/2019/08/21/politics/trump-twiller-iran-fact-check/index.html\n## Google Search Query: Purpose of Obama administration's $\\mathbf{\\$ 4 0 0}$ million payment to Iran\nInvestigating the Obama administration's $\\$ 400$ million payment to Iran - Aug 4, 2016 ... The Wall Street Journal revealed this week that in January, the Obama administration secretly airlifted $\\$ 400$ million in cash to Iran. https://www.pbs.org/newsHour/show/investigating-obama-administration-s400-million-payment-iran\nUS sent plane with $\\$ 400$ million in cash to Iran | CNN Politics - Aug 4, 2016 ... He described the payment as a \"conscious strategic decision that was made on the part of the Obama administration as we were implementing the ... https://www.cnn.com/2016/08/03/politics/us-sends-plane-iran-400-million-cash/index.html\nChairman Chaffetz seeks answers for $\\$ 400$ million sent to Iran ... - Aug 3, 2016 ... ... letter to Secretary of State John Kerry asking for information surrounding the Obama administration's $\\$ 400$ million cash payment to Iran. https://oversight.house.gov/release/chairman-chaffetz-seeks-answers-400-million-sent-iran\nU.S. Concedes $\\$ 400$ Million Payment to Iran Was Delayed as ... - Aug 16, 2016 ... For months the Obama administration had maintained that the payment was part of a settlement and did not amount to a \"ransom https://www.nytimes.com/2016/08/19/world/middleeast/iran-uk-cash-payment-antenary.html\nFinal Output: \"Full-trust. The Obama administration did send $\\$ 400$ million to Iran, but whether this payment was a \"ransom\" is disputed and seems to depend on one's interpretation of the term.\n\nPolitiFact Verdict: Trump said the Obama administration gave Iran \" $\\$ 400$ million in ransom payment cash.\" On the same day in January, the United States paid Iran $\\$ 400$ million dollars, and Iran released several American prisoners. While there appears to be a quid pro quo - the United States would not give Iran the cash until the prisoners were on their way home - this does not equate to ransom, experts told us. [...] Trump's statement contains some element of truth but ignores critical facts that would give a different impression. We rate it Mostly False.\n\nStatement: \"Miller Brewing is not a U.S. company any more. Neither is Anheuser-Busch,\" made by Paul Ryan on 9/8/2014\n\n## Google Search Query: Miller Brewing current ownership and headquarters\n\n- Miller Brewing Company - Wikipedia - The Miller Brewing Company is an American brewery and beer company in Milwaukee, Wisconsin. It was founded in 1855 by Frederick Miller. https://en.wikipedia.org/wiki/Miller Brewing Company\n- Molson Coors: Home - The official website of Molson Coors Beverage Company, which for more than two centuries has been brewing beverages that unite people for all of life's ... https://www.molsoncoors.com/\n- MillerCoors - Wikipedia - MillerCoors was a beer brewing company in the United States. MillerCoors was formed in 2008 as a joint venture between SABMiller and Molson Coors to combine ... https://en.wikipedia.org/wiki/MillerCoors\n- Office of Public Affairs | Justice Department Requires Anheuser ... - Jul 20, 2016 ... \"The two largest U.S. brewers - ABI and MillerCoors - will now remain ... MillerCoors owns and operates 12 breweries in the United States. https://www.justice.gov/cpe/pn/justice-department-requires-anheuser-busch-risks-digest-stake-millercoors-and-alter-beer\n- Where is Miller High Life Brewed? (Explained) - BeverageBeaver - Below, we'll tell you more about where the company was first founded, the different breweries and who currently owns Miller High Life. Where is this beer ... https://beveragebeaver.com/where-is-miller-high-life-brewed-explained/\nWho owns Miller Lite? | The US Sun - May 15, 2023 ... Although the beer brand was launched by Rheingold Brewery, the beverage has seen a shift in owner over the years. Advertisement. Miller Lite ... https://www.the-sun.com/money/4126579/who-owns-miller-lite/\n- Miller Brewing Co. | Milwaukee, WI | Beers | BeerAdvocate - In 2016 SABMiller sold its stake in MillerCoors to Molson Coors, which gained full ownership of the Miller brand portfolio outside the US and Puerto Rico, and ... https://www.beeradvocate.com/beer/profile/105/\n- Miller High Life shipment destroyed over 'Champagne of Beers ... - Apr 22, 2023 ... Molson Coors Beverage Co., which owns the Miller High Life brand, does not currently export it to the EU, and Belgian customs declined to ... https://www.npr.org/2023/04/22/117148042?/miller-beer-champagne-cans-deelmark/\nMiller Lite: Home of the Original Life Beer - The times change. But some things never do. The best times are spent together with our best friends drinking a few beers. That's why we started brewing Miller ... https://www.millerlife.com/\n\nFinal Output: False. Miller Brewing Company and Anheuser-Busch are still U.S. companies, although their ownership has changed. (sources: Wikipedia, U.S. Department of Justice, BeerAdvocate, NPR)\n\nPolitiFact Verdict: Ryan cited two well-known beer makers - including one founded in Milwaukee - when asked about inversions. He said the parent companies of Miller and Anheuser-Busch are located overseas. They were part of a long-standing and rapid world-wide consolidation of the major breweries. We rate his statement True.\n\nFigure 3: Examples of Politifact statements for which the model returns incorrect responses. We show the Google queries performed by the LLM and the first results of each query. The LLM is asked to verify whether the Obama administration paid a ransom payment to Iran. The LLM finds information on the payment but can't conclusively confirm the purpose of the payment. It concludes that the statement is half-true. PolitiFact argues that the statement is mostly-false, as the payment is not necessarily a ransom payment. In the second example, the LLM is asked to verify whether a beer brand is American. It finds information indicating that the company is American and returns False. The company has, however, been bought by foreign investors, making the statement true. The full examples, including all Google results, are shown in the Supplementary Material.\n\nfact-checks were uploaded until early 2019. Since then the dataset contains a relatively stable amount of around eight hundred fact-checks per month.\n\nTo use the Data Commons dataset we need to standardize the dataset. We reduced the number of unique verdicts, as many different fact-checking organizations use different ordinal scales that are hard to unify. By mapping all scales onto a coarse 4-level scale (\"False\", \"Mostly False\", \"Mostly True\", & \"True\"), we can then compare how the ability of our model to fact-check depends on the correct verdict assigned by the fact-checker. We translated all present verdicts from their original language to English. We then manually mapped all verdicts which appeared at least twice in the dataset (n = 468) to the four categories. We removed all observations that we were unable to map to one of these four categories. This included observations with a verdict such as \"Sarcasm\", \"Satire\", or \"unconfirmed\".\n\nSubsequently, we discarded all languages, that did not have at least 50 observations and languages that did not have at least 10 \"true\" or \"mostly true\" observations. From the remaining languages, we sampled up to 500 observations. To compare the ability of our approach across languages, we then used Googletrans³, a free python library that utilizes the Google Translate API, to translate all claims from their original language to English.\n\nIn this experiment, we compare the performance of GPT-3.5 both with context and without context in English and in the original language. In the context condition, we removed all fact-checking websites from the dataset from the Google search results.\n\n![img-1.jpeg](img-1.jpeg)\n\n**Figure 4:** Number of fact-checks per month in the Data Commons & PolitiFact datasets. Number of fact-checks Per Month in the Data Commons & PolitiFact Datasets. In blue (dashed) the number of fact-checks in the PolitFact Dataset are shown. The orange (solid) line indicates the number of fact-checks in the Data Commons dataset.\n\n<sup>3</sup>https://pypi.org/project/googletrans/"
    },
    {
      "id": 5,
      "title": "3 Results",
      "content": "# 3 Results\n\n### 3.1 Experiment on the PolitiFact Dataset\n\n| Correct Verdict | No Context |  |  | Context |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n## |  | GPT-3.5 | GPT-4 |  | GPT-3.5 | GPT-4 |  |\n| pants fire | 92.19 | 91.80 |  | 93.42 | 93.92 |  |\n| false | 81.63 | 88.08 |  | 88.75 | 86.13 |  |\n| mostly false | 79.64 | 68.70 |  | 71.11 | 55.82 |  |\n| half true | 36.16 | 51.90 |  | 51.58 | 67.26 |  |\n| mostly true | 42.35 | 79.29 |  | 67.27 | 80.88 |  |\n| true | 49.11 | 71.30 |  | 67.61 | 84.92 |  |\n\n| No Context |  |  | Context |  |\n| :--: | :--: | :--: | :--: | :--: |\n## | GPT-3.5 | GPT-4 |  | GPT-3.5 | GPT-4 |\n| 25.81 | 28.94 |  | 0.00 | 12.17 |\n| 64.17 | 47.49 |  | 86.32 | 55.82 |\n| 8.82 | 50.00 |  | 10.67 | 40.75 |\n| 2.75 | 11.58 |  | 0.00 | 3.57 |\n| 9.41 | 59.91 |  | 36.82 | 61.13 |\n| 37.05 | 22.22 |  | 33.80 | 33.00 |\n\nTable 1: Comparison of accuracy of all conditions on the PolitiFact dataset. For the table on the left, accuracy is calculated as the percentage of time predicting the correct overall category. Predicting any of pants-fire, false, or mostly false is correct when the claim is labeled as pants-fire, false, or mostly false. Conversely, predicting any half-true, mostly true, or true is correct when the claim is labeled as one of these categories. For the table on the right, accuracy is calculated as the percentage of time predicting the PolitiFact category. For example, predicting \"half-true\" when the claim is labeled as \"mostly-true\" is treated as a false prediction.\n\nTable 1 presents a comparative analysis of the GPT-3.5 and GPT-4 models in the task of automated factchecking using the PolitiFact dataset. The table categorizes the performance of both models across different veracity labels ranging from \"pants-fire\" to \"true.\" In the left table, the accuracy of the model is computed by considering only two categories, i.e. grouping \"pants-fire\", \"false\", and \"mostly-false\" together to false and \"half-true\", \"mostly-true\", and \"true\" to true. In the right table, the accuracy is computed using all the PolitiFact categories.\nIn the no-context condition, considering the overall accuracy (left table), GPT-4 generally outperforms GPT-3.5. GPT-3.5 predicts the veracity of a claim to be false in $58.2 \\%$ of the cases (compared to $22.89 \\%$ for GPT-4) achieving an accuracy of $36-49 \\%$ in the true-label categories. In the false-label categories, the difference between the two models is significantly smaller and GPT-3.5 outperforms GPT-4 in the mostly false category. Both models achieve an accuracy of over $90 \\%$ for claims that are labeled with \"pants-onfire\". In the context condition, GPT-3.5 is significantly better calibrated, meaning it exhibits a more balanced accuracy between true and false verdicts. While the accuracy in the false categories only differs insignificantly, GPT-3.5 achieves significantly better results in predicting true verdicts in the context condition than in the no-context condition. Similarly, the difference in false categories is smaller for GPT-4 than in true categories, where it achieves an increase of 10.19 percentage points on average. In the context condition, both GPT-3.5 and GPT-4 outperform the no-context conditions on average. Context both increases the ability of the models to discriminate true from false claims, but additionally, calibrates both models better, predicting true for more cases correctly.\n\nAcross all conditions, both models generally fare better in identifying false statements (\"pants-fire,\" \"mostlyfalse,\" and \"false\") than true ones (\"half-true,\" \"mostly-true,\" and \"true\"), mirroring the findings of [12] in the no-context settings. This could be attributed to the inherent complexity of verifying a statement's absolute truth compared to identifying falsehoods. Lastly, both models have reduced performance in less extreme categories like \"half-true\" and \"mostly-false.\" These categories are inherently ambiguous and represent statements that have elements of both truth and falsehood, making them more challenging to classify accurately. The table on the right only considers a claim to be correctly classified when the model predicts the exact category. The extremely low scores show that the models are unable to predict the exact shade of truth that PolitiFact assigned to the statement.\n\nBoth GPT-3.5 and GPT-4 were trained with data up to September of $2021^{4}$. As the training data of both models is private, data leakage is a significant concern for fact-checks published before that date. To investigate, whether the accuracy of the LLMs differs over time we plot the accuracy of all four conditions ( 2 models x 2 context conditions) in Fig. 5.\n\n[^0]\n[^0]:    ${ }^{4}$ https://platform.openai.com/docs/models/gpt-4\n\n![img-2.jpeg](img-2.jpeg)\n\nFigure 5: Accuracy of GPT-3.5 & GPT-4 overtime on the PolitiFact dataset. Yearly rolling average of the accuracy of LLMs over Time. Panel (A) displays the accuracy of GPT-4. Panel (B) shows the accuracy of GPT-3.5. The blue line indicates the context condition, and the orange line indicates the no-context condition. The vertical line represents the training end date of both models according to OpenAI. A faint line is the three-month average. The x-axis represents the date of the claim. The bands represent one standard error.\n\nThe accuracy of all four conditions exhibits an upward trend over time. For the context condition, this improvement can likely be attributed to an increasing availability of relevant information on Google over recent years. More surprisingly, the no-context condition also displays a similar trajectory of improving performance. One potential explanation is that the ground truth labels of some statements in the dataset may have evolved as more facts emerged. We do not observe any sudden decrease in accuracy after the official training cutoff for GPT-3.5 and GPT-4. This suggests that post-training refinements to the models via reinforcement learning from human feedback (RLHF) may introduce new knowledge, particularly for more recent events closer to the RLHF time period.\n\n### 3.2 Experiment on the Multilingual Dataset\n\n|  Language | Accuracy English | Accuracy Multilingual | F1 English | F1 Multilingual | Number of Samples  |\n| --- | --- | --- | --- | --- | --- |\n|  Turkish | 84.19 | 81.50 | 83.59 | 81.91 | 500  |\n|  Indonesian | 86.68 | 84.59 | 87.52 | 79.59 | 500  |\n|  French | 74.67 | 81.67 | 75.57 | 72.30 | 166  |\n|  English | - | 60.98 | - | 68.65 | 500  |\n|  Thai | 48.21 | 54.34 | 50.41 | 66.45 | 69  |\n|  Portuguese | 77.22 | 65.56 | 77.02 | 64.28 | 500  |\n|  Tamil | 70.33 | 59.19 | 67.68 | 51.59 | 500  |\n|  Spanish | 56.92 | 51.26 | 57.69 | 49.51 | 500  |\n|  Italian | 45.25 | 50.29 | 48.25 | 47.07 | 427  |\n|  Chinese | 64.86 | 43.75 | 68.56 | 42.88 | 174  |\n|  Hebrew | 49.61 | 41.17 | 56.51 | 40.65 | 287  |\n|  Farsi | 68.28 | 40.54 | 71.96 | 36.26 | 259  |\n|  Telugu | 59.79 | 26.99 | 67.03 | 27.24 | 500  |\n|  Azerbaijani | 37.23 | 22.89 | 42.26 | 15.64 | 98  |\n\nTable 2: Performance on the multilingual dataset without context. Accuracy and F1 score for the fact-checking without context on the multilingual dataset. Accuracy English and F1 English are the scores for the case where the models are provided with English translation of the claims while the other scores are computed in the case where the models are provided with the original claims.\n\nTable 2 shows the accuracy of GPT-3.5 in evaluating the veracity of fact-checking claims. The columns Accuracy English and Accuracy Multilingual show the percentage of correctly classified claims by language. English refers to the translations of the claims while multilingual refers to the original language. The F1 columns show the F1 score of the models for the Multilingual and English condition. We add the F1 score to the evaluation to account for the fact that the class distribution differs by language.\n\nIn all languages, except for Thai, we see an increase in the F1 score of the model when fact-checking the translated claim as compared to the original. This shows that the models that were tested are significantly better at predicting the verdict of a statement when it is presented in English, mirroring prior research indicating that the models struggle to correctly model and classify non-English language text. Similarly, the accuracy was significantly higher for the English language condition.\n\n|  Language | Accuracy English | Accuracy Multilingual | F1 English | F1 Multilingual | Number of Samples  |\n| --- | --- | --- | --- | --- | --- |\n|  Portuguese | 87.42 | 89.21 | 75.97 | 80.78 | 500  |\n|  Indonesian | 87.16 | 89.98 | 77.26 | 73.36 | 500  |\n|  English | - | 80.16 | - | 71.33 | 500  |\n|  Telugu | 83.15 | 83.80 | 77.21 | 69.97 | 500  |\n|  Thai | 77.27 | 50.00 | 55.66 | 66.66 | 69  |\n|  French | 89.55 | 87.09 | 79.58 | 56.13 | 166  |\n|  Chinese | 75.81 | 84.78 | 71.86 | 55.35 | 174  |\n|  Farsi | 77.78 | 59.00 | 69.15 | 48.58 | 259  |\n|  Turkish | 72.28 | 71.79 | 70.79 | 47.32 | 500  |\n|  Spanish | 82.68 | 75.18 | 63.01 | 46.30 | 500  |\n|  Italian | 56.93 | 56.29 | 40.15 | 45.54 | 427  |\n|  Tamil | 80.56 | 55.22 | 62.95 | 34.86 | 500  |\n|  Hebrew | 73.61 | 85.71 | 63.81 | 34.42 | 287  |\n|  Azerbaijani | 62.07 | 44.00 | 43.43 | 33.23 | 97  |\n\nTable 3: Performance on the multilingual dataset with context. Accuracy and F1 score for the fact-checking with context on the multilingual dataset. Accuracy English and F1 English are the scores for the case where the models are provided with English translation of the claims while the other scores are computed in the case where the models are provided with the original claims.\n\nTable 3 shows the accuracy of GPT-3.5 in evaluating the veracity of fact-checking claims with additional context provided. Similarly to table 2 the English language condition is significantly more accurate than the Multilingual condition. In all but three languages (Portugese, Thai, and Italian), translating increased the F1 score of the model. This difference was again significant.\n\nFigure 6 summarizes the findings offered in this section and displays a visualization that compares the F1 scores under the two conditions-'No Context' and 'Context Retrieval'-across various languages. Each language is represented once on the y-axis, with two corresponding horizontal lines plotted at different heights. The blue dashed lines indicate the performance under the 'No Context' condition, while the red solid lines represent the 'Context Retrieval' condition. Each line connects two points, corresponding to the F1 scores achieved when the model is trained on either English-only or Multilingual data. The circle marker displays the F1 score in the English language condition, while the square portrays the efficacy under the multilingual condition.\n\nSimilarly, to the PolitiFact experiment, we analyzed the performance of the model over time. Figure 7 shows the F1 score of all conditions over time. The confidence intervals represent the standard errors and are bootstrapped by sampling with replacement from the dataset and repeatedly calculating the F1 score. As in the PolitiFact experiment, we again see an increase in the performance of the models over time. Mirroring the previous findings, we do not see a decrease in the performance of the models following the official cut-off date for the training of GPT-3.5.\n\n![img-3.jpeg](img-3.jpeg)\n\nFigure 6: Comparison of all conditions on the Data Commons fact-checking dataset. The x-axis displays the F1 score of each language and condition. The y-axis displays each language for which the model was prompted. The blue, dashed line indicates the difference in the performance for the no context condition. The red, solid line shows the difference in performance for the context condition. The circles show the F1 score by language for the translations, the squares show the F1 score for the original scores."
    },
    {
      "id": 6,
      "title": "4 Conclusion",
      "content": "# 4 Conclusion\n\nIn this study, we investigated OpenAIs GPT-3.5 and GPT-4s ability to fact-check claims. We found that GPT-4 outperformed GPT-3.5. Notably, even without contextual information, GPT-3.5 and GPT-4 demonstrate good performance of $63-75 \\%$ accuracy on average, which further improves to above $80 \\%$ and $89 \\%$ for non-ambiguous verdicts when context is incorporated. Another crucial observation is the dependency on the language of the prompt. For non-English fact-checking tasks, translated prompts frequently outperformed the originals, even if the related claims pertained to non-English speaking countries and retrieved predominantly non-English information. The accuracy of these models varied significantly across languages, underscoring the importance of the original language of the claim.\n\nAs the training data of the GPT models includes vast amounts of web data, including a filtered version of the Common Crawl Corpus [41], there is a significant risk of data leakage for the task of fact-checking. Previous research has shown that misinformation does not exist in isolation, but is repeated [51] across platforms [52] and languages $[53,54]$. As misinformation is repeated and re-occurs, the ability of models to retain previously fact-checked claims can potentially be seen as a benefit rather than a drawback. Nevertheless, this presents a significant risk for the task of fact-checking novel misinformation. We address this in the paper by testing the ability of the models to detect misinformation after the training end date of the models. We found no reduction in performance for fact-checks after the training end date. It seems that incorporating real-time context for novel misinformation enables the models to reason about novel information. In summary, while\n\n![img-4.jpeg](img-4.jpeg)\n\nFigure 7: F1 score of context (A) and no-context-condition (B) over time. The orange (dashed) line shows the multilingual condition. The blue (solid) line showcases the F1 score for the English condition. The vertical line indicates the official training end date for GPT 3.5.\n\nData-leakage is a significant concern for any application that tests the abilities of LLMs, we argue that for the task of fact-checking it does not seem to degrade performance and might even be beneficial in view of recurring misinformation.\n\nComparing the performance of the models across languages is inherently difficult, as the fact-checks are not standardised across languages. Fact-checking organisations differ significantly in their choice of which fact-checks to dedicate time to, with some focussing exclusively on local issues and others researching any claims that are viral on social media. Similarly, some fact-checking organisations focus on current claims, while others debunk long-standing misinformation claims. All of these factors influence the ability of a large language model to discern the veracity of a claim. It is therefore possible that fact-checks in low-resource languages perform better than higher resource languages. The most salient point in the analysis of the multilingual misinformation is that the LLMs outperform the multilingual baseline when prompted with the English translation of the fact-checks. The variability in model performance across languages and the improvement of the accuracy when prompted with English language fact-checks indicates that the training regimen, in which the distribution of languages is highly skewed and English is dominant [55], significantly impacts accuracy. This suggests that the effectiveness of LLMs in fact-checking is not uniformly distributed across languages, likely due to the uneven representation of languages in training data.\n\nOur results suggest that these models cannot completely replace human fact-checkers as being wrong, even if infrequently, may have devastating implications in today's information ecosystem. Therefore, integrating mechanisms allowing for the verification of their verdict and reasoning is paramount. In particular, they hold potential as tools for content moderation and accelerating human fact-checkers' work.\n\nLooking ahead, it is important to delve deeper into the conditions under which Large Language Models excel or falter. As these models gain responsibilities in various high-stakes domains, it is crucial that their factual reliability is well-understood and that they are deployed judiciously under human supervision.\n\nWhile our study concentrated on OpenAI's GPT-3.5 and GPT-4, the rapid evolution of the field means newer, possibly fine-tuned, LLMs are emerging. One salient advantage of our methodology, distinguishing it from others, is the LLM Agents' capability to justify their conclusions. Future research should explore if, by critically examining the reasons and references provided by the LLMs, users can enhance the models' ability to fact-check claims effectively."
    },
    {
      "id": 7,
      "title": "References",
      "content": "# References\n\n1. Porter, E. \\& Wood, T. J. The global effectiveness of fact-checking: Evidence from simultaneous experiments in Argentina, Nigeria, South Africa, and the United Kingdom. Proceedings of the National Academy of Sciences 118, e2104235118 (2021).\n2. Morris, D. S., Morris, J. S. \\& Francia, P. L. A fake news inoculation? Fact checkers, partisan identification, and the power of misinformation. Politics, Groups, and Identities 8, 986-1005 (2020).\n3. Nyhan, B. \\& Reifler, J. Estimating fact-checking's effects. Arlington, VA: American Press Institute (2015).\n## 4. Mantzarlis, A. Fact-checking 101 85-100 (Unesco Publishing Paris, France, 2018).\n5. Bovet, A. \\& Makse, H. A. Influence of fake news in Twitter during the 2016 US presidential election. Nature communications 10, 7 (2019).\n6. Grinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B. \\& Lazer, D. Fake news on Twitter during the 2016 U.S. presidential election. Science 363, 374-378 (2019).\n7. Flamino, J. et al. Political polarization of news media and influencers on Twitter in the 2016 and 2020 US presidential elections. Nature Human Behaviour 7, 904-916 (2023).\n8. Graves, L. \\& Cherubini, F. The rise of fact-checking sites in Europe. Digital News Project Report (2016).\n9. Siwakoti, S. et al. How COVID drove the evolution of fact-checking. Harvard Kennedy School Misinformation Review (2021).\n10. Hassan, N. et al. The quest to automate fact-checking in Proceedings of the 2015 computation+ journalism symposium (2015).\n11. Adair, B., Li, C., Yang, J. \\& Yu, C. Progress toward \"the holy grail\": The continued quest to automate fact-checking in (2017).\n12. Hoes, E., Altay, S. \\& Bermeo, J. Leveraging ChatGPT for Efficient Fact-Checking. PsyArXiv [Preprint]. Available at: https://osf.io/preprints/psyarxiv/qnjkf/ Accessed November 16, 2023 (2023).\n13. Sawiński, M. et al. Openfact at CheckThat! 2023: Head-to-head GPT vs. BERT-a comparative study of transformers language models for the detection of check-worthy claims in CEUR Workshop Proceedings 3497 (2023).\n14. He, P., Gao, J. \\& Chen, W. DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. ArXiv [Preprint]. Available at: https://arxiv.org/ abs/2111.09543 Accessed November 16, 2023 (2021).\n15. Caramancion, K. M. News verifiers showdown: A comparative performance evaluation of chatgpt 3.5, chatgpt 4.0, bing ai, and bard in news fact-checking. ArXiv [Preprint]. Available at: https://arxiv. org/abs/2306.17176 Accessed November 16, 2023 (2023).\n16. Choi, E. C. \\& Ferrara, E. Automated Claim Matching with Large Language Models: Empowering FactCheckers in the Fight Against Misinformation. ArXiv [Preprint]. Available at: https://arxiv.org/ abs/2310.09223 (2023).\n17. Choudhury, A. \\& Shamszare, H. Investigating the Impact of User Trust on the Adoption and Use of ChatGPT: Survey Analysis. Journal of Medical Internet Research 25, e47184 (2023).\n18. World Economic Forum. The Global Risk Report Accessed: 2024-01-09. 2024. \\url (https://www3. weforum.org/docs/WEF_The_Global_Risks_Report_2024.pdf).\n19. Kotonya, N. \\& Toni, F. Explainable Automated Fact-Checking: A Survey in Proceedings of the 28th International Conference on Computational Linguistics (International Committee on Computational Linguistics, Barcelona, Spain (Online), 2020), 5430-5443.\n20. Gorrell, G. et al. SemEval-2019 Task 7: RumourEval, Determining Rumour Veracity and Support for Rumours in Proceedings of the 13th International Workshop on Semantic Evaluation (Association for Computational Linguistics, Minneapolis, Minnesota, USA, 2019), 845-854.\n21. Nakov, P. et al. Overview of the clef-2022 checkthat! lab on fighting the covid-19 infodemic and fake news detection in International Conference of the Cross-Language Evaluation Forum for European Languages (2022), 495-520.\n22. Nakov, P. et al. Overview of the CLEF-2022 CheckThat! lab task 1 on identifying relevant claims in tweets (2022).\n23. Nakov, P. et al. Overview of the CLEF-2022 CheckThat! lab task 2 on detecting previously fact-checked claims (2022).\n\n24. Köhler, J. et al. Overview of the CLEF-2022 CheckThat! lab task 3 on fake news detection. Working Notes of CLEF (2022).\n25. Thorne, J., Vlachos, A., Cocarascu, O., Christodoulopoulos, C. \\& Mittal, A. The Fact Extraction and VERification (FEVER) Shared Task in Proceedings of the First Workshop on Fact Extraction and VERification (FEVER) (Association for Computational Linguistics, Brussels, Belgium, 2018), 1-9.\n26. Das, A., Liu, H., Kovatchev, V. \\& Lease, M. The state of human-centered NLP technology for factchecking. Information processing & management 60, 103219 (2023).\n27. Hassan, N. et al. Claimbuster: The first-ever end-to-end fact-checking system. Proceedings of the VLDB Endowment 10, 1945-1948 (2017).\n28. Zeng, X., Abumansour, A. S. \\& Zubiaga, A. Automated fact-checking: A survey. Language and Linguistics Compass 15, e12438 (2021).\n29. Kenton, J. D. M.-W. C. \\& Toutanova, L. K. Bert: Pre-training of deep bidirectional transformers for language understanding in Proceedings of naacL-HLT 1 (2019), 2.\n30. Liu, Y. et al. Roberta: A robustly optimized bert pretraining approach. ArXiv [Preprint]. Available at: https://arxiv.org/abs/1907.11692 Accessed November 16, 2023 (2019).\n31. He, P., Liu, X., Gao, J. \\& Chen, W. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654 (2020).\n32. Cuartielles Saura, R., Ramon Vegas, X. \\& Pont Sorribes, C. Retraining fact-checkers: the emergence of ChatGPT in information verification. UPF Digital Repository (2023).\n33. Rashkin, H., Choi, E., Jang, J. Y., Volkova, S. \\& Choi, Y. Truth of varying shades: Analyzing language in fake news and political fact-checking in Proceedings of the 2017 conference on empirical methods in natural language processing (2017), 2931-2937.\n34. Guo, Z., Schlichtkrull, M. \\& Vlachos, A. A Survey on Automated Fact-Checking. Transactions of the Association for Computational Linguistics 10, 178-206 (2021).\n35. Thorne, J., Vlachos, A., Christodoulopoulos, C. \\& Mittal, A. FEVER: a Large-scale Dataset for Fact Extraction and VERification in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (Association for Computational Linguistics, New Orleans, Louisiana, 2018), 809-819.\n36. Wadden, D. et al. Fact or Fiction: Verifying Scientific Claims in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (Association for Computational Linguistics, Online, 2020), 7534-7550.\n37. Augenstein, I. et al. MultiFC: A real-world multi-domain dataset for evidence-based fact checking of claims. ArXiv [Preprint]. Available at: https://arxiv.org/abs/1909.03242 Accessed November 16, 2023 (2019).\n38. Weikum, G., Dong, L., Razniewski, S. \\& Suchanek, F. M. Machine Knowledge: Creation and Curation of Comprehensive Knowledge Bases. Found. Trends Databases 10, 108-490 (2020).\n39. Vaswani, A. et al. Attention is all you need. Advances in neural information processing systems 30 (2017).\n40. Buck, C., Heafield, K. \\& Van Ooyen, B. N-gram Counts and Language Models from the Common Crawl. in LREC 2 (2014), 4.\n41. Brown, T. et al. Language models are few-shot learners. Advances in neural information processing systems 33, 1877-1901 (2020).\n42. Robertson, S., Zaragoza, H., et al. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information Retrieval 3, 333-389 (2009).\n43. Yao, S. et al. ReAct: Synergizing Reasoning and Acting in Language Models. ArXiv [Preprint]. Available at: https://arxiv.org/abs/2210.03629 Accessed November 16, 2023 (2023).\n## 44. Chase, H. LangChain Oct. 2022. \\url\\{https://github.com/hwchase17/langchain\\}.\n45. OpenAI. Models - OpenAI API Accessed: 2024-01-02. 2023. https://platform.openai.com/docs/ models.\n46. Espejel, J. L., Ettifouri, E. H., Alassan, M. S. Y., Chouham, E. M. \\& Dahhane, W. GPT-3.5 vs GPT-4: Evaluating ChatGPT's Reasoning Performance in Zero-shot Learning. ArXiv [Preprint]. Available at: https://arxiv.org/abs/2305.12477 Accessed November 16, 2023 (2023).\n## 47. Misra, R. Politifact Fact Check Dataset 2022.\n\n48. Bang, Y. et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. ArXiv [Preprint]. Available at: https://arXiv:2302.04023 Accessed November 16, 2023 (2023).\n49. Jiao, W., Wang, W., Huang, J., Wang, X. \\& Tu, Z. Is ChatGPT a good translator? Yes with GPT-4 as the engine. ArXiv [Preprint]. Available at: https://arXiv:2301.08745 Accessed November 16, 2023 (2023).\n50. Zhu, W. et al. Multilingual machine translation with large language models: Empirical results and analysis. ArXiv [Preprint]. Available at: https://arXiv:2304.04675 Accessed November 16, 2023 (2023).\n51. Shaar, S., Babulkov, N., Da San Martino, G. \\& Nakov, P. That is a Known Lie: Detecting Previously Fact-Checked Claims in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (eds Jurafsky, D., Chai, J., Schluter, N. \\& Tetreault, J.) (Association for Computational Linguistics, Online, July 2020), 3607-3618. https://aclanthology.org/2020.acl-main. 332.\n52. Micallef, N. et al. Cross-platform multimodal misinformation: Taxonomy, characteristics and detection for textual posts and videos in Proceedings of the International AAAI Conference on Web and Social Media 16 (2022), 651-662.\n53. Kazemi, A, Li, Z, Peréz-Rosas, V, Hale, S. \\& Mihalcea, R. Matching Tweets with Applicable Fact-Checks across Languages in CEUR Workshop Proceedings (2022).\n54. Quelle, D., Cheng, C., Bovet, A. \\& Hale, S. A. Lost in Translation-Multilingual Misinformation and its Evolution. arXiv preprint arXiv:2310.18089 (2023).\n55. Common Crawl Foundation. Statistics of Common Crawl Monthly Archives - Distribution of Languages Retrieved: 2024-02-01. 2023. \\url\\{https://commoncrawl.github.io/cc-crawl-statistics/plots/ languages.html\\}."
    }
  ],
  "pdf_images": {
    "pdf_path": "inbox/2310.13549v2.pdf",
    "total_pages": 17,
    "pages": [
      {
        "page_number": 1,
        "filename": "page-001.png",
        "thumb_filename": "page-001-thumb.png",
        "mobile_filename": "page-001-mobile.png",
        "mobile_thumb_filename": "page-001-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 2,
        "filename": "page-002.png",
        "thumb_filename": "page-002-thumb.png",
        "mobile_filename": "page-002-mobile.png",
        "mobile_thumb_filename": "page-002-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 3,
        "filename": "page-003.png",
        "thumb_filename": "page-003-thumb.png",
        "mobile_filename": "page-003-mobile.png",
        "mobile_thumb_filename": "page-003-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 4,
        "filename": "page-004.png",
        "thumb_filename": "page-004-thumb.png",
        "mobile_filename": "page-004-mobile.png",
        "mobile_thumb_filename": "page-004-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 5,
        "filename": "page-005.png",
        "thumb_filename": "page-005-thumb.png",
        "mobile_filename": "page-005-mobile.png",
        "mobile_thumb_filename": "page-005-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 6,
        "filename": "page-006.png",
        "thumb_filename": "page-006-thumb.png",
        "mobile_filename": "page-006-mobile.png",
        "mobile_thumb_filename": "page-006-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 7,
        "filename": "page-007.png",
        "thumb_filename": "page-007-thumb.png",
        "mobile_filename": "page-007-mobile.png",
        "mobile_thumb_filename": "page-007-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 8,
        "filename": "page-008.png",
        "thumb_filename": "page-008-thumb.png",
        "mobile_filename": "page-008-mobile.png",
        "mobile_thumb_filename": "page-008-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 9,
        "filename": "page-009.png",
        "thumb_filename": "page-009-thumb.png",
        "mobile_filename": "page-009-mobile.png",
        "mobile_thumb_filename": "page-009-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 10,
        "filename": "page-010.png",
        "thumb_filename": "page-010-thumb.png",
        "mobile_filename": "page-010-mobile.png",
        "mobile_thumb_filename": "page-010-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 11,
        "filename": "page-011.png",
        "thumb_filename": "page-011-thumb.png",
        "mobile_filename": "page-011-mobile.png",
        "mobile_thumb_filename": "page-011-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 12,
        "filename": "page-012.png",
        "thumb_filename": "page-012-thumb.png",
        "mobile_filename": "page-012-mobile.png",
        "mobile_thumb_filename": "page-012-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 13,
        "filename": "page-013.png",
        "thumb_filename": "page-013-thumb.png",
        "mobile_filename": "page-013-mobile.png",
        "mobile_thumb_filename": "page-013-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 14,
        "filename": "page-014.png",
        "thumb_filename": "page-014-thumb.png",
        "mobile_filename": "page-014-mobile.png",
        "mobile_thumb_filename": "page-014-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 15,
        "filename": "page-015.png",
        "thumb_filename": "page-015-thumb.png",
        "mobile_filename": "page-015-mobile.png",
        "mobile_thumb_filename": "page-015-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 16,
        "filename": "page-016.png",
        "thumb_filename": "page-016-thumb.png",
        "mobile_filename": "page-016-mobile.png",
        "mobile_thumb_filename": "page-016-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 17,
        "filename": "page-017.png",
        "thumb_filename": "page-017-thumb.png",
        "mobile_filename": "page-017-mobile.png",
        "mobile_thumb_filename": "page-017-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      }
    ]
  },
  "metadata": {
    "model": "gpt-4o",
    "pdf_path": "inbox/2310.13549v2.pdf"
  },
  "attribution": {
    "type": "arxiv",
    "url": "https://arxiv.org/abs/2310.13549",
    "id": "2310.13549"
  }
}