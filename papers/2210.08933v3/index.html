<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DiffuSEQ: Sequence to Sequence Text Generation with Diffusion Models - RYO Analysis</title>
    <link rel="stylesheet" href="../../assets/style.css">
</head>
<body>
    <!-- Desktop Header -->
    <header class="desktop-header">
        <div class="header-left">
            <h1><a href="../../index.html">9030club</a> / DiffuSEQ: Sequence to Sequence Text Generation with Diffusion Models <a href="https://arxiv.org/abs/2210.08933" target="_blank" style="color: #666; text-decoration: none; font-size: 0.9rem;">(arXiv)</a></h1>
            <div class="filename-subtitle">2210.08933v3.pdf</div>
        </div>
        <div class="header-right">
            <div class="mode-toggle">
                <button class="mode-btn active" data-mode="markdown">markdown</button>
                <button class="mode-btn" data-mode="pdf">pdf</button>
                <button class="qr-btn" id="qr-btn" title="Show QR Code">
                    <img src="../../assets/qr-code-icon.png" alt="QR Code" style="width: 16px; height: 16px;">
                </button>
                <button class="collapse-btn" id="collapse-thumbnails" title="Collapse/Expand thumbnails">
                    <span class="collapse-icon">‚àí</span>
                </button>
            </div>
        </div>
    </header>
    
    <!-- Desktop 4-Column Layout -->
    <main class="four-column-layout">
        <section class="questions-column">
            <ul class="question-list">
                <li><button class="question-btn" data-question="1">1. What were they trying to do?</button></li>
<li><button class="question-btn" data-question="2">2. Why does it matter?</button></li>
<li><button class="question-btn" data-question="3">3. What did they try?</button></li>
<li><button class="question-btn" data-question="4">4. Core Analysis</button></li>
<li><button class="question-btn" data-question="5">5. Did it work?</button></li>
<li><button class="question-btn" data-question="6">6. What did they compare it to?</button></li>
<li><button class="question-btn" data-question="7">7. What was it tested on?</button></li>
<li><button class="question-btn" data-question="8">8. What's cool about it?</button></li>
<li><button class="question-btn" data-question="9">9. What's sketchy about it?</button></li>
<li><button class="question-btn" data-question="10">10. Can anyone use this?</button></li>
<li><button class="question-btn" data-question="11">11. What's still left to figure out?</button></li>

            </ul>
        </section>
        
        <section class="answers-column">
            <div class="answer-content">
                <p class="placeholder">Click a question to see the answer</p>
            </div>
        </section>
        
        <section class="page-column">
            <div class="page-content">
                <h3>DiffuSEQ: Sequence to Sequence Text Generation with Diffusion Models</h3>
<p><h1>DiffuSEQ: Sequence to Sequence Text Generation with Diffusion Models</h1></p><p>Shansan Gong ${ }^{1}$, Mukai $\mathbf{L i}^{1}$, Jiangtao Feng ${ }^{1}$, Zhiyong $\mathbf{W u}^{1}$, Lingpeng Kong ${ }^{2}$<br>${ }^{1}$ Shark-NLP, Shanghai AI Laboratory ${ }^{2}$ The University of Hong Kong<br>\{gongshansan, limukai,fengjiangtao,wuzhiyong\}@pjlab.org.cn<br>lpk@cs.hku.hk</p><p>#### Abstract</p><p>Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSEQ: a diffusion model designed for sequence-to-sequence (SEQ2SEQ) text generation tasks. Upon extensive evaluation over a wide range of SEQ2SEQ tasks, we find DiffuSEQ achieving comparable or even better performance than six established baselines, including a state-of-theart model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSEQ is its high diversity during generation, which is desired in many SEQ2SEQ tasks. We further include a theoretical analysis revealing the connection between DiffuSEQ and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks.</p><p><h2>## 1 INTRODUCTION</h2></p><p>Among existing generative models, GAN (Goodfellow et al., 2014) suffers from the instability issue (Salimans et al., 2016), subjecting to mode collapse (Metz et al., 2017); VAE (Kingma \& Welling, 2014) has to rely on surrogate objectives to approximate maximum likelihood training and Flow-based models (Dinh et al., 2017) has to use specialized architectures to construct reversible transform. Diffusion models (Ho et al., 2020; Nichol \& Dhariwal, 2021) have circumvented several of these limitations and emerged as a new paradigm for generative models, theoretically underpinned by non-equilibrium thermodynamics (Sohl-Dickstein et al., 2015) and score-matching network (Song \& Ermon, 2019). To date, the major breakthroughs are in domains using continuous signals, such as vision (Saharia et al., 2022a;b; Ramesh et al., 2022) and audio (Kong et al., 2020). However, extending continuous diffusion models to natural language remains an open challenge due to the inherently discrete nature of texts.</p><p>On the basis of unconditional generation in continuous space which is illustrated in Figure 1(a), existing efforts (Hoogeboom et al., 2021; Austin et al., 2021) start customizing diffusion models to text in discrete space on unconditional language modeling (i.e., free text generation). Diffusion-LM (Li et al., 2022), as in Figure 1(b), models texts in continuous space and proposes to use an extra-trained classifier as guidance (i.e., the condition signal $\mathbf{x}$ ) to impose subtle changes (usually complex, finegrained constraints) on generated sentences. Nonetheless, these models do not naturally generalize to conditional language modeling (i.e., the model assigns probabilities $p(\mathbf{w} \mid \mathbf{x})$ to sequences of words $\mathbf{w}$ given $\mathbf{x})$. In the more general sequence-to-sequence (SEQ2SEQ) setting where the condition $\mathbf{x}$ is also a sequence of words, applying Diffusion-LM can be difficult. The reason is that classifiers are attributes-oriented, and we can not train hundreds-of-thousands classifiers to model the semantic meaning between conditions and generated sentences.</p><p>SEQ2SEQ is an essential setting in NLP that covers a wide range of important tasks such as openended sentence generation, dialogue, paraphrasing, and text style transfer. In this paper, we propose</p><p>[^0]
[^0]:    ${ }^{1}$ Code is available at https://github.com/Shark-NLP/DiffuSeq</p><p>![img-0.jpeg](img-0.jpeg)</p><p>Figure 1: The demonstration of unconditional, classifier-guided, and classifier-free diffusion models.</p><p>DIFFUSEQ, depicted in Figure 1(c), a classifier-free diffusion model that supports SEQ2SEQ text generation tasks. By modeling the conditional probability of the target sentence $\mathbf{w}$ given context $\mathbf{x}$ using one single model, one advantage of DIFFUSEQ is that this paradigm allows a complete model to fit data distribution and utilize conditional guidance, rather than depending on a separate classifier.</p><p>Different from canonical generation approaches in an autoregressive (AR) left-to-right manner (Radford et al., 2019), DIFFUSEQ generates text tokens parallelly in the non-autoregressive (NAR) way. To corroborate the effectiveness of our DIFFUSEQ, we conduct experiments on four SEQ2SEQ tasks. Compared to AR and NAR models, which suffer from the "degeneration" problem (Holtzman et al., 2019) and rely on decoding strategies, DIFFUSEQ can achieve considerable sentence-level diversity without sacrificing the quality (see ¬ß 4.2).</p><p>To sum up, we make a series of technical and conceptual contributions: (a) we are the first to deploy the diffusion model on SEQ2SEQ text generation, and our proposed DIFFUSEQ as a conditional language model is trained end-to-end in a classifier-free manner; (b) we establish a theoretical connection among AR, NAR and DIFFUSEQ models, and justify DIFFUSEQ as an extension of iterative-NAR models; (c) with strong empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks.</p>
            </div>
        </section>
        
        <section class="thumbnails-column" id="thumbnails-column">
            <h3>Thumbnails</h3>
            <div class="thumbnail-content" id="thumbnail-content">
                <ul class="thumbnail-list">
                    <li><button class="thumbnail-btn" data-page="1">1. DiffuSEQ: Sequence to Sequence Text Generation with Diffusion Models</button></li>
<li><button class="thumbnail-btn" data-page="2">2. 2 Preliminary and Problem Statement</button></li>
<li><button class="thumbnail-btn" data-page="3">3. 5 Related Work</button></li>
<li><button class="thumbnail-btn" data-page="4">4. A Objective Derivations Of DiffuSEQ</button></li>
<li><button class="thumbnail-btn" data-page="5">5. B Graphical Models of AR, Fully NAR, iterative NAR and DIFFUSEQ MODELS</button></li>
<li><button class="thumbnail-btn" data-page="6">6. C From DiffuSeq to Iterative NAR and Diffusion Models</button></li>
<li><button class="thumbnail-btn" data-page="7">7. D. 4 Metrics</button></li>

                </ul>
            </div>
        </section>
    </main>
    
    <!-- Mobile Layout -->
    <div class="mobile-layout">
        <!-- Mobile Header -->
        <header class="mobile-header">
            <div class="mobile-title">
                <h1><a href="../../index.html">9030club</a></h1>
                <div class="mobile-paper-title">DiffuSEQ: Sequence to Sequence Text Generation with Diffusion Models <a href="https://arxiv.org/abs/2210.08933" target="_blank" style="color: #666; text-decoration: none; font-size: 0.9rem;">(arXiv)</a></div>
                <div class="mobile-filename">2210.08933v3.pdf</div>
            </div>
            <button class="mobile-qr-btn" id="mobile-qr-btn" title="Show QR Code">
                <img src="../../assets/qr-code-icon.png" alt="QR Code" style="width: 20px; height: 20px;">
            </button>
        </header>
        
        <!-- Mobile Navigation Container -->
        <div class="mobile-nav-container">
            <!-- Mobile Tab Navigation -->
            <div class="mobile-tabs">
                <button class="mobile-tab-btn active" data-tab="qa">QA</button>
                <button class="mobile-tab-btn" data-tab="pdf">PDF</button>
                <button class="mobile-tab-btn" data-tab="markdown">Markdown</button>
            </div>
            
            <!-- Mobile Navigation Bar -->
            <div class="mobile-nav" id="mobile-nav">
                <button class="mobile-nav-btn" id="mobile-prev-btn">‚Äπ</button>
                <div class="mobile-nav-info" id="mobile-nav-info">Q1 of 11</div>
                <button class="mobile-nav-btn" id="mobile-next-btn">‚Ä∫</button>
            </div>
        </div>
        
        <!-- Mobile Content Area -->
        <div class="mobile-content" id="mobile-content">
            <div class="mobile-content-inner">
                <p class="mobile-placeholder">Loading content...</p>
            </div>
        </div>
    </div>
    
    <!-- QR Code Overlay -->
    <div class="qr-overlay" id="qr-overlay">
        <img src="qr-code.png" alt="QR Code" id="qr-image">
    </div>
    
    <script src="../../assets/app.js"></script>
    <script>
        // Initialize with paper data
        window.paperData = {"title": "DiffuSEQ: Sequence to Sequence Text Generation with Diffusion Models", "slug": "2210.08933v3", "questions": {"1": {"question": "What were they trying to do?", "answer": "The main goal of the paper is to adapt diffusion models, which have been successful in continuous domains like vision and audio, to the discrete domain of natural language for sequence-to-sequence (SEQ2SEQ) text generation tasks. The authors propose DiffuSEQ, a diffusion model designed to handle the challenges of conditional text generation by modeling the conditional probability of target sentences given a context. The objective is to achieve high-quality and diverse text generation without relying on separate classifiers, demonstrating the potential of diffusion models in complex conditional language generation tasks.", "timestamp": "2025-09-01 17:26:35"}, "2": {"question": "Why does it matter?", "answer": "The paper \"DiffuSEQ: Sequence to Sequence Text Generation with Diffusion Models\" matters because it introduces a novel approach to sequence-to-sequence (SEQ2SEQ) text generation using diffusion models, which have previously been successful in continuous domains like vision and audio but underexplored in text due to its discrete nature. This is important because it offers a new method that achieves comparable or superior performance to existing models, including state-of-the-art pre-trained language models, while also providing high diversity in generated text. This matters to researchers and practitioners in natural language processing (NLP) as it expands the toolkit for text generation tasks, potentially leading to more diverse and high-quality text outputs in applications like dialogue systems, paraphrasing, and text simplification. Ultimately, this contributes to the broader field of AI by enhancing the capabilities and flexibility of generative models in handling complex language tasks.", "timestamp": "2025-09-01 17:26:41"}, "3": {"question": "What did they try?", "answer": "The authors of the paper proposed DiffuSEQ, a diffusion model specifically designed for sequence-to-sequence (SEQ2SEQ) text generation tasks. Their approach involves extending vanilla diffusion models to handle conditional text generation by modeling the conditional probability of a target sentence given a context. They introduced a classifier-free diffusion model that generates text tokens in a non-autoregressive manner, allowing for high diversity in generated sequences without sacrificing quality. The method employs a forward process with partial noising and a reverse process with conditional denoising, using a transformer architecture to model semantic relations between input and output sequences.", "timestamp": "2025-09-01 17:26:46"}, "4": {"question": "Core Analysis", "answer": "## üéØ Core Contribution\n\nThe core contribution of DiffuSEQ is the introduction of a classifier-free diffusion model specifically tailored for sequence-to-sequence (SEQ2SEQ) text generation tasks. This model overcomes the traditional obstacles faced by diffusion models in text applications, particularly in maintaining conditional informativeness without relying on separate classifiers. The approach involves embedding sequences into continuous vector spaces and applying partial Gaussian noise only to the target sequences. In this manner, the model preserves the context provided by the source sequences, allowing DiffuSEQ to restore original texts via reverse denoising steps. This innovative technique supports the generation of highly diverse textual outputs while ensuring high quality, positioning DiffuSEQ as a significant advance in NLP systems built on diffusion models.\n\n## üìä Method Breakdown\n\nThe methodology of DiffuSEQ unfolds in distinct steps, starting with transforming both source and target sequences into a unified continuous vector space through an embedding function. Once transformed, the forward diffusion process involves adding Gaussian noise only to the vectors corresponding to the target sequence, enabling the model to retain context from the source sequence without alteration. The subsequent reverse diffusion process then leverages these continuous representations to denoise and regenerate the target text while maintaining high fidelity to the conditioned sequence. A transformer architecture is employed to effectively model the non-linear interactions between the sequences, leveraging transformer attention mechanisms to facilitate contextually-aware denoising that aligns well with the original semantic meaning of the input text. In practical terms, DiffuSEQ employs a Minimum Bayes Risk (MBR) decoding strategy, enhancing the quality and diversity of outputs by evaluating multiple candidate sequences and selecting those with optimal expected outcomes.\n\n## üîß Subsystems/Parts\n\n1. Embedding Layer: Converts discrete text sequences into continuous vector spaces for diffusion processing. 2. Partial Noising Module: Applies Gaussian noise to target sequence embeddings, enabling context preservation in conditional generation. 3. Reverse Denoising Module: Recovers original target text from noised embeddings using iterative denoising techniques. 4. Transformer Network: Models semantic relations between source and target, ensuring contextual accuracy in generated outputs. 5. Decoding Strategy (MBR): Enhances output quality and diversity by selecting optimal sequences from generated candidates.\n\n## üîó Interactions\n\nThese subsystems are intricately linked through the forward and reverse diffusion processes. Initially, the embedding layer ensures that both source and target sequences are in a compatible format for subsequent operations. The partial noising module selectively targets the embeddings of the target sequence, leaving the source intact to retain full context. As the denoising module restores the text, it coordinates with the transformer network, which imposes linguistic and contextual coherency through attention mechanisms. The entire system is further optimized during the final decoding stage, where the MBR strategy is applied to select output sequences that best match pre-defined quality criteria, thus integrating the model's inherent capacity for diversity with the need for textual precision.\n\n## ‚ö° Delta vs Baseline\n\nDiffuSEQ fundamentally diverges from previous text-generating diffusion models by operating without external classifier constraints, thus removing the dependency on multiple pre-trained classifiers to ensure correct semantics in conditional generation tasks. By introducing partial noising‚Äîan innovative approach where only the target text is noised‚ÄîDiffuSEQ can emphasize the strong contextual ties of SEQ2SEQ tasks, which was a challenging endeavor using conventional autoregressive or non-autoregressive models that rely heavily on sequential predictions based on preceding words. Additionally, integration with MBR in selecting diverse outputs is notably different from standard diffusion methods.\n\n## üî¨ Evidence Anchor\n\nIn experiments, DiffuSEQ demonstrates its ability to achieve competitive or superior BLEU scores compared to traditional autoregressive and non-autoregressive models. Specifically, by increasing the candidate size in the MBR approach, the model showed enhanced performance beyond pretrained benchmarks, attesting to its capability for producing quality outputs with rich diversity. These results substantiate the theoretical and practical advantages claimed by adopting diffusion frameworks in SEQ2SEQ tasks.\n\n## üåç Transferability\n\nThe DiffuSEQ model, while specifically designed for SEQ2SEQ text generation, exemplifies a methodological approach that holds potential transferability across contexts where similar conditional generation patterns need implementing without reliance on external classifiers. However, integrating similar diffusion model strategies would necessitate suitable accommodations in domains other than text, especially those not innately reliant on sequence processing or differing inherently in structure from natural language.\n\n", "type": "core_analysis"}, "5": {"question": "Did it work?", "answer": "Yes, the idea worked. The DiffuSEQ model demonstrated comparable or superior performance in sequence-to-sequence (SEQ2SEQ) text generation tasks compared to six established baselines, including state-of-the-art models based on pre-trained language models. It achieved higher generation quality and diversity, outperforming autoregressive and non-autoregressive models in various tasks, such as open domain dialogue, question generation, text simplification, and paraphrasing. The results indicate that DiffuSEQ effectively balances quality and diversity in text generation.", "timestamp": "2025-09-01 17:26:51"}, "6": {"question": "What did they compare it to?", "answer": "In the paper, DiffuSEQ was compared to six established baselines, including autoregressive (AR) and non-autoregressive (NAR) models, as well as large pre-trained language models like GPT2. The baselines used for comparison included GRU with attention, Transformer, GPT2 (both base and large variants), GPVAE-T5, and NAR-LevT. The evaluation showed that DiffuSEQ achieved comparable or better performance in terms of quality and diversity across various sequence-to-sequence tasks, demonstrating its effectiveness over these prior models.", "timestamp": "2025-09-01 17:26:57"}, "7": {"question": "What was it tested on?", "answer": "The paper tested DiffuSEQ on four sequence-to-sequence (SEQ2SEQ) text generation tasks: open domain dialogue, question generation, text simplification, and paraphrasing. The evaluation setup involved comparing DiffuSEQ against six strong autoregressive (AR) and non-autoregressive (NAR) baselines, including models like GRU with attention, Transformer, GPT2, GPVAE, and LevT. The evaluation metrics used were BLEU, ROUGE, BERTScore for quality, and distinct unigram, self-BLEU, and diverse 4-gram for diversity. The experiments demonstrated that DiffuSEQ achieved comparable or better performance in terms of both quality and diversity compared to the baselines.", "timestamp": "2025-09-01 17:27:03"}, "8": {"question": "What's cool about it?", "answer": "DiffuSEQ introduces a novel approach by applying diffusion models to sequence-to-sequence (SEQ2SEQ) text generation, which is traditionally challenging due to the discrete nature of text. The model is innovative in its classifier-free design, allowing it to model the conditional probability of target sentences given context without relying on separate classifiers. This approach not only achieves comparable or superior performance to existing models but also enhances diversity in generated text, addressing common issues like degeneration in autoregressive models. The theoretical connection established between DiffuSEQ and autoregressive/non-autoregressive models further underscores its potential in complex language generation tasks.", "timestamp": "2025-09-01 17:27:07"}, "9": {"question": "What's sketchy about it?", "answer": "The paper presents several potential concerns and limitations. Firstly, the DiffuSEQ model, while innovative, is relatively new and may not have been extensively tested across diverse datasets and real-world applications, which could limit its generalizability. Additionally, the paper acknowledges that DiffuSEQ's token-level diversity still falls behind pre-trained models like GPT2, indicating room for improvement in generating varied word choices. The paper also notes the slow sampling speed of diffusion models as a major concern, which could hinder practical deployment. Lastly, the reliance on complex mathematical formulations and theoretical connections may pose challenges in understanding and implementing the model for practitioners not deeply familiar with diffusion models.", "timestamp": "2025-09-01 17:27:12"}, "10": {"question": "Can anyone use this?", "answer": "DiffuSEQ, as described in the paper, is a diffusion model designed for sequence-to-sequence text generation tasks. While it shows promising results in terms of quality and diversity of text generation, its practicality and accessibility may be limited. The model requires significant computational resources for training and inference, as indicated by the use of NVIDIA A100 GPUs. Additionally, the complexity of diffusion models and the need for specialized knowledge in machine learning and natural language processing may restrict its use to researchers and practitioners with expertise in these areas. Thus, it may not be easily accessible to the general public or those without technical expertise.", "timestamp": "2025-09-01 17:27:17"}, "11": {"question": "What's still left to figure out?", "answer": "The paper identifies several areas that remain unsolved or unclear, opening up new questions and future directions. One key challenge is improving the token-level choice diversity in diffusion models, as DiffuSEQ still lags behind pre-trained models like GPT2 in this aspect. Additionally, the paper suggests exploring pre-training techniques to enhance the model's performance further. The study also highlights the potential for integrating classifier-guided constraints to control text generation more effectively, which could be a promising area for future research.", "timestamp": "2025-09-01 17:27:22"}}, "core_analysis": {"input_data": {"title": "DiffuSEQ: Sequence to Sequence Text Generation with Diffusion Models", "content": "# 2210.08933v3\n\n*Generated from PDF: 2210.08933v3.pdf*\n\n---\n\n# DiffuSEQ: Sequence to Sequence Text Generation with Diffusion Models\n\nShansan Gong ${ }^{1}$, Mukai $\\mathbf{L i}^{1}$, Jiangtao Feng ${ }^{1}$, Zhiyong $\\mathbf{W u}^{1}$, Lingpeng Kong ${ }^{2}$<br>${ }^{1}$ Shark-NLP, Shanghai AI Laboratory ${ }^{2}$ The University of Hong Kong<br>\\{gongshansan, limukai,fengjiangtao,wuzhiyong\\}@pjlab.org.cn<br>lpk@cs.hku.hk\n\n#### Abstract\n\nRecently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSEQ: a diffusion model designed for sequence-to-sequence (SEQ2SEQ) text generation tasks. Upon extensive evaluation over a wide range of SEQ2SEQ tasks, we find DiffuSEQ achieving comparable or even better performance than six established baselines, including a state-of-theart model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSEQ is its high diversity during generation, which is desired in many SEQ2SEQ tasks. We further include a theoretical analysis revealing the connection between DiffuSEQ and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks.\n\n## ## 1 INTRODUCTION\n\nAmong existing generative models, GAN (Goodfellow et al., 2014) suffers from the instability issue (Salimans et al., 2016), subjecting to mode collapse (Metz et al., 2017); VAE (Kingma \\& Welling, 2014) has to rely on surrogate objectives to approximate maximum likelihood training and Flow-based models (Dinh et al., 2017) has to use specialized architectures to construct reversible transform. Diffusion models (Ho et al., 2020; Nichol \\& Dhariwal, 2021) have circumvented several of these limitations and emerged as a new paradigm for generative models, theoretically underpinned by non-equilibrium thermodynamics (Sohl-Dickstein et al., 2015) and score-matching network (Song \\& Ermon, 2019). To date, the major breakthroughs are in domains using continuous signals, such as vision (Saharia et al., 2022a;b; Ramesh et al., 2022) and audio (Kong et al., 2020). However, extending continuous diffusion models to natural language remains an open challenge due to the inherently discrete nature of texts.\n\nOn the basis of unconditional generation in continuous space which is illustrated in Figure 1(a), existing efforts (Hoogeboom et al., 2021; Austin et al., 2021) start customizing diffusion models to text in discrete space on unconditional language modeling (i.e., free text generation). Diffusion-LM (Li et al., 2022), as in Figure 1(b), models texts in continuous space and proposes to use an extra-trained classifier as guidance (i.e., the condition signal $\\mathbf{x}$ ) to impose subtle changes (usually complex, finegrained constraints) on generated sentences. Nonetheless, these models do not naturally generalize to conditional language modeling (i.e., the model assigns probabilities $p(\\mathbf{w} \\mid \\mathbf{x})$ to sequences of words $\\mathbf{w}$ given $\\mathbf{x})$. In the more general sequence-to-sequence (SEQ2SEQ) setting where the condition $\\mathbf{x}$ is also a sequence of words, applying Diffusion-LM can be difficult. The reason is that classifiers are attributes-oriented, and we can not train hundreds-of-thousands classifiers to model the semantic meaning between conditions and generated sentences.\n\nSEQ2SEQ is an essential setting in NLP that covers a wide range of important tasks such as openended sentence generation, dialogue, paraphrasing, and text style transfer. In this paper, we propose\n\n[^0]\n[^0]:    ${ }^{1}$ Code is available at https://github.com/Shark-NLP/DiffuSeq\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: The demonstration of unconditional, classifier-guided, and classifier-free diffusion models.\n\nDIFFUSEQ, depicted in Figure 1(c), a classifier-free diffusion model that supports SEQ2SEQ text generation tasks. By modeling the conditional probability of the target sentence $\\mathbf{w}$ given context $\\mathbf{x}$ using one single model, one advantage of DIFFUSEQ is that this paradigm allows a complete model to fit data distribution and utilize conditional guidance, rather than depending on a separate classifier.\n\nDifferent from canonical generation approaches in an autoregressive (AR) left-to-right manner (Radford et al., 2019), DIFFUSEQ generates text tokens parallelly in the non-autoregressive (NAR) way. To corroborate the effectiveness of our DIFFUSEQ, we conduct experiments on four SEQ2SEQ tasks. Compared to AR and NAR models, which suffer from the \"degeneration\" problem (Holtzman et al., 2019) and rely on decoding strategies, DIFFUSEQ can achieve considerable sentence-level diversity without sacrificing the quality (see ¬ß 4.2).\n\nTo sum up, we make a series of technical and conceptual contributions: (a) we are the first to deploy the diffusion model on SEQ2SEQ text generation, and our proposed DIFFUSEQ as a conditional language model is trained end-to-end in a classifier-free manner; (b) we establish a theoretical connection among AR, NAR and DIFFUSEQ models, and justify DIFFUSEQ as an extension of iterative-NAR models; (c) with strong empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks.\n\n# 2 Preliminary and Problem Statement\n\nPreliminary. A diffusion model typically contains forward and reverse processes. Given a data point sampled from a real-world data distribution $\\mathbf{z}_{0} \\sim q(\\mathbf{z})$, the forward process gradually corrupts $\\mathbf{z}_{0}$ into a standard Gaussian noise $\\mathbf{z}_{T} \\sim \\mathcal{N}(0, \\mathbf{I})$. For each forward step $t \\in[1,2, \\ldots, T]$, the perturbation is controlled by $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t} ; \\sqrt{1-\\beta_{t}} \\mathbf{z}_{t-1}, \\beta_{t} \\mathbf{I}\\right)$, with $\\beta_{t} \\in(0,1)$ as different variance scales. Once the forward process is completed, the reverse denoising process tries to gradually reconstruct the original data $\\mathbf{z}_{0}$ via sampling from $\\mathbf{z}_{T}$ by learning a diffusion model $f_{\\theta}$.\n\nProblem Statement. Many recent efforts have been devoted to adapting diffusion models to discrete texts (See ¬ß 5). However, they all focus on unconditional sequence modeling. In this paper, we target the sequence-to-sequence text generation tasks. In particular, given a $m$-length source sequence $\\mathbf{w}^{s}=\\left\\{w_{1}^{s}, \\ldots, w_{m}^{s}\\right\\}$, we aim to learn a diffusion model that can produce a $n$-length target sequence $\\mathbf{w}^{g}=\\left\\{w_{1}^{g}, \\ldots, w_{n}^{g}\\right\\}$ conditioning on the source sequence.\n\n## ## 3 DIFFUSEQ\n\nWe propose DIFFUSEQ to extend vanilla diffusion models to learn conditional text generation (as shown in Figure 2), concerning the model architecture and the training objective.\n\nForward Process with Partial Noising. In the beginning of forward process, we follow DiffusionLM (Li et al., 2022) to design an embedding function $\\operatorname{EMB}\\langle\\mathbf{w}\\rangle$ to map the discrete text $\\mathbf{w}$ into a continuous space. In particular, given a pair of sequence $\\mathbf{w}^{x}$ and $\\mathbf{w}^{g}$, DIFFUSEQ learns a unified\n\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: The diffusion process of our conditional diffusion language model DiffuSeq. Given the source $\\mathbf{w}^{x}$ and the target $\\mathbf{w}^{y}$, we pair-wisely transform them into continuous space $\\mathbf{z}_{0}$. The partial Gaussian noise is iteratively added on the target space of $\\mathbf{z}_{t}$.\nfeature space of $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$ by embedding transformation and concatenation as $\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right)=$ $\\left[\\operatorname{EMB}\\left(w_{1}^{x}\\right), \\ldots, \\operatorname{EMB}\\left(w_{m}^{x}\\right), \\operatorname{EMB}\\left(w_{1}^{y}\\right), \\ldots, \\operatorname{EMB}\\left(w_{n}^{y}\\right)\\right] \\in \\mathbb{R}^{(m+n) \\times d}$. The transformation allows us to adapt discrete textual input into the standard forward process, by extending the original forward chain to a new Markov transition $q_{\\phi}\\left(\\mathbf{z}_{0} \\mid \\mathbf{w}^{x \\oplus y}\\right)=\\mathcal{N}\\left(\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right), \\beta_{0} \\mathbf{I}\\right)$.\nWe denote $\\mathbf{z}_{t}=\\mathbf{x}_{t} \\oplus \\mathbf{y}_{t}$ to simplify the wordings, where $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ represent parts of $\\mathbf{z}_{t}$ that belong to $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$, respectively. For each forward step $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}\\right)$, we gradually inject noise into last step's hidden state $\\mathbf{z}_{t-1}$ to obtain $\\mathbf{z}_{t}$. Unlike conventional diffusion models that corrupt the whole $\\mathbf{z}_{t}$ (both $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ ) without distinction, we only impose noising on $\\mathbf{y}_{t}$. This modification (termed partial noising) allows us to adapt diffusion models for conditional language modeling.\n\nReverse Process with Conditional Denoising. The ultimate goal of the reverse process is to recover the original $\\mathbf{z}_{0}$ by denoising $\\mathbf{z}_{t}: p_{\\theta}\\left(\\mathbf{z}_{0: T}\\right):=p\\left(\\mathbf{z}_{T}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)$. We model the learning process $p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t-1} ; \\mu_{\\theta}\\left(\\mathbf{z}_{t}, t\\right), \\sigma_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right)$ using the proposed diffusion model DiffuSeq: $f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)$, where the $\\mu_{\\theta}(\\cdot)$ and $\\sigma_{\\theta}(\\cdot)$ is the parameterization of the predicted mean and standard deviation of $q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)$ in forward process, derived using Bayes' rule. The detailed derivations are in Appendix A. With the partial nosing strategy adopted in the forward process, we can impose the input as the condition when denoising as shown in Figure 1. The proposed conditional denoising is classifier-free by nature: we do not require extra-trained classifiers to control the denoising process.\nSpecifically, we use a transformer architecture to model $f_{\\theta}$, which spontaneously models the semantic relation between $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$. We compute the variational lower bound $\\left(\\mathcal{L}_{\\text {VLB }}\\right)$ following the original diffusion process. $\\mathcal{L}_{\\text {round }}$ corresponds to rounding operation in Figure 2.\n\n$$\n\\begin{aligned}\n\\mathcal{L}_{\\mathrm{VLB}} & =\\mathbb{E}_{q\\left(\\mathbf{z}_{1: T} \\mid \\mathbf{z}_{0}\\right)}[\\underbrace{\\log \\frac{q\\left(\\mathbf{z}_{T} \\mid \\mathbf{z}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{T}\\right)}}_{\\mathcal{L}_{T}}+\\sum_{t=2}^{T} \\underbrace{\\log \\frac{q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{0}, \\mathbf{z}_{t}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)}}_{\\mathcal{L}_{t-1}} \\\\\n& +\\underbrace{\\log \\frac{q_{\\phi}\\left(\\mathbf{z}_{0} \\mid \\mathbf{w}^{x \\oplus y}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{0} \\mid \\mathbf{z}_{1}\\right)}}_{\\mathcal{L}_{0}}-\\underbrace{\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)}_{\\mathcal{L}_{\\text {round }}}]\n\\end{aligned}\n$$\n\n## We further simplify the training objective as follows (details in Appendix A):\n\n$$\n\\begin{aligned}\n\\min _{\\theta} \\mathcal{L}_{\\mathrm{VLB}} & =\\min _{\\theta}\\left[\\sum_{t=2}^{T}\\left\\|\\mathbf{z}_{0}-f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right)-f_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}-\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)\\right] \\\\\n& \\rightarrow \\min _{\\theta}\\left[\\sum_{t=2}^{T}\\left\\|\\mathbf{y}_{0}-\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{y}\\right)-\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}+\\mathcal{R}\\left(\\left\\|\\mathbf{z}_{0}\\right\\|^{2}\\right)\\right]\n\\end{aligned}\n$$\n\nhere we use $\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)$ to denote the fractions of recovered $\\mathbf{z}_{0}$ corresponding to $\\mathbf{y}_{0}$. Note that although in the first term, we only compute the loss w.r.t $\\mathbf{y}_{0}$, due to the attention mechanism in the transformer, the reconstruction of $\\mathbf{y}_{0}$ also takes $\\mathbf{x}_{0}$ into account, thus the gradients from the first term\n\nwill also affect the learning of $\\mathbf{x}_{0}$. The mathematically equivalent regularization term $\\mathcal{R}\\left(\\left\\|\\mathbf{z}_{0}\\right\\|^{2}\\right)$ ) regularize the embedding learning. We further share the embedding function between source and target sequences, enabling the training of two different feature spaces jointly. This sets DiffuSeq away from existing solutions in vision such as GLIDE (Nichol et al., 2022).\n\nTraining and Inference Methods. In our preliminary experiments, we find that the high diversity in NLP datasets and long diffusion steps often result in insufficient training. We hypothesize the reason is that sampling step $t$ uniformly causes unnecessary noise in the $\\mathcal{L}_{\\text {VLB }}$ objective. We hence employ importance sampling (Nichol \\& Dhariwal, 2021) to address this problem.\n\n$$\n\\mathcal{L}_{\\mathrm{VLB}}=\\mathbb{E}_{t \\sim p_{t}}\\left[\\frac{\\mathcal{L}_{t}}{p_{t}}\\right], p_{t} \\propto \\sqrt{\\mathbb{E}\\left[\\mathcal{L}_{t}^{2}\\right]}, \\sum_{t=0}^{T-1} p_{t}=1\n$$\n\nIntuitively, the importance-weighted sampling algorithm will spend more steps on diffusion steps with larger $\\mathcal{L}_{t}$, and vice versa.\nTo conduct SEQ2SEQ generation given the condition $\\operatorname{EMB}\\left(\\mathbf{w}^{x}\\right)$, we randomly sample $\\mathbf{y}_{T} \\sim$ $\\mathcal{N}(0, I)$ and concatenate $\\mathbf{y}_{T}$ with $\\operatorname{EMB}\\left(\\mathbf{w}^{x}\\right)$ to obtain $\\mathbf{z}_{T}$. We can now repeat the reverse process until we arrive at $\\mathbf{z}_{0}$. At each sampling step, an anchoring function is executed towards reparameterized $\\mathbf{z}_{t}$. Specifically, the anchoring function: (a) operates rounding on $\\mathbf{z}_{t}$ to map it back to word embedding space following Li et al. (2022); (b) replaces the part of recovered $\\mathbf{z}_{t-1}$ that belongs to $\\mathbf{w}^{x}$ with the original $\\mathbf{x}_{0}$, considering that this part is recovered from corrupted $\\mathbf{z}_{t}$ via $f_{\\theta}$ and not strictly equals to $\\mathbf{x}_{0}$. Note that (b) is designed for DiffuSeq.\n\nTo improve the quality of generation, we apply the widely used Minimum Bayes Risk (MBR) decoding strategy (Koehn, 2004). We first generate a set of candidate samples $\\mathcal{S}$ from different random seeds of DiffuSeq and select the best output sequence that achieves the minimum expected risk under a meaningful loss function (e.g. BLEU or other cheaper metrics like precision). In practice, we use the negative BLEU score in our implementation.\n\nConnections to AR, Iter-NAR, and Fully-NAR Models. To better understand the behavior of DiffuSeq, we give the theoretical connection to autoregressive (AR), iterative non-autoregressive (iter-NAR), and fully non-autoregressive (fully-NAR) models. We argue that DiffuSeq can be seen as an extension of iter-NAR model. Detailed graphical learning discrepancies of these four cases are discussed in Appendix B for reference.\nAR models learn $p\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)$ by autoregressive decomposition based on left-context:\n\n$$\np_{\\mathrm{AR}}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\underbrace{p\\left(w_{1}^{y} \\mid \\mathbf{w}^{x}\\right)}_{\\text {initial prediction }} \\underbrace{\\prod_{i=1, \\ldots, n-1} p\\left(w_{i+1}^{y} \\mid \\mathbf{w}_{1: i}^{y}, \\mathbf{w}^{x}\\right)}_{\\text {progressive left-context prediction }},\n$$\n\nwhile fully-NAR models (Gu et al., 2018; Qian et al., 2021) learn the conditional probability given independent assumption for fast inference:\n\n$$\np_{\\text {fully-NAR }}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\prod_{i=1, \\ldots, n} p\\left(w_{i}^{y} \\mid \\mathbf{w}^{x}\\right)\n$$\n\nTo make a better analogy to AR and NAR models, we use a lossless way to formulate iterative NAR models (Gu et al., 2019; Ghazvininejad et al., 2019) by introducing a series of intermediate sequences $\\mathbf{w}_{1: K-1}^{y}, \\mathbf{w}_{K}^{y}=\\mathbf{w}^{y}$ with $K$ editable iterations:\n\n$$\np_{\\text {iter-NAR }}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} \\underbrace{\\prod_{i=1, . . n} p\\left(w_{1, i}^{y} \\mid \\mathbf{w}^{x}\\right)}_{\\text {initial prediction }} \\underbrace{\\prod_{k=1 . . K-1} \\prod_{i=1, . . n} p\\left(w_{k+1, i}^{y} \\mid \\mathbf{w}_{k, 1: n}^{y}, \\mathbf{w}^{x}\\right)}_{\\text {progressive full-context prediction }}\n$$\n\nPrevious study (Huang et al., 2022) shows that there is a gap called conditional total correlation between AR Eq. (4) and fully-NAR Eq. (5) learning paradigms, because of lossy decomposition of NAR models. However, when comparing iter-NAR Eq. (6) with AR Eq. (4) models, they both can be factorized into an initial prediction term and a progressive prediction process based on different context (i.e. left-context in AR and full-context in iter-NAR), and the discrepancy pointed out by\n\nHuang et al. (2022) is therefore closed in iter-NAR assuming sufficient steps. By showing DIFFUSEQ is an extension of the iter-NAR model, we offer a justification that it will not suffer from the conditional total correlation for the same reason.\n\nA straight-forward way to formulate pure continuous diffusion models is to introduce a series of Gaussian noise-corrupted features along with diffusion steps: $\\mathbf{y}_{1: T-1}, \\mathbf{y}_{0}=\\mathbf{y}, \\mathbf{y}_{T} \\sim \\mathcal{N}(0, \\mathbf{I})$.\n\n$$\np_{\\text {diffusion }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right)=\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)\n$$\n\nwhere $p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)$ describes the diffusion step on continuous representations $\\mathbf{y}$. The rounding operation in DIFFUSEQ maps the continuous vectors $\\mathbf{y}$ to discrete $\\mathbf{w}^{y}$ for each time step $t$, we in addition introduce this into Eq. (7):\n\n$$\n\\begin{aligned}\np_{\\text {DIFFUSEQ }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) & =\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) \\\\\n& =\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} p\\left(\\mathbf{y}_{t} \\mid \\mathbf{w}_{t+1}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\nBy rearranging Eq. (8) into Eq. (9), we can see DIFFUSEQ can be seen as a more generalized form of iter-NAR Eq. (6) before marginalizing out $\\left\\{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}\\right\\}$, despite the different initialization of $\\mathbf{y}_{T}{ }^{2}$. A more detailed derivation is shown in Appendix C.\n\n## # 4 EXPERIMENTS\n\nWe conduct experiments to validate the effectiveness of DIFFUSEQ on four different tasks, against six strong AR/NAR baselines.\n\n## ### 4.1 EXPERIMENTAL SETUP\n\nTasks and Datasets. SEQ2SEQ generation covers a wide range of tasks, among which we choose four typical and popular tasks. Open domain dialogue requires models to generate informative responses given a dialogue context. We use Commonsense Conversation Dataset (Zhou et al., 2018), which is extracted from Reddit single-round dialogs, with over 3 million conversational pairs. Question generation(QG) aims to generate questions given a context as input. To obtain sufficient training samples, we use the dataset Quasar-T (Dhingra et al., 2017) preprocessed by Lin et al. (2018), and then generate document-question pairs to obtain 119K training samples (details in Appendix D.1). Text simplification aims to revise the complex text into sequences with simplified grammar and word choice. Jiang et al. (2020) constructs a corpus consisting of 677 K complexsimple sentences with revision alignment. Paraphrase task generates an alternative surface form in the same language expressing the same semantic content. We adopt widely used QQP ${ }^{3}$ sourced from the community question answering forum Quora, with 147K positive pairs.\n\nBaselines. We consider three groups of models as baselines, covering both AR and NAR architectures. The first group of methods adopts encoder-decoder architecture (Cho et al., 2014) which is well-studied for SEQ2SEQ tasks, and we conduct experiments on two popular models: GRU with attention and Transformer (Vaswani et al., 2017). The second group is the finetuned large pre-trained language model (PLM), among which GPT2 (Radford et al., 2019) has demonstrated great success in almost all SEQ2SEQ tasks. We further compare to GPVAE (Du et al., 2022), which augments a pre-trained T5 (Raffel et al., 2020) with VAE to improve the generation diversity. For the last group of baselines, we consider LevT (Gu et al., 2019), a widely used, strong iterative NAR model. All baselines are trained following instructions in their papers, and details can be found in Appendix D.2.\n\n[^0]\n[^0]:    ${ }^{2}$ For NAR models, $\\mathbf{y}_{T}$ is uniform copied from the source sentence or $u n k$ 's token embedding (Gu et al., 2018); for diffusion models, $\\mathbf{y}_{T}$ is sampled from normal distribution $\\mathcal{N}(0, \\mathbf{I})$.\n${ }^{3}$ https://www.kaggle.com/c/quora-question-pairs\n\nTable 1: The overall results of different methods on different SEQ2SEQ tasks. The first group $\\diamond$ of methods adopt autoregressive encoder-decoder architecture and the second group $\\bullet$ is the finetuned large pre-trained language model (also in autoregressive manner) while the last group $\\ddagger$ is non-autoregressive. The best results are bold, and the best results without PLMs are underlined.\n\n| Tasks | Methods | BLEU $\\uparrow$ | R-L $\\uparrow$ | Score $\\uparrow$ | dist-1 $\\uparrow$ | selfB $\\downarrow / \\operatorname{div}-4 \\uparrow$ | Len |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Open <br> Domain <br> Dialogue | GRU-attention ${ }^{\\circ}$ | 0.0068 | 0.1054 | 0.4128 | 0.8998 | 0.8008/0.1824 | 4.46 |\n|  | Transformer-base ${ }^{\\circ}$ | 0.0189 | 0.1039 | 0.4781 | 0.7493 | 0.3698/0.6472 | 19.5 |\n|  | GPT2-base FT ${ }^{\\bullet}$ | 0.0108 | 0.1508 | 0.5279 | 0.9194 | 0.0182/0.9919 | 16.8 |\n|  | GPT2-large FT ${ }^{\\bullet}$ | 0.0125 | 0.1002 | 0.5293 | 0.9244 | 0.0213/0.9938 | 16.8 |\n|  | GPVAE-T5 ${ }^{\\bullet}$ | 0.0110 | 0.1009 | 0.4317 | 0.5625 | 0.3560/0.5551 | 20.1 |\n|  | NAR-LevT ${ }^{\\ddagger}$ | 0.0158 | 0.0550 | 0.4760 | 0.9726 | 0.7103/0.1416 | 4.11 |\n|  | DiffuSEQ (Ours) ${ }^{\\ddagger}$ | 0.0139 | 0.1056 | 0.5131 | 0.9467 | 0.0144/0.9971 | 13.6 |\n| Question <br> Generation | GRU-attention ${ }^{\\circ}$ | 0.0651 | 0.2617 | 0.5222 | 0.7930 | 0.9999/0.3178 | 10.1 |\n|  | Transformer-base ${ }^{\\circ}$ | 0.1663 | 0.3441 | 0.6307 | 0.9309 | 0.3265/0.7720 | 10.3 |\n|  | GPT2-base FT ${ }^{\\bullet}$ | 0.0741 | 0.2714 | 0.6052 | 0.9602 | 0.1403/0.9216 | 10.0 |\n|  | GPT2-large FT ${ }^{\\bullet}$ | 0.1110 | 0.3215 | 0.6346 | 0.9670 | 0.2910/0.8062 | 9.96 |\n|  | GPVAE-T5 ${ }^{\\bullet}$ | 0.1251 | 0.3390 | 0.6308 | 0.9381 | 0.3567/0.7282 | 11.4 |\n|  | NAR-LevT ${ }^{\\ddagger}$ | 0.0930 | 0.2893 | 0.5491 | 0.8914 | 0.9830/0.4776 | 6.93 |\n|  | DiffuSEQ (Ours) ${ }^{\\ddagger}$ | 0.1731 | 0.3665 | 0.6123 | 0.9056 | 0.2789/0.8103 | 11.5 |\n| Text <br> Simpli- <br> fication | GRU-attention ${ }^{\\circ}$ | 0.3256 | 0.5602 | 0.7871 | 0.8883 | 0.9998/0.3313 | 18.9 |\n|  | Transformer-base ${ }^{\\circ}$ | 0.2693 | 0.4907 | 0.7381 | 0.8886 | 0.6924/0.5095 | 18.5 |\n|  | GPT2-base FT ${ }^{\\bullet}$ | 0.3083 | 0.5461 | 0.8021 | 0.9439 | 0.5444/0.6047 | 16.1 |\n|  | GPT2-large FT ${ }^{\\bullet}$ | 0.2693 | 0.5111 | 0.7882 | 0.9464 | 0.6042/0.5876 | 15.4 |\n|  | GPVAE-T5 ${ }^{\\bullet}$ | 0.3392 | 0.5828 | 0.8166 | 0.9308 | 0.8147/0.4355 | 18.5 |\n|  | NAR-LevT ${ }^{\\ddagger}$ | 0.2052 | 0.4402 | 0.7254 | 0.9715 | 0.9907/0.3271 | 8.31 |\n|  | DiffuSEQ (Ours) ${ }^{\\ddagger}$ | 0.3622 | 0.5849 | 0.8126 | 0.9264 | 0.4642/0.6604 | 17.7 |\n| Paraphrase | GRU-attention ${ }^{\\circ}$ | 0.1894 | 0.5129 | 0.7763 | 0.9423 | 0.9958/0.3287 | 8.30 |\n|  | Transformer-base ${ }^{\\circ}$ | 0.2722 | 0.5748 | 0.8381 | 0.9748 | 0.4483/0.7345 | 11.2 |\n|  | GPT2-base FT ${ }^{\\bullet}$ | 0.1980 | 0.5212 | 0.8246 | 0.9798 | 0.5480/0.6245 | 9.67 |\n|  | GPT2-large FT ${ }^{\\bullet}$ | 0.2059 | 0.5415 | 0.8363 | 0.9819 | 0.7325/0.5020 | 9.53 |\n|  | GPVAE-T5 ${ }^{\\bullet}$ | 0.2409 | 0.5886 | 0.8466 | 0.9688 | 0.5604/0.6169 | 9.60 |\n|  | NAR-LevT ${ }^{\\ddagger}$ | 0.2268 | 0.5795 | 0.8344 | 0.9790 | 0.9995/0.3329 | 8.85 |\n|  | DiffuSEQ (Ours) ${ }^{\\ddagger}$ | 0.2413 | 0.5880 | 0.8365 | 0.9807 | 0.2732/0.8641 | 11.2 |\n\nEvaluation. We evaluate the generated sequences from two aspects: quality and diversity. To evaluate the quality, we use the standard metric BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) score. Since string-similarity-based metrics can be unsatisfactory for open-ended generation, we also report BERTScore (Zhang et al., 2019) that assesses the semantic similarity between generated sentences and references. Details are in Appendix D.4. Higher scores of BLEU, ROUGE and BERTScore reflect better performance. As for diversity, we use distinct unigram (dist-1) to measure intra-diversity within each generated sentence, where the lower dist-1 indicates that the generated sentence contains more repeated words. For sentence-level diversity evaluation, we consider sentence-level self-BLEU (Zhu et al., 2018) to measure the n-gram overlap between the set of outputs w.r.t one source sentence, and we additionally use diverse 4-gram (div-4) (Deshpande et al., 2019) to measure the ratio of distinct 4-grams in the set of outputs per source sentence. The lower self-BLEU and higher div-4 suggest higher diversity of generation. For each method including DiffuSEQ, we generate 3 samples for each source sentence to compute the diversity metrics.\n\nImplementation Details. Our DiffuSeq is based on the 12 layers of Transformer with 12 attention heads, where the time step embedding is plugged akin to the position embedding. The maximum sequence length is 128 , with embedding dimension $d=128$, diffusion steps $T=2,000$ and a square-root noise schedule. To reduce the out-of-vocabulary generation, we apply Byte Pair Encoding (Sennrich et al., 2016) to construct the vocabulary. After conducting the diversity beam\n\nTable 2: Sample outputs in QQP test set, conditioned on the same $\\mathbf{x}$.\n\n| Original sentence: How do I make friends. | Paraphrase reference: How to make friends ? |  |\n| :-- | :-- | :-- |\n| GPT2-large finetune | GPVAE-T5 | DIFFUSEQ |\n| How can I make friends? | How can I make friends? | How can I make friends better? |\n| How can I make friends? | How do I make friends? | How can I make friends? |\n| How can I make friends? | How can I make friends? | How do you make friends? |\n| How do I make friends and | How can I make friends? | What is the best way to make friends? |\n| keep them? | What's the best way to make | How can I make friends and more |\n|  | friends and make make friends? | something? |\n\nsearch (DBS) (Vijayakumar et al., 2016) for the Transformer-base model and GPT model, we find that DBS does not always promote diversity over temperature sampling and therefore we list the best diversity results. We compute the accuracy metrics of DIFFUSEQ using MBR with the size of candidate samples $|\\mathcal{S}|=10$. The experiment is deployed on NVIDIA A100 Tensor Core GPUs, and we use 4 GPUs on training and single GPU on sampling.\n\n## # 4.2 MAIN RESULTS\n\nAs shown in Table 1, we conclude that DiffuSeq achieves comparable or even higher generation quality compared with strong baselines. At the same time, DiffuSeq consistently demonstrates its superiority in generating diverse outputs given the same input sequence.\n\nAs we can see from Table 1, DiffuSeq wins competitions over at least one quality metric against 6 baselines $\\times 4$ tasks. Although NAR models such as LevT can also outperform AR baselines sometimes, they still lag well behind DIFFUSEQ by large margins (i.e., relative improvements over $50 \\%$ for BLEU in QG task and R-L in Dialogue task). Even compared with pre-trained then finetuned GPT2 models, DiffuSeq still delivers superior performance than the base variant, and is comparable with the large variant, which has 8.2 times more parameters than DiffuSeq. These empirical results amply support our findings in $\\S 3$, where we theoretically analyze the potential of diffusion models in modeling text sequences compared with AR models given sufficient diffusion steps.\n\nDiffuSeq, as a member of the deep generative model family, also exhibit the capacity to generate highly diverse sequences. As suggested by self-BLEU (lower is better) and div-4 (higher is better), in almost all cases, DiffuSeq significantly outperforms 4 AR baselines in terms of sentence-level diversity (i.e., producing diverse outputs given the same input). For diversity in word choice within one sentence, we consider dist-1: a higher dist-1 indicates less repetition within a sentence. As we can see from Table 1, DiffuSeq has less repetition compared with encoder-decoder methods, but still fall behind the pre-trained GPT2 models (the same situation with BERTScore). These results suggest there is still room for improvement (e.g., use pre-training techniques) in diffusion models' token-level choice. Different from NAR-LevT, DiffuSeq does not rely on an extra length prediction module but automatically decides by the padding token instead and is able to generate longer output sentences, indicated by the last column for average generation length.\n\nIn Table 2, we provide examples to showcase DiffuSeq's ability to generate diverse samples. More examples can be found in Appendix D.5.\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: The increase of BLEU score with different candidate sizes $|\\mathcal{S}|$.\n![img-3.jpeg](img-3.jpeg)\n\nFigure 4: Trade-off between quality and diversity (details in Appendix D.3).\n\n![img-4.jpeg](img-4.jpeg)\n\nFigure 5: The curve of BLEU/div-4 score along with generation process (percentage of steps).\n![img-5.jpeg](img-5.jpeg)\n\nFigure 6: The BLEU and inference speed of DIFFUSEQ and GPT2-large.\n\n## # 4.3 ANALYSIS\n\nWe conduct a series of analysis to investigate the effectiveness of different aspects in DIFFUSEQ.\nDiversity Ensures Quality. Generating high-quality texts with high diversity is an important requirement for many text generation applications and the trade-off between quality and diversity is always a critical concern in open-ended NLG tasks (Zhang et al., 2021). Different from AR models relying on the decoding strategy like temperature and nucleus sampling (Holtzman et al., 2019) and VAE models sampling latent variable from Gaussian Prior, the natural advantage of DIFFUSEQ is to generate different sentences along with a series of random Gaussian noise. In Figure 4, we elucidate that DIFFUSEQ have better trade-off between generation quality (BLEU) and sentence-level diversity (div-4). Here we further demonstrate that the high diversity provided by DIFFUSEQ can be turned into better quality.\n\nMBR is a common strategy to improve generation quality by aggregating and ranking candidate sequences, and we find that the upper bound of MBR is decided by a diversified candidate set. To valid this, we simultaneously apply MBR on both DIFFUSEQ and GPT2 with various candidate sizes $|\\mathcal{S}|$. The results are shown in Figure 3. As we can see, DIFFUSEQ lags behind GPT2 without using MBR $(|\\mathcal{S}|=1)$ or with a small candidate set $(|\\mathcal{S}|=3)$. However, as $|\\mathcal{S}|$ increases, DIFFUSEQ starts to outperform GPT2 by an increasing margin. The reason is that autoregressive models like GPT2 tend to generate highly similar candidates (as discussed in $\\S$ 4.2), which impedes the effectiveness of MBR. As $|\\mathcal{S}|$ increases to 20, DIFFUSEQ still shows better rising trends than GPT2. Our findings also stress the importance of better ranking methods in diffusion research.\n\nStep-wise Analysis against Iterative NAR. Given the underlying theoretical connection between iterative NAR and DIFFUSEQ discussed in $\\S 3$, we empirically investigate the behavior of LevT and DIFFUSEQ by analyzing their step-wise quality (i.e. BLEU) and diversity (i.e. div-4) curves. As is suggested in Figure 5, LevT grows fiercely in quality at the very beginning of generation, and quickly slows down in the successive refinement process. But DIFFUSEQ behaves differently, with BLEU score growing slowly at first, increasing rapidly as the diffusion process progresses and finally surpassing LevT. It is also observed that the diversity of both LevT and DIFFUSEQ is determined at the very early stage regardless of future refinement or diffusion, where DIFFUSEQ consistently outperforms LevT on diversity at any stage of generation. We conjecture that DIFFUSEQ explores more possible results at the first half of generation process, and soon converges to several potential candidates when it is closed to the end of steps. In this case, DIFFUSEQ shows its capacity to take both generation quality and diversity into consideration, and this is the capacity that iterative-NAR and even AR models can not obtain, due to the different learning paradigms.\n\nInference Speed. The slow sampling speed is one of the major concerns about diffusion models. Here we fix the number of diffusion steps during training for DIFFUSEQ while shrinking the inference steps following DDIM (Song et al., 2020). As we can see from Figure 6, when reducing the inference to 1,000 diffusion steps on single GPU, DIFFUSEQ achieves a higher BLEU score than GPT2-large yet registers a closer inference speed to GPT2-large.\n\nEffectiveness of Joint Training. In DIFFUSEQ, the representations of $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$ are jointly trained using the same embedding function $\\operatorname{EMB}(\\cdot)$ (stated in $\\S 3$ ). To validate the effectiveness\n\nTable 3: Results with or without joint training for Question Generation task.\n\n| Setting | BLEU $\\uparrow$ | R-L $\\uparrow$ | Score $\\uparrow$ | selfB $\\downarrow /$ div- $4 \\uparrow$ |\n| :-- | :-- | :-- | :-- | :-- |\n| DIFFUSEQ (w/o reranking) | 0.1567 | 0.3484 | 0.5947 | $0.2789 / 0.8103$ |\n| Fix $\\operatorname{EMB}\\left(\\mathbf{w}^{x}\\right)$ as pre-trained | 0.0110 | 0.0687 | 0.3769 | $0.0174 / 0.9376$ |\n\nof this joint training strategy, we compared it with the training strategy commonly used in text-toimage diffusion models (Nichol et al., 2022; Ramesh et al., 2022). In particular, we decouple the training of $\\operatorname{EMB}\\left(\\mathbf{w}^{x}\\right)$ and $\\operatorname{EMB}\\left(\\mathbf{w}^{y}\\right)$ by replacing $\\operatorname{EMB}\\left(\\mathbf{w}^{x}\\right)$ with representations extracted from a pre-trained BERT-tiny model (Turc et al., 2019). From Table 3, we find that the decoupled training strategy results in poor performance.\n\n# 5 Related Work\n\nDiffusion Models for Text Modeling. Text-to-Image generation using diffusion models has developed many potential applications. Models such as Imagen (Saharia et al., 2022b) and DALLE (Ramesh et al., 2022) are usually two-staged relying on the pre-trained models, requiring the alignment between the embedding vectors from two sources. GLIDE (Nichol et al., 2022) explores diffusion model with classifier-free (Ho \\& Salimans, 2022) guidance by setting guidance scale during training. The target space of these models is not discrete text space but stable vectors of pixel values. There are other works of diffusion on text generation, but they stick to the original encoderdecoder architecture and the diffusion process is interspersed on the decoder (Savinov et al., 2021), or the latent space (Yu et al., 2022).\n\nFor text generation using the diffusion models, Hoogeboom et al. (2021) introduce the multinomial diffusion for character-level text generation, the forward categorical noise is applied through the Markov transition matrix. Austin et al. (2021) generalize discrete text diffusion models by introducing the absorbing state ( (MASK )). However, discrete diffusion models may suffer from the scaling of the one-hot row vectors, and they only generate text samples unconditionally in discrete space. Diffusion-LM (Li et al., 2022) and Analog Bits (Chen et al., 2022) propose a new language model diffused on the continuous latent representations, with different mapping functions that connect the discrete and continuous space of texts. Compared with our work, we focus on the SEQ2SEQ diffusion models for text generation in the continuous space and our work is the first to explore this setting to the best of our knowledge.\n\nDiffusion Models for Conditional Generation. Related to conditional-VAE (Zhao et al., 2017), we can consider the latent encoded input $\\mathbf{x}$ as a condition. Diffusion-LM (Li et al., 2022) adopts the plug-and-play approaches (Dathathri et al., 2020) to compose fine-grained constraints on the generated sentences, but it fails to condition on the whole source sentence in SEQ2SEQ tasks. Noted that this controllable generation method is orthogonal to our DIFFSEQ, in other words, we can further add classifier-guided constraints on the SEQ2SEQ output to further control the text generation. There are other conditional diffusion models on the time series prediction like CSDI (Tashiro et al., 2021) or audio generation like WaveGrad (Chen et al., 2021), but their class conditions are usually attributes that are easy to model, while the contextual texts as conditions are much more complex.\n\n## ## 6 CONCLUSIONS\n\nWe propose DIFFUSEQ to tackle SEQ2SEQ tasks in a diffusion way, which contains the strong potential to achieve better generation quality and diversity trade-off. The capability enables favorable characteristics of DIFFUSEQ to further enhance the quality of final results, by leveraging a minimum Bayes risk decoding algorithm. Besides, we theoretically connect the AR and NAR models to DIFFUSEQ, and show that DIFFUSEQ is a powerful extension of iterative-NAR model. The empirical results demonstrate that DIFFUSEQ is also a powerful model for text generation, matching or even surpassing competitive AR, iterative NAR, and large-scale pre-trained models on quality and diversity. Given the limited progress of current diffusion models on text generation, our study addresses promising achievements by such a new sequence-to-sequence learning paradigm.\n\n## # ACKNOWLEDGMENTS\n\nWe would like to thank the anonymous reviewers and other peers for their valuable advice, and we also acknowledge Chenxin An's efforts to update the generation results for the Transformer-base model on QG and Paraphrasing tasks. This work is partially supported by the Shanghai Committee of Science and Technology (Grant No. 21DZ1100100) and the joint research scheme of the National Natural Science Foundation of China (NSFC) and the Research Grants Council (RGC) under grant number N_HKU714/21.\n\n## ## REFERENCES\n\nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 2021.\n\nNanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In International Conference on Learning Representations, ICLR, 2021.\n\nTing Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022.\n\nKyunghyun Cho, Bart van Merrienboer, √áaglar G√ºl√ßehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing, EMNLP, 2014.\n\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. In 8th International Conference on Learning Representations, ICLR, 2020.\n\nAditya Deshpande, Jyoti Aneja, Liwei Wang, Alexander G Schwing, and David Forsyth. Fast, diverse and accurate image captioning guided by part-of-speech. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10695-10704, 2019.\n\nBhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. Quasar: Datasets for question answering by search and reading. arXiv preprint arXiv:1707.03904, 2017.\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In International Conference on Learning Representations, ICLR, 2017.\n\nWanyu Du, Jianqiao Zhao, Liwei Wang, and Yangfeng Ji. Diverse text generation via variational encoder-decoder models with gaussian process priors. In Proceedings of 6th Workshop on Structured Prediction for NLP of the Association for Computational Linguistics, ACL, 2022.\n\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 6112-6121. Association for Computational Linguistics, 2019.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014.\n\nJiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. In International Conference on Learning Representations, ICLR, 2018.\n\nJiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. Advances in Neural Information Processing Systems, 2019.\n\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 2020.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, ICLR, 2019.\n\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr√©, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 2021.\n\nFei Huang, Tianhua Tao, Hao Zhou, Lei Li, and Minlie Huang. On the learning of non-autoregressive transformers. In International Conference on Machine Learning, ICML, 2022.\n\nChao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. Neural crf model for sentence alignment in text simplification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL, 2020.\n\nDiederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on Learning Representations, ICLR, 2014.\n\nPhilipp Koehn. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 conference on empirical methods in natural language processing, pp. 388-395, 2004.\n\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, ICLR, 2020.\n\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. arXiv preprint arXiv:2205.14217, 2022.\n\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, 2004.\n\nYankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. Denoising distantly supervised opendomain question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL, 2018.\n\nLuke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. In International Conference on Learning Representations, ICLR, 2017.\n\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, ICML, 2022.\n\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, ICML, 2021.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL, 2002.\n\nLihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, and Lei Li. Glancing transformer for non-autoregressive neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nChitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, 2022a.\n\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b.\n\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, 2016.\n\nNikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van den Oord. Stepunrolled denoising autoencoders for text generation. In International Conference on Learning Representations, ICLR, 2021.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715-1725. Association for Computational Linguistics, 2016.\n\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, ICML, 2015.\n\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, ICLR, 2020.\n\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.\n\nYusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. Advances in Neural Information Processing Systems, 2021.\n\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact of student initialization on knowledge distillation. CoRR, abs/1908.08962, 2019.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017.\n\nAshwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424, 2016.\n\nP Yu, S Xie, X Ma, B Jia, B Pang, R Gao, Y Zhu, S-C Zhu, and YN Wu. Latent diffusion energybased model for interpretable text modeling. In International Conference on Machine Learning, ICML, 2022.\n\nHugh Zhang, Daniel Duckworth, Daphne Ippolito, and Arvind Neelakantan. Trading off diversity and quality in natural language generation. In Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pp. 25-33. Association for Computational Linguistics, 2021.\n\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, ICLR, 2019 .\n\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, 2017.\n\nHao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. Commonsense knowledge aware conversation generation with graph attention. In International Joint Conference on Artificial Intelligence, IJCAI, 2018.\n\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: A benchmarking platform for text generation models. In The 41st International ACM SIGIR Conference on Research \\& Development in Information Retrieval, pp. 1097-1100, 2018.\n\n# A Objective Derivations Of DiffuSEQ\n\nThe diffusion model is well-known as its ability to achieve the trade-off between flexibility and tractability of the models' probability distributions, compared with GAN, VAE and Flow-based models. Following Ho et al. (2020); Nichol \\& Dhariwal (2021); Song et al. (2020), we systematically define the forward noising process and reverse denoising process on latent continuous space $\\mathbf{z}$.\nThe forward noising is to perturb the structure of data $\\mathbf{z}_{0} . \\mathbf{z}_{0}$ is finally changed into the partial Gaussian noise with $\\mathbf{y}_{T} \\sim \\mathcal{N}(0, \\mathbf{I})$ through $T$-step forward random disturbance\n\n$$\nq\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t} ; \\sqrt{1-\\beta_{t}} \\mathbf{z}_{t-1}, \\beta_{t} \\mathbf{I}\\right)\n$$\n\nwith $t=1,2, \\ldots, T$ and $\\left\\{\\beta_{t} \\in(0,1)\\right\\}_{t=1}^{T}$ are the variance schedule. Let $\\alpha_{t}=1-\\beta_{t}$ and $\\bar{\\alpha}_{t}=$ $\\prod_{i=1}^{t} \\alpha_{i}$, we have:\n\n$$\n\\begin{aligned}\n\\mathbf{z}_{t} & =\\sqrt{\\alpha_{t}} \\mathbf{z}_{t-1}+\\sqrt{1-\\alpha_{t}} \\epsilon_{t-1}=\\sqrt{\\alpha_{t} \\alpha_{t-1}} \\mathbf{z}_{t-2}+\\sqrt{1-\\alpha_{t} \\alpha_{t-1}} \\bar{\\epsilon}_{t-2} \\\\\n& =\\ldots=\\sqrt{\\bar{\\alpha}_{t}} \\mathbf{z}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}} \\epsilon\n\\end{aligned}\n$$\n\nwhere $\\epsilon$ stands for Gaussian noises. In the end, $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{0}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t} ; \\sqrt{\\bar{\\alpha}_{t}} \\mathbf{z}_{0},\\left(1-\\bar{\\alpha}_{t}\\right) \\mathbf{I}\\right)$. We use a sqrt noise schedule in Diffusion-LM (Li et al., 2022), that is, $\\bar{\\alpha}_{t}=1-\\sqrt{t / T+s}$ with $s$ as a small constant at the start of noise level. The reverse process then denoises $\\mathbf{z}_{t}$, aiming to recover original $\\mathbf{z}_{0}$, and is defined as:\n\n$$\np_{\\theta}\\left(\\mathbf{z}_{0: T}\\right):=p\\left(\\mathbf{z}_{T}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right), \\quad p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t-1} ; \\mu_{\\theta}\\left(\\mathbf{z}_{t}, t\\right), \\sigma_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right)\n$$\n\nThe learning of $p_{\\theta}$ is based on our diffusion model DiffuSEQ: $f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)$, where the $\\mu_{\\theta}(\\cdot)$ and $\\sigma_{\\theta}(\\cdot)$ is the predicted parameterization of the mean and standard variation of $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}\\right)$ in forward process. Using Bayes' rule:\n\n$$\nq\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}, \\mathbf{z}_{0}\\right)=q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}, \\mathbf{z}_{0}\\right) \\frac{q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{0}\\right)}{q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{0}\\right)}\n$$\n\nSubstitute Eq. (11) to it and we can get the parameterized mean of $q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}, \\mathbf{z}_{0}\\right)$ :\n\n$$\n\\mu_{t}\\left(\\mathbf{z}_{t}, \\mathbf{z}_{0}\\right)=\\frac{\\sqrt{\\alpha_{t}}\\left(1-\\bar{\\alpha}_{t-1}\\right)}{1-\\bar{\\alpha}_{t}} \\mathbf{z}_{t}+\\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_{t}}{1-\\bar{\\alpha}_{t}} \\mathbf{z}_{0}\n$$\n\nand for brevity, we short the coefficient of $\\mathbf{z}_{t}$ and $\\mathbf{z}_{0}$ as $\\mathcal{U}$ and $\\mathcal{E}$ respectively.\nWe can use the variational lower bound to optimize the negative log-likelihood $\\mathbb{E}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{0}\\right)\\right] \\leq$ $\\mathcal{L}_{\\text {VLB }}$. The objective can be further rewritten to be a combination of several KL-divergence and entropy terms following Sohl-Dickstein et al. (2015).\n\n$$\n\\begin{aligned}\n\\mathcal{L}_{\\mathrm{VLB}}=\\mathcal{L}_{T}+\\mathcal{L}_{T-1}+\\cdots+\\mathcal{L}_{0} & =\\mathbb{E}_{q\\left(\\mathbf{z}_{1: T} \\mid \\mathbf{z}_{0}\\right)}\\left[\\log \\frac{q\\left(\\mathbf{z}_{T} \\mid \\mathbf{z}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{T}\\right)}+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{0}, \\mathbf{z}_{t}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)}\\right. \\\\\n& \\left.+\\log \\frac{q_{\\phi}\\left(\\mathbf{z}_{0} \\mid \\mathbf{w}^{x \\oplus y}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{0} \\mid \\mathbf{z}_{1}\\right)}-\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)\\right]\n\\end{aligned}\n$$\n\nFor $1 \\leq t \\leq T-1$, we compute the parameterization of $\\mathcal{L}_{t}$ by substituting Eq. (14) to minimize the difference from $\\mu_{t}$ and $\\mu_{\\theta}$ following Ho et al. (2020):\n\n$$\n\\begin{aligned}\n\\mathcal{L}_{t} & =\\mathbb{E}_{\\mathbf{z}_{0}}\\left[\\log \\frac{q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{0}, \\mathbf{z}_{t+1}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t+1}\\right)}\\right]=\\mathbb{E}_{\\mathbf{z}_{0}}\\left[\\frac{1}{\\mathcal{C}}\\left\\|\\mu_{t}\\left(\\mathbf{z}_{t}, \\mathbf{z}_{0}\\right)-\\mu_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}\\right] \\\\\n& =\\mathbb{E}_{\\mathbf{z}_{0}}\\left[\\frac{1}{\\mathcal{C}}\\left\\|\\mathcal{U} \\mathbf{z}_{t}+\\mathcal{E} \\mathbf{z}_{0}-\\left(\\mathcal{U} \\mathbf{z}_{t}+\\mathcal{E} f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right)\\right\\|^{2}\\right]=\\frac{\\mathcal{E}}{\\mathcal{C}} \\mathbb{E}_{\\mathbf{z}_{0}}\\left[\\left\\|\\mathbf{z}_{0}-f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}\\right]\n\\end{aligned}\n$$\n\nwhere $\\mathcal{C}=2\\left\\|\\sigma_{\\theta}\\right\\|^{2}$ is a loss independent constant. Then the optimization of training loss $\\min _{\\theta} \\mathcal{L}_{\\text {VLB }}$ can be further simplified as:\n\n$$\n\\begin{aligned}\n& \\min _{\\theta}\\left[\\left\\|\\mu\\left(\\mathbf{z}_{T}\\right)\\right\\|^{2}+\\sum_{t=2}^{T}\\left\\|\\mathbf{z}_{0}-f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right)-f_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}-\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)\\right] \\\\\n\\rightarrow & \\min _{\\theta}\\left[\\sum_{t=2}^{T}\\left\\|\\mathbf{z}_{0}-f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right)-f_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}-\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)\\right] \\\\\n\\rightarrow & \\min _{\\theta}\\left[\\sum_{t=2}^{T}\\left\\|\\mathbf{y}_{0}-\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{y}\\right)-\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}+\\mathcal{R}\\left(\\left\\|\\mathbf{z}_{0}\\right\\|^{2}\\right)\\right]\n\\end{aligned}\n$$\n\n# B Graphical Models of AR, Fully NAR, iterative NAR and DIFFUSEQ MODELS\n\nWe start from the conditional sequence generation problem, which aims to learn a conditional probability $p\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)$ with $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$. AR models learn $p\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)$ by autoregressive decomposition based on left-context:\n\n$$\np_{\\mathrm{AR}}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\underbrace{p\\left(w_{1}^{y} \\mid \\mathbf{w}^{x}\\right)}_{\\text {initial prediction }} \\underbrace{\\prod_{i=1, \\ldots, n-1} p\\left(w_{i+1}^{y} \\mid \\mathbf{w}_{1: i}^{y}, \\mathbf{w}^{x}\\right)}_{\\text {progressive left-context prediction }}\n$$\n\nconsisting of an initial prediction and an autoregressive left-context prediction process, while fullyNAR models (Gu et al., 2018; Qian et al., 2021) learn the conditional probability given independent assumption for fast inference:\n\n$$\np_{\\text {fully-NAR }}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\prod_{i=1, \\ldots, n} p\\left(w_{i}^{y} \\mid \\mathbf{w}^{x}\\right)\n$$\n\nTo make a better analogy to AR and NAR models, we use a lossless way to formulate iterative NAR models (Gu et al., 2019; Ghazvininejad et al., 2019) by introducing a series of intermediate sequences $\\mathbf{w}_{1: K-1}^{y}, \\mathbf{w}_{K}^{y}=\\mathbf{w}^{y}$ as:\n\n$$\n\\begin{aligned}\n& p_{\\text {iter-NAR }}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} p\\left(\\mathbf{w}_{1}^{y} \\mid \\mathbf{w}^{x}\\right) \\prod_{k=1 \\ldots K-1} p\\left(\\mathbf{w}_{k+1}^{y} \\mid \\mathbf{w}_{k}^{y}, \\mathbf{w}^{x}\\right) \\\\\n& =\\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} p\\left(\\mathbf{w}_{1}^{y} \\mid \\mathbf{w}^{x}\\right) \\prod_{k=1 \\ldots K-1} p\\left(\\mathbf{w}_{k+1}^{y} \\mid \\mathbf{w}_{k}^{y}, \\mathbf{w}^{x}\\right) \\\\\n& =\\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} \\underbrace{\\prod_{i=1 \\ldots n} p\\left(w_{1, i}^{y} \\mid \\mathbf{w}^{x}\\right)}_{\\text {initial prediction }} \\underbrace{\\prod_{k=1 \\ldots K-1} \\prod_{i=1 \\ldots n} p\\left(w_{k+1, i}^{y} \\mid \\mathbf{w}_{k, 1: n}^{y}, \\mathbf{w}^{x}\\right)}_{\\text {progressive full-context prediction }}\n\\end{aligned}\n$$\n\nPrevious study (Huang et al., 2022) shows that there is a gap called conditional total correlation between AR and fully-NAR learning paradigms, because of the lossy decomposition of NAR models. This gap is mainly responsible for the performance drop from AR to NAR models. However, when comparing iter-NAR, Eq. (20), with AR models, they both can be factorized into an initial prediction term and a progressive prediction process based on different context (i.e. left-context in AR and full-context in iter-NAR). The discrepancy as pointed out by Huang et al. (2022) is therefore closed in iter-NAR assuming sufficient steps. By showing DIFFUSEQ is an extension of the iter-NAR model, we offer a justification that it will not suffer from the conditional total correlation for the same reason.\n\nA straight-forward way to formulate naive diffusion models is to introduce a series of Gaussian noise-corrupted features $\\mathbf{y}_{1: T-1}, \\mathbf{y}_{0}=\\mathbf{y}, \\mathbf{y}_{T} \\sim \\mathcal{N}(0, \\mathbf{I})$ on continuous space as:\n\n$$\n\\begin{aligned}\np_{\\text {diffusion }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) & =\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} \\underbrace{p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right)}_{\\begin{array}{c}\n\\text { final-step prediction } \\\\\n\\text { progressive full-context diffusion }\n\\end{array}} \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n& =\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} \\prod_{i=1, \\ldots, n} p\\left(\\mathbf{w}_{i}^{y} \\mid \\mathbf{y}_{0, i}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} \\prod_{i=1, \\ldots, n} p\\left(\\mathbf{y}_{t-1, i} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\nwhere $p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)$ describes the diffusion process on contiguous representations $\\mathbf{y}$. The total number of diffusion steps is denoted as $T$. Thereafter we omit the independent decomposition on $\\mathbf{w}^{y}$ and $\\mathbf{y}_{t}$. To apply diffusion models on discrete space, the rounding operation in DIFFUSEQ maps the continuous vectors $\\mathbf{y}$ to discrete $\\mathbf{w}^{y}$ for each time step $t$, we hence in addition introduce both contiguous feature $\\mathbf{y}$ and the discrete text $\\mathbf{w}^{y}$ to represent the discrete text into Eq. (21):\n\n$$\n\\begin{aligned}\np\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) & =\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) p\\left(\\mathbf{y}_{t} \\mid \\mathbf{w}_{t+1}^{y}\\right) \\\\\n& =\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n& =\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n& =\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} \\sum_{\\mathbf{w}_{t}^{y}} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\nBy rearranging Eq. (23) and Eq. (24), we can see that DIFFUSEQ can be seen as a more generalized form of iter-NAR before marginalizing out $\\left\\{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}\\right\\}$, where Eq. (23) and Eq. (24) are equivalent with different computation order, despite the different initialization of $\\mathbf{y}_{T}$. For NAR models, $\\mathbf{y}_{T}$ is uniform copied from the source sentence or unk's token embedding (Gu et al., 2018); for diffusion models, $\\mathbf{y}_{T}$ is sampled from normal distribution $\\mathcal{N}(0, \\mathbf{I})$.\nIt is notable that unlike AR and fully NAR models generating text all at once, iterative NAR and diffusion models feature a self-corrected text generation process. The graphical comparison is shown in Figure 7.\n\n# C From DiffuSeq to Iterative NAR and Diffusion Models\n\nFrom DiffuSeq to Iterative NAR We show how to derive DiffuSeq to iterative nonautoregressive model on discrete space.\n\n![img-6.jpeg](img-6.jpeg)\n\nFigure 7: Graphical Models of AR, Fully NAR, iterative NAR and DiffuSeq models. For simplicity, we omit source node $\\mathbf{w}^{x}$. Gray nodes indicate dependency on the source node while white nodes are independent to the source node.\n\n$$\n\\begin{aligned}\n& p_{\\text {DiffuSeq }} \\cdot\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) p\\left(\\mathbf{y}_{t} \\mid \\mathbf{w}_{t+1}^{y}\\right) \\quad \\text { reorder computation } \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} \\int_{\\mathbf{y}_{t}} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) p\\left(\\mathbf{y}_{t} \\mid \\mathbf{w}_{t+1}^{y}\\right) \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{w}_{t+1}^{y}, \\mathbf{w}^{x}\\right) \\quad \\text { marginalize over } \\mathbf{y} \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} p\\left(\\mathbf{w}_{1}^{y} \\mid \\mathbf{w}^{x}\\right) \\prod_{k=1 \\ldots K-1} p\\left(\\mathbf{w}_{k+1}^{y} \\mid \\mathbf{w}_{k}^{y}, \\mathbf{w}^{x}\\right) \\quad \\text { align } t \\text { and } k \\text { reversely. } \\\\\n= & p_{\\text {iter-NAR }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\nFrom DiffuSeq to diffusion model We show how to derive DiffuSeq to the straight-forward diffusion model on continuous space.\n\n$$\n\\begin{aligned}\n& p_{\\text {DiffUSEQ }} \\cdot\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n= & \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} \\sum_{\\mathbf{w}_{t}^{y}} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n= & \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n= & p_{\\text {diffusion }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\n## # D DETAILS OF EXPERIMENTS\n\n## D. 1 Processing of Question Generation Dataset\n\nTo construct high-quality document-question pairs from the Quasar-T dataset, which consists of $\\left\\langle\\right.$ document $_{i}$, question, answer $\\rangle$ triplets, we extract $\\left\\langle\\right.$ document $_{i}$, question $\\rangle$ pairs if answer exactly matches document ${ }_{i}$. After pre-processing, we obtain 119 K document-question training pairs.\n\n## ## D. 2 SETTINGS OF BASELINES\n\nWe compare the settings of different models, including the number of parameters and how to sample the different output sentences, as shown in Table 4. For plain GRU-based encoder-decoder methods, we do not implement diversity search algorithms on it, thus its sentence-level diversity could be very poor. For NAR-LevT, we set max iteration to 9 and follow the termination condition mentioned in the original paper. For GPVAE-T5, we tune the scalar to find the best trade-off between quality and diversity on the dev set. The scalars of all four tasks are set to 2 . We implement GPT2 baselines using HuggingFace Transformers and for the baseline Transformer-base, we use Fairseq.\n\n## Table 4: The comparison for different models\n\n| Models | \\# Parameters | Learning Paradigm | Diversity Source |\n| :-- | :-- | :-- | :-- |\n| GRU-attention | 65 M | encoder-decoder | - |\n| Transformer-base | 80 M | encoder-decoder | Temperature/DBS |\n| GPT2-base FT | 117 M | pretrain-finetune | Hybrid strategy |\n| GPT2-large FT | 774 M | pretrain-finetune | Hybrid strategy |\n| GPVAE-T5 | 220 M | pretrain+VAE | Gaussian sampling |\n| NAR-LevT | 80 M | non-autoregressive | - |\n| DIFFUSEQ | 91 M | non-autoregressive | Gaussian sampling |\n\n## D. 3 Diversity and Quality Trade-off Settings\n\nWe list the details to obtain Figure 4. For GPVAE-T5, we set different scalars as 1, 2, 3, 4. For DIFFUSEQ, we choose trained models at different training steps to achieve different trade-off points. For other baselines, there is no explicit factor to control the diversity generation, so we leave them as single points in the figure.\n\n[^0]\n[^0]:    ${ }^{4}$ Including top-p sampling, temperature, diversity beam search (DBS) and etc. Implement using HuggingFace Transformers https://github.com/huggingface/transformers\n\n# D. 4 Metrics\n\nThe used BLEU score is sentence-level smoothed from BLEU-1 to 4, and used ROUGE-L score is longest common subsequence based statistics. The implementation is based on NLTK ${ }^{5}$ and torchmetrics. The n-gram based metrics may fail to capture the semantic meaning of sentences, so we consider using BERTScore ${ }^{6}$. Specifically, we use microsoft/deberta-xlarge-mnli to help BERTScore correlate better with human scores.\n\n## ## D. 5 GENERATION RESULTS\n\nFor different tasks we list some generation examples. As we can see in Table 5, Table 6 and Table 7, DiffuSeq tends to generate diverse outputs, but sometimes the sentence is not as fluent as finetuned GPT2.\n\nTable 5: Sample outputs with different random seed in Dialogue test set.\n\n| Utterance: How long does the dye last? <br> Response: Just did this two days ago, not sure how it'll fade yet! |  |\n| :--: | :--: |\n| GPVAE-T5 | NAR-LevT |\n| * I'm not sure, I'm not sure. I've tested it a few times, but I don't know for sure. I've | * half . |\n| * I'm not sure. I'm not sure how long it lasts, I'm sure it 'll get better. It's been a while since | * half . |\n| * I've been using it for about a year and a half. I've been using it for about a year and a half. | * half . |\n| GPT2-large finetune | DiffuSeq |\n| * Two weeks in my case. | * About an hour, 5 days or so. |\n| * I've had it for about a year. | * 4 days. |\n| * The dye can sit around for a month then you can wash it. | * I'm not sure about this, about the same kind of time. |\n\nTable 6: Sample outputs with different random seed in Question Generation test set.\nStatement: The Japanese yen is the official and only currency recognized in Japan.\n## Question: What is the Japanese currency?\n\n| GPVAE-T5 | NAR-LevT |\n| :-- | :-- |\n| * What is the japanese currency | * What is the basic unit of currency for Japan ? |\n| * What is the japanese currency | * What is the basic unit of currency for Japan ? |\n| * What is the japanese currency | * What is the basic unit of currency for Japan ? |\n| GPT2-large finetune | DiffuSeq |\n| * What is the basic unit of currency for Japan? | * What is the Japanese currency |\n| * What is the Japanese currency | * Which country uses the \"yen yen\" in currency |\n| * What is the basic unit of currency for Japan? | * What is the basic unit of currency? |\n\n[^0]\n[^0]:    ${ }^{5}$ https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n${ }^{6}$ https://github.com/Tiiiger/bert_score\n\nTable 7: Sample outputs with different random seed in Text Simplification test set.\nComplex sentence: People can experience loneliness for many reasons, and many life events may cause it, such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person.\nSimplified: One cause of loneliness is a lack of friends during childhood and teenage years.\n\n## ## GPVAE-T5\n\n## People can experience loneliness for many reasons, and many life events may cause it,\n\n* such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person\n\n## People can experience loneliness for many reasons, and many life events may cause it,\n\n* such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person\n\n## People can experience loneliness for many reasons, and many life events may cause it,\n\n* such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person\n\n## GPT2-large finetune\n\n* Loneliness can be caused by many things.\n* Loneliness can affect people in many ways.\n* Loneliness can be caused by many things.\n\n## NAR-LevT\n\n* People may experience reashapphapphapphapphappabout life reasit.\n* People may experience reashapphapphapphapphappabout life reasit.\n* People may experience reashapphapphapphapphappabout life reasit.\n\n## ## DIFFUSEQ\n\n* Many life events may cause of loneliness\n* People can also be very experience loneliness for many reasons.\n* People can experience loneliness for many reasons, and many life events may, cause it.\n", "source": "inbox/2210.08933v3.pdf"}, "output_data": {"core_contribution": "The core contribution of DiffuSEQ is the introduction of a classifier-free diffusion model specifically tailored for sequence-to-sequence (SEQ2SEQ) text generation tasks. This model overcomes the traditional obstacles faced by diffusion models in text applications, particularly in maintaining conditional informativeness without relying on separate classifiers. The approach involves embedding sequences into continuous vector spaces and applying partial Gaussian noise only to the target sequences. In this manner, the model preserves the context provided by the source sequences, allowing DiffuSEQ to restore original texts via reverse denoising steps. This innovative technique supports the generation of highly diverse textual outputs while ensuring high quality, positioning DiffuSEQ as a significant advance in NLP systems built on diffusion models.", "method_breakdown": "The methodology of DiffuSEQ unfolds in distinct steps, starting with transforming both source and target sequences into a unified continuous vector space through an embedding function. Once transformed, the forward diffusion process involves adding Gaussian noise only to the vectors corresponding to the target sequence, enabling the model to retain context from the source sequence without alteration. The subsequent reverse diffusion process then leverages these continuous representations to denoise and regenerate the target text while maintaining high fidelity to the conditioned sequence. A transformer architecture is employed to effectively model the non-linear interactions between the sequences, leveraging transformer attention mechanisms to facilitate contextually-aware denoising that aligns well with the original semantic meaning of the input text. In practical terms, DiffuSEQ employs a Minimum Bayes Risk (MBR) decoding strategy, enhancing the quality and diversity of outputs by evaluating multiple candidate sequences and selecting those with optimal expected outcomes.", "subsystems_parts": "1. Embedding Layer: Converts discrete text sequences into continuous vector spaces for diffusion processing.\n2. Partial Noising Module: Applies Gaussian noise to target sequence embeddings, enabling context preservation in conditional generation.\n3. Reverse Denoising Module: Recovers original target text from noised embeddings using iterative denoising techniques.\n4. Transformer Network: Models semantic relations between source and target, ensuring contextual accuracy in generated outputs.\n5. Decoding Strategy (MBR): Enhances output quality and diversity by selecting optimal sequences from generated candidates.", "interactions": "These subsystems are intricately linked through the forward and reverse diffusion processes. Initially, the embedding layer ensures that both source and target sequences are in a compatible format for subsequent operations. The partial noising module selectively targets the embeddings of the target sequence, leaving the source intact to retain full context. As the denoising module restores the text, it coordinates with the transformer network, which imposes linguistic and contextual coherency through attention mechanisms. The entire system is further optimized during the final decoding stage, where the MBR strategy is applied to select output sequences that best match pre-defined quality criteria, thus integrating the model's inherent capacity for diversity with the need for textual precision.", "delta_vs_baseline": "DiffuSEQ fundamentally diverges from previous text-generating diffusion models by operating without external classifier constraints, thus removing the dependency on multiple pre-trained classifiers to ensure correct semantics in conditional generation tasks. By introducing partial noising‚Äîan innovative approach where only the target text is noised‚ÄîDiffuSEQ can emphasize the strong contextual ties of SEQ2SEQ tasks, which was a challenging endeavor using conventional autoregressive or non-autoregressive models that rely heavily on sequential predictions based on preceding words. Additionally, integration with MBR in selecting diverse outputs is notably different from standard diffusion methods.", "evidence_anchor": "In experiments, DiffuSEQ demonstrates its ability to achieve competitive or superior BLEU scores compared to traditional autoregressive and non-autoregressive models. Specifically, by increasing the candidate size in the MBR approach, the model showed enhanced performance beyond pretrained benchmarks, attesting to its capability for producing quality outputs with rich diversity. These results substantiate the theoretical and practical advantages claimed by adopting diffusion frameworks in SEQ2SEQ tasks.", "transferability": "The DiffuSEQ model, while specifically designed for SEQ2SEQ text generation, exemplifies a methodological approach that holds potential transferability across contexts where similar conditional generation patterns need implementing without reliance on external classifiers. However, integrating similar diffusion model strategies would necessitate suitable accommodations in domains other than text, especially those not innately reliant on sequence processing or differing inherently in structure from natural language."}, "conversation_log": ["Generator: {\"core_diagram_summary\":\"The paper introduces a figure (Figure 2) illustrating the DiffuSEQ conditional diffusion language model process. It depicts transforming source (w^x) and target (w^y) sequences into continuous vector spaces, followed by iteratively adding Gaussian noise to these target sequences (z_t). This process enables conditional generation without needing separate classifiers.\",\"unique_move\":\"DiffuSEQ introduces a classifier-free diffusion model for SEQ2SEQ tasks in textual generation, utilizing partial noising where only the target sequence is noised, allowing the model to be fully conditional without the aid for external classifiers.\",\"distilled_recipe\":\"Embed text sequences to continuous vectors ‚Üí Apply partial Gaussian noise to target vectors ‚Üí Apply reverse denoising to reconstruct original texts ‚Üí Use transformer to model semantic relations.\",\"contribution_type\":\"System\",\"evidence_anchor\":\"The experiment comparing BLEU scores with varying candidate sizes shows that as the candidate size increases, DIFFUSEQ outperforms pre-trained models, demonstrating its ability to produce diverse and quality outputs.\"}", "Decider 1: {\"scratchpad_updates\":\"1. Accuracy of Technical Claims: The core technical claims are mostly accurate. However, the description of 'partial noising' could be expanded with more detail on how it differentiates DiffuSEQ from existing models. Additionally, clarify the theoretical connections between DiffuSEQ and autoregressive/non-autoregressive models with more explicit mathematical representations rather than only conceptual ones. \\n\\n2. Completeness of Distilled Recipe: The distilled recipe is concise and captures the process well, but it should briefly explain why 'partial Gaussian noise' is used, which is central to DiffuSEQ's novelty.\\n\\n3. Type Classification: The contribution type correctly identifies DiffuSEQ as a 'System'; however, supplementary details might accentuate whether it's more about the 'system' of diffusion mechanisms or applied methodologies for SEQ2SEQ tasks beyond a mere structural innovation.\\n\\n4. Evidence Anchor: The evidence anchor is appropriate, but would benefit from more context around why the diversity in outputs matters, emphasizing its empirical findings regarding SEQ2SEQ performance improvements.\\n\\n5. Diagram Summary: Generally clear and precise, but could benefit from elaborating on the meaning of some elements like the 'z_t' components and the importance of using continuous space. Illustrating how the transformation supports the round-trip digital-to-continuous-to-digital conversion effectively would aid clarity.\",\"response_to_researcher\":\"The core claims seem mostly solid, but I suggest delving deeper into the mathematical justification for how partial noising specifically benefits SEQ2SEQ tasks over traditional noising methods. Furthermore, can the authors provide more insight into the performance with more contextual data regarding diversity versus quality trade-offs? This might help in comprehending the broader impact of DiffuSEQ in practical applications.\"}", "Researcher 1: {\"correction_scratchpad\":\"1. **Accuracy of Technical Claims:**\\n   - **Partial Noising:** The concept of partial noising is central to DiffuSEQ‚Äôs innovation. The paper uses partial noising only on the target sequence, which allows the source sequence to remain unchanged and provides the contextual information necessary during reverse denoising. This technique helps maintain the conditional nature of SEQ2SEQ tasks without requiring separate classifiers, distinguishing DiffuSEQ from other diffusion language models that do not differentiate between source and target during noising.\\n   - **Theoretical Connections:** The explanation of DiffuSEQ's theoretical connections to AR and NAR models is solid, demonstrating its role as an extension of iterative-NAR models by factorizing the prediction process into initial and progressive steps. However, additional mathematical derivations could clarify the equivalence with iter-NAR models by focusing on specific examples or scenarios under different modeling assumptions.\\n\\n2. **Completeness of Distilled Recipe:**\\n   - The recipe should highlight the importance of 'partial Gaussian noise' more explicitly, explaining its core role in allowing conditional modeling without classifiers, a requirement in traditional diffusion models. Adding a step that specifically mentions the retention of contextual information in the source sequence during noising might also be beneficial.\\n\\n3. **Type Classification:**\\n   - The classification as a 'System' is correct, but could further emphasize how it represents a methodological innovation in using diffusion models for sequence-to-sequence tasks, impacting both theoretical understanding and empirical effectiveness.\\n\\n4. **Evidence Anchor:**\\n   - The evidence anchor regarding BLEU scores is accurate, recognizing DiffuSEQ's superiority in generating diverse, high-quality outputs. Further context could underscore the importance of diversity and quality trade-offs in specific applications, strengthening its relevance in practical scenarios.\\n\\n5. **Diagram Summary:**\\n   - While succinctly capturing the model's essence, it could enhance clarity by further explaining the intricacies of the 'z_t' components. Additionally, detailing how continuous space transformations allow seamless integration during both noising and denoising processes could fortify understanding.\",\"response_to_decider\":\"Upon reviewing the concerns regarding the mathematical justification and emphasis on diversity versus quality trade-offs, it is evident that DiffuSEQ‚Äôs novel partial noising provides a strategic advantage for SEQ2SEQ tasks. This approach bypasses the need for classifier models by retaining source context, which is crucial during denoising. Furthermore, elucidating the diversity versus quality balance, as shown by the BLEU score variations, highlights DiffuSEQ's potential in generating varied and high-quality outputs, crucial for numerous practical applications in NLP. Overall, DiffuSEQ‚Äôs method offers both theoretical rigor and empirical efficacy, validating its contributions to sequence-to-sequence text generation paradigms.\",\"done\":true}", "Finalizer: {\"core_contribution\":\"The core contribution of DiffuSEQ is the introduction of a classifier-free diffusion model specifically tailored for sequence-to-sequence (SEQ2SEQ) text generation tasks. This model overcomes the traditional obstacles faced by diffusion models in text applications, particularly in maintaining conditional informativeness without relying on separate classifiers. The approach involves embedding sequences into continuous vector spaces and applying partial Gaussian noise only to the target sequences. In this manner, the model preserves the context provided by the source sequences, allowing DiffuSEQ to restore original texts via reverse denoising steps. This innovative technique supports the generation of highly diverse textual outputs while ensuring high quality, positioning DiffuSEQ as a significant advance in NLP systems built on diffusion models.\",\"method_breakdown\":\"The methodology of DiffuSEQ unfolds in distinct steps, starting with transforming both source and target sequences into a unified continuous vector space through an embedding function. Once transformed, the forward diffusion process involves adding Gaussian noise only to the vectors corresponding to the target sequence, enabling the model to retain context from the source sequence without alteration. The subsequent reverse diffusion process then leverages these continuous representations to denoise and regenerate the target text while maintaining high fidelity to the conditioned sequence. A transformer architecture is employed to effectively model the non-linear interactions between the sequences, leveraging transformer attention mechanisms to facilitate contextually-aware denoising that aligns well with the original semantic meaning of the input text. In practical terms, DiffuSEQ employs a Minimum Bayes Risk (MBR) decoding strategy, enhancing the quality and diversity of outputs by evaluating multiple candidate sequences and selecting those with optimal expected outcomes.\",\"subsystems_parts\":\"1. Embedding Layer: Converts discrete text sequences into continuous vector spaces for diffusion processing.\\n2. Partial Noising Module: Applies Gaussian noise to target sequence embeddings, enabling context preservation in conditional generation.\\n3. Reverse Denoising Module: Recovers original target text from noised embeddings using iterative denoising techniques.\\n4. Transformer Network: Models semantic relations between source and target, ensuring contextual accuracy in generated outputs.\\n5. Decoding Strategy (MBR): Enhances output quality and diversity by selecting optimal sequences from generated candidates.\",\"interactions\":\"These subsystems are intricately linked through the forward and reverse diffusion processes. Initially, the embedding layer ensures that both source and target sequences are in a compatible format for subsequent operations. The partial noising module selectively targets the embeddings of the target sequence, leaving the source intact to retain full context. As the denoising module restores the text, it coordinates with the transformer network, which imposes linguistic and contextual coherency through attention mechanisms. The entire system is further optimized during the final decoding stage, where the MBR strategy is applied to select output sequences that best match pre-defined quality criteria, thus integrating the model's inherent capacity for diversity with the need for textual precision.\",\"delta_vs_baseline\":\"DiffuSEQ fundamentally diverges from previous text-generating diffusion models by operating without external classifier constraints, thus removing the dependency on multiple pre-trained classifiers to ensure correct semantics in conditional generation tasks. By introducing partial noising‚Äîan innovative approach where only the target text is noised‚ÄîDiffuSEQ can emphasize the strong contextual ties of SEQ2SEQ tasks, which was a challenging endeavor using conventional autoregressive or non-autoregressive models that rely heavily on sequential predictions based on preceding words. Additionally, integration with MBR in selecting diverse outputs is notably different from standard diffusion methods.\",\"evidence_anchor\":\"In experiments, DiffuSEQ demonstrates its ability to achieve competitive or superior BLEU scores compared to traditional autoregressive and non-autoregressive models. Specifically, by increasing the candidate size in the MBR approach, the model showed enhanced performance beyond pretrained benchmarks, attesting to its capability for producing quality outputs with rich diversity. These results substantiate the theoretical and practical advantages claimed by adopting diffusion frameworks in SEQ2SEQ tasks.\",\"transferability\":\"The DiffuSEQ model, while specifically designed for SEQ2SEQ text generation, exemplifies a methodological approach that holds potential transferability across contexts where similar conditional generation patterns need implementing without reliance on external classifiers. However, integrating similar diffusion model strategies would necessitate suitable accommodations in domains other than text, especially those not innately reliant on sequence processing or differing inherently in structure from natural language.\"}"], "processed_at": "2025-09-01 17:28:19.478714", "iterations": 1, "domain": "core_extraction"}, "markdown_pages": [{"id": 1, "title": "DiffuSEQ: Sequence to Sequence Text Generation with Diffusion Models", "content": "# DiffuSEQ: Sequence to Sequence Text Generation with Diffusion Models\n\nShansan Gong ${ }^{1}$, Mukai $\\mathbf{L i}^{1}$, Jiangtao Feng ${ }^{1}$, Zhiyong $\\mathbf{W u}^{1}$, Lingpeng Kong ${ }^{2}$<br>${ }^{1}$ Shark-NLP, Shanghai AI Laboratory ${ }^{2}$ The University of Hong Kong<br>\\{gongshansan, limukai,fengjiangtao,wuzhiyong\\}@pjlab.org.cn<br>lpk@cs.hku.hk\n\n#### Abstract\n\nRecently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSEQ: a diffusion model designed for sequence-to-sequence (SEQ2SEQ) text generation tasks. Upon extensive evaluation over a wide range of SEQ2SEQ tasks, we find DiffuSEQ achieving comparable or even better performance than six established baselines, including a state-of-theart model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSEQ is its high diversity during generation, which is desired in many SEQ2SEQ tasks. We further include a theoretical analysis revealing the connection between DiffuSEQ and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks.\n\n## ## 1 INTRODUCTION\n\nAmong existing generative models, GAN (Goodfellow et al., 2014) suffers from the instability issue (Salimans et al., 2016), subjecting to mode collapse (Metz et al., 2017); VAE (Kingma \\& Welling, 2014) has to rely on surrogate objectives to approximate maximum likelihood training and Flow-based models (Dinh et al., 2017) has to use specialized architectures to construct reversible transform. Diffusion models (Ho et al., 2020; Nichol \\& Dhariwal, 2021) have circumvented several of these limitations and emerged as a new paradigm for generative models, theoretically underpinned by non-equilibrium thermodynamics (Sohl-Dickstein et al., 2015) and score-matching network (Song \\& Ermon, 2019). To date, the major breakthroughs are in domains using continuous signals, such as vision (Saharia et al., 2022a;b; Ramesh et al., 2022) and audio (Kong et al., 2020). However, extending continuous diffusion models to natural language remains an open challenge due to the inherently discrete nature of texts.\n\nOn the basis of unconditional generation in continuous space which is illustrated in Figure 1(a), existing efforts (Hoogeboom et al., 2021; Austin et al., 2021) start customizing diffusion models to text in discrete space on unconditional language modeling (i.e., free text generation). Diffusion-LM (Li et al., 2022), as in Figure 1(b), models texts in continuous space and proposes to use an extra-trained classifier as guidance (i.e., the condition signal $\\mathbf{x}$ ) to impose subtle changes (usually complex, finegrained constraints) on generated sentences. Nonetheless, these models do not naturally generalize to conditional language modeling (i.e., the model assigns probabilities $p(\\mathbf{w} \\mid \\mathbf{x})$ to sequences of words $\\mathbf{w}$ given $\\mathbf{x})$. In the more general sequence-to-sequence (SEQ2SEQ) setting where the condition $\\mathbf{x}$ is also a sequence of words, applying Diffusion-LM can be difficult. The reason is that classifiers are attributes-oriented, and we can not train hundreds-of-thousands classifiers to model the semantic meaning between conditions and generated sentences.\n\nSEQ2SEQ is an essential setting in NLP that covers a wide range of important tasks such as openended sentence generation, dialogue, paraphrasing, and text style transfer. In this paper, we propose\n\n[^0]\n[^0]:    ${ }^{1}$ Code is available at https://github.com/Shark-NLP/DiffuSeq\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: The demonstration of unconditional, classifier-guided, and classifier-free diffusion models.\n\nDIFFUSEQ, depicted in Figure 1(c), a classifier-free diffusion model that supports SEQ2SEQ text generation tasks. By modeling the conditional probability of the target sentence $\\mathbf{w}$ given context $\\mathbf{x}$ using one single model, one advantage of DIFFUSEQ is that this paradigm allows a complete model to fit data distribution and utilize conditional guidance, rather than depending on a separate classifier.\n\nDifferent from canonical generation approaches in an autoregressive (AR) left-to-right manner (Radford et al., 2019), DIFFUSEQ generates text tokens parallelly in the non-autoregressive (NAR) way. To corroborate the effectiveness of our DIFFUSEQ, we conduct experiments on four SEQ2SEQ tasks. Compared to AR and NAR models, which suffer from the \"degeneration\" problem (Holtzman et al., 2019) and rely on decoding strategies, DIFFUSEQ can achieve considerable sentence-level diversity without sacrificing the quality (see ¬ß 4.2).\n\nTo sum up, we make a series of technical and conceptual contributions: (a) we are the first to deploy the diffusion model on SEQ2SEQ text generation, and our proposed DIFFUSEQ as a conditional language model is trained end-to-end in a classifier-free manner; (b) we establish a theoretical connection among AR, NAR and DIFFUSEQ models, and justify DIFFUSEQ as an extension of iterative-NAR models; (c) with strong empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks."}, {"id": 2, "title": "2 Preliminary and Problem Statement", "content": "# 2 Preliminary and Problem Statement\n\nPreliminary. A diffusion model typically contains forward and reverse processes. Given a data point sampled from a real-world data distribution $\\mathbf{z}_{0} \\sim q(\\mathbf{z})$, the forward process gradually corrupts $\\mathbf{z}_{0}$ into a standard Gaussian noise $\\mathbf{z}_{T} \\sim \\mathcal{N}(0, \\mathbf{I})$. For each forward step $t \\in[1,2, \\ldots, T]$, the perturbation is controlled by $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t} ; \\sqrt{1-\\beta_{t}} \\mathbf{z}_{t-1}, \\beta_{t} \\mathbf{I}\\right)$, with $\\beta_{t} \\in(0,1)$ as different variance scales. Once the forward process is completed, the reverse denoising process tries to gradually reconstruct the original data $\\mathbf{z}_{0}$ via sampling from $\\mathbf{z}_{T}$ by learning a diffusion model $f_{\\theta}$.\n\nProblem Statement. Many recent efforts have been devoted to adapting diffusion models to discrete texts (See ¬ß 5). However, they all focus on unconditional sequence modeling. In this paper, we target the sequence-to-sequence text generation tasks. In particular, given a $m$-length source sequence $\\mathbf{w}^{s}=\\left\\{w_{1}^{s}, \\ldots, w_{m}^{s}\\right\\}$, we aim to learn a diffusion model that can produce a $n$-length target sequence $\\mathbf{w}^{g}=\\left\\{w_{1}^{g}, \\ldots, w_{n}^{g}\\right\\}$ conditioning on the source sequence.\n\n## ## 3 DIFFUSEQ\n\nWe propose DIFFUSEQ to extend vanilla diffusion models to learn conditional text generation (as shown in Figure 2), concerning the model architecture and the training objective.\n\nForward Process with Partial Noising. In the beginning of forward process, we follow DiffusionLM (Li et al., 2022) to design an embedding function $\\operatorname{EMB}\\langle\\mathbf{w}\\rangle$ to map the discrete text $\\mathbf{w}$ into a continuous space. In particular, given a pair of sequence $\\mathbf{w}^{x}$ and $\\mathbf{w}^{g}$, DIFFUSEQ learns a unified\n\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: The diffusion process of our conditional diffusion language model DiffuSeq. Given the source $\\mathbf{w}^{x}$ and the target $\\mathbf{w}^{y}$, we pair-wisely transform them into continuous space $\\mathbf{z}_{0}$. The partial Gaussian noise is iteratively added on the target space of $\\mathbf{z}_{t}$.\nfeature space of $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$ by embedding transformation and concatenation as $\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right)=$ $\\left[\\operatorname{EMB}\\left(w_{1}^{x}\\right), \\ldots, \\operatorname{EMB}\\left(w_{m}^{x}\\right), \\operatorname{EMB}\\left(w_{1}^{y}\\right), \\ldots, \\operatorname{EMB}\\left(w_{n}^{y}\\right)\\right] \\in \\mathbb{R}^{(m+n) \\times d}$. The transformation allows us to adapt discrete textual input into the standard forward process, by extending the original forward chain to a new Markov transition $q_{\\phi}\\left(\\mathbf{z}_{0} \\mid \\mathbf{w}^{x \\oplus y}\\right)=\\mathcal{N}\\left(\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right), \\beta_{0} \\mathbf{I}\\right)$.\nWe denote $\\mathbf{z}_{t}=\\mathbf{x}_{t} \\oplus \\mathbf{y}_{t}$ to simplify the wordings, where $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ represent parts of $\\mathbf{z}_{t}$ that belong to $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$, respectively. For each forward step $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}\\right)$, we gradually inject noise into last step's hidden state $\\mathbf{z}_{t-1}$ to obtain $\\mathbf{z}_{t}$. Unlike conventional diffusion models that corrupt the whole $\\mathbf{z}_{t}$ (both $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ ) without distinction, we only impose noising on $\\mathbf{y}_{t}$. This modification (termed partial noising) allows us to adapt diffusion models for conditional language modeling.\n\nReverse Process with Conditional Denoising. The ultimate goal of the reverse process is to recover the original $\\mathbf{z}_{0}$ by denoising $\\mathbf{z}_{t}: p_{\\theta}\\left(\\mathbf{z}_{0: T}\\right):=p\\left(\\mathbf{z}_{T}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)$. We model the learning process $p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t-1} ; \\mu_{\\theta}\\left(\\mathbf{z}_{t}, t\\right), \\sigma_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right)$ using the proposed diffusion model DiffuSeq: $f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)$, where the $\\mu_{\\theta}(\\cdot)$ and $\\sigma_{\\theta}(\\cdot)$ is the parameterization of the predicted mean and standard deviation of $q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)$ in forward process, derived using Bayes' rule. The detailed derivations are in Appendix A. With the partial nosing strategy adopted in the forward process, we can impose the input as the condition when denoising as shown in Figure 1. The proposed conditional denoising is classifier-free by nature: we do not require extra-trained classifiers to control the denoising process.\nSpecifically, we use a transformer architecture to model $f_{\\theta}$, which spontaneously models the semantic relation between $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$. We compute the variational lower bound $\\left(\\mathcal{L}_{\\text {VLB }}\\right)$ following the original diffusion process. $\\mathcal{L}_{\\text {round }}$ corresponds to rounding operation in Figure 2.\n\n$$\n\\begin{aligned}\n\\mathcal{L}_{\\mathrm{VLB}} & =\\mathbb{E}_{q\\left(\\mathbf{z}_{1: T} \\mid \\mathbf{z}_{0}\\right)}[\\underbrace{\\log \\frac{q\\left(\\mathbf{z}_{T} \\mid \\mathbf{z}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{T}\\right)}}_{\\mathcal{L}_{T}}+\\sum_{t=2}^{T} \\underbrace{\\log \\frac{q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{0}, \\mathbf{z}_{t}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)}}_{\\mathcal{L}_{t-1}} \\\\\n& +\\underbrace{\\log \\frac{q_{\\phi}\\left(\\mathbf{z}_{0} \\mid \\mathbf{w}^{x \\oplus y}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{0} \\mid \\mathbf{z}_{1}\\right)}}_{\\mathcal{L}_{0}}-\\underbrace{\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)}_{\\mathcal{L}_{\\text {round }}}]\n\\end{aligned}\n$$\n\n## We further simplify the training objective as follows (details in Appendix A):\n\n$$\n\\begin{aligned}\n\\min _{\\theta} \\mathcal{L}_{\\mathrm{VLB}} & =\\min _{\\theta}\\left[\\sum_{t=2}^{T}\\left\\|\\mathbf{z}_{0}-f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right)-f_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}-\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)\\right] \\\\\n& \\rightarrow \\min _{\\theta}\\left[\\sum_{t=2}^{T}\\left\\|\\mathbf{y}_{0}-\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{y}\\right)-\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}+\\mathcal{R}\\left(\\left\\|\\mathbf{z}_{0}\\right\\|^{2}\\right)\\right]\n\\end{aligned}\n$$\n\nhere we use $\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)$ to denote the fractions of recovered $\\mathbf{z}_{0}$ corresponding to $\\mathbf{y}_{0}$. Note that although in the first term, we only compute the loss w.r.t $\\mathbf{y}_{0}$, due to the attention mechanism in the transformer, the reconstruction of $\\mathbf{y}_{0}$ also takes $\\mathbf{x}_{0}$ into account, thus the gradients from the first term\n\nwill also affect the learning of $\\mathbf{x}_{0}$. The mathematically equivalent regularization term $\\mathcal{R}\\left(\\left\\|\\mathbf{z}_{0}\\right\\|^{2}\\right)$ ) regularize the embedding learning. We further share the embedding function between source and target sequences, enabling the training of two different feature spaces jointly. This sets DiffuSeq away from existing solutions in vision such as GLIDE (Nichol et al., 2022).\n\nTraining and Inference Methods. In our preliminary experiments, we find that the high diversity in NLP datasets and long diffusion steps often result in insufficient training. We hypothesize the reason is that sampling step $t$ uniformly causes unnecessary noise in the $\\mathcal{L}_{\\text {VLB }}$ objective. We hence employ importance sampling (Nichol \\& Dhariwal, 2021) to address this problem.\n\n$$\n\\mathcal{L}_{\\mathrm{VLB}}=\\mathbb{E}_{t \\sim p_{t}}\\left[\\frac{\\mathcal{L}_{t}}{p_{t}}\\right], p_{t} \\propto \\sqrt{\\mathbb{E}\\left[\\mathcal{L}_{t}^{2}\\right]}, \\sum_{t=0}^{T-1} p_{t}=1\n$$\n\nIntuitively, the importance-weighted sampling algorithm will spend more steps on diffusion steps with larger $\\mathcal{L}_{t}$, and vice versa.\nTo conduct SEQ2SEQ generation given the condition $\\operatorname{EMB}\\left(\\mathbf{w}^{x}\\right)$, we randomly sample $\\mathbf{y}_{T} \\sim$ $\\mathcal{N}(0, I)$ and concatenate $\\mathbf{y}_{T}$ with $\\operatorname{EMB}\\left(\\mathbf{w}^{x}\\right)$ to obtain $\\mathbf{z}_{T}$. We can now repeat the reverse process until we arrive at $\\mathbf{z}_{0}$. At each sampling step, an anchoring function is executed towards reparameterized $\\mathbf{z}_{t}$. Specifically, the anchoring function: (a) operates rounding on $\\mathbf{z}_{t}$ to map it back to word embedding space following Li et al. (2022); (b) replaces the part of recovered $\\mathbf{z}_{t-1}$ that belongs to $\\mathbf{w}^{x}$ with the original $\\mathbf{x}_{0}$, considering that this part is recovered from corrupted $\\mathbf{z}_{t}$ via $f_{\\theta}$ and not strictly equals to $\\mathbf{x}_{0}$. Note that (b) is designed for DiffuSeq.\n\nTo improve the quality of generation, we apply the widely used Minimum Bayes Risk (MBR) decoding strategy (Koehn, 2004). We first generate a set of candidate samples $\\mathcal{S}$ from different random seeds of DiffuSeq and select the best output sequence that achieves the minimum expected risk under a meaningful loss function (e.g. BLEU or other cheaper metrics like precision). In practice, we use the negative BLEU score in our implementation.\n\nConnections to AR, Iter-NAR, and Fully-NAR Models. To better understand the behavior of DiffuSeq, we give the theoretical connection to autoregressive (AR), iterative non-autoregressive (iter-NAR), and fully non-autoregressive (fully-NAR) models. We argue that DiffuSeq can be seen as an extension of iter-NAR model. Detailed graphical learning discrepancies of these four cases are discussed in Appendix B for reference.\nAR models learn $p\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)$ by autoregressive decomposition based on left-context:\n\n$$\np_{\\mathrm{AR}}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\underbrace{p\\left(w_{1}^{y} \\mid \\mathbf{w}^{x}\\right)}_{\\text {initial prediction }} \\underbrace{\\prod_{i=1, \\ldots, n-1} p\\left(w_{i+1}^{y} \\mid \\mathbf{w}_{1: i}^{y}, \\mathbf{w}^{x}\\right)}_{\\text {progressive left-context prediction }},\n$$\n\nwhile fully-NAR models (Gu et al., 2018; Qian et al., 2021) learn the conditional probability given independent assumption for fast inference:\n\n$$\np_{\\text {fully-NAR }}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\prod_{i=1, \\ldots, n} p\\left(w_{i}^{y} \\mid \\mathbf{w}^{x}\\right)\n$$\n\nTo make a better analogy to AR and NAR models, we use a lossless way to formulate iterative NAR models (Gu et al., 2019; Ghazvininejad et al., 2019) by introducing a series of intermediate sequences $\\mathbf{w}_{1: K-1}^{y}, \\mathbf{w}_{K}^{y}=\\mathbf{w}^{y}$ with $K$ editable iterations:\n\n$$\np_{\\text {iter-NAR }}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} \\underbrace{\\prod_{i=1, . . n} p\\left(w_{1, i}^{y} \\mid \\mathbf{w}^{x}\\right)}_{\\text {initial prediction }} \\underbrace{\\prod_{k=1 . . K-1} \\prod_{i=1, . . n} p\\left(w_{k+1, i}^{y} \\mid \\mathbf{w}_{k, 1: n}^{y}, \\mathbf{w}^{x}\\right)}_{\\text {progressive full-context prediction }}\n$$\n\nPrevious study (Huang et al., 2022) shows that there is a gap called conditional total correlation between AR Eq. (4) and fully-NAR Eq. (5) learning paradigms, because of lossy decomposition of NAR models. However, when comparing iter-NAR Eq. (6) with AR Eq. (4) models, they both can be factorized into an initial prediction term and a progressive prediction process based on different context (i.e. left-context in AR and full-context in iter-NAR), and the discrepancy pointed out by\n\nHuang et al. (2022) is therefore closed in iter-NAR assuming sufficient steps. By showing DIFFUSEQ is an extension of the iter-NAR model, we offer a justification that it will not suffer from the conditional total correlation for the same reason.\n\nA straight-forward way to formulate pure continuous diffusion models is to introduce a series of Gaussian noise-corrupted features along with diffusion steps: $\\mathbf{y}_{1: T-1}, \\mathbf{y}_{0}=\\mathbf{y}, \\mathbf{y}_{T} \\sim \\mathcal{N}(0, \\mathbf{I})$.\n\n$$\np_{\\text {diffusion }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right)=\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)\n$$\n\nwhere $p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)$ describes the diffusion step on continuous representations $\\mathbf{y}$. The rounding operation in DIFFUSEQ maps the continuous vectors $\\mathbf{y}$ to discrete $\\mathbf{w}^{y}$ for each time step $t$, we in addition introduce this into Eq. (7):\n\n$$\n\\begin{aligned}\np_{\\text {DIFFUSEQ }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) & =\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) \\\\\n& =\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} p\\left(\\mathbf{y}_{t} \\mid \\mathbf{w}_{t+1}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\nBy rearranging Eq. (8) into Eq. (9), we can see DIFFUSEQ can be seen as a more generalized form of iter-NAR Eq. (6) before marginalizing out $\\left\\{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}\\right\\}$, despite the different initialization of $\\mathbf{y}_{T}{ }^{2}$. A more detailed derivation is shown in Appendix C.\n\n## # 4 EXPERIMENTS\n\nWe conduct experiments to validate the effectiveness of DIFFUSEQ on four different tasks, against six strong AR/NAR baselines.\n\n## ### 4.1 EXPERIMENTAL SETUP\n\nTasks and Datasets. SEQ2SEQ generation covers a wide range of tasks, among which we choose four typical and popular tasks. Open domain dialogue requires models to generate informative responses given a dialogue context. We use Commonsense Conversation Dataset (Zhou et al., 2018), which is extracted from Reddit single-round dialogs, with over 3 million conversational pairs. Question generation(QG) aims to generate questions given a context as input. To obtain sufficient training samples, we use the dataset Quasar-T (Dhingra et al., 2017) preprocessed by Lin et al. (2018), and then generate document-question pairs to obtain 119K training samples (details in Appendix D.1). Text simplification aims to revise the complex text into sequences with simplified grammar and word choice. Jiang et al. (2020) constructs a corpus consisting of 677 K complexsimple sentences with revision alignment. Paraphrase task generates an alternative surface form in the same language expressing the same semantic content. We adopt widely used QQP ${ }^{3}$ sourced from the community question answering forum Quora, with 147K positive pairs.\n\nBaselines. We consider three groups of models as baselines, covering both AR and NAR architectures. The first group of methods adopts encoder-decoder architecture (Cho et al., 2014) which is well-studied for SEQ2SEQ tasks, and we conduct experiments on two popular models: GRU with attention and Transformer (Vaswani et al., 2017). The second group is the finetuned large pre-trained language model (PLM), among which GPT2 (Radford et al., 2019) has demonstrated great success in almost all SEQ2SEQ tasks. We further compare to GPVAE (Du et al., 2022), which augments a pre-trained T5 (Raffel et al., 2020) with VAE to improve the generation diversity. For the last group of baselines, we consider LevT (Gu et al., 2019), a widely used, strong iterative NAR model. All baselines are trained following instructions in their papers, and details can be found in Appendix D.2.\n\n[^0]\n[^0]:    ${ }^{2}$ For NAR models, $\\mathbf{y}_{T}$ is uniform copied from the source sentence or $u n k$ 's token embedding (Gu et al., 2018); for diffusion models, $\\mathbf{y}_{T}$ is sampled from normal distribution $\\mathcal{N}(0, \\mathbf{I})$.\n${ }^{3}$ https://www.kaggle.com/c/quora-question-pairs\n\nTable 1: The overall results of different methods on different SEQ2SEQ tasks. The first group $\\diamond$ of methods adopt autoregressive encoder-decoder architecture and the second group $\\bullet$ is the finetuned large pre-trained language model (also in autoregressive manner) while the last group $\\ddagger$ is non-autoregressive. The best results are bold, and the best results without PLMs are underlined.\n\n| Tasks | Methods | BLEU $\\uparrow$ | R-L $\\uparrow$ | Score $\\uparrow$ | dist-1 $\\uparrow$ | selfB $\\downarrow / \\operatorname{div}-4 \\uparrow$ | Len |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Open <br> Domain <br> Dialogue | GRU-attention ${ }^{\\circ}$ | 0.0068 | 0.1054 | 0.4128 | 0.8998 | 0.8008/0.1824 | 4.46 |\n|  | Transformer-base ${ }^{\\circ}$ | 0.0189 | 0.1039 | 0.4781 | 0.7493 | 0.3698/0.6472 | 19.5 |\n|  | GPT2-base FT ${ }^{\\bullet}$ | 0.0108 | 0.1508 | 0.5279 | 0.9194 | 0.0182/0.9919 | 16.8 |\n|  | GPT2-large FT ${ }^{\\bullet}$ | 0.0125 | 0.1002 | 0.5293 | 0.9244 | 0.0213/0.9938 | 16.8 |\n|  | GPVAE-T5 ${ }^{\\bullet}$ | 0.0110 | 0.1009 | 0.4317 | 0.5625 | 0.3560/0.5551 | 20.1 |\n|  | NAR-LevT ${ }^{\\ddagger}$ | 0.0158 | 0.0550 | 0.4760 | 0.9726 | 0.7103/0.1416 | 4.11 |\n|  | DiffuSEQ (Ours) ${ }^{\\ddagger}$ | 0.0139 | 0.1056 | 0.5131 | 0.9467 | 0.0144/0.9971 | 13.6 |\n| Question <br> Generation | GRU-attention ${ }^{\\circ}$ | 0.0651 | 0.2617 | 0.5222 | 0.7930 | 0.9999/0.3178 | 10.1 |\n|  | Transformer-base ${ }^{\\circ}$ | 0.1663 | 0.3441 | 0.6307 | 0.9309 | 0.3265/0.7720 | 10.3 |\n|  | GPT2-base FT ${ }^{\\bullet}$ | 0.0741 | 0.2714 | 0.6052 | 0.9602 | 0.1403/0.9216 | 10.0 |\n|  | GPT2-large FT ${ }^{\\bullet}$ | 0.1110 | 0.3215 | 0.6346 | 0.9670 | 0.2910/0.8062 | 9.96 |\n|  | GPVAE-T5 ${ }^{\\bullet}$ | 0.1251 | 0.3390 | 0.6308 | 0.9381 | 0.3567/0.7282 | 11.4 |\n|  | NAR-LevT ${ }^{\\ddagger}$ | 0.0930 | 0.2893 | 0.5491 | 0.8914 | 0.9830/0.4776 | 6.93 |\n|  | DiffuSEQ (Ours) ${ }^{\\ddagger}$ | 0.1731 | 0.3665 | 0.6123 | 0.9056 | 0.2789/0.8103 | 11.5 |\n| Text <br> Simpli- <br> fication | GRU-attention ${ }^{\\circ}$ | 0.3256 | 0.5602 | 0.7871 | 0.8883 | 0.9998/0.3313 | 18.9 |\n|  | Transformer-base ${ }^{\\circ}$ | 0.2693 | 0.4907 | 0.7381 | 0.8886 | 0.6924/0.5095 | 18.5 |\n|  | GPT2-base FT ${ }^{\\bullet}$ | 0.3083 | 0.5461 | 0.8021 | 0.9439 | 0.5444/0.6047 | 16.1 |\n|  | GPT2-large FT ${ }^{\\bullet}$ | 0.2693 | 0.5111 | 0.7882 | 0.9464 | 0.6042/0.5876 | 15.4 |\n|  | GPVAE-T5 ${ }^{\\bullet}$ | 0.3392 | 0.5828 | 0.8166 | 0.9308 | 0.8147/0.4355 | 18.5 |\n|  | NAR-LevT ${ }^{\\ddagger}$ | 0.2052 | 0.4402 | 0.7254 | 0.9715 | 0.9907/0.3271 | 8.31 |\n|  | DiffuSEQ (Ours) ${ }^{\\ddagger}$ | 0.3622 | 0.5849 | 0.8126 | 0.9264 | 0.4642/0.6604 | 17.7 |\n| Paraphrase | GRU-attention ${ }^{\\circ}$ | 0.1894 | 0.5129 | 0.7763 | 0.9423 | 0.9958/0.3287 | 8.30 |\n|  | Transformer-base ${ }^{\\circ}$ | 0.2722 | 0.5748 | 0.8381 | 0.9748 | 0.4483/0.7345 | 11.2 |\n|  | GPT2-base FT ${ }^{\\bullet}$ | 0.1980 | 0.5212 | 0.8246 | 0.9798 | 0.5480/0.6245 | 9.67 |\n|  | GPT2-large FT ${ }^{\\bullet}$ | 0.2059 | 0.5415 | 0.8363 | 0.9819 | 0.7325/0.5020 | 9.53 |\n|  | GPVAE-T5 ${ }^{\\bullet}$ | 0.2409 | 0.5886 | 0.8466 | 0.9688 | 0.5604/0.6169 | 9.60 |\n|  | NAR-LevT ${ }^{\\ddagger}$ | 0.2268 | 0.5795 | 0.8344 | 0.9790 | 0.9995/0.3329 | 8.85 |\n|  | DiffuSEQ (Ours) ${ }^{\\ddagger}$ | 0.2413 | 0.5880 | 0.8365 | 0.9807 | 0.2732/0.8641 | 11.2 |\n\nEvaluation. We evaluate the generated sequences from two aspects: quality and diversity. To evaluate the quality, we use the standard metric BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) score. Since string-similarity-based metrics can be unsatisfactory for open-ended generation, we also report BERTScore (Zhang et al., 2019) that assesses the semantic similarity between generated sentences and references. Details are in Appendix D.4. Higher scores of BLEU, ROUGE and BERTScore reflect better performance. As for diversity, we use distinct unigram (dist-1) to measure intra-diversity within each generated sentence, where the lower dist-1 indicates that the generated sentence contains more repeated words. For sentence-level diversity evaluation, we consider sentence-level self-BLEU (Zhu et al., 2018) to measure the n-gram overlap between the set of outputs w.r.t one source sentence, and we additionally use diverse 4-gram (div-4) (Deshpande et al., 2019) to measure the ratio of distinct 4-grams in the set of outputs per source sentence. The lower self-BLEU and higher div-4 suggest higher diversity of generation. For each method including DiffuSEQ, we generate 3 samples for each source sentence to compute the diversity metrics.\n\nImplementation Details. Our DiffuSeq is based on the 12 layers of Transformer with 12 attention heads, where the time step embedding is plugged akin to the position embedding. The maximum sequence length is 128 , with embedding dimension $d=128$, diffusion steps $T=2,000$ and a square-root noise schedule. To reduce the out-of-vocabulary generation, we apply Byte Pair Encoding (Sennrich et al., 2016) to construct the vocabulary. After conducting the diversity beam\n\nTable 2: Sample outputs in QQP test set, conditioned on the same $\\mathbf{x}$.\n\n| Original sentence: How do I make friends. | Paraphrase reference: How to make friends ? |  |\n| :-- | :-- | :-- |\n| GPT2-large finetune | GPVAE-T5 | DIFFUSEQ |\n| How can I make friends? | How can I make friends? | How can I make friends better? |\n| How can I make friends? | How do I make friends? | How can I make friends? |\n| How can I make friends? | How can I make friends? | How do you make friends? |\n| How do I make friends and | How can I make friends? | What is the best way to make friends? |\n| keep them? | What's the best way to make | How can I make friends and more |\n|  | friends and make make friends? | something? |\n\nsearch (DBS) (Vijayakumar et al., 2016) for the Transformer-base model and GPT model, we find that DBS does not always promote diversity over temperature sampling and therefore we list the best diversity results. We compute the accuracy metrics of DIFFUSEQ using MBR with the size of candidate samples $|\\mathcal{S}|=10$. The experiment is deployed on NVIDIA A100 Tensor Core GPUs, and we use 4 GPUs on training and single GPU on sampling.\n\n## # 4.2 MAIN RESULTS\n\nAs shown in Table 1, we conclude that DiffuSeq achieves comparable or even higher generation quality compared with strong baselines. At the same time, DiffuSeq consistently demonstrates its superiority in generating diverse outputs given the same input sequence.\n\nAs we can see from Table 1, DiffuSeq wins competitions over at least one quality metric against 6 baselines $\\times 4$ tasks. Although NAR models such as LevT can also outperform AR baselines sometimes, they still lag well behind DIFFUSEQ by large margins (i.e., relative improvements over $50 \\%$ for BLEU in QG task and R-L in Dialogue task). Even compared with pre-trained then finetuned GPT2 models, DiffuSeq still delivers superior performance than the base variant, and is comparable with the large variant, which has 8.2 times more parameters than DiffuSeq. These empirical results amply support our findings in $\\S 3$, where we theoretically analyze the potential of diffusion models in modeling text sequences compared with AR models given sufficient diffusion steps.\n\nDiffuSeq, as a member of the deep generative model family, also exhibit the capacity to generate highly diverse sequences. As suggested by self-BLEU (lower is better) and div-4 (higher is better), in almost all cases, DiffuSeq significantly outperforms 4 AR baselines in terms of sentence-level diversity (i.e., producing diverse outputs given the same input). For diversity in word choice within one sentence, we consider dist-1: a higher dist-1 indicates less repetition within a sentence. As we can see from Table 1, DiffuSeq has less repetition compared with encoder-decoder methods, but still fall behind the pre-trained GPT2 models (the same situation with BERTScore). These results suggest there is still room for improvement (e.g., use pre-training techniques) in diffusion models' token-level choice. Different from NAR-LevT, DiffuSeq does not rely on an extra length prediction module but automatically decides by the padding token instead and is able to generate longer output sentences, indicated by the last column for average generation length.\n\nIn Table 2, we provide examples to showcase DiffuSeq's ability to generate diverse samples. More examples can be found in Appendix D.5.\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: The increase of BLEU score with different candidate sizes $|\\mathcal{S}|$.\n![img-3.jpeg](img-3.jpeg)\n\nFigure 4: Trade-off between quality and diversity (details in Appendix D.3).\n\n![img-4.jpeg](img-4.jpeg)\n\nFigure 5: The curve of BLEU/div-4 score along with generation process (percentage of steps).\n![img-5.jpeg](img-5.jpeg)\n\nFigure 6: The BLEU and inference speed of DIFFUSEQ and GPT2-large.\n\n## # 4.3 ANALYSIS\n\nWe conduct a series of analysis to investigate the effectiveness of different aspects in DIFFUSEQ.\nDiversity Ensures Quality. Generating high-quality texts with high diversity is an important requirement for many text generation applications and the trade-off between quality and diversity is always a critical concern in open-ended NLG tasks (Zhang et al., 2021). Different from AR models relying on the decoding strategy like temperature and nucleus sampling (Holtzman et al., 2019) and VAE models sampling latent variable from Gaussian Prior, the natural advantage of DIFFUSEQ is to generate different sentences along with a series of random Gaussian noise. In Figure 4, we elucidate that DIFFUSEQ have better trade-off between generation quality (BLEU) and sentence-level diversity (div-4). Here we further demonstrate that the high diversity provided by DIFFUSEQ can be turned into better quality.\n\nMBR is a common strategy to improve generation quality by aggregating and ranking candidate sequences, and we find that the upper bound of MBR is decided by a diversified candidate set. To valid this, we simultaneously apply MBR on both DIFFUSEQ and GPT2 with various candidate sizes $|\\mathcal{S}|$. The results are shown in Figure 3. As we can see, DIFFUSEQ lags behind GPT2 without using MBR $(|\\mathcal{S}|=1)$ or with a small candidate set $(|\\mathcal{S}|=3)$. However, as $|\\mathcal{S}|$ increases, DIFFUSEQ starts to outperform GPT2 by an increasing margin. The reason is that autoregressive models like GPT2 tend to generate highly similar candidates (as discussed in $\\S$ 4.2), which impedes the effectiveness of MBR. As $|\\mathcal{S}|$ increases to 20, DIFFUSEQ still shows better rising trends than GPT2. Our findings also stress the importance of better ranking methods in diffusion research.\n\nStep-wise Analysis against Iterative NAR. Given the underlying theoretical connection between iterative NAR and DIFFUSEQ discussed in $\\S 3$, we empirically investigate the behavior of LevT and DIFFUSEQ by analyzing their step-wise quality (i.e. BLEU) and diversity (i.e. div-4) curves. As is suggested in Figure 5, LevT grows fiercely in quality at the very beginning of generation, and quickly slows down in the successive refinement process. But DIFFUSEQ behaves differently, with BLEU score growing slowly at first, increasing rapidly as the diffusion process progresses and finally surpassing LevT. It is also observed that the diversity of both LevT and DIFFUSEQ is determined at the very early stage regardless of future refinement or diffusion, where DIFFUSEQ consistently outperforms LevT on diversity at any stage of generation. We conjecture that DIFFUSEQ explores more possible results at the first half of generation process, and soon converges to several potential candidates when it is closed to the end of steps. In this case, DIFFUSEQ shows its capacity to take both generation quality and diversity into consideration, and this is the capacity that iterative-NAR and even AR models can not obtain, due to the different learning paradigms.\n\nInference Speed. The slow sampling speed is one of the major concerns about diffusion models. Here we fix the number of diffusion steps during training for DIFFUSEQ while shrinking the inference steps following DDIM (Song et al., 2020). As we can see from Figure 6, when reducing the inference to 1,000 diffusion steps on single GPU, DIFFUSEQ achieves a higher BLEU score than GPT2-large yet registers a closer inference speed to GPT2-large.\n\nEffectiveness of Joint Training. In DIFFUSEQ, the representations of $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$ are jointly trained using the same embedding function $\\operatorname{EMB}(\\cdot)$ (stated in $\\S 3$ ). To validate the effectiveness\n\nTable 3: Results with or without joint training for Question Generation task.\n\n| Setting | BLEU $\\uparrow$ | R-L $\\uparrow$ | Score $\\uparrow$ | selfB $\\downarrow /$ div- $4 \\uparrow$ |\n| :-- | :-- | :-- | :-- | :-- |\n| DIFFUSEQ (w/o reranking) | 0.1567 | 0.3484 | 0.5947 | $0.2789 / 0.8103$ |\n| Fix $\\operatorname{EMB}\\left(\\mathbf{w}^{x}\\right)$ as pre-trained | 0.0110 | 0.0687 | 0.3769 | $0.0174 / 0.9376$ |\n\nof this joint training strategy, we compared it with the training strategy commonly used in text-toimage diffusion models (Nichol et al., 2022; Ramesh et al., 2022). In particular, we decouple the training of $\\operatorname{EMB}\\left(\\mathbf{w}^{x}\\right)$ and $\\operatorname{EMB}\\left(\\mathbf{w}^{y}\\right)$ by replacing $\\operatorname{EMB}\\left(\\mathbf{w}^{x}\\right)$ with representations extracted from a pre-trained BERT-tiny model (Turc et al., 2019). From Table 3, we find that the decoupled training strategy results in poor performance."}, {"id": 3, "title": "5 Related Work", "content": "# 5 Related Work\n\nDiffusion Models for Text Modeling. Text-to-Image generation using diffusion models has developed many potential applications. Models such as Imagen (Saharia et al., 2022b) and DALLE (Ramesh et al., 2022) are usually two-staged relying on the pre-trained models, requiring the alignment between the embedding vectors from two sources. GLIDE (Nichol et al., 2022) explores diffusion model with classifier-free (Ho \\& Salimans, 2022) guidance by setting guidance scale during training. The target space of these models is not discrete text space but stable vectors of pixel values. There are other works of diffusion on text generation, but they stick to the original encoderdecoder architecture and the diffusion process is interspersed on the decoder (Savinov et al., 2021), or the latent space (Yu et al., 2022).\n\nFor text generation using the diffusion models, Hoogeboom et al. (2021) introduce the multinomial diffusion for character-level text generation, the forward categorical noise is applied through the Markov transition matrix. Austin et al. (2021) generalize discrete text diffusion models by introducing the absorbing state ( (MASK )). However, discrete diffusion models may suffer from the scaling of the one-hot row vectors, and they only generate text samples unconditionally in discrete space. Diffusion-LM (Li et al., 2022) and Analog Bits (Chen et al., 2022) propose a new language model diffused on the continuous latent representations, with different mapping functions that connect the discrete and continuous space of texts. Compared with our work, we focus on the SEQ2SEQ diffusion models for text generation in the continuous space and our work is the first to explore this setting to the best of our knowledge.\n\nDiffusion Models for Conditional Generation. Related to conditional-VAE (Zhao et al., 2017), we can consider the latent encoded input $\\mathbf{x}$ as a condition. Diffusion-LM (Li et al., 2022) adopts the plug-and-play approaches (Dathathri et al., 2020) to compose fine-grained constraints on the generated sentences, but it fails to condition on the whole source sentence in SEQ2SEQ tasks. Noted that this controllable generation method is orthogonal to our DIFFSEQ, in other words, we can further add classifier-guided constraints on the SEQ2SEQ output to further control the text generation. There are other conditional diffusion models on the time series prediction like CSDI (Tashiro et al., 2021) or audio generation like WaveGrad (Chen et al., 2021), but their class conditions are usually attributes that are easy to model, while the contextual texts as conditions are much more complex.\n\n## ## 6 CONCLUSIONS\n\nWe propose DIFFUSEQ to tackle SEQ2SEQ tasks in a diffusion way, which contains the strong potential to achieve better generation quality and diversity trade-off. The capability enables favorable characteristics of DIFFUSEQ to further enhance the quality of final results, by leveraging a minimum Bayes risk decoding algorithm. Besides, we theoretically connect the AR and NAR models to DIFFUSEQ, and show that DIFFUSEQ is a powerful extension of iterative-NAR model. The empirical results demonstrate that DIFFUSEQ is also a powerful model for text generation, matching or even surpassing competitive AR, iterative NAR, and large-scale pre-trained models on quality and diversity. Given the limited progress of current diffusion models on text generation, our study addresses promising achievements by such a new sequence-to-sequence learning paradigm.\n\n## # ACKNOWLEDGMENTS\n\nWe would like to thank the anonymous reviewers and other peers for their valuable advice, and we also acknowledge Chenxin An's efforts to update the generation results for the Transformer-base model on QG and Paraphrasing tasks. This work is partially supported by the Shanghai Committee of Science and Technology (Grant No. 21DZ1100100) and the joint research scheme of the National Natural Science Foundation of China (NSFC) and the Research Grants Council (RGC) under grant number N_HKU714/21.\n\n## ## REFERENCES\n\nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 2021.\n\nNanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In International Conference on Learning Representations, ICLR, 2021.\n\nTing Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022.\n\nKyunghyun Cho, Bart van Merrienboer, √áaglar G√ºl√ßehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing, EMNLP, 2014.\n\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. In 8th International Conference on Learning Representations, ICLR, 2020.\n\nAditya Deshpande, Jyoti Aneja, Liwei Wang, Alexander G Schwing, and David Forsyth. Fast, diverse and accurate image captioning guided by part-of-speech. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10695-10704, 2019.\n\nBhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. Quasar: Datasets for question answering by search and reading. arXiv preprint arXiv:1707.03904, 2017.\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In International Conference on Learning Representations, ICLR, 2017.\n\nWanyu Du, Jianqiao Zhao, Liwei Wang, and Yangfeng Ji. Diverse text generation via variational encoder-decoder models with gaussian process priors. In Proceedings of 6th Workshop on Structured Prediction for NLP of the Association for Computational Linguistics, ACL, 2022.\n\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 6112-6121. Association for Computational Linguistics, 2019.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014.\n\nJiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. In International Conference on Learning Representations, ICLR, 2018.\n\nJiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. Advances in Neural Information Processing Systems, 2019.\n\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 2020.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, ICLR, 2019.\n\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr√©, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 2021.\n\nFei Huang, Tianhua Tao, Hao Zhou, Lei Li, and Minlie Huang. On the learning of non-autoregressive transformers. In International Conference on Machine Learning, ICML, 2022.\n\nChao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. Neural crf model for sentence alignment in text simplification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL, 2020.\n\nDiederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on Learning Representations, ICLR, 2014.\n\nPhilipp Koehn. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 conference on empirical methods in natural language processing, pp. 388-395, 2004.\n\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, ICLR, 2020.\n\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. arXiv preprint arXiv:2205.14217, 2022.\n\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, 2004.\n\nYankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. Denoising distantly supervised opendomain question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL, 2018.\n\nLuke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. In International Conference on Learning Representations, ICLR, 2017.\n\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, ICML, 2022.\n\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, ICML, 2021.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL, 2002.\n\nLihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, and Lei Li. Glancing transformer for non-autoregressive neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nChitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, 2022a.\n\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b.\n\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, 2016.\n\nNikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van den Oord. Stepunrolled denoising autoencoders for text generation. In International Conference on Learning Representations, ICLR, 2021.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715-1725. Association for Computational Linguistics, 2016.\n\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, ICML, 2015.\n\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, ICLR, 2020.\n\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.\n\nYusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. Advances in Neural Information Processing Systems, 2021.\n\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact of student initialization on knowledge distillation. CoRR, abs/1908.08962, 2019.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017.\n\nAshwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424, 2016.\n\nP Yu, S Xie, X Ma, B Jia, B Pang, R Gao, Y Zhu, S-C Zhu, and YN Wu. Latent diffusion energybased model for interpretable text modeling. In International Conference on Machine Learning, ICML, 2022.\n\nHugh Zhang, Daniel Duckworth, Daphne Ippolito, and Arvind Neelakantan. Trading off diversity and quality in natural language generation. In Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pp. 25-33. Association for Computational Linguistics, 2021.\n\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, ICLR, 2019 .\n\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, 2017.\n\nHao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. Commonsense knowledge aware conversation generation with graph attention. In International Joint Conference on Artificial Intelligence, IJCAI, 2018.\n\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: A benchmarking platform for text generation models. In The 41st International ACM SIGIR Conference on Research \\& Development in Information Retrieval, pp. 1097-1100, 2018."}, {"id": 4, "title": "A Objective Derivations Of DiffuSEQ", "content": "# A Objective Derivations Of DiffuSEQ\n\nThe diffusion model is well-known as its ability to achieve the trade-off between flexibility and tractability of the models' probability distributions, compared with GAN, VAE and Flow-based models. Following Ho et al. (2020); Nichol \\& Dhariwal (2021); Song et al. (2020), we systematically define the forward noising process and reverse denoising process on latent continuous space $\\mathbf{z}$.\nThe forward noising is to perturb the structure of data $\\mathbf{z}_{0} . \\mathbf{z}_{0}$ is finally changed into the partial Gaussian noise with $\\mathbf{y}_{T} \\sim \\mathcal{N}(0, \\mathbf{I})$ through $T$-step forward random disturbance\n\n$$\nq\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t} ; \\sqrt{1-\\beta_{t}} \\mathbf{z}_{t-1}, \\beta_{t} \\mathbf{I}\\right)\n$$\n\nwith $t=1,2, \\ldots, T$ and $\\left\\{\\beta_{t} \\in(0,1)\\right\\}_{t=1}^{T}$ are the variance schedule. Let $\\alpha_{t}=1-\\beta_{t}$ and $\\bar{\\alpha}_{t}=$ $\\prod_{i=1}^{t} \\alpha_{i}$, we have:\n\n$$\n\\begin{aligned}\n\\mathbf{z}_{t} & =\\sqrt{\\alpha_{t}} \\mathbf{z}_{t-1}+\\sqrt{1-\\alpha_{t}} \\epsilon_{t-1}=\\sqrt{\\alpha_{t} \\alpha_{t-1}} \\mathbf{z}_{t-2}+\\sqrt{1-\\alpha_{t} \\alpha_{t-1}} \\bar{\\epsilon}_{t-2} \\\\\n& =\\ldots=\\sqrt{\\bar{\\alpha}_{t}} \\mathbf{z}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}} \\epsilon\n\\end{aligned}\n$$\n\nwhere $\\epsilon$ stands for Gaussian noises. In the end, $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{0}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t} ; \\sqrt{\\bar{\\alpha}_{t}} \\mathbf{z}_{0},\\left(1-\\bar{\\alpha}_{t}\\right) \\mathbf{I}\\right)$. We use a sqrt noise schedule in Diffusion-LM (Li et al., 2022), that is, $\\bar{\\alpha}_{t}=1-\\sqrt{t / T+s}$ with $s$ as a small constant at the start of noise level. The reverse process then denoises $\\mathbf{z}_{t}$, aiming to recover original $\\mathbf{z}_{0}$, and is defined as:\n\n$$\np_{\\theta}\\left(\\mathbf{z}_{0: T}\\right):=p\\left(\\mathbf{z}_{T}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right), \\quad p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t-1} ; \\mu_{\\theta}\\left(\\mathbf{z}_{t}, t\\right), \\sigma_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right)\n$$\n\nThe learning of $p_{\\theta}$ is based on our diffusion model DiffuSEQ: $f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)$, where the $\\mu_{\\theta}(\\cdot)$ and $\\sigma_{\\theta}(\\cdot)$ is the predicted parameterization of the mean and standard variation of $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}\\right)$ in forward process. Using Bayes' rule:\n\n$$\nq\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}, \\mathbf{z}_{0}\\right)=q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}, \\mathbf{z}_{0}\\right) \\frac{q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{0}\\right)}{q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{0}\\right)}\n$$\n\nSubstitute Eq. (11) to it and we can get the parameterized mean of $q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}, \\mathbf{z}_{0}\\right)$ :\n\n$$\n\\mu_{t}\\left(\\mathbf{z}_{t}, \\mathbf{z}_{0}\\right)=\\frac{\\sqrt{\\alpha_{t}}\\left(1-\\bar{\\alpha}_{t-1}\\right)}{1-\\bar{\\alpha}_{t}} \\mathbf{z}_{t}+\\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_{t}}{1-\\bar{\\alpha}_{t}} \\mathbf{z}_{0}\n$$\n\nand for brevity, we short the coefficient of $\\mathbf{z}_{t}$ and $\\mathbf{z}_{0}$ as $\\mathcal{U}$ and $\\mathcal{E}$ respectively.\nWe can use the variational lower bound to optimize the negative log-likelihood $\\mathbb{E}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{0}\\right)\\right] \\leq$ $\\mathcal{L}_{\\text {VLB }}$. The objective can be further rewritten to be a combination of several KL-divergence and entropy terms following Sohl-Dickstein et al. (2015).\n\n$$\n\\begin{aligned}\n\\mathcal{L}_{\\mathrm{VLB}}=\\mathcal{L}_{T}+\\mathcal{L}_{T-1}+\\cdots+\\mathcal{L}_{0} & =\\mathbb{E}_{q\\left(\\mathbf{z}_{1: T} \\mid \\mathbf{z}_{0}\\right)}\\left[\\log \\frac{q\\left(\\mathbf{z}_{T} \\mid \\mathbf{z}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{T}\\right)}+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{0}, \\mathbf{z}_{t}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)}\\right. \\\\\n& \\left.+\\log \\frac{q_{\\phi}\\left(\\mathbf{z}_{0} \\mid \\mathbf{w}^{x \\oplus y}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{0} \\mid \\mathbf{z}_{1}\\right)}-\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)\\right]\n\\end{aligned}\n$$\n\nFor $1 \\leq t \\leq T-1$, we compute the parameterization of $\\mathcal{L}_{t}$ by substituting Eq. (14) to minimize the difference from $\\mu_{t}$ and $\\mu_{\\theta}$ following Ho et al. (2020):\n\n$$\n\\begin{aligned}\n\\mathcal{L}_{t} & =\\mathbb{E}_{\\mathbf{z}_{0}}\\left[\\log \\frac{q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{0}, \\mathbf{z}_{t+1}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t+1}\\right)}\\right]=\\mathbb{E}_{\\mathbf{z}_{0}}\\left[\\frac{1}{\\mathcal{C}}\\left\\|\\mu_{t}\\left(\\mathbf{z}_{t}, \\mathbf{z}_{0}\\right)-\\mu_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}\\right] \\\\\n& =\\mathbb{E}_{\\mathbf{z}_{0}}\\left[\\frac{1}{\\mathcal{C}}\\left\\|\\mathcal{U} \\mathbf{z}_{t}+\\mathcal{E} \\mathbf{z}_{0}-\\left(\\mathcal{U} \\mathbf{z}_{t}+\\mathcal{E} f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right)\\right\\|^{2}\\right]=\\frac{\\mathcal{E}}{\\mathcal{C}} \\mathbb{E}_{\\mathbf{z}_{0}}\\left[\\left\\|\\mathbf{z}_{0}-f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}\\right]\n\\end{aligned}\n$$\n\nwhere $\\mathcal{C}=2\\left\\|\\sigma_{\\theta}\\right\\|^{2}$ is a loss independent constant. Then the optimization of training loss $\\min _{\\theta} \\mathcal{L}_{\\text {VLB }}$ can be further simplified as:\n\n$$\n\\begin{aligned}\n& \\min _{\\theta}\\left[\\left\\|\\mu\\left(\\mathbf{z}_{T}\\right)\\right\\|^{2}+\\sum_{t=2}^{T}\\left\\|\\mathbf{z}_{0}-f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right)-f_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}-\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)\\right] \\\\\n\\rightarrow & \\min _{\\theta}\\left[\\sum_{t=2}^{T}\\left\\|\\mathbf{z}_{0}-f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right)-f_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}-\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)\\right] \\\\\n\\rightarrow & \\min _{\\theta}\\left[\\sum_{t=2}^{T}\\left\\|\\mathbf{y}_{0}-\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{y}\\right)-\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}+\\mathcal{R}\\left(\\left\\|\\mathbf{z}_{0}\\right\\|^{2}\\right)\\right]\n\\end{aligned}\n$$"}, {"id": 5, "title": "B Graphical Models of AR, Fully NAR, iterative NAR and DIFFUSEQ MODELS", "content": "# B Graphical Models of AR, Fully NAR, iterative NAR and DIFFUSEQ MODELS\n\nWe start from the conditional sequence generation problem, which aims to learn a conditional probability $p\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)$ with $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$. AR models learn $p\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)$ by autoregressive decomposition based on left-context:\n\n$$\np_{\\mathrm{AR}}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\underbrace{p\\left(w_{1}^{y} \\mid \\mathbf{w}^{x}\\right)}_{\\text {initial prediction }} \\underbrace{\\prod_{i=1, \\ldots, n-1} p\\left(w_{i+1}^{y} \\mid \\mathbf{w}_{1: i}^{y}, \\mathbf{w}^{x}\\right)}_{\\text {progressive left-context prediction }}\n$$\n\nconsisting of an initial prediction and an autoregressive left-context prediction process, while fullyNAR models (Gu et al., 2018; Qian et al., 2021) learn the conditional probability given independent assumption for fast inference:\n\n$$\np_{\\text {fully-NAR }}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\prod_{i=1, \\ldots, n} p\\left(w_{i}^{y} \\mid \\mathbf{w}^{x}\\right)\n$$\n\nTo make a better analogy to AR and NAR models, we use a lossless way to formulate iterative NAR models (Gu et al., 2019; Ghazvininejad et al., 2019) by introducing a series of intermediate sequences $\\mathbf{w}_{1: K-1}^{y}, \\mathbf{w}_{K}^{y}=\\mathbf{w}^{y}$ as:\n\n$$\n\\begin{aligned}\n& p_{\\text {iter-NAR }}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} p\\left(\\mathbf{w}_{1}^{y} \\mid \\mathbf{w}^{x}\\right) \\prod_{k=1 \\ldots K-1} p\\left(\\mathbf{w}_{k+1}^{y} \\mid \\mathbf{w}_{k}^{y}, \\mathbf{w}^{x}\\right) \\\\\n& =\\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} p\\left(\\mathbf{w}_{1}^{y} \\mid \\mathbf{w}^{x}\\right) \\prod_{k=1 \\ldots K-1} p\\left(\\mathbf{w}_{k+1}^{y} \\mid \\mathbf{w}_{k}^{y}, \\mathbf{w}^{x}\\right) \\\\\n& =\\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} \\underbrace{\\prod_{i=1 \\ldots n} p\\left(w_{1, i}^{y} \\mid \\mathbf{w}^{x}\\right)}_{\\text {initial prediction }} \\underbrace{\\prod_{k=1 \\ldots K-1} \\prod_{i=1 \\ldots n} p\\left(w_{k+1, i}^{y} \\mid \\mathbf{w}_{k, 1: n}^{y}, \\mathbf{w}^{x}\\right)}_{\\text {progressive full-context prediction }}\n\\end{aligned}\n$$\n\nPrevious study (Huang et al., 2022) shows that there is a gap called conditional total correlation between AR and fully-NAR learning paradigms, because of the lossy decomposition of NAR models. This gap is mainly responsible for the performance drop from AR to NAR models. However, when comparing iter-NAR, Eq. (20), with AR models, they both can be factorized into an initial prediction term and a progressive prediction process based on different context (i.e. left-context in AR and full-context in iter-NAR). The discrepancy as pointed out by Huang et al. (2022) is therefore closed in iter-NAR assuming sufficient steps. By showing DIFFUSEQ is an extension of the iter-NAR model, we offer a justification that it will not suffer from the conditional total correlation for the same reason.\n\nA straight-forward way to formulate naive diffusion models is to introduce a series of Gaussian noise-corrupted features $\\mathbf{y}_{1: T-1}, \\mathbf{y}_{0}=\\mathbf{y}, \\mathbf{y}_{T} \\sim \\mathcal{N}(0, \\mathbf{I})$ on continuous space as:\n\n$$\n\\begin{aligned}\np_{\\text {diffusion }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) & =\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} \\underbrace{p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right)}_{\\begin{array}{c}\n\\text { final-step prediction } \\\\\n\\text { progressive full-context diffusion }\n\\end{array}} \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n& =\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} \\prod_{i=1, \\ldots, n} p\\left(\\mathbf{w}_{i}^{y} \\mid \\mathbf{y}_{0, i}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} \\prod_{i=1, \\ldots, n} p\\left(\\mathbf{y}_{t-1, i} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\nwhere $p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)$ describes the diffusion process on contiguous representations $\\mathbf{y}$. The total number of diffusion steps is denoted as $T$. Thereafter we omit the independent decomposition on $\\mathbf{w}^{y}$ and $\\mathbf{y}_{t}$. To apply diffusion models on discrete space, the rounding operation in DIFFUSEQ maps the continuous vectors $\\mathbf{y}$ to discrete $\\mathbf{w}^{y}$ for each time step $t$, we hence in addition introduce both contiguous feature $\\mathbf{y}$ and the discrete text $\\mathbf{w}^{y}$ to represent the discrete text into Eq. (21):\n\n$$\n\\begin{aligned}\np\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) & =\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) p\\left(\\mathbf{y}_{t} \\mid \\mathbf{w}_{t+1}^{y}\\right) \\\\\n& =\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n& =\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n& =\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} \\sum_{\\mathbf{w}_{t}^{y}} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\nBy rearranging Eq. (23) and Eq. (24), we can see that DIFFUSEQ can be seen as a more generalized form of iter-NAR before marginalizing out $\\left\\{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}\\right\\}$, where Eq. (23) and Eq. (24) are equivalent with different computation order, despite the different initialization of $\\mathbf{y}_{T}$. For NAR models, $\\mathbf{y}_{T}$ is uniform copied from the source sentence or unk's token embedding (Gu et al., 2018); for diffusion models, $\\mathbf{y}_{T}$ is sampled from normal distribution $\\mathcal{N}(0, \\mathbf{I})$.\nIt is notable that unlike AR and fully NAR models generating text all at once, iterative NAR and diffusion models feature a self-corrected text generation process. The graphical comparison is shown in Figure 7."}, {"id": 6, "title": "C From DiffuSeq to Iterative NAR and Diffusion Models", "content": "# C From DiffuSeq to Iterative NAR and Diffusion Models\n\nFrom DiffuSeq to Iterative NAR We show how to derive DiffuSeq to iterative nonautoregressive model on discrete space.\n\n![img-6.jpeg](img-6.jpeg)\n\nFigure 7: Graphical Models of AR, Fully NAR, iterative NAR and DiffuSeq models. For simplicity, we omit source node $\\mathbf{w}^{x}$. Gray nodes indicate dependency on the source node while white nodes are independent to the source node.\n\n$$\n\\begin{aligned}\n& p_{\\text {DiffuSeq }} \\cdot\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) p\\left(\\mathbf{y}_{t} \\mid \\mathbf{w}_{t+1}^{y}\\right) \\quad \\text { reorder computation } \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} \\int_{\\mathbf{y}_{t}} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) p\\left(\\mathbf{y}_{t} \\mid \\mathbf{w}_{t+1}^{y}\\right) \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{w}_{t+1}^{y}, \\mathbf{w}^{x}\\right) \\quad \\text { marginalize over } \\mathbf{y} \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} p\\left(\\mathbf{w}_{1}^{y} \\mid \\mathbf{w}^{x}\\right) \\prod_{k=1 \\ldots K-1} p\\left(\\mathbf{w}_{k+1}^{y} \\mid \\mathbf{w}_{k}^{y}, \\mathbf{w}^{x}\\right) \\quad \\text { align } t \\text { and } k \\text { reversely. } \\\\\n= & p_{\\text {iter-NAR }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\nFrom DiffuSeq to diffusion model We show how to derive DiffuSeq to the straight-forward diffusion model on continuous space.\n\n$$\n\\begin{aligned}\n& p_{\\text {DiffUSEQ }} \\cdot\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n= & \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} \\sum_{\\mathbf{w}_{t}^{y}} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n= & \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n= & p_{\\text {diffusion }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\n## # D DETAILS OF EXPERIMENTS\n\n## D. 1 Processing of Question Generation Dataset\n\nTo construct high-quality document-question pairs from the Quasar-T dataset, which consists of $\\left\\langle\\right.$ document $_{i}$, question, answer $\\rangle$ triplets, we extract $\\left\\langle\\right.$ document $_{i}$, question $\\rangle$ pairs if answer exactly matches document ${ }_{i}$. After pre-processing, we obtain 119 K document-question training pairs.\n\n## ## D. 2 SETTINGS OF BASELINES\n\nWe compare the settings of different models, including the number of parameters and how to sample the different output sentences, as shown in Table 4. For plain GRU-based encoder-decoder methods, we do not implement diversity search algorithms on it, thus its sentence-level diversity could be very poor. For NAR-LevT, we set max iteration to 9 and follow the termination condition mentioned in the original paper. For GPVAE-T5, we tune the scalar to find the best trade-off between quality and diversity on the dev set. The scalars of all four tasks are set to 2 . We implement GPT2 baselines using HuggingFace Transformers and for the baseline Transformer-base, we use Fairseq.\n\n## Table 4: The comparison for different models\n\n| Models | \\# Parameters | Learning Paradigm | Diversity Source |\n| :-- | :-- | :-- | :-- |\n| GRU-attention | 65 M | encoder-decoder | - |\n| Transformer-base | 80 M | encoder-decoder | Temperature/DBS |\n| GPT2-base FT | 117 M | pretrain-finetune | Hybrid strategy |\n| GPT2-large FT | 774 M | pretrain-finetune | Hybrid strategy |\n| GPVAE-T5 | 220 M | pretrain+VAE | Gaussian sampling |\n| NAR-LevT | 80 M | non-autoregressive | - |\n| DIFFUSEQ | 91 M | non-autoregressive | Gaussian sampling |\n\n## D. 3 Diversity and Quality Trade-off Settings\n\nWe list the details to obtain Figure 4. For GPVAE-T5, we set different scalars as 1, 2, 3, 4. For DIFFUSEQ, we choose trained models at different training steps to achieve different trade-off points. For other baselines, there is no explicit factor to control the diversity generation, so we leave them as single points in the figure.\n\n[^0]\n[^0]:    ${ }^{4}$ Including top-p sampling, temperature, diversity beam search (DBS) and etc. Implement using HuggingFace Transformers https://github.com/huggingface/transformers"}, {"id": 7, "title": "D. 4 Metrics", "content": "# D. 4 Metrics\n\nThe used BLEU score is sentence-level smoothed from BLEU-1 to 4, and used ROUGE-L score is longest common subsequence based statistics. The implementation is based on NLTK ${ }^{5}$ and torchmetrics. The n-gram based metrics may fail to capture the semantic meaning of sentences, so we consider using BERTScore ${ }^{6}$. Specifically, we use microsoft/deberta-xlarge-mnli to help BERTScore correlate better with human scores.\n\n## ## D. 5 GENERATION RESULTS\n\nFor different tasks we list some generation examples. As we can see in Table 5, Table 6 and Table 7, DiffuSeq tends to generate diverse outputs, but sometimes the sentence is not as fluent as finetuned GPT2.\n\nTable 5: Sample outputs with different random seed in Dialogue test set.\n\n| Utterance: How long does the dye last? <br> Response: Just did this two days ago, not sure how it'll fade yet! |  |\n| :--: | :--: |\n| GPVAE-T5 | NAR-LevT |\n| * I'm not sure, I'm not sure. I've tested it a few times, but I don't know for sure. I've | * half . |\n| * I'm not sure. I'm not sure how long it lasts, I'm sure it 'll get better. It's been a while since | * half . |\n| * I've been using it for about a year and a half. I've been using it for about a year and a half. | * half . |\n| GPT2-large finetune | DiffuSeq |\n| * Two weeks in my case. | * About an hour, 5 days or so. |\n| * I've had it for about a year. | * 4 days. |\n| * The dye can sit around for a month then you can wash it. | * I'm not sure about this, about the same kind of time. |\n\nTable 6: Sample outputs with different random seed in Question Generation test set.\nStatement: The Japanese yen is the official and only currency recognized in Japan.\n## Question: What is the Japanese currency?\n\n| GPVAE-T5 | NAR-LevT |\n| :-- | :-- |\n| * What is the japanese currency | * What is the basic unit of currency for Japan ? |\n| * What is the japanese currency | * What is the basic unit of currency for Japan ? |\n| * What is the japanese currency | * What is the basic unit of currency for Japan ? |\n| GPT2-large finetune | DiffuSeq |\n| * What is the basic unit of currency for Japan? | * What is the Japanese currency |\n| * What is the Japanese currency | * Which country uses the \"yen yen\" in currency |\n| * What is the basic unit of currency for Japan? | * What is the basic unit of currency? |\n\n[^0]\n[^0]:    ${ }^{5}$ https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n${ }^{6}$ https://github.com/Tiiiger/bert_score\n\nTable 7: Sample outputs with different random seed in Text Simplification test set.\nComplex sentence: People can experience loneliness for many reasons, and many life events may cause it, such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person.\nSimplified: One cause of loneliness is a lack of friends during childhood and teenage years.\n\n## ## GPVAE-T5\n\n## People can experience loneliness for many reasons, and many life events may cause it,\n\n* such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person\n\n## People can experience loneliness for many reasons, and many life events may cause it,\n\n* such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person\n\n## People can experience loneliness for many reasons, and many life events may cause it,\n\n* such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person\n\n## GPT2-large finetune\n\n* Loneliness can be caused by many things.\n* Loneliness can affect people in many ways.\n* Loneliness can be caused by many things.\n\n## NAR-LevT\n\n* People may experience reashapphapphapphapphappabout life reasit.\n* People may experience reashapphapphapphapphappabout life reasit.\n* People may experience reashapphapphapphapphappabout life reasit.\n\n## ## DIFFUSEQ\n\n* Many life events may cause of loneliness\n* People can also be very experience loneliness for many reasons.\n* People can experience loneliness for many reasons, and many life events may, cause it."}], "pdf_images": {"pdf_path": "inbox/2210.08933v3.pdf", "total_pages": 20, "pages": [{"page_number": 1, "filename": "page-001.png", "thumb_filename": "page-001-thumb.png", "mobile_filename": "page-001-mobile.png", "mobile_thumb_filename": "page-001-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 2, "filename": "page-002.png", "thumb_filename": "page-002-thumb.png", "mobile_filename": "page-002-mobile.png", "mobile_thumb_filename": "page-002-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 3, "filename": "page-003.png", "thumb_filename": "page-003-thumb.png", "mobile_filename": "page-003-mobile.png", "mobile_thumb_filename": "page-003-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 4, "filename": "page-004.png", "thumb_filename": "page-004-thumb.png", "mobile_filename": "page-004-mobile.png", "mobile_thumb_filename": "page-004-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 5, "filename": "page-005.png", "thumb_filename": "page-005-thumb.png", "mobile_filename": "page-005-mobile.png", "mobile_thumb_filename": "page-005-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 6, "filename": "page-006.png", "thumb_filename": "page-006-thumb.png", "mobile_filename": "page-006-mobile.png", "mobile_thumb_filename": "page-006-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 7, "filename": "page-007.png", "thumb_filename": "page-007-thumb.png", "mobile_filename": "page-007-mobile.png", "mobile_thumb_filename": "page-007-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 8, "filename": "page-008.png", "thumb_filename": "page-008-thumb.png", "mobile_filename": "page-008-mobile.png", "mobile_thumb_filename": "page-008-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 9, "filename": "page-009.png", "thumb_filename": "page-009-thumb.png", "mobile_filename": "page-009-mobile.png", "mobile_thumb_filename": "page-009-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 10, "filename": "page-010.png", "thumb_filename": "page-010-thumb.png", "mobile_filename": "page-010-mobile.png", "mobile_thumb_filename": "page-010-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 11, "filename": "page-011.png", "thumb_filename": "page-011-thumb.png", "mobile_filename": "page-011-mobile.png", "mobile_thumb_filename": "page-011-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 12, "filename": "page-012.png", "thumb_filename": "page-012-thumb.png", "mobile_filename": "page-012-mobile.png", "mobile_thumb_filename": "page-012-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 13, "filename": "page-013.png", "thumb_filename": "page-013-thumb.png", "mobile_filename": "page-013-mobile.png", "mobile_thumb_filename": "page-013-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 14, "filename": "page-014.png", "thumb_filename": "page-014-thumb.png", "mobile_filename": "page-014-mobile.png", "mobile_thumb_filename": "page-014-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 15, "filename": "page-015.png", "thumb_filename": "page-015-thumb.png", "mobile_filename": "page-015-mobile.png", "mobile_thumb_filename": "page-015-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 16, "filename": "page-016.png", "thumb_filename": "page-016-thumb.png", "mobile_filename": "page-016-mobile.png", "mobile_thumb_filename": "page-016-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 17, "filename": "page-017.png", "thumb_filename": "page-017-thumb.png", "mobile_filename": "page-017-mobile.png", "mobile_thumb_filename": "page-017-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 18, "filename": "page-018.png", "thumb_filename": "page-018-thumb.png", "mobile_filename": "page-018-mobile.png", "mobile_thumb_filename": "page-018-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 19, "filename": "page-019.png", "thumb_filename": "page-019-thumb.png", "mobile_filename": "page-019-mobile.png", "mobile_thumb_filename": "page-019-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}, {"page_number": 20, "filename": "page-020.png", "thumb_filename": "page-020-thumb.png", "mobile_filename": "page-020-mobile.png", "mobile_thumb_filename": "page-020-mobile-thumb.png", "width": 1275, "height": 1650, "mobile_width": 2550, "mobile_height": 3300}]}, "metadata": {"model": "gpt-4o", "pdf_path": "inbox/2210.08933v3.pdf"}, "attribution": {"type": "arxiv", "url": "https://arxiv.org/abs/2210.08933", "id": "2210.08933"}};
        initializeColumnInterface();
    </script>
</body>
</html>