{
  "title": "AlCuNA: Large Language Models Meet New Knowledge",
  "slug": "2310.14820v1",
  "questions": {
    "1": {
      "question": "What were they trying to do?",
      "answer": "The main goal of the paper was to address the lack of benchmarks for evaluating large language models' (LLMs) ability to handle new knowledge, which is crucial in a rapidly changing world. The authors aimed to create a benchmark, AlCuNA, using a method called KnowGen to generate new knowledge by altering existing entity attributes and relationships. This benchmark assesses LLMs' capabilities in understanding, differentiating, and associating new knowledge with existing knowledge, ultimately driving the development of LLMs to better handle new scenarios and information.",
      "timestamp": "2025-07-21 16:31:35"
    },
    "2": {
      "question": "Why does it matter?",
      "answer": "The paper \"AlCuNA: Large Language Models Meet New Knowledge\" addresses the critical need for benchmarks that evaluate large language models (LLMs) on their ability to handle new knowledge, which is essential in a rapidly changing world. This matters because LLMs frequently encounter new information, and retraining them for every update is impractical. Evaluating LLMs with new knowledge helps ensure their reliability and adaptability without data leakage concerns. This is important for developers and researchers who aim to improve LLMs' performance in dynamic environments, ultimately benefiting industries relying on AI for real-time data processing and decision-making.",
      "timestamp": "2025-07-21 16:31:41"
    },
    "3": {
      "question": "What did they try?",
      "answer": "The authors of the paper proposed a method called KnowGen to generate new knowledge by altering existing entity attributes and relationships. This approach involves creating artificial entities that are distinct from real-world entities by simulating biological processes like heredity and variation. They applied KnowGen to structured biological taxonomic data to create a benchmark named AlCuNA, which assesses large language models' abilities in understanding, differentiating, and associating new knowledge. The benchmark includes questions about these artificial entities to evaluate the models' cognitive abilities in handling new knowledge scenarios.",
      "timestamp": "2025-07-21 16:31:45"
    },
    "4": {
      "question": "Did it work?",
      "answer": "The idea proposed in the paper, which involves creating a benchmark called AlCuNA to evaluate large language models' (LLMs) ability to handle new knowledge, revealed that current LLMs, including ChatGPT, Alpaca, Vicuna, and ChatGLM, perform poorly when faced with new knowledge. The benchmark showed that while ChatGPT performed relatively better in understanding and differentiating new knowledge, all models struggled significantly with knowledge association, particularly in reasoning between new and internal knowledge. This highlights the need for caution when using LLMs in new scenarios and suggests that improvements are necessary for LLMs to effectively integrate and reason with new knowledge.",
      "timestamp": "2025-07-21 16:31:48"
    },
    "5": {
      "question": "What did they compare it to?",
      "answer": "The paper compares the proposed benchmark, AlCuNA, to existing benchmarks that evaluate large language models (LLMs) on traditional tasks. AlCuNA specifically assesses LLMs' abilities to handle new knowledge, which is not adequately measured by existing benchmarks. The authors evaluated several popular LLMs, including ChatGPT, Alpaca, Vicuna, and ChatGLM, using AlCuNA. The results showed that these models perform poorly with new knowledge, particularly in reasoning tasks that require linking new and existing knowledge, highlighting the limitations of current models in handling new information effectively.",
      "timestamp": "2025-07-21 16:31:54"
    },
    "6": {
      "question": "What was it tested on?",
      "answer": "The paper tested the proposed approach, KnowGen, and the benchmark, AlCuNA, on several popular large language models (LLMs) including ChatGPT, Alpaca, Vicuna, and ChatGLM. The evaluation involved generating artificial entities with new knowledge using structured biological taxonomic information and assessing the models' abilities in knowledge understanding, differentiation, and association. The evaluation setup included zero-shot and few-shot settings with both vanilla and Chain-of-Thought (CoT) reasoning forms. The models were tested on a dataset consisting of 84,351 questions about 3,554 artificial entities, divided into subsets to measure different capabilities. The results highlighted the models' challenges, particularly in reasoning between new and existing knowledge.",
      "timestamp": "2025-07-21 16:31:57"
    },
    "7": {
      "question": "What's cool about it?",
      "answer": "The paper introduces a novel approach called KnowGen to generate new knowledge by altering existing entity attributes and relationships, creating artificial entities distinct from real-world ones. This is used to construct a benchmark named AlCuNA, which evaluates large language models (LLMs) on their ability to handle new knowledge. The approach is clever because it simulates real-world scenarios where LLMs encounter new information, without the risk of data leakage from training data. The novelty lies in its structured, ontological representation of knowledge, allowing for a more reliable assessment of LLMs' capabilities in understanding, differentiating, and associating new knowledge with existing knowledge.",
      "timestamp": "2025-07-21 16:32:03"
    },
    "8": {
      "question": "What's sketchy about it?",
      "answer": "The paper presents several concerns and limitations. Firstly, the benchmark's reliance on artificially generated knowledge may not fully capture the complexity and unpredictability of real-world new knowledge. Additionally, the method is only implemented on biological data, limiting its applicability across other domains. The evaluation is conducted on a limited number of models, which may not provide a comprehensive understanding of LLMs' capabilities. Furthermore, the artificial nature of the knowledge raises ethical concerns about potential misuse or misinterpretation. Lastly, the benchmark's closed-source nature and the potential for models to eventually incorporate this benchmark into their training data could limit its long-term effectiveness.",
      "timestamp": "2025-07-21 16:32:08"
    },
    "9": {
      "question": "Can anyone use this?",
      "answer": "The AlCuNA benchmark is designed to be accessible to researchers and developers working with large language models (LLMs). It is not inherently expensive or limited to certain cases, as it uses a method called KnowGen to generate new knowledge by altering existing entity attributes and relationships. This approach is practical for evaluating LLMs' abilities to handle new knowledge, as it does not require retraining models, which can be costly and time-consuming. However, using the benchmark effectively may require some understanding of LLM evaluation and the ability to interpret structured data, which could pose a challenge for those without a technical background in NLP.",
      "timestamp": "2025-07-21 16:32:11"
    },
    "10": {
      "question": "What's still left to figure out?",
      "answer": "The paper identifies several unresolved challenges and opens up new questions for future research. It highlights the difficulty in constructing benchmarks that ensure the knowledge is genuinely new for large language models (LLMs) and remains so over time. The study reveals that current LLMs struggle with reasoning between new and existing knowledge, particularly in multi-hop reasoning tasks. Future directions include improving LLMs' ability to handle new knowledge, exploring the impact of entity similarity on model performance, and developing benchmarks in other domains using the KnowGen method. Additionally, the paper suggests the need for community coordination to manage benchmark data and prevent it from being used in LLM training.",
      "timestamp": "2025-07-21 16:32:15"
    }
  },
  "markdown_pages": [
    {
      "id": 1,
      "title": "AlCuNA: Large Language Models Meet New Knowledge",
      "content": "# AlCuNA: Large Language Models Meet New Knowledge\n\nXunjian Yin ${ }^{\\star}$ and Baizhou Huang* and Xiaojun Wan<br>Wangxuan Institute of Computer Technology, Peking University<br>Center for Data Science, Peking University<br>The MOE Key Laboratory of Computational Linguistics, Peking University<br>\\{xjyin, hbz19, wanxiaojun\\}@pku.edu.cn\n\n#### Abstract\n\nWith the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models' capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs' ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named AlCuNA to assess LLMs' abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model's understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.\n\n## 1 Introduction\n\nLarge-scale language models (LLMs) have made impressive progress in the last few years (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023; OpenAI, 2023), which perform surprisingly well on various tasks on various domains, to the extent that many traditional benchmarks (Thorne et al., 2018; Wang et al., 2018) are no longer sufficient to measure the capabilities of LLMs. Therefore, some new benchmarks have been proposed to evaluate the ability of the model to solve more complex tasks such as college entrance exams, law\n\n[^0]school admission tests, math competitions and so on (Hendrycks et al., 2021; Guo et al., 2023; Zhong et al., 2023). LLMs also achieve promising results on these benchmarks.\n\nHowever, it is surprising that there is not yet a benchmark to evaluate the ability of large models in face of new knowledge, which is very important and challenging. Why is this evaluation important? Firstly, we are in a changing world, where models encounter new knowledge frequently in practice. And some work (Peng et al., 2023) is exploring retrieval methods to augment large models, which will also cause the models to meet new knowledge frequently. So of course we expect the model to perform well in such situations, because re-training the model every time is very expensive and unrealistic. Secondly, as Elangovan et al. (2021) mentioned, the presence of overlap between the training and test data can lead to a misestimation of the model's memory ability as generalization ability, especially nowadays when LLMs are trained on an enormous amount of data. Whereas, evaluation on new knowledge does not need to worry about such data leakage, as new knowledge can usually lead to new data and thus more reliable and valuable assessment of the model's ability.\n\nWhile such evaluation is important, it is challenging to construct the benchmark. The reason is that it is difficult to ensure that the knowledge contained in the benchmark is new for LLMs, since training data for some models are large and nonpublicly available. Furthermore, it is also difficult to ensure that the knowledge used for benchmarking will be not outdated and inefficient, as there are many LLMs that may soon include data from the benchmark in their training. In summary, such benchmark for new knowledge needs to exhibit three basic characteristics: it contains enough new knowledge for sufficient evaluation (sufficient), the knowledge is new to all models (model-agnostic) and the knowledge can remain new for a long time\n\n[^0]:    *These authors contributed equally to this work.\n\n(long-lasting).\nThere are several possible solutions to the above challenges. One option is to always use the most updated data such as the news of the day (temporal knowledge). However, this is both labor-intensive to race against the LLM training and unclear about the lifetime of the proposed data. Another option is to keep the benchmark closed-source, with an authoritative committee managing the data and users calling API when evaluating, thus preventing using them for training LLMs. To reach this point further requires community coordination.\n\nTo better address these challenges, we propose an approach to GENerate new KNOWledge conveniently (KnowGen) based on the structured representation of existing entity knowledge by making reasonable changes to entity attributes and relationships. There are differences and associations between artificial entities and existing entities. Particularly, we apply KnowGen with structured biological taxonomic information data to rationally create a group of organisms that do not exist in the world. To test the model's ability in face of new knowledge, we construct a variety of questions about these artificial entities that can examine the model's ability to understand new knowledge (Knowledge Understanding), distinguish between model's internal knowledge and new knowledge (Knowledge Differentiation) and make multi-hop reasoning by linking model's internal and new knowledge (Knowledge Association). We use the ArtificialLy ConstrUcted kNowledge to Assess LLMs as a benchmark (ALCUNA).\n\nWe evaluate and analyze several popular large models based on AlCUNA, including ChatGPT1, Alpaca, Vicuna, and ChatGLM with vanilla, CoT (Chain-of-Thought), Zero-Shot and Few-Shot settings (Brown et al., 2020; Kojima et al., 2023; Wei et al., 2023). We find that neither ChatGPT nor other models perform very well in face of new knowledge. ChatGPT does a good job of understanding and differentiating new knowledge, but almost all models fail to reason between the new knowledge and the internal knowledge. This reminds us to remain cautious when large models encounter new scenarios and knowledge. In addition, we explore the impact of entity similarity on the model's understanding of entity knowledge, the impact of contextual entities, etc.\n\n## The contributions of our work are listed below:\n\n[^0]1) we propose a new method KnowGen to generate new knowledge for simulating real scenarios. 2) we apply our method to produce an artificial biological entity knowledge dataset, ALCUNA, as a benchmark for evaluating the performance of models in face of new knowledge. 3) we evaluate and analyze several popular large models and obtain insightful observations and conclusions.\n\nOur benchmark has been released to the community to facilitate future research ${ }^{2}$.\n\n```\n## Algorithm 1: Knowledge Generation\ninput :One Class \\(C\\)\noutput: Property Set \\(\\mathcal{T}(\\hat{e})\\) of \\(\\hat{e}\\)\n\\(e^{p} \\leftarrow\\) RandomSelect \\((C)\\);\n\\(E^{p a b} \\leftarrow \\operatorname{sib}\\left(e^{p}\\right)\\)\n// Get the triplet set for heredity, variation, dropout\nand extension\n\\(\\mathcal{T}_{R}^{h}, \\mathcal{T}_{R}^{v}, \\mathcal{T}_{R}^{p} \\leftarrow\\) RandomSplit \\(\\left(\\mathcal{T}_{R}\\left(e^{p}\\right)\\right)\\)\n\\(\\mathcal{T}_{A}^{h}, \\mathcal{T}_{A}^{v}, \\mathcal{T}_{A}^{p} \\leftarrow\\) RandomSplit \\(\\left(\\mathcal{T}_{A}\\left(e^{p}\\right)\\right)\\)\n\\(\\mathcal{T}^{a} \\leftarrow\\) RandomSample \\(\\left(\\mathcal{T}\\left(E^{p a b}\\right)\\right)\\)\n// Heredity and Dropout\n\\(\\mathcal{T}_{R}(\\hat{e}) \\leftarrow \\mathcal{T}_{R}(C) \\cup \\mathcal{T}_{R}^{h}\\);\n\\(\\mathcal{T}_{A}(\\hat{e}) \\leftarrow \\mathcal{T}_{A}(C) \\cup \\mathcal{T}_{A}^{h}\\)\n// Variation: replacing the object with an entity from\nthe same class\nfor \\(\\left(e^{p}, r, e^{\\prime}\\right)\\) in \\(\\mathcal{T}_{R}^{v}\\) do\n\\(E^{\\prime a b} \\leftarrow \\operatorname{sib}\\left(e^{\\prime}\\right)\\)\n\\(e^{\\prime a b} \\leftarrow\\) RandomSelect \\(\\left(E^{\\prime a b}\\right)\\)\n\\(\\mathcal{T}_{R}(\\hat{e}) \\leftarrow \\mathcal{T}_{R}(\\hat{e}) \\cup\\left\\{\\left(\\hat{e}, r, e^{\\prime a b}\\right)\\right\\}\\)\n// Variation: add gaussian noise to the value or copy\nfrom \\(E^{p a b}\\)\nfor \\(\\left(e^{p}, a, v\\right)\\) in \\(\\mathcal{T}_{A}^{v}\\) do\nif isnum \\((v)\\) then\n\\(\\tilde{v} \\leftarrow v+\\mathcal{N}(0, v / 10)\\)\nelse\n\\(e^{p a b} \\leftarrow\\) RandomSelect \\(\\left(E^{p a b}\\right)\\)\n\\(\\tilde{v} \\leftarrow\\) GetValue \\(\\left(e^{p a b}, a\\right)\\)\n\\(\\mathcal{T}_{A}(\\hat{e}) \\leftarrow \\mathcal{T}_{A}(\\hat{e}) \\cup\\{(\\hat{e}, a, \\tilde{v})\\}\\)\n// Extension and get final property\n\\(\\mathcal{T}(\\hat{e}) \\leftarrow \\mathcal{T}_{A}(\\hat{e}) \\cup \\mathcal{T}_{R}(\\hat{e}) \\cup \\mathcal{T}^{v}\\)\n```\n\n## 2 KnowGen: Knowledge Generation\n\nIn this section, we begin by presenting our inspiration, then formally introduce our knowledge generation method KnowGen.\n\n### 2.1 Inspiration\n\nAccording to the ontological form (Sowa, 1995; Noy et al., 2001), we can represent most knowledge\n\n[^1]\n[^0]:    ${ }^{1}$ https://chat.openai.com/chat\n\n[^1]:    ${ }^{2}$ https://github.com/Arvid-pku/ALCUNA\n\nas entities, the classes to which they belong, their attributes and the relations between them. And inspired by organisms: organisms can produce organisms with new properties naturally through heredity and variation. Can knowledge also be \"inherited\" and \"varied\" in a broader context? Different entities of the same class have different properties as well as commonalities. Generally speaking, entities of the same class are similar to some extent, while they have some different properties. By analogy with biological terminology, we refer to this characteristic of knowledge of similar entities as \"hybridizability\". This inspires us to use different entities of the same class to fuse their properties, simulating the process of biological inheritance and variation, to generate new entity knowledge.\n\nIn the following we will formally define and describe how knowledge is \"inherited\" and \"varied\" in our approach.\n\n### 2.2 Knowledge Formalization\n\nBased on the design of the ontology, We represent knowledge from the viewpoint of Entity. Entity $e \\in \\mathcal{E}$ is a distinct and identifiable object or individual in the world. Each entity can possess various attributes $a \\in \\mathcal{A}$, which describe the properties of the entity with a value $v \\in \\mathcal{V}$. At the same time, each entity can participate in relations $r \\in \\mathcal{R}$ with other entities. Both the attributes and relations of entity $e$ can be represented as a set of property triplets: $\\{(e, a, v)\\}=\\mathcal{T}_{A}(e) \\subset \\mathcal{T}_{A}$ and $\\left\\{\\left(e, r, e^{\\prime}\\right)\\right\\}=\\mathcal{T}_{R}(e) \\subset \\mathcal{T}_{R}$. Entities with similar characteristics may be aggregated into a class $C \\subset \\mathcal{E}$. For convenience, we denote the same properties across all entities in class $C$ as $\\mathcal{T}(C)=\\mathcal{T}_{A}(C) \\cup \\mathcal{T}_{R}(C)$. Without any special description, the triplet $\\mathcal{T}(e)$ of entity $e$ refers to its unique properties.\n\nFor example, Figure 1 shows an example in the form of such structured knowledge, where both Alpaca and Vicuna are existing entities belonging to the class Camels and Alcuna is an artificial entity generated by us. Alpaca has attributes such as \"Body mass\" and relations such as \"Eaten by\", and can be formed into triplets such as (Alpaca, Body mass, 60 kg ) and (Alpaca, Eaten by, Cougar).\n\n### 2.3 Construction of New Entities\n\nFocusing on knowledge in the form of ontology, we aim to construct artificial entities reasonably to accelerate the process of new knowledge generation in the real world. When creating an artificial entity\nwithin a specific class, it must adhere to certain common properties shared by other entities in the same class. Furthermore, it is essential for the artificial entity to possess some unique properties that differentiate it from existing entities. To address these requirements for individuality and commonality, we propose a fast and effective method to construct an artificial entity that fuses attributes and relations of entities within the same class.\n\nInitially, we select an existing entity $e^{p}$ from a specific class $C$ to serve as the parent entity for our artificial entity $\\tilde{e}$ and consider other entities within the same class $C$ as sibling entities of parent, denoted by $\\operatorname{sib}\\left(e^{p}\\right)=\\left\\{e_{1}, \\ldots e_{n}\\right\\}$. Our goal is to construct an artificial entity that exhibits high similarity to the parent entity and conforms to the commonality of the class (heredity) while incorporating properties from the sibling entities or reasonably changing property values (variation). As an example, in Figure 1, the artificial entity Alcuna inherits the \"Diet\" and other attributes from the parent Alpaca, while the \"Body mass\" is varied.\n\nBesides the above operations of heredity and variation, we construct new entities with additional extension and dropout of the properties of the new entities, in order to mimic human progressive cognitive processes of entities. As the example in Figure 1 shows, we extend the attribute of \"First appearance\" from Vicuna to Alcuna, and drop out the \"Life span\" from the parent entity Alpaca.\n\nThe whole method can be seen in Algorithm 1. A detailed description of KnowGen with natural language and expressions is shown in Appendix A. The entities constructed in this way are not only reasonable but also convenient for us to assess the model's cognitive ability to associate and differentiate new knowledge with the existing knowledge.\n\n### 2.4 Question Answering as Evaluation Task\n\nBased on the constructed artificial entities, a natural form of evaluation is to ask questions to LLMs in the context of new knowledge. In specific, we leverage an attribute triplet $(\\tilde{e}, a, v)$ for generating a onehop question $q(\\tilde{e}, a, v)$ by specifically asking about the object $v$, given the subject $\\tilde{e}$ and attribute $a$. With a chain of relation triplets $\\mathcal{T}_{C}=\\left(\\tilde{e}, r, e_{1}\\right) \\rightarrow$ $\\left(e_{1}, r_{1}, e_{2}\\right) \\rightarrow \\ldots \\rightarrow\\left(e_{N-1}, r_{N-1}, e_{N}\\right)$, we construct a multi-hop question $q\\left(\\mathcal{T}_{C}\\right)$ asking about the tail object $e_{N}$, given the head subject $\\tilde{e}$ and relations $\\left\\{r, r_{1}, \\ldots, r_{N-1}\\right\\}$.\n\n## We propose that LLM requires the knowledge understanding, knowledge differentiation and\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Demonstration of AlcuNA, including heredity, variation, extension and dropout operations in KnowGen, generated artificial entity named Alcuna and three types of questions related to it.\n\nknowledge association abilities when faced with new knowledge. To enable a more detailed assessment, we design diverse categories of questions to evaluate each ability, as shown on the right side of Figure 1.\n\nSpecifically, we sample some attribute triplets in the variation set $$ \\mathcal{T}_A^c(\\hat{e}) $$ and the dropout set $$ \\mathcal{T}_A^d(\\hat{e}) $$ to create KD question set, which is ideally suited for evaluating the knowledge differentiation ability of a model to distinguish between the parent entity $$ e^{p} $$ existing in its internal knowledge and the newly constructed artificial entity $$ \\hat{e} $$ provided in the context.\n\nThe proficiency of LLMs in reasoning along mastered knowledge graph has been demonstrated in previous studies (Hu et al., 2022; Choudhary and Reddy, 2023). However, it remains uncertain whether it can effectively establish connection between newly encountered artificial entity and the existing knowledge for multi-hop reasoning task. To investigate this, we incorporate the relations of artificial entity to construct a new graph, which encompasses both existing entities and the artificial entity. We then perform a breadth-first-search on the relation graph to identify a chain of relation triplets $$ \\mathcal{T}_C $$ with the artificial entity serving as root (e.g., [(*Alcuna, Eaten by, Jaguar*), (*Jaguar, Compete with, Maned Wolf*)]), and then utilize the chain to generate a multi-hop question $$ q(\\mathcal{T}_C) $$ (e.g., *What organism is the competitor of the Alcuna's natural enemy?*). We group such questions into KA question set.\n\nFor the rest of the artificial entity's property triplets, we utilize them to evaluate the ability of remembering and understanding new knowledge by simply asking questions about the objects in triplets. We group such questions into KU question set.\n\n## 3 AlcuNA: Our Benchmark\n\nWith our proposed method, one can create a large amount of new entities quickly based on existing structured knowledge. A natural attempt is to construct new organisms on already discovered ones, since the biological taxonomy is perfectly adapted to our proposed approach. Therefore, we utilize KnowGen to propose AlcuNA, a biological dataset for evaluating the ability of the model in face of new knowledge as shown in Figure 1.\n\n### 3.1 EOL Database\n\nWe utilize the structured data from the EOL (Encyclopedia of Life) database (Parr et al., 2014) to provide existing knowledge, which is an online, freely accessible database that aims to provide information about all known species on Earth. EOL organizes all biological taxons into a taxonomic tree, in which each entity belongs to a class. The most intriguing feature of EOL is that it constructs rich structured data in the form of key-value pairs for each biological entity, including taxonomic rank, attributes, relationships and information source. As a whole, it contains 2404790 entities with a total of 13625612 properties consisted of 669 property types. The substantial volume of data, coupled with\n\n<sup>3</sup>https://eol.org/\n\nits well-organized format, renders EOL the most suitable data source for constructing AlCuna.\n\n### 3.2 Artificial Entity Construction Details\n\nEach time we select a class $\\mathcal{C}$ from the taxonomic tree of EOL and consider its members as entities. We then divide them into one parent entity $e^{p}$ and its siblings $\\operatorname{sib}\\left(e^{p}\\right)$ from which we construct the artificial entity. Since a high-level taxon is usually not a specific organism, the properties it possesses may be too homogeneous, so we screen out all taxons belonging to kingdom, phylum and domain.\n\nIn the real world, the naming scheme of a newly found creature usually incorporates the same prefix or suffix of other creatures of the same species. In order to mimic the real world scenario and considering the tokenization algorithms used in LLMs, we firstly split the names of related existing entities (i.e. the parent entity and sibling entities of parent) into subwords ${ }^{4}$. Then we randomly select names of related entities, and for the $i$-th selected entity we choose its $i$-th subword. For example, \"AlCuna\" is created from Alpaca and Vicuna.\n\n### 3.3 Template-based Question Generation\n\nGiven a property triplet $(\\bar{e}, a, v)$ (one-hop setting) or a chain of property triplets $\\mathcal{T}_{C}=\\left(\\bar{e}, r, e_{1}\\right) \\rightarrow$ $\\left(e_{1}, r_{1}, e_{2}\\right) \\rightarrow \\ldots \\rightarrow\\left(e_{N-1}, r_{N-1}, e_{N}\\right)$ (multi-hop setting), we aim to generate natural language question asking about the tail object.\n\nWe leverage ChatGPT in the process of question generation to avoid expensive labor costs following petroni2019gpt. Specifically, we use ChatGPT to generate a question template with a placeholder [T] given only the relevant properties to avoid introducing too much model's knowledge of a specific entity. Then we generate questions from the question template by replacing [T] with the name of head subject. We generate five question templates for each property group in form of multiple choice, fill-in-the-blank and Boolean questions. The details about the prompts used for question generation and examples are shown in Appendix B.2. To ensure the quality of automatic question generation by this method, we randomly sample 100 questions each for one-hop and multi-hop questions for human checking. It turns out that for the generated onehop questions, $98 \\%$ are correct; for the multi-hop questions, $95 \\%$ are correct. It shows that this way of constructing questions is acceptable.\n\n[^0]![img-1.jpeg](img-1.jpeg)\n\nFigure 2: (a) The number of entities with different counts of attributes. (b) The number of entities with different counts of relations.\n\n### 3.4 Dataset Summary\n\nWith the previous steps, we constructed a dataset, AlCuna, for evaluating the ability of LLMs in face of new knowledge. The AlCuna dataset consists of a total of 84351 questions about 3554 artificial entities. We ensure that the constructed artificial entities contain rich and unique attributes and relationships by filtering out parent entities with less than three properties. Specifically, each artificial entity contains 11.75 property triples and 25.39 siblings on average. The distribution of the number of property triplets is shown in Figure 2.\n\nWe organize the dataset in terms of questions, and for each question we collect the corresponding property triplets as evidence and the relevant artificial entities' information as new knowledge. We divide all questions into three subsets, KU, KD and KA, as mentioned in Section 2.4 to measure the corresponding capabilities of LLMs in a fine-grained manner. In specific, KU, KD and KA contain 11316, 27186 and 15353 questions respectively. The details about question forms in the three subsets are shown in Appendix B.1.\n\n## 4 Evaluation of LLMs\n\n### 4.1 LLMs Selected for Evaluation\n\nWe select several popular LLMs for evaluation and analysis on our benchmarks, including ChatGPT, Alpaca-7B, Vicuna-13B and ChatGLM-6B. The detailed description of our selected models can be found in Appendix C.\n\n### 4.2 Evaluation Methods\n\nIn order to adapt the approach in the era of large models and to match the application scenarios in practice, we introduce two types of evaluation methods: zero-shot and few-shot. We implement both the vanilla and \"Chain-of-Thought\" (CoT) reasoning forms for zero-shot and few-shot setting.\n\n[^0]:    ${ }^{4}$ We utilize the tokenizer of GPT-2 for tokenization.\n\nFor experiments in the zero-shot setting, our inputs are structured representations of new knowledge and questions to be answered by the model. For experiments in the few-shot setting, we include several examples of the same form together with the answers, which we hope will help the model understand. For the zero-shot CoT, we append \"Let's think step by step.\" at the end of questions, and for the few-shot CoT, the reasoning process for the answers of examples is also attached. Please refer to Appendix D for the detailed method description. An example of prompt used in our experiment is shown in Table 12.\n\n### 4.3 Evaluation Metric\n\nSince our questions are in the form of multiple choice, fill-in-the-blank or Boolean questions, the golden answers are usually just one or a few words. Therefore, we determine the correctness of the model output by matching it with the answer (like Accuracy). Since there may be more than one possible answer to some questions, such as those asking about the geographical distribution of entities, etc., we consider the answer to be correct as long as it matches one of the correct answers. This is a less stringent measurement, but as seen in Section 5, most models still perform poorly. Using the proposed dataset, we evaluate each model's ability with each method for knowledge understanding, knowledge differentiation, and knowledge association, respectively. We report the average score on the entire benchmark in each setting.\n\n### 4.4 Data Filtering\n\nSince there are differences of internal/existing knowledge of different models due to the different model size and training data, we further filter the samples in our dataset for each model before testing, based on previous work (Petroni et al., 2019), in order not to be influenced by the difference (which is not the theme of our paper) and to compare the models' performance in face of new knowledge in a more focused and fair way. For our method of filtering questions, please refer to Appendix E. We experiment and analyze the four models mentioned in Section 4.1 based on the filtered new knowledge, using the evaluation settings introduced in Section 4.2.\n\n## 5 Result and Analysis\n\n### 5.1 Overall Results\n\nThe performance of the LLMs on our benchmark under different settings is shown in Table 1. We can see that ChatGPT has the best performance in all settings, which is consistent with our usual beliefs. Vicuna has the second best performance among all models. In terms of methods, the few-shot setting performs better than the zero-shot setting overall, and CoT performs better than the vanilla form in most cases.\n\nIn face of new knowledge, as seen in Table 1, LLMs do perform poorly except for ChatGPT on KU and KD experiments. Among all abilities, knowledge association is obviously the most difficult for LLMs, and all of them have difficulty in relating to their internal knowledge through new knowledge provided, and thus in making multi-hop reasoning correctly. The performance of knowledge understanding and knowledge differentiation is better than that of knowledge association, but yet not satisfactory for most LLMs.\n\nIn summary, current LLMs perform relatively poorly in face of new knowledge, slightly better in knowledge understanding and knowledge differentiation, and have more difficulty in reasoning across new and existing knowledge. In order to have a clearer view of models output, please refer to Appendix F for the analysis of models output.\n\nConsidering that it is expensive, slow and unstable to call ChatGPT's API, without loss of generality, all the following comparison experiments for analysis are conducted on three other models. In addition, for convenience, the following analysis experiments are performed in the setting of the vanilla few-shot method, and structured input artificial entity knowledge, if not specifically stated.\n\n### 5.2 Impact of Entity Similarity\n\nIn this section we explore the effect of the similarity between the artificial entity and the parent entity on the model performance over the KD questions, which are designed to assess the model's ability to distinguish between new knowledge and existing knowledge. Specifically, we explore attribute similarity and name similarity.\n\nThe More Similar, the More Confusing (unless powerful enough) We define the proportion of overlap of properties between entities as the property similarity of entities. As shown in Figure 3,\n\n|  | ChatGPT | Alpaca | Vicuna | ChatGLM | ChatGPT | Alpaca | Vicuna | ChatGLM |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Zero-Shot-Vanilla |  |  |  |  | Zero-Shot-CoT |  |  |  |\n## | KU | 50.19 | 31.02 | 34.12 | 34.64 | 68.75 | 39.81 | 39.61 | 24.85 |\n## | KD | 58.70 | 15.35 | 38.65 | 32.29 | 61.78 | 14.29 | 38.53 | 22.84 |\n## | KA | 28.44 | 24.60 | 29.71 | 10.29 | 35.36 | 19.66 | 29.55 | 4.97 |\n| Avg. | 52.85 | 25.12 | 35.98 | 31.26 | 63.34 | 29.44 | 38.00 | 22.04 |\n| Few-Shot-Vanilla |  |  |  |  | Few-Shot-CoT |  |  |  |\n## | KU | 75.44 | 33.80 | 41.22 | 47.97 | 82.18 | 40.77 | 43.67 | 40.91 |\n## | KD | 64.20 | 38.97 | 46.76 | 41.42 | 74.99 | 36.24 | 55.81 | 37.19 |\n## | KA | 41.52 | 27.63 | 30.10 | 27.47 | 37.88 | 25.73 | 25.07 | 26.93 |\n| Avg. | 64.37 | 35.09 | 42.74 | 42.56 | 74.11 | 38.02 | 47.73 | 37.64 |\n\n## Table 1: Performance of LLMs at our benchmark under different settings\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: Relationship between model performance on KD questions and the property similarity between the artificial entity and its parental entity.\n\n|  | Alpaca | Vicuna | ChatGLM |\n| :--: | :--: | :--: | :--: |\n| similar | 38.74 | 47.23 | 41.61 |\n| random | 39.52 | 47.64 | 43.20 |\n\nTable 2: Results on KD questions of the entity with the same property and different name.\nwe divide questions according to the property similarity between the parental entities and the artificial entities, and calculate the scores of models in different similarity ranges on the KD problem respectively. We can find that the performance of the robust ChatGPT to differentiate entities is almost independent of the similarity. But other than it, all other models are affected by entity similarity. The more similar the new entities are to the existing entities, the harder they are to differentiate them, which illustrates the flaws of LLMs. The further analysis is shown in Appendix ??\n\nThe Name also Plays a Role Since each artificial entity is identified by its name, we think that the name might have some effect on the model's ability to distinguish new knowledge, so we conducted experiments on KD questions for comparison. We\n\n| Context | Alpaca | Vicuna | ChatGLM |\n| :-- | :--: | :--: | :--: |\n| artificial | 38.97 | 46.76 | 41.42 |\n| w/ parent | 37.09 | 35.04 | 24.71 |\n| w/ irrelevant | 37.12 | 37.17 | 35.00 |\n\nTable 3: Results on KD questions with different knowledge in context.\nassign two different names to artificial entities with the same properties: one is a randomly generated sequence of characters (random), and the other is a random substitution of one character (similar) for the name of its parent entity. The results of the experiments on the KD questions are shown in Table 2. We can find that the influence of names on model cognition does exist, and similar names are more likely to cause confusion in the model. However, the effect is not very large and both results are lower, which shows that the modeling of new entities by LLM is both derived from the names and influenced by the properties.\n\n### 5.3 Impact of Provided Knowledge\n\nTo further explore the performance of LLMs when new knowledge is provided in the context, we vary the knowledge content provided for the analysis experiments.\n\nParent Entity Aggravates Confusion According to Section 5.2, the models suffer from a certain degree of confusion when faced with new knowledge that overlaps in name or properties with internal knowledge. To further verify this, we explicitly introduce parent entities in the context. Specifically, we conducted two comparison experiments: 1) adding parent entity knowledge to the context; 2) adding a random existing entity knowledge as control variables. As shown in Table 3, the performance is affected by both parent and irrelevant\n\n| Context | Alpaca | Vicuna | ChatGLM |\n| --- | --- | --- | --- |\n| artificial | 27.63 | 30.1 | 27.47 |\n| w/ chain | 29.21 | 33.25 | 44.43 |\n| w/ irrelevant | 25.38 | 22.98 | 18.56 |\n\nTable 4: Results on KA questions with different knowledge in context.\n\n|  | Alpaca |  | Vicuna |  | ChatGLM |  |\n| --- | --- | --- | --- | --- | --- | --- |\n## |  | JSON | NL | JSON | NL | JSON | NL |\n## | KU | 33.8 | 30.63 | 41.22 | 28.04 | 47.97 | 20.07 |\n## | KD | 38.97 | 36.00 | 46.76 | 36.12 | 41.42 | 22.44 |\n## | KA | 27.63 | 26.07 | 30.10 | 25.48 | 27.47 | 9.82 |\n| Avg. | 35.09 | 32.16 | 42.74 | 31.92 | 42.56 | 20.54 |\n\nTable 5: Results on AlCuna with knowledge in JSON and natural language format (NL).\nentities in the context, which is consistent with previous work (Shi et al., 2023). More significantly, for Vicuna and ChatGLM models, the parent entity brings more substantial performance degradation compared to the irrelevant entity, again confirming the confusion problem of existing large models in face of new knowledge.\n\nChain Entities are Key to Knowledge Association To more clearly analyze why all models performs poorly on the knowledge association problem, we conduct two additional sets of experiments on the KA questions: 1) adding knowledge about the entities involved in the reasoning chain to the context. 2) randomly sampling the same number of entities to the context for a fair comparison. The final results are shown in Table 4. We can find that the score of all models improves very much after adding the information of the entities required for inference, and the performance of all models decreases after adding irrelevant entities. This also shows that the main problem is that LLMs really cannot make the association between new and existing knowledge well, and not just the problem of not being able to make reasoning.\n\nStructured Knowledge is Better Since our knowledge is represented structurally, the input of knowledge in our experiments is also structured, as shown in Table 12. To explore the effect of the form of the knowledge representation, we additionally do comparison experiments with knowledge in natural language form as input (NL). We use templates to transform each attribute into a language description for input similar to the template-based question generation process in Section 3.3. As can be seen from Table 5, all models generally perform better with the structured input setting (JSON). The models' understanding of this structured text may come from the code in the training data. This indicates that for this kind of high-density knowledge input, a clear and structured representation is more helpful for the model's understanding.\n\n## 6 Assess New Models with AlCuna\n\nIn this section, we discuss how to apply the proposed AlCuna benchmark to other models. There are two different application scenarios of our benchmark. First, if one wants to assess the knowledge understanding performance of different LLMs in the face of new knowledge, AlCuna can be directly utilized for evaluation. On the other hand, if one wants to compare the knowledge differentiation and association abilities, the different background knowledge inside the different models may lead to an unfair comparison. Therefore, we need to conduct an additional filtering on AlCuna to ensure the existing entities are possessed by all models, which will cause a shrinkage of our benchmark. Despite this, the current benchmark has been filtered on models such as Alpaca. A reasonable assumption is that the models that come after that will be more and more powerful, so the resulting shrinkage won't be very severe.\n\n## 7 Related Work\n\nLarge Language Models In recent years, significant advancements in Large Language Models (LLMs) like FLAN-T5(Chung et al., 2022), GPT3(Brown et al., 2020), OPT(Zhang et al., 2022), LLama(Touvron et al., 2023) and GPT-4(OpenAI, 2023) have led to exceptional performance in natural language processing tasks. At the same time, open-source LLMs based on LLama and other fundamental models for further instruction fine-tuning have emerged recently, such as Alpaca(Taori et al., 2023), Vicuna(Chiang et al., 2023), Koala(Geng et al., 2023), ChatGLM(Du et al., 2022), etc., which have also shown strong capabilities.\n\nThese models have shown breakthroughs on a variety of tasks, and some have been applied as a commercial product in daily work. Since the world is constantly changing, the ability of the models to perform when faced with new knowledge is critical.\n\nExisting Benchmarks Benchmarks, such as SQuAD(Rajpurkar et al., 2016), SNLI(Bowman\n\net al., 2015), GLUE(Wang et al., 2018), SuperGLUE (Wang et al., 2020), LAMBADA (Paperno et al., 2016), etc., is essential for setting evaluation criteria and tracking model performance. While some traditional benchmarks like SQuAD and SNLI assess single-task abilities, GLUE and SuperGLUE evaluate general language models across various NLP tasks. More recently, with rapid development of LLMs, new benchmarks have been proposed to evaluate on more complex tasks such as college entrance exams, law school admission tests and so on (Hendrycks et al., 2021; Guo et al., 2023; Zhong et al., 2023). However, these benchmarks are still based on existing knowledge, which is abundant in the training data of LLM.\n\nSome knowledge benchmarks (Onoe et al., 2021; Mallen et al., 2023; Arodi et al., 2023) evaluate model's ability of knowledge integration while they either only evaluate on small models which are very different with LLMs (training data, ability, etc.) or simply use knowledge that already exists. Therefore, benchmarks that assess LLMs' ability with new knowledge rather than existing knowledge are urgently needed.\n\nSource of New Knowledge Many works use temporal knowledge as a source of new knowledge to evaluate new knowledge behavior of LLMs (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Jang et al., 2023; Zaporojets et al., 2023). There is also some work that uses entity or attribute substitution to create new knowledge (Longpre et al., 2022; Zhou et al., 2023). A discussion of the differences and strengths and weaknesses of our work versus prior work is in Appendix I.\n\n## 8 Conclusion\n\nWe propose a new approach KnowGen to construct new knowledge and build an artificial biological entity benchmark AlCuna for evaluating the ability of LLMs faced with new knowledge. We test and analyze several popular models with commonly used methods, and find some useful conclusions. Our proposed approach and benchmark can help the development of more powerful LLMs that can understand, differentiate and reason across new and existing knowledge. In the future, we expect that more LLMs will be evaluated on our benchmark. More benchmarks in other disciplines also can be constructed based our method.\n\n## Limitations\n\nAlthough the method we design can be used for the construction of any knowledge that satisfies the ontological representation, we have implemented it only on biological data. It is absolutely possible to use this approach to create new knowledge in other domains for evaluating LLMs. It is because KnowGen method for creating new knowledge only requires some defined classes, and some entities in the classes, entities with their own attributes, and connections between the entities. Any knowledge base with such a structure can be used to create new knowledge with our KnowGen method. In the future, we hope that new knowledge datasets from more domains will be available.\n\nWe evaluate only a few powerful models due to the fact that some models are closed source or the number of parameters is too large. We expect that more models can be measured using our benchmark.\n\n## Ethics Statement\n\nThe knowledge of biological entities in our benchmark is artificially constructed, so it does not exist in the real world. Therefore, for humans, it is necessary to be careful not to be misled by the knowledge inside; for models, the hazard of using our dataset for training on model knowledge is unknown. This is in line with our expectation that we want AlCuna to be used as a new knowledge benchmark only for the evaluation of model capabilities in face of new knowledge.\n\n## Acknowledgements\n\nThis work was supported by National Key R\\&D Program of China (2021YFF0901502), National Science Foundation of China (No. 62161160339), State Key Laboratory of Media Convergence Production Technology and Systems and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). We appreciate the anonymous reviewers for their helpful comments. Xiaojun Wan is the corresponding author.\n\n## References\n\nOshin Agarwal and Ani Nenkova. 2022. Temporal effects on pre-trained models for language processing tasks.\n\nAkshatha Arodi, Martin Pömsl, Kaheer Suleman, Adam Trischler, Alexandra Olteanu, and Jackie Chi Kit Cheung. 2023. The kitmus test: Evaluating knowledge integration from multiple sources in natural language understanding systems.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Computational Linguistics.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality.\n\nNurendra Choudhary and Chandan K. Reddy. 2023. Complex logical reasoning over knowledge graphs using large language models.\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models.\n\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335.\n\nAparna Elangovan, Jiayuan He, and Karin Verspoor. 2021. Memorization vs. generalization: Quantifying data leakage in nlp performance evaluation.\n\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post.\n\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection.\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding.\n\nZiniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Kai-Wei Chang, and Yizhou Sun. 2022. Empowering language models with knowledge graph reasoning for question answering.\n\nJoel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, and Minjoon Seo. 2023. Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large language models are zero-shot reasoners.\n\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tomás Kociský, Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, and Phil Blunsom. 2021. Mind the gap: Assessing temporal generalization in neural language models. In Neural Information Processing Systems.\n\nShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2022. Entity-based knowledge conflicts in question answering.\n\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.\n\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work?\n\nNatalya F Noy, Deborah L McGuinness, et al. 2001. Ontology development 101: A guide to creating your first ontology.\n\nYasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and Greg Durrett. 2021. Creak: A dataset for commonsense reasoning over entity knowledge.\n\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder,\n\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.\n\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525-1534, Berlin, Germany. Association for Computational Linguistics.\n\nCynthia S Parr, Mr Nathan Wilson, Mr Patrick Leary, Katja S Schulz, Ms Kristen Lans, Ms Lisa Walley, Jennifer A Hammock, Mr Anthony Goddard, Mr Jeremy Rice, Mr Marie Studer, et al. 2014. The encyclopedia of life v2: providing global access to knowledge about life on earth. Biodiversity data journal, (2).\n\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback.\n\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 2019. Language models as knowledge bases?\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.\n\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context.\n\nJohn F. Sowa. 1995. Top-level ontological categories. International Journal of Human-Computer Studies, 43(5):669-685.\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.\n\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809-819, New Orleans, Louisiana. Association for Computational Linguistics.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2020. Superglue: A stickier benchmark for general-purpose language understanding systems.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models.\n\nKlim Zaporojets, Lucie-Aimee Kaffee, Johannes Deleu, Thomas Demeester, Chris Develder, and Isabelle Augenstein. 2023. Tempel: Linking dynamically evolving and newly emerging entities.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models.\n\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models.\n\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful prompting for large language models.\n\n## A KnowGen Method\n\nHere, we will describe knowgen in the form of natural language and math formulas.\n\nFollowing the symbolic representation in Section 2.2, we divide the property triplets $\\mathcal{T}(e)=$ $\\mathcal{T}_{A}(e) \\cup \\mathcal{T}_{R}(e)$ of parent entity into three sets: remain, change and delete, denoted as $\\mathcal{T}^{*}(e)=$ $\\mathcal{T}_{A}^{*}(e) \\cup \\mathcal{T}_{R}^{*}(e), * \\in\\{r, c, d\\}$. We randomly sample some triplets from sibling entities as add set $\\mathcal{T}^{a}(\\tilde{e})=\\mathcal{T}_{A}^{a}(\\tilde{e}) \\cup \\mathcal{T}_{R}^{a}(\\tilde{e})$. Subsequently, we fuse the above four sets ${ }^{5}$ to form the properties of artificial entity. We retain $T^{r}(e)$ for high similarity between the parent and artificial entities.\n\n$$\n\\begin{aligned}\n& \\mathcal{T}_{A}^{r}(\\tilde{e})=\\left\\{(\\tilde{e}, a, v) \\mid(e, a, v) \\in \\mathcal{T}_{A}^{r}(e)\\right\\} \\\\\n& \\mathcal{T}_{R}^{r}(\\tilde{e})=\\left\\{\\left(\\tilde{e}, r, e^{\\prime}\\right) \\mid\\left(e, r, e^{\\prime}\\right) \\in \\mathcal{T}_{R}^{r}(e)\\right\\}\n\\end{aligned}\n$$\n\nWe modify the object of triplets in $T^{c}(e)$ with some perturbation to create uniqueness in the properties of the artificial entity, but without introducing overly unusual values.\n\n$$\n\\begin{aligned}\n& \\mathcal{T}_{A}^{c}(\\tilde{e})=\\left\\{(\\tilde{e}, a, \\operatorname{perturb}(v)) \\mid(e, a, v) \\in \\mathcal{T}_{A}^{c}(e)\\right\\} \\\\\n& \\mathcal{T}_{R}^{c}(\\tilde{e})=\\left\\{(\\tilde{e}, r, \\operatorname{perturb}\\left(e^{\\prime}\\right)) \\mid\\left(e, r, e^{\\prime}\\right) \\in \\mathcal{T}_{R}^{c}(e)\\right\\}\n\\end{aligned}\n$$\n\n, in which perturb() represents a perturbation function that can modify values according to specific needs. For numeric attribute value $v$, we add a gaussian noise to it. For non-numeric attribute value $v$, we use the object of the same attribute from siblings entities. For association object entity $e^{\\prime}$, we randomly choose one of its siblings as the modified value.\n\nWe also include $T^{a}(\\tilde{e})$ to ensure the commonality of entities within the specific class $C$.\n\n$$\n\\begin{aligned}\n\\mathcal{T}_{A}^{a}(\\tilde{e})= & \\operatorname{RandomSample}(\\{(\\tilde{e}, r, v) \\mid \\\\\n& \\left.\\left(e_{i}, r, v\\right) \\in \\mathcal{T}_{A}\\left(e_{i}\\right), e_{i} \\in \\operatorname{Sib}(\\tilde{e})\\right\\} \\\\\n\\mathcal{T}_{R}^{a}(\\tilde{e})= & \\operatorname{RandomSample}\\left(\\left\\{\\left(\\tilde{e}, r, e^{\\prime}\\right)\\right.\\right. \\\\\n& \\left.\\left.\\left(e_{i}, r, e^{\\prime}\\right) \\in \\mathcal{T}_{R}\\left(e_{i}\\right), e_{i} \\in \\operatorname{Sib}(\\tilde{e})\\right\\}\n\\end{aligned}\n$$\n\n## B Dataset Details\n\n## B. 1 Question Types Distribution\n\nAs mentioned in Section 2.4, we divide the AlCuNA into three subsets. Each subset contains a certain number of multiple choice, fill-in-the-blank,\n\n[^0]Table 6: Number of different forms of KU, KD and KA questions.\n\n|  | Total |  |  |  |\n| :-- | --: | --: | --: | --: |\n|  | ChatGPT | Vicuna | Alpaca | ChatGLM |\n| Refuse | 20.01 | 17.3 | 20.69 | 17.45 |\n| Multi | 0.5 | 0.2 | 0.04 | 0.06 |\n| Wrong | 79.49 | 82.5 | 79.27 | 82.49 |\n| Total | 100.00 | 100.00 | 100.00 | 100.00 |\n\nTable 7: The percentage of categories of incorrect responses from large models.\nand Boolean questions. The specific distribution of question types is shown in Table 6. Note that we generate only multiple choice questions for the DA question set. This is done for two reasons: one is that the mutli-hop questions are too hard for current LLMs as shown in Table 1, and the other is that there may be multiple answers to one multi-hop question, and to evaluate the results more accurately and avoid false negatives, we utilize choices to limit the answer space.\n\n## B. 2 Natural Language Question Generation\n\nWe prompt ChatGPT to generate question templates given a property triplet in one-hop setting or a chain of property triplets in multi-hop setting. We provide the prompt to generate Boolean, fill-in-the-blank question templates under both settings in Figure 4, 5 and 6. We also show some generated question templates in Table 8. To generate multiple choice questions, we simply append four choices to the corresponding fill-in-the-blank questions.\n\nIn order to guarantee the accuracy of the questions generated through this approach, we select a random sample of 100 questions for both one-hop and multi-hop questions to be checked by humans. The results indicate that over $98 \\%$ of the one-hop questions and over $95 \\%$ of the multi-hop questions generated are accurate.\n\n## C Introduction to Our Model We Select\n\nWe choose four representative and popular LLMs. ChatGPT, is one of the most powerful models, but is close-source, so we can only call it through the API. The other three are all excellent open-source\n\n[^0]:    ${ }^{5}$ We process these four collections in a manner that ensures no overlapping or shared properties between them, thereby eliminating any potential contradictions.\n\n| Relation Chain or Property | Generated Templates |\n| :--: | :-- |\n| prey on | What are the animals that [T] prey on? |\n| What animals are preyed on by [T]? |  |\n| cingulum location | Where is the cingulum located in the [T]'s mouth? |\n| What is the type of cingulum in the teeth of [T]? |  |\n| frost free days | How many frost free days are required for the growth of [T]? |\n| How many frost-free days does the habitat of [T] have on average? |  |\n| (have host, co-roost with) | What is the species that co-roosts with the host of [T]? |\n| What species shares a roosting habitat with the host of [T]? |  |\n| (parasitize, visit flowers of, eat) | What is the food source of a species that feeds on the flowers visited by an organism parasitized by [T]? |\n| What is the food source of the species whose flowers are visited by an organism parasitized by '[T]? |  |\n\nTable 8: Question templates generated from properties of entities or chains of reasoning.\n\nSystem: You are a powerful multi-hop question generator with biological knowledge. Users will provide a chain of RDF triples, and you will help write a question to ask the tail entity from the head entity. You shouldn't include bridge entities in the generated question. The question should only include the head entity [T], which is placeholder. If you can't create a valid question, reply with $\\backslash$ \"[None] $\\backslash$ \" only. Don't reply with any explanation or other information.\n\n```\n/* exemplars */\nUser: Given RDF triples ([T], compete with, x1), (x1, are eaten by, x2), (x2, are host of, x3), write a question to ask\nx3. Don't mention x1, x2, ... Write the possible question in natural English.\nAgent: Multi-hop question: Which entities serve as the habitats for the creatures that feed on the organisms\ncompeting with [T]?\nUser: Given RDF triples ([T], parasitize, x1), (x1, prey on, x2), write a question to ask x2. Don't mention x1, x2, ...\nWrite the possible question in natural English.\n## Agent: Multi-hop question: What is the prey of an organism that is parasitized by [T]?\nUser: Given RDF triples ([T], eat, x1), (x1, compete with, x2), (x2, are pathogens of, x3), write a question to ask\nx3. Don't mention x1, x2, ... Write the possible question in natural English.\nAgent: Multi-hop question: Which species has pathogens that compete with species that are eaten by [T]?\n/* query */\nUser: Given RDF triples (\\$Triple chain), write a question to ask (\\$Tail entity). Don't mention x1, x2, ... Write the\npossible question in natural English.\n```\n\nFigure 4: Prompt for generating multi-hop question templates.\n\nSystem: You are a powerful question generation model with biological knowledge. Given a biological taxon's property name and a set of possible values of the property. You need to generate a yes-no question template with placeholders [T] and [V] about the given property satisfying the following constraints:\n\n## 1. Most important: the generated question template must contain [T] and [V].\n## 2. The placeholder $[T]$ can be replaced with any taxons name.\n3. The placeholder $[\\mathrm{V}]$ can be replaced with possible value of the given property. You can refer to the given value set for the format of possible property value. But don't use any value in the value set directly.\n## 4. Don't do the replacement, just provide the question template with [T] and [V] placeholders.\n## 5. Try to create the question even if the property is not a biological property.\n## 6. Don't reply with any explanation or other information.\n\n```\n/* exemplars */\n## User: Property name: skeleton contains Value set: ['chitin']\n## Agent: Question template: Does the skeleton of [T] contain [V]?\nUser: Property name: body shape Value set: ['cylinder', 'elongated', 'bilaterally symmetric', 'radially symmetric']\n## Agent: Question template: What's the body shape of [T]\nUser: Property name: litters per year Value set: ['1.0/year','3.0/year','1.5/year','2.0/year','2.5/year']\n## Agent: Question template: Does one [T] produce [V] litters per year?\n```\n\n/* query */\n## User: Property name: \\{\\$Property name\\} Value set: \\{\\$Possible values of Property\\}\n\nFigure 5: Prompt for generating Boolean question templates.\n\n| System: You are a powerful question generation model with biological knowledge. Given a biological taxon's |\n| :-- |\n| property name. You need to generate a question template with a placeholder [T] about the given property so that |\n| the placeholder [T] can be replaced with any taxons name. Try to create the question even if the property is not a |\n| biological property. Don't reply with any explanation or other information. |\n| /* exemplars */ |\n| User: Property name: skeleton contains |\n| Agent: Question template: Which organic compound is a component of a [T]'s skeleton? |\n| User: Property name: body shape |\n| Agent: Question template: What's the body shape of [T] |\n| User: Property name: litters per year |\n| Agent: Question template: How many litters can [T] have per year? |\n| /* query */ |\n| User: Property name: \\{\\$Property name\\} |\n\nFigure 6: Prompt for generating fill-in-the-blank question templates.\nmodels. Due to the limitation of GPU memory space, we choose those open-source models ranging from 6B-13B.\n\n- ChatGPT: ChatGPT is a sibling model to InstructGPT (Ouyang et al., 2022), which is trained on a vast instruction dataset and further tuned by reinforcement learning with human feedbacks (RLHF).\n- Alpaca-7B: Alpaca is an open-source instruction-following LLM trained for academic purposes, which is fine-tuned from the LLaMA 7B model (Touvron et al., 2023) on 52 K instruction-following demonstrations.\n- Vicuna-13B: Vicuna is trained by fine-tuning LLaMA using 70K conversations with ChatGPT shared by users. A preliminary evaluation using GPT-4 as a judge shows that Vicuna13B achieves more than $90 \\%$ of the quality of ChatGPT.\n- ChatGLM-6B: ChatGLM is also an opensource instruction-following LLM, which is based on General Language Model (Du et al., 2022) framework.\n\nWe download the above three open source models from Huggingface ${ }^{6}$, and thanks to the convenient design of the FastChat ${ }^{7}$ library, we unify the testing framework for all models and call them through the API.\n\nWe also consider several models with 7B parameters for evaluation to compare the performance of models of same size, which may help to analyze the\n\n[^0]impact of different training processes on the ability in face of new knowledge. We select Llama2Chat-7B, Alpaca-7B, Vicuna-7B, and ChatGLM7B specifically. The results are presented on Table 9 .\n\n## D Introduction to Our Experiment Method\n\n### D. 1 Zero/Few-shot Evaluation Setting\n\nThe zero-shot setting is where the model is given explicit instructions to directly complete the mission. This scenario evaluates the ability of the original model to solve the problem autonomously without training. In our benchmark, the input to the zero-shot is the new knowledge of the artificial entity and a question to be asked about it.\n\nCompared to the zero-shot setting, in the fewshot setting the model is given several additional examples from the same task as a reference. This allows evaluating the ability of the model to learn the task quickly based on a limited number of samples, and is also consistent with practical situations where supervised training is not convenient. According to the Min et al. (2022)'s study, in our benchmark assessment, we provided 3 to 5 examples for each type of problem, expecting that it would be sufficient to be able to demonstrate the labeling space for this type of problem.\n\n### D. 2 Chain-of-Thought (CoT) Form\n\nFor both the zero-shot and few-shot evaluation settings, we add the design of the CoT form. For the zero-shot setting, we added the words \"Let's think step by step.\" at the end of the question, expecting the model to output the thinking process, which can help LLM to reason about complex problems. For\n\n[^0]:    ${ }^{6}$ https://huggingface.co/\n${ }^{7}$ https://github.com/lm-sys/FastChat\n\n|  | Llama2-Chat-7B | Alpaca-7B | Vicuna-7B | ChatGLM-6B | Llama2-Chat-7B | Alpaca-7B | Vicuna-7B | ChatGLM-6B |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Zero-Shot-Vanilla |  |  |  |  | Zero-Shot-CoT |  |  |  |\n## | KU | 28.92 | 31.02 | 28.00 | 34.64 | 47.75 | 39.81 | 32.74 | 24.85 |\n## | KD | 32.61 | 15.35 | 34.70 | 32.29 | 37.48 | 14.29 | 33.61 | 22.84 |\n## | KA | 18.71 | 24.60 | 23.40 | 10.29 | 24.61 | 19.66 | 17.56 | 4.97 |\n| Avg. | 29.57 | 25.12 | 30.88 | 31.26 | 39.50 | 29.44 | 31.67 | 22.04 |\n| Few-Shot-Vanilla |  |  |  |  | Few-Shot-CoT |  |  |  |\n## | KU | 37.06 | 33.80 | 32.78 | 47.97 | 65.20 | 40.77 | 38.50 | 40.91 |\n## | KD | 44.38 | 38.97 | 39.39 | 41.42 | 44.71 | 36.24 | 38.71 | 37.19 |\n## | KA | 24.42 | 27.63 | 24.39 | 27.47 | 28.72 | 25.73 | 27.11 | 26.93 |\n| Avg. | 39.27 | 35.09 | 35.14 | 42.56 | 50.15 | 38.02 | 37.21 | 37.64 |\n\nTable 9: Performance of LLMs of 7B parameters at our benchmark under different settings.\n\n|  | multiple choice | Boolean | fill-in-the-blank |\n| :-- | :--: | :--: | :--: |\n## | KU | 2025 | 3138 | 3814 |\n## | KD | 2486 | 24592 | 12601 |\n## | KA | 8757 | 0 | 0 |\n| Total | 13268 | 27730 | 16415 |\n\nTable 10: Number of different forms of KU, KD and KA questions after filtering.\nthe few-shot setting, we add the thought process in the answer to each sample question shown to inspire the model.\n\n## E Details of Our Approach to Filtering Questions\n\nSpecifically, we retain only those artificial entities whose parent entities could be perfectly recalled by the model. In addition, since answering multi-hop questions requires the model to make use of each single-hop knowledge, we then filter out any reasoning chains that contain knowledge that cannot be correctly recalled by the model.\n\nThe method used for the above two filtering is to construct question templates for the knowledge involved, including attributes and relationships, based on previous work [Petroni et al. (2019)], and then to query the model using the few-shot setting. We filter samples in our benchmark for every evaluated model to ensure that our questions are specific to the ability about new knowledge, and then select the intersection of filtered questions for fair experimentation and analysis. The number of questions per category left is shown in Table 10.\n\n## F Analysis about Models' Output\n\nAn example of the output of the model is shown in Table 11. To better analyze the models' responses, as shown in Table 7, we divide all the models' error outputs into 3 categories, including rejecting\nresponses, answering multiple options, and other incorrect responses.\n\nWe can find that the percentage of answering multiple options for all models is very small, which indicates that all models can understand and comply with our requirements very well. In addition, some of the questions are rejected by some models, probably because some models recognizes that it cannot answer the corresponding question and responds with \"I don't know\" or \"I am sorry\".\n\n## G Impact of Different Modifications\n\nAs shown in Section 5.2, models commonly struggle with knowledge differentiation when the artificial entity and the parent entity are similar. In this section, we further conduct an ablation study to investigate the specific impact of different modifications (i.e. variation and dropout).\n\nTo ablate one type of modification, we reconstruct artificial entities in the KD dataset. For each question in the original KD dataset, we have a parent entity $e^{p}$ and the corresponding attribute $a$ to be queried. We then reconstruct several artificial entities by modifying one attribute of $e^{p}$ (except $a$ ) at a time. From this, we obtain several artificial entities with different similarity caused by the same type of modification. For a fair comparison, we only experiment with $e^{p}$ with 10 attributes. We randomly sample 1000 parent entities and create a total of $1000 \\times 10=10000$ new artificial entities. Finally, we conduct experiments by querying models about them with the same question about $a$.\n\nThe experiment results are presented in Table 7. Different models exhibit almost the same trend under both modifications. It is clear that dropout yields a stable improvement as the similarity decreases while the impact of variation is relatively weak and insignificant.\n\n## Question: What organism is parasitized by an organism that feeds on Agabus ajax?\n## Answer: Rangifer tarandus\nModel's Output: \"Thought process: Agabus ajax is a species of aquatic beetle that feeds on Acari, Culicidae, Daphnia magna, Daphnia pulex, and Ceratopogonidae. To answer the question, we need to identify an organism that is parasitized by an organism that feeds on Agabus ajax.\n## Final answer: Thala gorii\"]\n## Question: Is Sorghum versinatum a type of hyperhydate?\n## Answer: Yes\n## Model's Output: \"ANSWER: Yes\"\n## Question: What's the preferred water depth range for Plana?\nAnswer: 500.0 cm\nModel's Output: ANSWER: $500 \\mathrm{~cm} \\ln \\ln \\mathrm{~B}$ is important to note that the information\n\nTable 11: Example of question and answer from our models.\n![img-3.jpeg](img-3.jpeg)\n\nFigure 7: Relationship between model performance on KD questions and the property similarity between the artificial entity and its parental entity due to different levels of modification, i.e. variation and dropout.\n\n## H Prompts in Experiments\n\nIn this section, we will show prompts used in our settings. Since our questions contain Boolean, multiple choice, and fill-in-the-blank forms, the rest of the prompt is identical except for the restrictions on the output format for each type of question that are targeted. Therefore, without loss of generality, we provide the prompts for the fill-in-the-blank questions as a demonstration. We present the CoT zero-shot prompt as an example in Table 12. The other methods of prompt are basically similar.\n\n## I Source of New Knowledge\n\nTemporal Knowledge Using temporal knowledge as a source of new knowledge has the following disadvantages:\n\n1. the expense of collecting data. With the continuous emergence of new LLMs, the tempo-\nral knowledge used needs to be re-collected each time, requiring labor and resources to race with the training of the LLM.\n2. uncertain validity and risk of information leakage. Some LLMs do not announce the training data they use, so it is not known whether the temporal knowledge collected each time is still valid or not.\n3. fairness of comparison. Since the range of timestamps for training data of each model is different, the new temporal knowledge is different for each model. Therefore, the test data used for each evaluation needs to be different, and it is uncertain whether such a comparison is fair.\n\nIn contrast, our proposed KnowGen method solves the above problems. Since it is artificial knowledge, it will be valid for a long time and no\n\nYou are a powerful question-answering system with knowledge in the field of biology. Users will provide some biological information along with a question.\nYour task is to combine the information provided by the user with your biological knowledge to answer the question.\nIf you are unable to answer the question, simply respond with \"I don't know.\"\n## Here is the basic information about a taxon you can refer:\n\\#\\#\\#\n\\{\n\"name\": \"Bainvillevillea spinosa\",\n\"property\": \\{\n\"cellularity\": [\"multicellular\"],\n\"conservation status\": [\"least concern\"],\n\"geographic distribution\": [\"Ecuador\"],\n\"habitat\": [\"terrestrial\"],\n\"leaf complexity\": [\"compound\"],\n\"leaf morphology\": [\"broad\"],\n\"leaf sheddability\": [\"evergreen\"],\n\"plant growth form\": [\"branched\"],\n\"produces\": [\"oxygen\"],\n\"woodiness\": [\"woody\"]\n\\},\n\"rank\": \"species\"\n\\}\n\\#\\#\\#\n## Answer the following question a few words: What is the habitat of Bainvillevillea spinosa?\nDesired format: Thought process: <Thought process>, Final answer: [Final answer].\nLet's think step by step.\nTable 12: Demonstration of the zero-shot prompt in the CoT form.\n\nmore effort is required to collect data repeatedly. Moreover, for all models, the knowledge is definitely new, so we can also use the same test data to evaluate the models, which also ensures the validity and fairness of the evaluation.\n\nPrevious methods of entity and attribute substitutions As for the relationship to previous work (Longpre et al., 2022; Zhou et al., 2023), all of us \"construct\" knowledge, but our knowledge forms, construction purposes, and implementation methods are different.\n\n1. Knowledge form: The knowledge in Longpre et al. (2022) and Zhou et al. (2023) is a statement of a few sentences describing a simple fact. Whereas our knowledge is represented in ontological form, with complete properties, relations and classes, which is more structured and complete.\n2. Purpose of construction: both of them are designed to construct knowledge that contradicts the internal knowledge of the model, as a way to make the model hallucinatory or unreliable predictions. Instead, our work is intended to simulate the generation of new knowledge that is associated and consistent with the existing knowledge.\n3. Implementation: the methods of them are limited by the form of knowledge representation, and merely construct counterfactuals by replacing the name of the entity in the sentence with the name of another existing entity. Our method, on the other hand, will create a completely new entity with a new name and which reasonably exists in the original knowledge system through operations such as heredity, variation and dropout."
    }
  ],
  "pdf_images": {
    "pdf_path": "inbox/2310.14820v1.pdf",
    "total_pages": 18,
    "pages": [
      {
        "page_number": 1,
        "filename": "page-001.png",
        "thumb_filename": "page-001-thumb.png",
        "mobile_filename": "page-001-mobile.png",
        "mobile_thumb_filename": "page-001-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 2,
        "filename": "page-002.png",
        "thumb_filename": "page-002-thumb.png",
        "mobile_filename": "page-002-mobile.png",
        "mobile_thumb_filename": "page-002-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 3,
        "filename": "page-003.png",
        "thumb_filename": "page-003-thumb.png",
        "mobile_filename": "page-003-mobile.png",
        "mobile_thumb_filename": "page-003-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 4,
        "filename": "page-004.png",
        "thumb_filename": "page-004-thumb.png",
        "mobile_filename": "page-004-mobile.png",
        "mobile_thumb_filename": "page-004-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 5,
        "filename": "page-005.png",
        "thumb_filename": "page-005-thumb.png",
        "mobile_filename": "page-005-mobile.png",
        "mobile_thumb_filename": "page-005-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 6,
        "filename": "page-006.png",
        "thumb_filename": "page-006-thumb.png",
        "mobile_filename": "page-006-mobile.png",
        "mobile_thumb_filename": "page-006-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 7,
        "filename": "page-007.png",
        "thumb_filename": "page-007-thumb.png",
        "mobile_filename": "page-007-mobile.png",
        "mobile_thumb_filename": "page-007-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 8,
        "filename": "page-008.png",
        "thumb_filename": "page-008-thumb.png",
        "mobile_filename": "page-008-mobile.png",
        "mobile_thumb_filename": "page-008-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 9,
        "filename": "page-009.png",
        "thumb_filename": "page-009-thumb.png",
        "mobile_filename": "page-009-mobile.png",
        "mobile_thumb_filename": "page-009-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 10,
        "filename": "page-010.png",
        "thumb_filename": "page-010-thumb.png",
        "mobile_filename": "page-010-mobile.png",
        "mobile_thumb_filename": "page-010-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 11,
        "filename": "page-011.png",
        "thumb_filename": "page-011-thumb.png",
        "mobile_filename": "page-011-mobile.png",
        "mobile_thumb_filename": "page-011-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 12,
        "filename": "page-012.png",
        "thumb_filename": "page-012-thumb.png",
        "mobile_filename": "page-012-mobile.png",
        "mobile_thumb_filename": "page-012-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 13,
        "filename": "page-013.png",
        "thumb_filename": "page-013-thumb.png",
        "mobile_filename": "page-013-mobile.png",
        "mobile_thumb_filename": "page-013-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 14,
        "filename": "page-014.png",
        "thumb_filename": "page-014-thumb.png",
        "mobile_filename": "page-014-mobile.png",
        "mobile_thumb_filename": "page-014-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 15,
        "filename": "page-015.png",
        "thumb_filename": "page-015-thumb.png",
        "mobile_filename": "page-015-mobile.png",
        "mobile_thumb_filename": "page-015-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 16,
        "filename": "page-016.png",
        "thumb_filename": "page-016-thumb.png",
        "mobile_filename": "page-016-mobile.png",
        "mobile_thumb_filename": "page-016-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 17,
        "filename": "page-017.png",
        "thumb_filename": "page-017-thumb.png",
        "mobile_filename": "page-017-mobile.png",
        "mobile_thumb_filename": "page-017-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      },
      {
        "page_number": 18,
        "filename": "page-018.png",
        "thumb_filename": "page-018-thumb.png",
        "mobile_filename": "page-018-mobile.png",
        "mobile_thumb_filename": "page-018-mobile-thumb.png",
        "width": 1241,
        "height": 1754,
        "mobile_width": 2481,
        "mobile_height": 3508
      }
    ]
  },
  "metadata": {
    "model": "gpt-4o",
    "pdf_path": "inbox/2310.14820v1.pdf"
  },
  "attribution": {
    "type": "arxiv",
    "url": "https://arxiv.org/abs/2310.14820",
    "id": "2310.14820"
  }
}