<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Variational Lossy Autoencoder - RYO Analysis</title>
    <link rel="stylesheet" href="../../assets/style.css">
</head>
<body>
    <!-- Desktop Header -->
    <header class="desktop-header">
        <div class="header-left">
            <h1><a href="../../index.html">9030club</a> / Variational Lossy Autoencoder <a href="https://arxiv.org/abs/1611.02731" target="_blank" style="color: #666; text-decoration: none; font-size: 0.9rem;">(arXiv)</a></h1>
            <div class="filename-subtitle">1611.02731v2.pdf</div>
        </div>
        <div class="header-right">
            <div class="mode-toggle">
                <button class="mode-btn active" data-mode="markdown">markdown</button>
                <button class="mode-btn" data-mode="pdf">pdf</button>
                <button class="qr-btn" id="qr-btn" title="Show QR Code">
                    <img src="../../assets/qr-code-icon.png" alt="QR Code" style="width: 16px; height: 16px;">
                </button>
            </div>
        </div>
    </header>
    
    <!-- Desktop 4-Column Layout -->
    <main class="four-column-layout">
        <section class="questions-column">
            <ul class="question-list">
                <li><button class="question-btn" data-question="1">1. What were they trying to do?</button></li>
<li><button class="question-btn" data-question="2">2. Why does it matter?</button></li>
<li><button class="question-btn" data-question="3">3. What did they try?</button></li>
<li><button class="question-btn" data-question="4">4. Did it work?</button></li>
<li><button class="question-btn" data-question="5">5. What did they compare it to?</button></li>
<li><button class="question-btn" data-question="6">6. What was it tested on?</button></li>
<li><button class="question-btn" data-question="7">7. What's cool about it?</button></li>
<li><button class="question-btn" data-question="8">8. What's sketchy about it?</button></li>
<li><button class="question-btn" data-question="9">9. Can anyone use this?</button></li>
<li><button class="question-btn" data-question="10">10. What's still left to figure out?</button></li>

            </ul>
        </section>
        
        <section class="answers-column">
            <div class="answer-content">
                <p class="placeholder">Click a question to see the answer</p>
            </div>
        </section>
        
        <section class="page-column">
            <div class="page-content">
                <h3>3 Variational Lossy Autoencoder</h3>
<p><h1>3 Variational Lossy Autoencoder</h1></p><p>The discussion in Section 2.2 suggests that autoregressive models cannot be combined with VAE since information will be preferred to be modeled by autoregressive models. Nevertheless, in this section, we present two complementary classes of improvements to VAE that utilize autoregressive models fruitfully to explicitly control representation learning and improve density estimation.</p><p><h2>### 3.1 LOSSY CODE VIA EXPLICIT INFORMATION PLACEMENT</h2></p><p>Even though the information preference property of VAE might suggest that one should always use the full autoregressive models to achieve a better code length/log-likelihood, especially when slow data generation is not a concern, we argue that this information preference property can be exploited to turn the VAE into a powerful representation learning method that gives us fine-grained control over the kind of information that gets included in the learned representation.</p><p>When we try to learn a lossy compression/representation of data, we can simply construct a decoding distribution that's capable of modeling the part of information that we don't want the lossy representation to capture, but, critically, that's incapable of modelling the information that we do want the lossy representation to capture.</p><p>For instance, if we are interested in learning a global representation for 2D images that doesn't encode information about detailed texture, we can construct a specific factorization of the autoregressive distribution such that it has a small local receptive field as decoding distribution, e.g., $p_{\text {local }}(\mathbf{x} \mid \mathbf{z})=\prod_{i} p\left(\mathbf{x}_{i} \mid \mathbf{z}, \mathbf{x}_{\text {WindowAround }(i)}\right)$. Notice that, as long as $\mathbf{x}_{\text {WindowAround }}(i)$ is smaller than $\mathbf{x}_{<i}, p_{\text {local }}(\mathbf{x} \mid \mathbf{z})$ won't be able to represent arbitrarily complex distribution over $\mathbf{x}$ without dependence on $\mathbf{z}$ since the receptive field is limited such that not all distributions over $\mathbf{x}$ admit such factorizations. In particular, the receptive field window can be a small rectangle adjacent to a pixel $\mathbf{x}_{i}$ and in this case long-range dependency will be encoded in the latent code $\mathbf{z}$. On the other hand, if the true data distribution admits such factorization for a given datum $\mathbf{x}$ and dimension $i$, i.e.</p><p>$p_{\text {data }}\left(\mathbf{x}_{i} \mid \mathbf{x}_{\text {WindowAround }(i)}\right)=p_{\text {data }}\left(\mathbf{x}_{i} \mid \mathbf{x}_{<i}\right)$, then the information preference property discussed in Section 2.2 will apply here, which means that all the information will be encoded in local autoregressive distribution for $\mathbf{x}_{i}$. Local statistics of 2D images like texture will likely be modeled completely by a small local window, whereas global structural information of an images like shapes of objects is long-range dependency that can only be communicated through latent code $\mathbf{z}$. Therefore we have given an example VAE that will produce a lossy compression of 2D images carrying exclusively global information that can't be modeled locally.</p><p>Notice that a global representation is only one of many possible lossy representations that we can construct using this information preference property. For instance, the conditional of an autoregressive distribution might depend on a heavily down-sampled receptive field so that it can only model long-range pattern whereas local high-frequency statistics need to be encoded into the latent code. Hence we have demonstrated that we can achieve explicit placement of information by constraining the receptive field/factorization of an autoregressive distribution that's used as decoding distribution.</p><p>We want to additionally emphasize the information preference property is an asymptotic view in a sense that it only holds when the variational lowerbound can be optimized well. Thus, we are not proposing an alternative to techniques like free bits Kingma et al. (2016) or KL annealing, and indeed they are still useful methods to smooth the optimization problem and used in this paper's experiments.</p>
            </div>
        </section>
        
        <section class="thumbnails-column">
            <ul class="thumbnail-list">
                <li><button class="thumbnail-btn" data-page="1">1. 3 Variational Lossy Autoencoder</button></li>
<li><button class="thumbnail-btn" data-page="2">2. 3.2 Learned Prior with Autoregressive Flow</button></li>
<li><button class="thumbnail-btn" data-page="3">3. 4.3 Natural Images: CIFAR10</button></li>
<li><button class="thumbnail-btn" data-page="4">4. 5 Related Work</button></li>
<li><button class="thumbnail-btn" data-page="5">5. C Soft Free Bits</button></li>

            </ul>
        </section>
    </main>
    
    <!-- Mobile Layout -->
    <div class="mobile-layout">
        <!-- Mobile Header -->
        <header class="mobile-header">
            <div class="mobile-title">
                <h1><a href="../../index.html">9030club</a></h1>
                <div class="mobile-paper-title">Variational Lossy Autoencoder <a href="https://arxiv.org/abs/1611.02731" target="_blank" style="color: #666; text-decoration: none; font-size: 0.9rem;">(arXiv)</a></div>
                <div class="mobile-filename">1611.02731v2.pdf</div>
            </div>
            <button class="mobile-qr-btn" id="mobile-qr-btn" title="Show QR Code">
                <img src="../../assets/qr-code-icon.png" alt="QR Code" style="width: 20px; height: 20px;">
            </button>
        </header>
        
        <!-- Mobile Navigation Container -->
        <div class="mobile-nav-container">
            <!-- Mobile Tab Navigation -->
            <div class="mobile-tabs">
                <button class="mobile-tab-btn active" data-tab="qa">QA</button>
                <button class="mobile-tab-btn" data-tab="pdf">PDF</button>
                <button class="mobile-tab-btn" data-tab="markdown">Markdown</button>
            </div>
            
            <!-- Mobile Navigation Bar -->
            <div class="mobile-nav" id="mobile-nav">
                <button class="mobile-nav-btn" id="mobile-prev-btn">‹</button>
                <div class="mobile-nav-info" id="mobile-nav-info">Q1 of 10</div>
                <button class="mobile-nav-btn" id="mobile-next-btn">›</button>
            </div>
        </div>
        
        <!-- Mobile Content Area -->
        <div class="mobile-content" id="mobile-content">
            <div class="mobile-content-inner">
                <p class="mobile-placeholder">Loading content...</p>
            </div>
        </div>
    </div>
    
    <!-- QR Code Overlay -->
    <div class="qr-overlay" id="qr-overlay">
        <img src="qr-code.png" alt="QR Code" id="qr-image">
    </div>
    
    <script src="../../assets/app.js"></script>
    <script>
        // Initialize with paper data
        window.paperData = {"title": "Variational Lossy Autoencoder", "slug": "1611.02731v2", "questions": {"1": {"question": "What were they trying to do?", "answer": "The authors aimed to develop a method for learning global representations of data through a Variational Lossy Autoencoder (VLAE) that combines Variational Autoencoders (VAEs) with autoregressive models. Their goal was to control the information captured in the latent code, allowing for the discarding of irrelevant details, such as texture in images, while improving generative modeling performance on various datasets.", "timestamp": "/Users/atobe/Dropbox/OODA/1071_ryo"}, "2": {"question": "Why does it matter?", "answer": "The research presented in the paper matters because it advances the field of representation learning by introducing a method that effectively combines Variational Autoencoders (VAEs) with autoregressive models, allowing for the creation of lossy representations that capture essential global structures while discarding irrelevant local details. This has significant implications for various applications, such as image classification and generative modeling, by improving the efficiency and performance of machine learning models. Ultimately, this work contributes to the broader goal of enhancing how machines understand and process complex data, which is crucial for advancements in artificial intelligence.", "timestamp": "2025-06-28 10:16:16"}, "3": {"question": "What did they try?", "answer": "The authors proposed a method called the Variational Lossy Autoencoder (VLAE), which combines Variational Autoencoders (VAEs) with autoregressive models to control the information captured in the latent representation. Their approach involves designing a decoding distribution with a limited receptive field to ensure that only global information is encoded in the latent code, effectively discarding local details like texture. Additionally, they introduced an autoregressive flow to parameterize the prior distribution, enhancing the model's density estimation performance while maintaining the ability to learn lossy representations.", "timestamp": "2025-07-07 13:45:19"}, "4": {"question": "Did it work?", "answer": "Yes, their idea worked effectively. The proposed Variational Lossy Autoencoder (VLAE) achieved state-of-the-art results in density estimation tasks on datasets like MNIST, OMNIGLOT, and Caltech-101 Silhouettes, and demonstrated improved performance over traditional VAEs by successfully learning lossy representations that capture global statistics while discarding local details. The results showed that VLAE can generate high-quality reconstructions and significantly reduce the number of bits used for encoding compared to standard VAEs.", "timestamp": "2025-07-07 13:45:23"}, "5": {"question": "What did they compare it to?", "answer": "The authors compared their proposed Variational Lossy Autoencoder (VLAE) to standard Variational Autoencoders (VAEs) and other generative models, achieving state-of-the-art results on datasets like MNIST, OMNIGLOT, and Caltech-101 Silhouettes. They demonstrated that VLAE outperforms traditional VAEs by using an autoregressive decoder that effectively models local statistics while allowing the latent code to capture global structure, leading to a lossy representation. The benchmarks included comparisons of negative log-likelihood (NLL) scores, where VLAE consistently showed lower NLL values than its predecessors, indicating improved density estimation performance.", "timestamp": "2025-07-07 13:45:27"}, "6": {"question": "What was it tested on?", "answer": "The proposed Variational Lossy Autoencoder (VLAE) was tested on binary image datasets, specifically MNIST, OMNIGLOT, and Caltech-101 Silhouettes, as well as the CIFAR10 dataset for natural images. The evaluation setup involved using a PixelCNN with a small receptive field as the autoregressive decoder, which allowed the model to focus on local statistics while encoding global structure in the latent code. The experiments demonstrated that VLAE achieved state-of-the-art results in density estimation tasks, indicating its effectiveness in learning lossy representations.", "timestamp": "2025-07-07 13:45:29"}, "7": {"question": "What's cool about it?", "answer": "The paper presents a novel approach by combining Variational Autoencoders (VAEs) with autoregressive models to create a Variational Lossy Autoencoder (VLAE), which allows for controlled representation learning by explicitly managing what information is retained or discarded. This clever exploitation of the information preference property enables the model to focus on global structures while ignoring local details, leading to improved performance in density estimation tasks and achieving state-of-the-art results on various datasets. The ability to design lossy representations through the architecture is both surprising and elegant, showcasing a significant advancement in generative modeling techniques.", "timestamp": "2025-07-07 13:45:32"}, "8": {"question": "What's sketchy about it?", "answer": "The paper raises several concerns regarding the Variational Lossy Autoencoder (VLAE). One limitation is the potential inefficiency in the Bits-Back Coding mechanism, which may lead to suboptimal use of the latent code due to the autoregressive model's preference for local information. Additionally, the reliance on specific architectural choices and hyperparameter tuning may limit the generalizability of the results across different datasets. Lastly, the slower generation speed due to the sequential nature of autoregressive models could hinder practical applications.", "timestamp": "2025-07-07 13:45:41"}, "9": {"question": "Can anyone use this?", "answer": "The Variational Lossy Autoencoder (VLAE) is designed to be practical and accessible, as it leverages existing neural network architectures and techniques. However, it may require a certain level of expertise in machine learning to implement effectively, given its reliance on autoregressive models and variational inference. While the model can achieve state-of-the-art results, the complexity and computational demands may limit its use to those with sufficient resources and technical knowledge.", "timestamp": "2025-07-07 14:04:19"}, "10": {"question": "What's still left to figure out?", "answer": "The paper identifies several unresolved issues and future directions, including the need for further exploration of how to effectively specify the type of information to be encoded in the latent representation for different tasks and datasets. Additionally, while the proposed Variational Lossy Autoencoder (VLAE) shows promise, extending this approach to other forms of data, particularly those with temporal aspects like audio and video, remains an open question. The authors also highlight the potential for designing representations that focus solely on information relevant to downstream tasks, which could enhance semi-supervised learning.", "timestamp": "2025-07-07 14:04:22"}}, "markdown_pages": [{"id": 1, "title": "3 Variational Lossy Autoencoder", "content": "# 3 Variational Lossy Autoencoder\n\nThe discussion in Section 2.2 suggests that autoregressive models cannot be combined with VAE since information will be preferred to be modeled by autoregressive models. Nevertheless, in this section, we present two complementary classes of improvements to VAE that utilize autoregressive models fruitfully to explicitly control representation learning and improve density estimation.\n\n## ### 3.1 LOSSY CODE VIA EXPLICIT INFORMATION PLACEMENT\n\nEven though the information preference property of VAE might suggest that one should always use the full autoregressive models to achieve a better code length/log-likelihood, especially when slow data generation is not a concern, we argue that this information preference property can be exploited to turn the VAE into a powerful representation learning method that gives us fine-grained control over the kind of information that gets included in the learned representation.\n\nWhen we try to learn a lossy compression/representation of data, we can simply construct a decoding distribution that's capable of modeling the part of information that we don't want the lossy representation to capture, but, critically, that's incapable of modelling the information that we do want the lossy representation to capture.\n\nFor instance, if we are interested in learning a global representation for 2D images that doesn't encode information about detailed texture, we can construct a specific factorization of the autoregressive distribution such that it has a small local receptive field as decoding distribution, e.g., $p_{\\text {local }}(\\mathbf{x} \\mid \\mathbf{z})=\\prod_{i} p\\left(\\mathbf{x}_{i} \\mid \\mathbf{z}, \\mathbf{x}_{\\text {WindowAround }(i)}\\right)$. Notice that, as long as $\\mathbf{x}_{\\text {WindowAround }}(i)$ is smaller than $\\mathbf{x}_{<i}, p_{\\text {local }}(\\mathbf{x} \\mid \\mathbf{z})$ won't be able to represent arbitrarily complex distribution over $\\mathbf{x}$ without dependence on $\\mathbf{z}$ since the receptive field is limited such that not all distributions over $\\mathbf{x}$ admit such factorizations. In particular, the receptive field window can be a small rectangle adjacent to a pixel $\\mathbf{x}_{i}$ and in this case long-range dependency will be encoded in the latent code $\\mathbf{z}$. On the other hand, if the true data distribution admits such factorization for a given datum $\\mathbf{x}$ and dimension $i$, i.e.\n\n$p_{\\text {data }}\\left(\\mathbf{x}_{i} \\mid \\mathbf{x}_{\\text {WindowAround }(i)}\\right)=p_{\\text {data }}\\left(\\mathbf{x}_{i} \\mid \\mathbf{x}_{<i}\\right)$, then the information preference property discussed in Section 2.2 will apply here, which means that all the information will be encoded in local autoregressive distribution for $\\mathbf{x}_{i}$. Local statistics of 2D images like texture will likely be modeled completely by a small local window, whereas global structural information of an images like shapes of objects is long-range dependency that can only be communicated through latent code $\\mathbf{z}$. Therefore we have given an example VAE that will produce a lossy compression of 2D images carrying exclusively global information that can't be modeled locally.\n\nNotice that a global representation is only one of many possible lossy representations that we can construct using this information preference property. For instance, the conditional of an autoregressive distribution might depend on a heavily down-sampled receptive field so that it can only model long-range pattern whereas local high-frequency statistics need to be encoded into the latent code. Hence we have demonstrated that we can achieve explicit placement of information by constraining the receptive field/factorization of an autoregressive distribution that's used as decoding distribution.\n\nWe want to additionally emphasize the information preference property is an asymptotic view in a sense that it only holds when the variational lowerbound can be optimized well. Thus, we are not proposing an alternative to techniques like free bits Kingma et al. (2016) or KL annealing, and indeed they are still useful methods to smooth the optimization problem and used in this paper's experiments."}, {"id": 2, "title": "3.2 Learned Prior with Autoregressive Flow", "content": "# 3.2 Learned Prior with Autoregressive Flow\n\nInefficiency in Bits-Back Coding, i.e., the mismatch between approximate posterior and true posterior, can be exploited to construct a lossy code but it's still important to minimize such inefficiency to improve overall modeling performance/coding efficiency. We propose to parametrize the prior distribution $p(\\mathbf{z} ; \\theta)$ with an autoregressive model and show that a type of autoregressive latent code can in theory reduce inefficiency in Bits-Back coding.\n\nIt is well-known that limited approximate posteriors impede learning and therefore various expressive posterior approximations have been proposed to improve VAE's density estimation performance (Turner et al., 2008; Mnih \\& Gregor, 2014; Salimans et al., 2014; Rezende \\& Mohamed, 2015; Kingma et al., 2016). One such class of approximate posteriors that has been shown to attain good empirical performance is based on the idea of Normalizing Flow, which is to apply an invertible mapping to a simple random variable, for example a factorized Gaussian as commonly used for $q(\\mathbf{z} \\mid \\mathbf{x})$, in order to obtain a complicated random variable. For an invertible transformation between a simple distribution $\\mathbf{y}$ and a more flexible $\\mathbf{z}$, we know from the change-of-variable technique that $\\log q(\\mathbf{z} \\mid \\mathbf{x})=\\log q(\\mathbf{y} \\mid \\mathbf{x})-\\log \\operatorname{det} \\frac{d \\mathbf{z}}{d \\mathbf{y}}$ and using $q(\\mathbf{z} \\mid \\mathbf{x})$ as approximate posterior will decrease the coding efficiency gap $D_{K L}(q(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z} \\mid \\mathbf{x}))$ provided the transformation is sufficiently expressive.\n\nKingma et al. (2016) introduced Inverse Autoregressive Flow, which is a powerful class of such invertible mappings that have simple determinant: $z_{i}=\\frac{y_{i}-\\mu_{i}\\left(y_{1: i-1}\\right)}{\\sigma_{i}\\left(y_{1: i-1}\\right)}$, where $\\mu_{i}($.$) \\in \\mathcal{R}, \\sigma_{i}($.$) \\in \\mathcal{R}^{+}$ are general functions that can be parametrized by expressive neural networks, such as MADE and PixelCNN variants (Germain et al., 2015; van den Oord et al., 2016a). Inverse autoregressive flow is the inverse/whitening of autoregressive flow: $y_{i}=z_{i} \\sigma_{i}\\left(y_{1: i-1}\\right)+\\mu_{i}\\left(y_{1: i-1}\\right)$. We refer interested readers to (Rezende \\& Mohamed, 2015; Kingma et al., 2016) for in-depth discussions on related topics.\n\nIn this paper, we propose to parametrize our learnable prior as an autoregressive flow from some simple noise source like spherical Gaussian. Next, we show that using latent code transformed by autoregressive flow (AF) is equivalent to using inverse autoregressive flow (IAF) approximate posterior, which explains why it can similarly improve Bits-Back Coding efficiency. Moreover, compared with an IAF posterior, an AF prior has a more expressive generative model that essentially \"comes for free\".\n\nFor an autoregressive flow $f$, some continuous noise source $\\epsilon$ is transformed into latent code $\\mathbf{z}$ : $\\mathbf{z}=f(\\epsilon)$. Assuming the density function for noise source is $u(\\epsilon)$, we similarly know that $\\log p(\\mathbf{z})=$ $\\log u(\\epsilon)+\\log \\operatorname{det} \\frac{g_{\\epsilon}}{d \\mathbf{z}}$.\n\nSimply re-arranging the variational lowerbound for using AF prior reveals that having an AF latent code $\\mathbf{z}$ is equivalent to using an IAF posterior for $\\epsilon$ that we can interpret as the new latent code:\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{x} ; \\theta) & =\\mathbb{E}_{\\mathbf{z} \\sim q(\\mathbf{z} \\mid \\mathbf{x})}[\\log p(\\mathbf{x} \\mid \\mathbf{z})+\\log p(\\mathbf{z})-\\log q(\\mathbf{z} \\mid \\mathbf{x})] \\\\\n& =\\mathbb{E}_{\\mathbf{z} \\sim q(\\mathbf{z} \\mid \\mathbf{x}), \\epsilon=f^{-1}(\\mathbf{z})}[\\log p(\\mathbf{x} \\mid f(\\epsilon))+\\log u(\\epsilon)+\\log \\operatorname{det} \\frac{d \\epsilon}{d \\mathbf{z}}-\\log q(\\mathbf{z} \\mid \\mathbf{x})] \\\\\n& =\\mathbb{E}_{\\mathbf{z} \\sim q(\\mathbf{z} \\mid \\mathbf{x}), \\epsilon=f^{-1}(\\mathbf{z})}[\\log p(\\mathbf{x} \\mid f(\\epsilon))+\\log u(\\epsilon)-\\underbrace{(\\log q(\\mathbf{z} \\mid \\mathbf{x})-\\log \\operatorname{det} \\frac{d \\epsilon}{d \\mathbf{z}})}_{\\text {IAF Posterior }}\n\\end{aligned}\n$$\n\nAF prior is the same as IAF posterior along the encoder path, $f^{-1}(q(\\mathbf{z} \\mid \\mathbf{x}))$, but differs along the decoder/generator path: IAF posterior has a shorter decoder path $p(\\mathbf{x} \\mid \\mathbf{z})$ whereas AF prior has a deeper decoder path $p(\\mathbf{x} \\mid f(\\epsilon))$. The crucial observation is that AF prior and IAF posterior have the same computation cost under the expectation of $\\mathbf{z} \\sim q(\\mathbf{z} \\mid \\mathbf{x})$, so using AF prior makes the model more expressive at no training time cost.\n\n## # 4 EXPERIMENTS\n\nIn this paper, we evaluate VLAE on 2D images and leave extensions to other forms of data to future work. For the rest of the section, we define a VLAE model as a VAE that uses AF prior and autoregressive decoder. We choose to implement conditional distribution $p(\\mathbf{x} \\mid \\mathbf{z})$ with a small-receptive-field PixelCNN (van den Oord et al., 2016a), which has been proved to be a scalable autoregressive model.\n\nFor evaluation, we use binary image datasets that are commonly used for density estimation tasks: MNIST (LeCun et al., 1998) (both statically binarized ${ }^{1}$ and dynamically binarized version (Burda et al., 2015a)), OMNIGLOT (Lake et al., 2013; Burda et al., 2015a) and Caltech-101 Silhouettes (Marlin et al., 2010). All datasets uniformly consist of 28 x 28 binary images, which allow us to use a unified architecture. VAE networks used in binary image datasets are simple variants of ResNet VAEs described in (Salimans et al., 2014; Kingma et al., 2016). For the decoder, we use a variant of PixelCNN that has 6 layers of masked convolution with filter size 3, which means the window of dependency, $\\mathbf{x}_{\\text {WindowAround }(i)}$, is limited to a small local patch. During training, \"free bits\" (Kingma et al., 2016) is used improve optimization stability. Experimental setup and hyperparameters are detailed in the appendix. Reported marginal NLL is estimated using Importance Sampling with 4096 samples.\n\n## We designed experiments to answer the following questions:\n\n- Can VLAE learn lossy codes that encode global statistics?\n- Does using AF priors improves upon using IAF posteriors as predicted by theory?\n- Does using autoregressive decoding distributions improve density estimation performance?\n\n### 4.1 Lossy Compression\n\nFirst we are interested in whether VLAE can learn a lossy representation/compression of data by using the PixelCNN decoder to model local statistics. We trained VLAE model on Statically Binarized MNIST and the converged model has $\\mathbb{E}\\left[D_{K L}(q(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z}))\\right]=13.3$ nats $=19.2$ bits, which is the number of bits it uses on average to encode/compress one MNIST image. By comparison, an identical VAE model with factorized decoding distribution will uses on average 37.3 bits in latent code, and this thus indicates that VLAE can learn a lossier compression than a VAE with regular factorized conditional distribution.\n\nThe next question is whether VLAE's lossy compression encodes global statistics and discards local statistics. In Fig 1a, we visualize original images $\\mathbf{x}_{\\text {data }}$ and one random \"decompression\" $\\mathbf{x}_{\\text {decompressed }}$ from VLAE: $\\mathbf{z} \\sim q\\left(\\mathbf{z} \\mid \\mathbf{x}_{\\text {data }}\\right), \\mathbf{x}_{\\text {decompressed }} \\sim p(\\mathbf{x} \\mid \\mathbf{z})$. We observe that none of the\n\n[^0]\n[^0]:    ${ }^{1}$ We use the version provided by Hugo Larochelle.\n\n![img-0.jpeg](img-0.jpeg)\n\n## Figure 1: Statically Binarized MNIST\ndecompressions is an exact reconstruction of the original image but instead the global structure of the image was encoded in the lossy code $\\mathbf{z}$ and regenerated. Also worth noting is that local statistics are not preserved but a new set of likely local statistics are generated in the decompressed images: the binary masks are usually different and local styles like stroke width are sometimes slightly different.\n\nHowever, we remark that the lossy code $\\mathbf{z}$ doesn't always capture the kind of global information that we care about and it's dependent on the type of constraint we put on the decoder. For instance, in Fig 4b, we show decompressions for OMNIGLOT dataset, which has more meaningful variations in small patches than MNIST, and we can observe that semantics are not preserved in some cases. This highlights the need to specify the type of statistics we care about in a representation, which will be different across tasks and datasets, and design decoding distribution accordingly.\n![img-1.jpeg](img-1.jpeg)\n(a) Original test-set images (left) and \"decompressioned\" versions from VLAE's lossy code (right)\n(b) Samples from VLAE\n\n## Figure 2: OMNIGLOT\n\n## # 4.2 DENSITY ESTIMATION\n\nNext we investigate whether leveraging autoregressive models as latent distribution $p(\\mathbf{z})$ and as decoding distribution $p(\\mathbf{x} \\mid \\mathbf{z})$ would improve density estimation performance.\n\nTo verify whether AF prior is able to improve upon IAF posterior alone, it's desirable to test this model without using autoregressive decoder but instead using the conventional independent Bernoulli distribution for $p(\\mathbf{x} \\mid \\mathbf{z})$. Hence we use the best performing model from Kingma et al.\n\n## Table 1: Statically Binarized MNIST\n\n| Model | NLL Test |\n| --- | --- |\n| Normalizing flows (Rezende & Mohamed, 2015) | 85.10 |\n| DRAW (Gregor et al., 2015) | < 80.97 |\n| Discrete VAE (Rolfe, 2016) | 81.01 |\n| PixelRNN (van den Oord et al., 2016a) | 79.20 |\n| IAF VAE (Kingma et al., 2016) | 79.88 |\n## | AF VAE | 79.30 |\n## | VLAE | 79.03 |\n\n(2016) on statically binarized MNIST and make the single modification of replacing the original IAF posterior with an equivalent AF prior, removing the context. As seen in Table 1, VAE with AF prior is outperforming VAE with an equivalent IAF posterior, indicating that the deeper generative model from AF prior is beneficial. A similar gain carries over when an autoregressive decoder is used: on statically binarized MNIST, using AF prior instead of IAF posterior reduces train NLL by 0.8 nat and test NLL by 0.6 nat.\n\nNext we evaluate whether using autoregressive decoding distribution can improve performance and we show in Table 1 that a VLAE model, with AF prior and PixelCNN conditional, is able to outperform a VAE with just AF prior and achieves new state-of-the-art results on statically binarized MNIST.\n\nIn addition, we hypothesize that the separation of different types of information, the modeling global structure in latent code and local statistics in PixelCNN, likely has some form of good inductive biases for 2D images. In order to evaluate if VLAE is an expressive density estimator with good inductive biases, we will test a single VLAE model, with the same network architecture, on all binary datasets. We choose hyperparameters manually on statically binarized MNIST and use the same hyperparameters to evaluate on dynamically binarized MNIST, OMNIGLOT and Caltech-101 Silhouettes. We also note that better performance can be obtained if we individually tune hyperparameters for each dataset. As a concrete demonstration, we report the performance of a fine-tuned VLAE on OMNIGLOT dataset in Table 3.\n\n## Table 2: Dynamically binarized MNIST\n\n| Model | NLL Test |\n| --- | --- |\n| Convolutional VAE + HVI (Salimans et al., 2014) | 81.94 |\n| DLGM 2hl + IWAE (Burda et al., 2015a) | 82.90 |\n| Discrete VAE (Rolfe, 2016) | 80.04 |\n| LVAE (Kaae Sønderby et al., 2016) | 81.74 |\n| DRAW + VGP (Tran et al., 2015) | < 79.88 |\n| IAF VAE (Kingma et al., 2016) | 79.10 |\n| Unconditional Decoder | 87.55 |\n## | VLAE | 78.53 |\n\nTable 3: OMNIGLOT. [1] (Burda et al., 2015a), [2] (Burda et al., 2015b), [3] (Gregor et al., 2015), [4] (Gregor et al., 2016),\n\n| Model | NLL Test |\n| --- | --- |\n## | VAE [1] | 106.31 |\n## | IWAE [1] | 103.38 |\n| RBM (500 hidden) [2] | 100.46 |\n## | DRAW [3] | < 96.50 |\n| Conv DRAW [4] | < 91.00 |\n| Unconditional Decoder | 95.02 |\n## | VLAE | 90.98 |\n| VLAE (fine-tuned) | 89.83 |\n\nTable 4: Caltech-101 Silhouettes. [1] (Bornschein & Bengio, 2014), [2] (Cho et al., 2011), [3] (Du et al., 2015), [4] (Rolfe, 2016), [5] (Goessling & Amit, 2015),\n\n| Model | NLL Test |\n| --- | --- |\n## | RWS SBN [1] | 113.3 |\n## | RBM [2] | 107.8 |\n## | NAIS NADE [3] | 100.0 |\n| Discrete VAE [4] | 97.6 |\n| SpARN [5] | 88.48 |\n| Unconditional Decoder | 89.26 |\n## | VLAE | 77.36 |\n\nAs seen in Table 2,3,4, with the same set of hyperparameters tuned on statically binarized MNIST, VLAE is able to perform well on the rest of datasets, significantly exceeding previous state-of-the-art results on dynamically binarized MNIST and Caltech-101 Silhouettes and tying statistically with best previous result on OMNIGLOT. In order to isolate the effect of expressive PixelCNN as decoder, we also report performance of the same PixelCNN trained without VAE part under the name \"Unconditional Decoder\"."}, {"id": 3, "title": "4.3 Natural Images: CIFAR10", "content": "# 4.3 Natural Images: CIFAR10\n\nIn addition to binary image datasets, we have applied VLAE to the CIFAR10 dataset of natural images. Density estimation of CIFAR10 images has been a challenging benchmark problem used by many recent generative models and hence is great task to position VLAE among existing methods.\n\nWe investigated using ResNet (He et al., 2016) and DenseNet (Huang et al., 2016) as building blocks for VAE networks and observed that DenseNet reduces overfitting. We also propose a new optimization technique that blends the advantages of KL annealing (Serban et al., 2016) and \"free bits\" (Kingma et al., 2016) to stabilize learning on this challenging dataset. Detailed experimental setup is described in Appendix.\n\nVLAE is compared to other methods on CIFAR10 in Table 5. We show that VLAE models attain new state-of-the-art performance among other variationally trained latent-variable models. DenseNet VLAE model also outperforms most other tractable likelihood models including Gated PixelCNN and PixelRNN and has results only slightly worse than currently unarchived state-of-the-art Pixel$\\mathrm{CNN}++$.\n\nTable 5: CIFAR10. Likelihood for VLAE is approximated with 512 importance samples. [1] (van den Oord et al., 2016a), [2] (Dinh et al., 2014), [3] (van den Oord \\& Schrauwen, 2014), [4] (Dinh et al., 2016), [5] (van den Oord et al., 2016b), [6] (Salimans et al., 2017), [7] (Sohl-Dickstein et al., 2015), [8] (Gregor et al., 2016), [9] (Kingma et al., 2016)\n\n| Method | bits/dim $\\leq$ |\n| :-- | :--: |\n| Results with tractable likelihood models: |  |\n| Uniform distribution [1] | 8.00 |\n| Multivariate Gaussian [1] | 4.70 |\n## | NICE [2] | 4.48 |\n| Deep GMMs [3] | 4.00 |\n| Real NVP [4] | 3.49 |\n| PixelCNN [1] | 3.14 |\n| Gated PixelCNN [5] | 3.03 |\n| PixelRNN [1] | 3.00 |\n| PixelCNN++ [6] | $\\mathbf{2 . 9 2}$ |\n| Results with variationally trained latent-variable models: |  |\n| Deep Diffusion [7] | 5.40 |\n| Convolutional DRAW [8] | 3.58 |\n| ResNet VAE with IAF [9] | 3.11 |\n| ResNet VLAE | 3.04 |\n| DenseNet VLAE | $\\mathbf{2 . 9 5}$ |\n\nWe also investigate learning lossy codes on CIFAR10 images. To illustrate how does the receptive field size of PixelCNN decoder influence properties of learned latent codes, we show visualizations of similar VLAE models with receptive fields of different sizes. Specifically we say a receptive field, $\\mathbf{x}_{\\text {WindowAround }(i)}$, has size $A \\mathrm{x} B$ when a pixel $x_{i}$ can depend on the rectangle block of size $A \\mathrm{x} B$ immediately on top of $x_{i}$ as well as the $\\left\\lceil\\frac{A-1}{2}\\right\\rceil$ pixels immediately to the left of $x_{i}$. We use this notation to refer to different types of PixelCNN decoders in Figure 3.\n\nFrom (a)-(c) in Figure 3, we can see that larger receptive fields progressively make autoregressive decoders capture more structural information. In (a), a smaller receptive field tends to preserve rather detailed shape information in the lossy code whereas the latent code only retains rough shape in (c) with a larger receptive field.\n\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: CIFAR10: Original test-set images (left) and \"decompressioned\" versions from VLAE's lossy code (right) with different types of receptive fields\n\nIt's interesting to also note that in (a)-(c), oftentimes color information is partially omitted from latent codes and one explanation can be that color is very predictable locally. However, color information can be important to preserve if our task is, for example, object classification. To demonstrate how we can encode color information in the lossy code, we can choose to make PixelCNN decoder depend only on images' grayscale versions. In other words, instead of choosing the decoder to be $p_{\\text {local }}(\\mathbf{x} \\mid \\mathbf{z})=\\prod_{i} p\\left(\\mathbf{x}_{i} \\mid \\mathbf{z}, \\mathbf{x}_{\\text {WindowAround }(i)}\\right)$, we use a decoder of the form $p_{\\text {local }}(\\mathbf{x} \\mid \\mathbf{z})=\\prod_{i} p\\left(\\mathbf{x}_{i} \\mid \\mathbf{z}, \\operatorname{Grayscale}\\left(\\mathbf{x}_{\\text {WindowAround }(i)}\\right)\\right)$. In (d) of Figure 3, we visualize lossy codes for a VLAE that has the same receptive field size as (c) but uses a \"grayscale receptive field\". We note that the lossy codes in (d) encode roughly the same structural information as those in (c) but generally generate objects that are more recognizable due to the preservation of color information. This serves as one example of how we can design the lossy latent code carefully to encode what's important and what's not."}, {"id": 4, "title": "5 Related Work", "content": "# 5 Related Work\n\nWe investigate a fusion between variational autoencoders with continuous latent variables (Kingma \\& Welling, 2013; Rezende et al., 2014) and neural autoregressive models. For autoregression, we specifically apply a novel type of architecture where autoregression is realised through a carefully\n\nconstructed deep convolutional network, introduced in the PixelCNN model for images (van den Oord et al., 2016a,b). These family of convolutional autoregressive models was further explored, and extended, for audio in WaveNet (Oord et al., 2016), video in Video Pixel Networks (Kalchbrenner et al., 2016b) and language in ByteNet (Kalchbrenner et al., 2016a).\n\nThe combination of latent variables with expressive decoder was previously explored using recurrent networks mainly in the context of language modeling (Chung et al., 2015; Bowman et al., 2015; Serban et al., 2016; Fraccaro et al., 2016; Xu \\& Sun, 2016). Bowman et al. (2015) has also proposed to weaken an otherwise too expressive decoder by dropout to force some information into latent codes.\n\nConcurrent with our work, PixelVAE (Gulrajani et al., 2016) also explored using conditional PixelCNN as a VAE's decoder and has obtained impressive density modeling results through the use of multiple levels of stochastic units.\n\nUsing autoregressive model on latent code was explored in the context of discrete latent variables in DARN (Gregor et al., 2013). Kingma et al. (2016), Kaae Sønderby et al. (2016), Gregor et al. (2016) and Salimans (2016) explored VAE architecture with an explicitly deep autoregressive prior for continuous latent variables, but the autoregressive data likelihood is intractable in those architectures and needs to inferred variationally. In contrast, we use multiple steps of autoregressive flows that has exact likelihood and analyze the effect of using expressive latent code.\n\nOptimization challenges for using (all levels of) continuous latent code were discussed before and practical solutions were proposed (Bowman et al., 2015; Kaae Sønderby et al., 2016; Kingma et al., 2016). In this paper, we present a complementary perspective on when/how should the latent code be used by appealing to a Bits-Back interpretation of VAE.\n\nLearning a lossy compressor with latent variable model has been investigated with ConvDRAW (Gregor et al., 2016). It learns a hierarchy of latent variables and just using high-level latent variables will result in a lossy compression that performs similarly to JPEG. Our model similarly learns a lossy compressor but it uses an autoregressive model to explicitly control what kind of information should be lost in compression.\n\n## # 6 CONCLUSION\n\nIn this paper, we analyze the condition under which the latent code in VAE should be used, i.e. when does VAE autoencode, and use this observation to design a VAE model that's a lossy compressor of observed data. At modeling level, we propose two complementary improvements to VAE that are shown to have good empirical performance.\n\nVLAE has the appealing properties of controllable representation learning and improved density estimation performance but these properties come at a cost: compared with VAE models that have simple prior and decoder, VLAE is slower at generation due to the sequential nature of autoregressive model.\n\nMoving forward, we believe it's exciting to extend this principle of learning lossy codes to other forms of data, in particular those that have a temporal aspect like audio and video. Another promising direction is to design representations that contain only information for downstream tasks and utilize those representations to improve semi-supervised learning.\n\n## ## REFERENCES\n\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.\n\nJörg Bornschein and Yoshua Bengio. Reweighted wake-sleep. arXiv preprint arXiv:1406.2751, 2014.\n\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.\n\nYuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015a.\n\nYuri Burda, Roger B Grosse, and Ruslan Salakhutdinov. Accurate and conservative estimates of mrf log-likelihood using reverse annealing. In AISTATS, 2015b.\n\nKyungHyun Cho, Tapani Raiko, and Alexander T Ihler. Enhanced gradient and adaptive learning rate for training restricted boltzmann machines. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 105-112, 2011.\n\nJunyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, pp. 2980-2988, 2015.\n\nLaurent Dinh, David Krueger, and Yoshua Bengio. Nice: non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014.\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv preprint arXiv:1605.08803, 2016.\n\nChao Du, Jun Zhu, and Bo Zhang. Learning deep generative models with doubly stochastic mcmc. arXiv preprint arXiv:1506.04557, 2015.\n\nOtto Fabius and Joost R van Amersfoort. Variational recurrent auto-encoders. arXiv preprint arXiv:1412.6581, 2014.\n\nMarco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, and Ole Winther. Sequential neural models with stochastic layers. arXiv preprint arXiv:1605.07571, 2016.\n\nMathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation. arXiv preprint arXiv:1502.03509, 2015.\n\nMarc Goessling and Yali Amit. Sparse autoregressive networks. arXiv preprint arXiv:1511.04776, 2015.\n\nKarol Gregor, Andriy Mnih, and Daan Wierstra. Deep AutoRegressive Networks. arXiv preprint arXiv:1310.8499, 2013.\n\nKarol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. DRAW: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.\n\nKarol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. arXiv preprint arXiv:1604.08772, 2016.\n\nIshaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, and Aaron Courville. Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013, 2016.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. arXiv preprint arXiv:1603.05027, 2016.\n\nGeoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pp. 5-13. ACM, 1993.\n\nGeoffrey E Hinton and Richard S Zemel. Autoencoders, minimum description length, and Helmholtz free energy. Advances in neural information processing systems, pp. 3-3, 1994.\n\nAntti Honkela and Harri Valpola. Variational learning and bits-back coding: an informationtheoretic view to bayesian learning. IEEE Transactions on Neural Networks, 15(4):800-810, 2004.\n\nGao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016.\n\nCasper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. How to train deep variational autoencoders and probabilistic ladder networks. arXiv preprint arXiv:1602.02282, 2016.\n\nNal Kalchbrenner, Lasse Espheholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. eural machine translation in linear time. arXiv preprint arXiv:1610.00527, 2016a.\n\nNal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks. arXiv preprint arXiv:1610.00527, 2016b.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational Bayes. Proceedings of the 2nd International Conference on Learning Representations, 2013.\n\nDiederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse autoregressive flow. arXiv preprint arXiv:1606.04934, 2016.\n\nBrenden M Lake, Ruslan R Salakhutdinov, and Josh Tenenbaum. One-shot learning by inverting a compositional causal process. In Advances in neural information processing systems, pp. 25262534, 2013.\n\nYann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\nBenjamin M Marlin, Kevin Swersky, Bo Chen, and Nando de Freitas. Inductive principles for restricted boltzmann machine learning. In AISTATS, pp. 509-516, 2010.\n\nAndriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv preprint arXiv:1402.0030, 2014.\n\nVinod Nair, Josh Susskind, and Geoffrey E Hinton. Analysis-by-synthesis by learning to invert generative black boxes. In International Conference on Artificial Neural Networks, pp. 971-981. Springer, 2008.\n\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.\n\nDanilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings of The 32nd International Conference on Machine Learning, pp. 1530-1538, 2015.\n\nDanilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 1278-1286, 2014.\n\nJason Tyler Rolfe. Discrete variational autoencoders. arXiv preprint arXiv:1609.02200, 2016.\nTim Salimans. A structured variational auto-encoder for learning deep hierarchies of sparse features. arXiv preprint arXiv:1602.08734, 2016.\n\nTim Salimans, Diederip P. Kingma, and Max Welling. Markov chain Monte Carlo and variational inference: Bridging the gap. arXiv preprint arXiv:1410.6460, 2014.\n\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017.\n\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for generating dialogues. arXiv preprint arXiv:1605.06069, 2016.\n\nJascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015.\n\nDustin Tran, Rajesh Ranganath, and David M Blei. Variational gaussian process. arXiv preprint arXiv:1511.06499, 2015.\n\nRichard E Turner, Pietro Berkes, and Maneesh Sahani. Two problems with variational expectation maximisation for time-series models. In Proceedings of the Workshop on Inference and Estimation in Probabilistic Time-Series Models, pp. 107-115, 2008.\n\nAaron van den Oord and Benjamin Schrauwen. Factoring variations in natural images with deep gaussian mixture models. In Advances in Neural Information Processing Systems, pp. 35183526, 2014.\n\nAaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016a.\n\nAaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with pixelcnn decoders. arXiv preprint arXiv:1606.05328, 2016b.\n\nWeidi Xu and Haoze Sun. Semi-supervised variational autoencoders for sequence classification. arXiv preprint arXiv:1603.02514, 2016.\n\nAlan Yuille and Daniel Kersten. Vision as bayesian inference: analysis by synthesis? Trends in cognitive sciences, 10(7):301-308, 2006.\n\nBiao Zhang, Deyi Xiong, and Jinsong Su. Variational neural machine translation. arXiv preprint arXiv:1605.07869, 2016.\n\n## # APPENDIX\n\n## ## A DETAILED EXPERIMENT SETUP FOR BINARY IMAGES\n\nFor VAE's encoder and decoder, we use the same ResNet (He et al., 2015) VAE architecture as the one used in IAF MNIST experiment (Kingma et al., 2016). The only difference is that the decoder network now, instead of outputing a $28 \\times 28 \\times 1$ spatial feature map to specify the mean of a factorized bernoulli distribution, outputs a $28 \\times 28 \\times 4$ spatial feature map that's concatenated with the original binary image channel-wise, forming a $28 \\times 28 \\times 5$ feature map that's then fed through a typical masked PixelCNN (van den Oord et al., 2016a). As such even though the PixelCNN conditions on the latent code, we don't call it a Conditional PixelCNN because it doesn't use the specific architecture that was proposed in van den Oord et al. (2016b). For the PixelCNN, it has 6 masked convolution layers with 123 x 3 filters organized in ResNet blocks, and it has 4 additional 1x1 convolution ResNet block between every other masked convolution layer to increase processing capacity since it employs fewer masked convolutions than usual. All the masked convolution layer have their weights tied to reduce overfitting on statically binarized MNIST, and untying the weights will increase performance for other datasets. Experiments are tuned on the validation set and then final experiment was run with train and validation set, with performance evaluated with test set. Exponential Linear Units (Clevert et al., 2015) are used as activation functions in both VAE network and PixelCNN network. Weight normalization is everywhere with data-dependent initialization (Salimans \\& Kingma, 2016).\n\nA latent code of dimension 64 was used. For AF prior, it's implemented with MADE (Germain et al., 2015) as detailed in Kingma et al. (2016). We used 4 steps of autoregressive flow and each flow is implemented by a 3-layer MADE that has 640 hidden units and uses Relu (Nair \\& Hinton, 2010) as activation functions. Differing from the practice of Kingma et al. (2016), we use mean-only autoregressive flow, which we found to be more numerically stable.\n\nIn terms of training, Adamax (Kingma \\& Ba, 2014) was used with a learning rate of 0.002 . 0.01 nats/data-dim free bits (Kingma et al., 2016) was found to be effective in dealing with the problem of all the latent code being ignored early in training. Polyak averaging (Polyak \\& Juditsky, 1992) was used to compute the final parameters, with $\\alpha=0.998$.\n\nAll experiments are implemented using TensorFlow (Abadi et al., 2016).\n\n## ## B ADDITIONAL EXPERIMENT SETUP FOR CIFAR10\n\nLatent codes are represented by 16 feature maps of size $8 \\times 8$, and this choice of spatial stochastic units are inspired by ResNet IAF VAE (Kingma et al., 2016). Prior distribution is factorized Gaussian noise transformed by 6 autoregressive flows, each of which is implemented by a PixelCNN (van den Oord et al., 2016a) with 2 hidden layers and 128 feature maps. Between every other autoregressive flow, the ordering of stochastic units is reversed.\n\nResNet VLAE has the following structure for encoder: 2 ResNet blocks, Conv w/ stride=2, 2 ResNet blocks, Conv w/ stride=2, 3 ResNet blocks, 1x1 convolution and has a symmetric decoder. Channel size $=48$ for $32 \\times 32$ feature maps and 96 for other feature maps. DenseNet VLAE follows a similar structure: replacing 2 ResNet blocks with one DenseNet block of 3 steps and each step produces a certain number of feature maps such that at the end of a block, the concatenated feature maps is slightly more than the ResNet VLAE at the same stage.\n\nConditional PixelCNN++ (Salimans et al., 2017) is used as the decoder. Specifically the channelautoregressive variant is used to ensure there is sufficient capacity even when the receptive field is small. Specifically, the decoder PixelCNN has 4 blocks of 64 feature maps where each block is conditioned on previous blocks with Gated ResNet connections and hence the PixelCNN decoders we use are shallow but very wide. For $4 \\times 2$ receptive field experiment, we use 1 layer of vertical stack convolutions and 2 layers of horizontal stack convolutions; for $5 \\times 3$ receptive field experiment, we use 2 layers of vertical stack convolutions and 2 layers of horizontal stack convolutions; For $5 \\times 3$ receptive field experiment, we use 2 layers of vertical stack convolutions and 2 layers of horizontal stack convolutions; For $7 \\times 4$ receptive field experiment, we use 3 layers of vertical stack convolutions and 3 layers of horizontal stack convolutions; for $7 \\times 4$ Grayscale experiment, we transform RGB\n\nimages into gray-scale images via this specific transformation: $(0.299 * R)+(0.587 G)+(0.114 B)$. Best density estimation result is obtained with 7x4 receptive field experiments."}, {"id": 5, "title": "C Soft Free Bits", "content": "# C Soft Free Bits\n\n\"Free bits\" was a technique proposed in (Kingma et al., 2016) where $K$ groups of stochastic units are encouraged to be used through the following surrogate objective:\n\n$$\n\\widetilde{\\mathcal{L}}_{\\lambda}=\\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{M}}\\left[\\mathbb{E}_{q(\\mathbf{z} \\mid \\mathbf{x})}[\\log p(\\mathbf{x} \\mid \\mathbf{z})]\\right]-\\sum_{j=1}^{K} \\operatorname{maximum}\\left(\\lambda, \\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{M}}\\left[D_{K L}\\left(q\\left(\\mathbf{z}_{j} \\mid \\mathbf{x}\\right) \\| p\\left(\\mathbf{z}_{j}\\right)\\right)\\right]\\right)\n$$\n\nThis technique is easy to use since it's usually easy to determine the minimum number of bits/nats, $\\lambda$, stochastic units need to encode. Choosing $\\lambda$ is hence easier than setting a fixed KL annealing schedule (Serban et al., 2016).\n\nOn the other hand, Kl annealing has the benefit of the surrogate objective will smoothly become the true objective, the variational lower bound where as \"free bits\" has a sharp transition at the boundary. Therefore, we propose to still use $\\lambda$ as hyperparameter to specify at least $\\lambda$ nats should be used but try to change the optimization objective as slowly as possible:\n\n$$\n\\mathcal{L}_{\\text {SoftFreeBits }}(\\mathbf{x} ; \\theta)=\\mathbb{E}_{q(\\mathbf{z} \\mid \\mathbf{x})}[\\log p(\\mathbf{x} \\mid \\mathbf{z})]-\\gamma D_{K L}(q(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z}))\n$$\n\nwhere $0<\\gamma \\leq 1$.\nAnd we make the optimization smoother by changing $\\gamma$ slowly online to make sure at least $\\lambda$ nats are used: when Kl is too much higher than $\\lambda$ (we experimented wide range of thresholds from $3 \\%$ to $30 \\%$, all of which yield improved results, and we tend to use $5 \\%$ us a threshold), $\\gamma$ is increased, and when Kl lower than $\\lambda, \\gamma$ is decreased to encourage information flow.\n\nWe found it sufficient to increase/decrease at $10 \\%$ increment and didn't further tune this parameter.\n\n## D Autoregressive DECODER WITHOUT AUTOREGRESSIVE PRIOR\n\nIn this section, we investigate the scenario of just using an autoregressive decoder without using an autoregressive prior. We compare the exact same model in three configurations: 1) using small-receptive-field PixelCNN as an unconditional density estimator; 2) using small-receptive-field as a decoder in a VAE with Gaussian latent variables; 3) replacing Gaussian latent variables with autoregressive flow latent variables in 2).\n\n## Table 1: Ablation on Dynamically binarized MNIST\n\n| Model | NLL Test | KL |\n| :-- | :--: | :--: |\n| Unconditional PixelCNN | 87.55 | 0 |\n| PixelCNN Decoder + Gaussian Prior | 79.48 | 10.60 |\n| PixelCNN Decoder + AF Prior | $\\mathbf{7 8 . 9 4}$ | 11.73 |\n\nIn Table 1, we can observe that each step of modification improves density estimation performance. In addition, using an autoregressive latent code makes the latent code transmit more information as shown in the difference of $\\mathbb{E}\\left[D_{K L}(q(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z}))\\right]$.\n\n## ## E CIFAR10 GENERATED SAMPLES\n\n## ## REFERENCES\n\nMartın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.\n\n![img-3.jpeg](img-3.jpeg)\n\n## Figure 4: CIFAR10: Generated samples for different models\n\nDjork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by Exponential Linear Units (ELUs). arXiv preprint arXiv:1511.07289, 2015.\n\nMathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation. arXiv preprint arXiv:1502.03509, 2015.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.\n\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nDiederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse autoregressive flow. arXiv preprint arXiv:1606.04934, 2016.\n\nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807-814, 2010.\n\nBoris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838-855, 1992.\n\nTim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. arXiv preprint arXiv:1602.07868, 2016.\n\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017.\n\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for generating dialogues. arXiv preprint arXiv:1605.06069, 2016.\n\nAaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016a.\n\nAaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with pixelcnn decoders. arXiv preprint arXiv:1606.05328, 2016b."}], "pdf_images": {"pdf_path": "inbox/1611.02731v2.pdf", "total_pages": 17, "pages": [{"page_number": 1, "filename": "page-001.png", "thumb_filename": "page-001-thumb.png", "width": 1241, "height": 1754}, {"page_number": 2, "filename": "page-002.png", "thumb_filename": "page-002-thumb.png", "width": 1241, "height": 1754}, {"page_number": 3, "filename": "page-003.png", "thumb_filename": "page-003-thumb.png", "width": 1241, "height": 1754}, {"page_number": 4, "filename": "page-004.png", "thumb_filename": "page-004-thumb.png", "width": 1241, "height": 1754}, {"page_number": 5, "filename": "page-005.png", "thumb_filename": "page-005-thumb.png", "width": 1241, "height": 1754}, {"page_number": 6, "filename": "page-006.png", "thumb_filename": "page-006-thumb.png", "width": 1241, "height": 1754}, {"page_number": 7, "filename": "page-007.png", "thumb_filename": "page-007-thumb.png", "width": 1241, "height": 1754}, {"page_number": 8, "filename": "page-008.png", "thumb_filename": "page-008-thumb.png", "width": 1241, "height": 1754}, {"page_number": 9, "filename": "page-009.png", "thumb_filename": "page-009-thumb.png", "width": 1241, "height": 1754}, {"page_number": 10, "filename": "page-010.png", "thumb_filename": "page-010-thumb.png", "width": 1241, "height": 1754}, {"page_number": 11, "filename": "page-011.png", "thumb_filename": "page-011-thumb.png", "width": 1241, "height": 1754}, {"page_number": 12, "filename": "page-012.png", "thumb_filename": "page-012-thumb.png", "width": 1241, "height": 1754}, {"page_number": 13, "filename": "page-013.png", "thumb_filename": "page-013-thumb.png", "width": 1241, "height": 1754}, {"page_number": 14, "filename": "page-014.png", "thumb_filename": "page-014-thumb.png", "width": 1241, "height": 1754}, {"page_number": 15, "filename": "page-015.png", "thumb_filename": "page-015-thumb.png", "width": 1241, "height": 1754}, {"page_number": 16, "filename": "page-016.png", "thumb_filename": "page-016-thumb.png", "width": 1241, "height": 1754}, {"page_number": 17, "filename": "page-017.png", "thumb_filename": "page-017-thumb.png", "width": 1241, "height": 1754}]}, "metadata": {"model": "gpt-4o-mini", "pdf_path": "inbox/1611.02731v2.pdf"}, "attribution": {"type": "arxiv", "url": "https://arxiv.org/abs/1611.02731", "id": "1611.02731"}};
        initializeColumnInterface();
    </script>
</body>
</html>