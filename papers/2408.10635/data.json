{
  "title": "Strategist: Self - Improvement of Llm Decision Making Via Bi - Level Tree Search",
  "slug": "2408.10635",
  "questions": {
    "1": {
      "question": "What were they trying to do?",
      "answer": "The main goal of the paper is to develop a framework called STRATEGIST that enhances decision-making in large language models (LLMs) for complex, multi-agent environments. The authors aim to integrate the generalization capabilities of LLMs with the precise planning abilities of Monte Carlo Tree Search (MCTS) to improve strategy learning and execution without extensive training data. This approach is designed to optimize strategies for competitive games with partial information, outperforming traditional reinforcement learning methods and other LLM-based techniques.",
      "timestamp": "2026-02-16 13:09:25"
    },
    "2": {
      "question": "Why does it matter?",
      "answer": "The paper introduces STRATEGIST, a framework that combines the generalization capabilities of large language models (LLMs) with the precise planning of Monte Carlo Tree Search (MCTS) to improve decision-making in complex, multi-agent environments. This matters because it addresses the limitations of LLMs in strategic planning and decision-making, enhancing their performance in competitive games like GOPS and Avalon. By outperforming traditional reinforcement learning methods and existing LLM-based agents, STRATEGIST demonstrates the potential for more effective AI strategies in adversarial settings, which could be valuable for applications in AI-driven strategic planning and autonomous systems.",
      "timestamp": "2026-02-16 13:09:34"
    },
    "3": {
      "question": "What did they try?",
      "answer": "The authors introduced STRATEGIST, a novel framework that combines the strengths of large language models (LLMs) and Monte Carlo Tree Search (MCTS) to improve decision-making in complex environments. Their approach involves using LLMs to generate and refine high-level strategies, which are then translated into executable policies through MCTS. This bi-level method allows for strategy optimization via population-based self-play simulations without requiring training data, effectively integrating LLMs' generalization capabilities with MCTS's precise planning. The framework was demonstrated to be effective in learning optimal strategies for games like Game of Pure Strategy (GOPS) and The Resistance: Avalon.",
      "timestamp": "2026-02-16 13:09:46"
    },
    "4": {
      "question": "Core Analysis",
      "answer": "## üéØ Core Contribution\n\nSTRATEGIST introduces a bi-level framework combining large language model (LLM)-based strategy abstraction with Monte Carlo Tree Search (MCTS) for strategy refinement. This innovative procedure leverages a modular improvement method through population-based self-play without needing any training data. STRATEGIST excels in complex multi-agent games, demonstrating superior performance over traditional reinforcement learning (RL) and LLM-based methods, achieving strategies comparable to human-level play.\n\n## üìä Method Breakdown\n\nThe STRATEGIST framework involves a hierarchical approach where high-level strategy abstractions are developed using LLM guidance through an iterative process of self-play. Initial high-level strategies are generated by the LLM, forming a strategy tree, where each strategy's performance is tested and refined via Monte Carlo Tree Search (MCTS). The LLM contributes to proposing improvement ideas, guided by a bandit algorithm, which are evaluated for effectiveness in self-play simulations. The resulting strategies are continually refined until an optimal solution is approached.\n\n## üîß Subsystems/Parts\n\n1. LLM-Based Strategy Abstraction: Generates high-level strategies in text form. 2. Strategy Tree: Hierarchical storage of strategies for refinement. 3. MCTS Executor: Refines abstractions into executable low-level policies. 4. Improvement Idea Queue: Stores and prioritizes strategy improvement suggestions generated by the LLM. 5. Self-Play Simulation Environment: Tests and evaluates strategy performance and improvement.\n\n## üîó Interactions\n\nThe LLM creates high-level strategy abstractions which are refined in a strategy tree. The MCTS executor executes these abstractions into low-level policies, testing them through self-play simulations. Improvement ideas generated from the LLM are queued and prioritized, guiding further strategy refinements throughout the process. Individual components collaborate to iteratively enhance and adapt strategies based on feedback.\n\n## ‚ö° Delta vs Baseline\n\nCompared to baseline methods, STRATEGIST uniquely integrates LLM-generated strategies with iterative refinement through MCTS, leveraging self-play without needing training data. Its bi-level approach optimizes both strategy abstraction and detailed execution, significantly improving results over traditional RL and existing LLM strategies.\n\n## üî¨ Evidence Anchor\n\nTable 2 effectively demonstrates STRATEGIST's superior performance, showing its ability to consistently outperform traditional RL and other LLM-based methods in both the Game of Pure Strategy (GOPS) and Avalon, evidenced by higher win rates and score improvements.\n\n## üåç Transferability\n\nThe core STRATEGIST framework is adaptable to various multi-agent game settings. The approach is generalizable, capable of optimizing strategies in contexts requiring sophisticated decision-making and strategic play, making it potentially applicable beyond the specific games studied.\n\n",
      "type": "core_analysis"
    },
    "5": {
      "question": "Did it work?",
      "answer": "Yes, the STRATEGIST framework was effective in improving decision-making and strategy learning in complex games. The results showed that agents using STRATEGIST outperformed those trained with traditional reinforcement learning methods and other LLM-based techniques in games like the Game of Pure Strategy (GOPS) and The Resistance: Avalon. STRATEGIST achieved higher win rates than existing agents and matched human performance in Avalon, demonstrating its ability to handle strategic complexity and social interaction effectively.",
      "timestamp": "2026-02-16 13:09:52"
    },
    "6": {
      "question": "What did they compare it to?",
      "answer": "The paper compares the STRATEGIST framework to several baselines, including traditional reinforcement learning (RL) methods like AlphaGo and DeepRole, as well as other LLM-based agents such as ReAct and ReCon. STRATEGIST demonstrates superior performance over these methods, achieving higher win rates in games like Game of Pure Strategy (GOPS) and The Resistance: Avalon. The effectiveness of STRATEGIST is shown through its ability to outperform existing LLM-based self-improvement methods and traditional RL approaches, while also achieving comparable performance against human players.",
      "timestamp": "2026-02-16 13:10:00"
    },
    "7": {
      "question": "What was it tested on?",
      "answer": "The STRATEGIST framework was tested on two games: the Game of Pure Strategy (GOPS) and The Resistance: Avalon. These games were chosen due to their strategic complexity and differing levels of natural language interaction. The evaluation setup involved comparing STRATEGIST's performance against traditional reinforcement learning methods, other LLM-based agents, and human players. The framework's effectiveness was demonstrated through population-based self-play simulations, where STRATEGIST agents outperformed existing methods and achieved comparable performance to human players in these multi-agent, adversarial environments.",
      "timestamp": "2026-02-16 13:10:11"
    },
    "8": {
      "question": "What's cool about it?",
      "answer": "The novel aspect of the STRATEGIST framework is its integration of large language models (LLMs) with Monte Carlo Tree Search (MCTS) to enhance decision-making in complex, multi-agent environments without requiring extensive training data. This bi-level approach leverages LLMs for high-level strategy generation and refinement, while MCTS provides precise low-level policy execution, allowing for effective self-improvement through population-based self-play simulations. The framework's ability to outperform traditional reinforcement learning methods and existing LLM-based agents, while achieving human-comparable performance in games like Avalon, highlights its innovative combination of generalization and detailed planning capabilities.",
      "timestamp": "2026-02-16 13:10:27"
    },
    "9": {
      "question": "What's sketchy about it?",
      "answer": "The paper presents several potential red flags and limitations. One concern is the high variance in individual runs, which is attributed to the noisy feedback from multi-agent adversarial game settings, particularly in Avalon where performance depends on multiple players' policies. Additionally, the inherent noisiness in LLM generations can impact results, and the method has not been tested in non-adversarial environments, which limits its generalizability. Furthermore, while the method shows promise, it relies heavily on simulated self-play, which may not fully capture the complexities of real-world interactions.",
      "timestamp": "2026-02-16 13:10:37"
    },
    "10": {
      "question": "Can anyone use this?",
      "answer": "STRATEGIST is a sophisticated framework that combines large language models (LLMs) with Monte Carlo Tree Search (MCTS) to improve decision-making in complex, multi-agent environments. While it demonstrates superior performance in strategic games like GOPS and Avalon, its practicality and accessibility may be limited. The framework requires significant computational resources, such as those provided by advanced GPUs, and involves complex processes like bi-level tree search and population-based self-play, which may not be easily accessible or affordable for all users. Additionally, its application is primarily focused on specific strategic games, potentially limiting its use in other contexts.",
      "timestamp": "2026-02-16 13:10:44"
    },
    "11": {
      "question": "What's still left to figure out?",
      "answer": "The paper identifies several areas that remain unsolved or unclear, opening up new questions and future directions. One key area is the inherent noisiness in feedback from multi-agent adversarial environments, which can affect the performance of the STRATEGIST framework. The paper suggests that running more simulations with different opponent policies might help reduce this noise. Additionally, while the method shows strong performance in adversarial settings, it has not been tested in non-adversarial environments, leaving open the question of how well it might perform in those contexts.",
      "timestamp": "2026-02-16 13:10:51"
    }
  },
  "core_analysis": {
    "input_data": {
      "title": "Strategist: Self - Improvement of Llm Decision Making Via Bi - Level Tree Search",
      "content": "# 2408.10635\n\n*Generated from PDF: 2408.10635.pdf*\n\n---\n\n## Published as a conference paper at ICLR 2025\n\n## # STRATEGIST: SELF-IMPROVEMENT OF LLM DECISION MAKING VIA BI-LEVEL TREE SEARCH\n\nJonathan Light $^{1}$  Min Cai $^{2}$  Weiqin Chen $^{1}$  Guanzhi Wang $^{5}$  Xiusi Chen $^{3}$  Wei Cheng $^{4}$  Yisong Yue $^{5}$  Ziniu Hu $^{5}$\n\n$^{1}$ Rensselaer Polytechnic Institute,  $^{2}$ Shenzhen University,\n$^{3}$ University of California, Los Angeles,  $^{4}$ NEC laboratories America,\n## California Institute of Technology\n\nhttps://llm-strategist.github.io\n\n## # ABSTRACT\n\nTraditional reinforcement learning and planning typically requires vast amounts of data and training to develop effective policies. In contrast, large language models (LLMs) exhibit strong generalization and zero-shot capabilities, but struggle with tasks that require detailed planning and decision-making in complex action spaces. We introduce STRATEGIST, a novel approach that integrates the strengths of both methods. Our approach leverages LLMs to search and update high-level strategies (as text), which are then refined and executed by low-level Monte Carlo Tree Search (MCTS). STRATEGIST is a generalizable framework to optimize the strategy through population-based self-play simulations without the need for any training data. We demonstrate the effectiveness of STRATEGIST in learning optimal strategies for competitive, multi-turn games with partial information, including Game of Pure Strategy (GOPS) and multi-agent, hidden-identity discussion games like The Resistance: Avalon. Our results show that agents equipped with STRATEGIST outperform those trained with traditional RL methods, other LLM-based skill acquisition techniques, pre-existing LLM agents across both game environments and achieves comparable performance against human players.\n\n## # 1 INTRODUCTION\n\nRecent studies have shown the potential of Large Language Models (LLMs) for learning skills and improving decision-making in interactive environments Wang et al. (2022; 2023a; 2024); Xi et al. (2023); Zhao et al. (2024); Liu et al. (2023). However, adversarial, multi-agent environments present a significant challenge. LLMs must reason about opponent actions, plan ahead strategically, and navigate a vast policy space Kambhampati et al. (2024); Valmeekam et al. (2023); Kambhampati (2024). This complexity hinders the LLM's ability to identify, understand, and translate optimal policies into effective actions. Simply generating decision rules by querying the LLM proves inefficient and impractical in such settings. This raises two critical questions: (1) How can LLMs effectively learn complex policies in adversarial multi-agent environments? (2) What is the best approach to improve these policies?\n\nThis paper introduces STRATEGIST, a novel framework that leverages LLMs to develop high-level strategies, represented as interpretable text,\n\nwhich are then refined and translated into executable policies using a low-level executor (Figure 1). Our approach combines LLM self-improvement with a hierarchical search process, operating on both a high-level strategy space and a low-level action space.\n\n![img-0.jpeg](img-0.jpeg)\nFigure 1: Overview of STRATEGIST. We use an LLM to generate and improve high-level strategy abstractions using environment feedback, then refine strategy into a policy using tree search or CoT.\n\nAt the high level, Strategist constructs a strategy tree through an evolutionary process, iteratively testing and improving strategies via LLM-guided revisions based on self-play feedback. This process fosters the emergence of robust, adaptable strategies. To further enhance strategy improvement, we maintain a queue of \"improvement ideas\" sampled via a bandit algorithm to guide LLM revisions. At the low level, we employ fine-grained tree search techniques (e.g., MCTS) guided by the high-level strategy to refine the agent‚Äôs policy. We can run the policy against itself through self-play in the multi-agent strategic game, and use win-rate as final outcome reward. The reward can be backpropagated to help update the strategy. This bi-level approach effectively integrates the generalization capabilities of LLMs with the precise planning of MCTS. Both levels are updated through simulated self-play, eliminating the need for extensive training data.\n\nWe demonstrate the effectiveness of Strategist in two challenging games: the Game of Pure Strategy (GOPS) and Resistance: Avalon *(Ross, 1971; Light et al., 2023)*, chosen for their strategic complexity and differing levels of natural language interaction. Our results show that Strategist outperforms existing LLM-based self-improvement methods and traditional RL approaches, achieving superior performance within the same computational and data budget. Furthermore, we highlight the successful integration of feedback across the different levels of our framework, demonstrating the synergistic interplay between high-level strategy improvement and low-level policy refinement. Moreover, Strategist matches human performance in Avalon while employing a more sophisticated mixed strategy and strategic randomization to deceive others and conceal its identity.\n\n## To summarize, our main contributions are:\n\n- [leftmargin=*]\n- We propose Strategist, a general non-parametric bilevel skill-learning framework that uses LLMs to learn high-level strategy abstractions, refined into low-level policies by a modular executor. These abstractions can define strategic profiles for all players in the game.\n- We introduce a modular improvement method for high-level strategies using population-based self-play without training data, demonstrating superior performance over existing LLM-based skill learning methods.\n- We apply Strategist to GOPS and Avalon, demonstrating its effectiveness in learning policies for both dialogue and non-dialogue actions, achieving higher win rates than both existing agents and human players. Examples of learned strategies are provided in H.3 and I.3.\n\n## 2 Methodology\n\n### 2.1 Decision making setting and games\n\nThe general goal in our decision-making setting is to learn a good policy function in a sequential decision-making setting (generally formulated as a partially observable Markov decision game (POMDG)). Our strategy-learning framework is generalizable to any decision making setting.\n\nProblem definition. Given state space $\\mathcal{S}$ and action space $\\mathcal{A}$, a policy function $\\phi$ in policy space $\\Phi$ is a mapping $\\phi:\\mathcal{S}\\rightarrow\\Delta\\mathcal{A}$ where we allow $\\phi$ to output a probability distribution over the actions ($\\Delta\\mathcal{A}$). An environment $\\mathcal{E}=\\langle\\mathcal{S},\\mathcal{A},\\mathcal{N},T,R,A,\\phi_{\\epsilon}\\rangle$ defines the state space $\\mathcal{S}$, the action space $\\mathcal{A}$, a set of actors $\\mathcal{N}$, a transition function $T:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathcal{S}$, a reward function $R:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}^{|\\mathcal{N}|}$ that specifies intermediate rewards for each actor, an action function $A:\\mathcal{S}\\rightarrow\\mathcal{N},\\mathcal{P}(\\mathcal{A})$ that specifies which actor may take what legal actions at some state where $\\mathcal{P}$ is power set, and $\\phi_{\\epsilon}$, the policy function for the environment actor. Note that transitions are deterministic in our notation, and stochastic transitions are handled by the environment actor $\\epsilon\\in\\mathcal{N}$ instead, so we cover stochastic, deterministic, multi-agent, and single agent cases. For a partial information setting, we also have a space of information sets $\\mathcal{I}$ and a function $H:\\mathcal{S}\\times\\mathcal{N}\\rightarrow\\mathcal{I}$ that maps from hidden states and actors to hidden information sets. Hence, $\\phi:\\mathcal{I}\\rightarrow\\Delta\\mathcal{A}$ is a function from information sets to action distributions instead. A strategic profile $\\bm{\\phi}=[\\phi_{i}]_{|\\mathcal{N}|}$ describes the policies for every player $i\\in\\mathcal{N}$.\n\nLet $f:\\Sigma\\rightarrow\\Phi$ be the function that maps strategies to policies. A high-level strategy $\\sigma\\in\\Sigma$ helps parameterize policies so that we can search over the lower dimension $\\Sigma$ space instead of $\\Phi$. Let $\\Phi_{-i}$ denote the space of possible opponent policies, where $-i$ are the indices of players other than i. Then our goal is to find the optimal strategy $\\sigma_{i}$ that approximates finding the optimal policy given the policies of the other agents $\\phi_{-i}$, i.e.\n\n$\\operatorname*{argmax}_{\\sigma_{i}}\\operatorname*{\\mathbb{E}}_{\\tau\\sim(f(\\sigma_{i}),\\phi_{-i})}\\left[\\sum_{(s,a)\\in\\tau}R_{i}(s,a)\\right]\\approx\\operatorname*{argmax}_{\\tau\\sim(\\phi_{i},\\phi_{-i})}\\operatorname*{\\mathbb{E}}\\left[\\sum_{(s,a)\\in\\tau}R_{i}(s,a)\\right]$\n\n$\\tau=(s_{0},a_{0},...)$ is the simulated trajectory according to the strategic profile $(\\phi_{i},\\phi_{-i})$ and the transition function $T$, with $a_{t}\\sim\\phi(a_{t}|s_{t})$ and $s_{t+1}=T(s_{t},a_{t})$. Thus, a key part of these settings involves learning the probable strategic profile of other players, usually by assuming that they play optimally like in a Nash equilibrium *(Perolat et al., 2022)*.\n\nThe state space, action space, and actor space are different depending on the setting. In non-stochastic, single agent settings such as question answering *(Yang et al., 2018; Wang et al., 2022; Shridhar et al., 2020; Thorne et al., 2018)*, $\\mathcal{N}=\\{0\\}$. In stochastic single agent settings such as web browsing *(Yao et al., 2022a; Deng et al., 2024; Zhou et al., 2023)*, $\\mathcal{N}=\\{\\epsilon,0\\}$ where we add an environment agent $\\epsilon$. We specifically focus on adversarial, multi-agent stochastic game ($\\mathcal{N}=\\{\\epsilon,0,1,...\\}$) settings where the other agents are actively working against each other. In non-dialogue-based card games such as GOPS (see B for rules) for example, $\\mathcal{S}$ consists of the cards played so far, $\\mathcal{N}=\\{\\epsilon,0,1\\}$ for players 1 and 2 respectively, and $\\mathcal{A}$ consists of the cards you can play. In dialogue-based games such as Avalon (see A for rules), $\\mathcal{A}$ consists of both the discrete actions (moves) such as voting, and language actions which consist of text. Similarly, $\\mathcal{S}$ consists of both the historical moves and historical dialogue record, and $\\mathcal{N}=\\{\\epsilon,0,1,...\\}$ depending on the number of players involved, with $|\\mathcal{N}|\\geq 6$.\n\n### 2.2 Abstracting high level strategies as processes\n\nA key part of our framework abstracts key features of the policy $\\phi$, representing them as a high-level strategy $\\sigma$, which is more suitable for LLM-driven improvement. $\\sigma$ is then executed and refined by a low-level executor, often during inference, resulting in the execution policy $\\phi_{\\sigma}$.\n\n$\\text{feedback from environment}\\rightarrow\\texttt{LLM-Improver}\\rightarrow\\sigma\\rightarrow\\texttt{Executor}(\\sigma)\\rightarrow\\phi_{\\sigma}$\n\nAbstraction offers two key benefits: (1) High-level strategies provide a more abstract, compressed, and intuitive representation of the problem, which the LLM better understands, enabling its generalization and reasoning capabilities. (2) Searching over high-level strategies is more efficient than low-level policy exploration, as it simplifies the search space by focusing on core principles rather than fine details. This approach guides the search toward promising areas more quickly, leveraging domain knowledge, patterns, or heuristics that generalize across scenarios.\n\nFor non-dialogue actions, while the action spaces $\\mathcal{A}$ and state spaces $\\mathcal{S}$ are typically discrete and finite, the number of possible functions $\\Phi$ mapping state to action is vast. Most LLM-agents query the LLM directly with state information to decide the next action in decision-making environments *(Yao et al., 2023; Shinn et al., 2024; Zhao et al., 2024)*. However, this is costly because the LLM must be queried for every move, and in Avalon, each player makes at least 20 moves. This becomes even more expensive when incorporating look-ahead search, which requires querying the LLM for future actions and states. Traditionally, policy-gradient reinforcement learning addresses the large policy space by parameterizing the policy and optimizing these parameters, reducing the search space.\n\nWe abstract and ‚Äúparameterize‚Äù the policy $\\phi$ as a value heuristic (VH) $\\sigma\\coloneqq v:\\mathcal{S}\\rightarrow\\mathbb{R}^{|\\mathcal{N}|}$ that estimates the expected cumulative returns for each player at a given state, typically the expected probability of winning for each player. Using a value heuristic simplifies reasoning, as it‚Äôs easier to describe how good a state is than to specify the optimal action. This makes it intuitive for the LLM to reason about winning probabilities. Additionally, we represent the value heuristic as Python code, allowing the LLM to easily process and modify the logic for *computing the value* for each state.\n\nExample LLM Generated Value Heuristic Function def evaluate_state(state): # Calculating the potential scores for each player player_0_potential_score = sum(state,player_0_hand) player_1_potential_score = sum(state,player_1_hand) # Calculating the potential final scores for each player player_0_final_score = player_0_score + player_0_potential_score player_1_final_score = player_1_score + player_1_potential_score # Storing the intermediate values used to calculate the scores intermediate_values = { ‚Äôplayer_0_potential_score‚Äô: player_0_potential_score, ‚Äôplayer_1_potential_score‚Äô: player_1_potential_score } return player_score, intermediate_values\n\nAt the low-level, we execute the strategy (value function) by selecting the action that leads to the best state: $\\phi_{i}(s)=\\operatorname*{argmax}_{a\\in\\mathcal{A}}Q(s,a)=\\operatorname*{argmax}_{a\\in\\mathcal{A}}R_{i}(s,a)+v_{i}(s^{\\prime})|s^{\\prime}=T(s,a)$. Since\n\n## Published as a conference paper at ICLR 2025\n\nwe learn the value  $v_{i}$  for each actor, we can compute the best action for each actor  $i$ , refining the value function into a strategic profile  $\\phi$ . Since value heuristics can be inaccurate, we can refine the policy using search such as MCTS to look ahead through multiple action-state sequences, resulting in a better policy (Grill et al., 2020; Silver et al., 2017; Schrittwieser et al., 2020). MCTS also generates additional feedback by comparing the updated value estimate from MCTS with the initial value heuristic. The estimated win-rate from search provides a shaped reward signal, which is more informative than the simple win/lose outcome reward. We focus on key states with the largest discrepancy between the MCTS estimate and the episode outcome for self-improvement feedback. More details on our MCTS implementation are in Appendix F.\n\nDialogue generation poses a unique challenge for self-improvement. Both the action space  $\\mathcal{A}$ , representing possible word sequences for each discussion round, and the state space  $S$ , representing prior input dialogues, are enormous. As a result, the number of possible dialogue generation policies  $\\Phi$  is vast, making traditional parameter training approaches difficult to optimize. To address this, we develop a high-level strategy guide that the agent processes before speaking. This guide formalizes a chain-of-thought process to analyze the situation before generating dialogue (Wei et al., 2022b). It is implemented as a question-and-answer worksheet, where the agent answers all questions in the guide before using it as a prompt for dialogue generation.\n\n# Example (fully generated) Chain of Thought Dialogue Strategy Guide\n\n## Q1: Who are the players you suspect are on the Evil side and why?\n\nA1: I suspect Player 2 and Player 4 are on the Evil side. Player 2's suggestion for the team consisting of themselves and Player 1 seems too convenient, as it would allow Evil to easily fail the quest. Player 4's agreement with Player 1 and Player 2's proposed team without offering any alternative suggestions raises suspicion.\n\nQ2: Draft a subtle statement that casts doubt on these players without directly accusing them.\n\nA2: \"I can't help but wonder if some players are pushing for certain team compositions to benefit themselves rather than considering what is truly best for the success of our Quest.\"\n\nGenerated dialogue: \"I have observed patterns in the voting that suggest certain players may be working together to push for specific team compositions. This raises suspicion and calls into question their true intentions. We must not allow ourselves to be swayed by potentially deceitful tactics.\"\n\n## # 2.3 MODULAR SEARCH FOR HIGH-LEVEL STRATEGY IMPROVEMENT\n\n![img-1.jpeg](img-1.jpeg)\n## High-level strategy improvement\nFigure 2: Overview of our high-level strategy improvement method: The process alternates between two steps‚Äîidea generation and strategy improvement. For more details see Figure 11.\n\nOur framework alternates between two steps: idea generation and strategy improvement, as shown in Figure 2 and detailed in App. D. Each cycle refines strategies stored in a tree structure based on feedback from self-play simulations and generates new improvement ideas stored in a priority queue.\n\n#### 2.3.1 Idea Generation\n\nIn the idea generation step, a strategy $\\sigma$ and its feedback trajectory $\\tau_{\\sigma}$ are selected from the strategy tree using an adaptive selection policy (e.g., UCB or BFS). Feedback consists of trajectories from previous self-play simulations, including visited states, actions taken, estimated win rates, final outcomes, and intermediate values. To avoid processing lengthy trajectories, we select key states that best capture discrepancies between the strategy‚Äôs value heuristic and search-based estimates.\n\nThese key states are translated into natural language and used to prompt the LLM for new improvement ideas. The ideas are added to the idea queue with a prior score estimating their potential effectiveness:\n\n## Idea-queue $\\leftarrow$ LLM-idea-inventor(Sampled strategy, strategy feedback)\n\n#### 2.3.2 Strategy Improvement\n\nIn the strategy improvement step, we sample a strategy $\\sigma$ from the strategy tree and an idea $d$ from the queue, using a method that balances exploration and exploitation (e.g., UCB). The LLM refines $\\sigma$ using $d$, generating a new strategy $\\sigma_{\\text{new}}$. This new strategy is evaluated via self-play simulations, which produce win rates $W[\\sigma]$ and trajectory feedback $\\mathcal{T}[\\sigma]$.\n\nDuring simulations, players conduct MCTS tree searches to estimate win rates at different states, providing additional feedback. The strategy tree is updated with the new strategy and its performance:\n\n## Strategy-library $\\leftarrow$ LLM-reviser(Sampled idea, sampled prior strategy)\n\nThe improvement score of the idea $d$ is updated based on how much it improved $\\sigma$.\n\n#### 2.3.3 Search Modularization\n\nA key feature of our framework is the use of an *idea queue* to modularize the search process. By refining strategies incrementally rather than globally, we avoid confounding factors and ensure interpretability of changes. The queue also tracks successful improvements that are generalizable across strategies, enabling transfer and reuse of ideas. Improvements are often additive (e.g., penalties or adjustments) and enhance performance when applied to similar strategies (Table 2).\n\n## We use UCB sampling to balance exploration of new ideas and exploitation of proven ones:\n\n$UCB(\\text{idea})=\\overline{z}_{\\text{idea}}+c\\sqrt{\\frac{\\ln(N_{\\text{total}})}{N_{\\text{idea}}}}$\n\nwhere $\\overline{z}$ is the empirical average improvement score, $N_{\\text{total}}$ is the total number of ideas implemented, and $N_{\\text{idea}}$ is the number of implementations of the specific idea.\n\nTo simplify the improvement process, we optimize dialogue generation and gameplay moves separately in Avalon before integrating them into a unified agent (Appendix E). This modular approach mirrors strategies used in related domains, such as Diplomacy *(1)*.\n\n### 2.4 Population based self-play simulation\n\nRecent works focus on using either another LLM for critique-based feedback *(Madaan et al., 2024; Shinn et al., 2024)*, real environment interactions *(Nottingham et al., 2024)*, or a combination of both *(Wang et al., 2023a)* during the LLM-improvement process. Our method improves on this by simulating opponents using the learned high-level strategy, enabling higher-quality feedback. Since our high-level strategies can be refined into a policy for any player, we can easily pit different strategies against each other by refining them into specific policies for different players. Specifically, we use population based self-play, where we conduct round-robin games among the top ten strategies generated and use the average performance as the strategy score for each strategy *(Xu et al., 2023b)*. During round-robin games, strategies are given the same level of low-level refinement to ensure fairness. This evolutionary approach makes the final improved strategy more robust to different opponent policies, since it will have survived multiple round-robin tournaments. We demonstrate the effectiveness of this method in section 3.5.\n\n## Published as a conference paper at ICLR 2025\n\n## # 3 EXPERIMENTS\n\nWe evaluate the effectiveness of our bi-level framework through four key experiments: demonstrating STRATEGIST's competitiveness with human players, its superiority over RL-based deep learning methods, its advantage over LLM-based agents, and the effectiveness of our LLM-improvement method compared to other techniques. Our approach is tested on two games with distinct challenges. Game of Pure Strategy (GOPS) is a two-player, zero-sum card game requiring strategic decisions under partial information and simultaneous moves (see B) (Ross, 1971; Lanctot et al., 2009). The Resistance: Avalon is a multi-agent (5+ players), team-based game combining language, deception, and deduction (see A) (Light et al., 2023). These games highlight STRATEGIST's versatility in handling strategic and social complexities.\n\n![img-2.jpeg](img-2.jpeg)\nFigure 3: Action analysis of STRATEGIST and humans when playing against each other in Avalon. STRATEGIST conducts more strategic randomization than humans, making its identity harder to deduce based on actions.\n\n## # 3.1 HUMAN EVALUATIONS\n\nWe conducted human experiments in the six-player Avalon setting, using the following character roles: Merlin, 3 Servants of Arthur, 1 Assassin, and 1 Minion of Mordred. Ten participants were recruited and randomly assigned to one of the roles, playing against five STRATEGIST agents. In total, 30 games were collected and analyzed. As presented in Table 1, STRATEGIST achieved a win rate comparable to human players. After each session, participants completed a survey evaluating the performance of STRATEGIST across seven key metrics, while an experimenter independently assessed human performance. The survey results are visualized in Figure 4, revealing that STRATEGIST outperformed humans in concealment and adaptability to human playstyles, but lagged behind in reasoning, deduction, and cooperation.\n\n## Additionally, we conducted an action analysis to compare the behavior of STRATEGIST and\n\n![img-3.jpeg](img-3.jpeg)\nFigure 4: Human vs STRATEGIST performance metrics in Avalon. We ask humans to evaluate the performance of STRATEGIST and human players in ad-hoc human-AI games. STRATEGIST performs better than humans at concealment, but worse at reasoning and cooperation.\n\nhuman players, shown in Figure 3. Specifically, we examined the distribution of approval votes‚Äîa pivotal action in Avalon. The data revealed that human players exhibited distinct voting patterns based on their roles, facilitating the deduction of their identities. In contrast, STRATEGIST demonstrated strategic randomization, producing more uniform action distributions across roles and thereby making its identity harder to infer. More details on human evaluations can be found in Appendix O.\n\n## Published as a conference paper at ICLR 2025\n\nTable 2: Comparison of different self-improvement methods. The improvement process was run with the same number of strategies generated for each method (40 for GOPS, 24 for Avalon) on the same seed functions, all with GPT3.5. We collect 9 functions from each process and play them against each other (a total of  $5 \\times 9 = 40$  different agents), reporting the average number of points you win over your opponents for GOPS and the average winrate for Avalon. For the dialogue guide, we show the improvement over the baseline seed function, which has a score  $z = -0.875 \\in [-2, 2]$ , a rating from opponents which we describe in G.\n\n|  Self-Improvement Methodology | Line search (Madaan et al., 2024; Shinn et al., 2024) | Greedy search (Ma et al., 2023) | Best First Search (BFS) (Yao et al., 2024) | BFS with thought | STRATEGIST  |\n| --- | --- | --- | --- | --- | --- |\n|  GOPS Value Heuristic | -0.47 ¬±0.74 | -0.54 ¬±0.45 | 0.092 ¬±0.67 | -0.48¬±0.375 | 1.5 ¬±0.99  |\n|  Avalon Value Heuristic | 0.54 ¬±0.11 | 0.47 ¬±0.11 | 0.50 ¬±0.085 | 0.55 ¬±0.065 | 0.59 ¬±0.11  |\n|  Merlin Dialogue Guide | 0.37 ¬±0.19 | 0.62 ¬±0.13 | 0.49 ¬±0.063 | 0.37 ¬±0.06 | 0.88 ¬±0.063  |\n|  Assassin Dialogue Guide | 1.83 ¬±0.25 | 1.81 ¬±0.063 | 1.82 ¬±0.063 | 1.89¬±0.063 | 1.98 ¬±0.042  |\n\nWe further analyzed the proportion of correct votes, defined as rejecting teams with at least one Evil player or approving teams with only Good players. This metric indicates a player's ability to infer Good and Evil identities, a skill particularly crucial for the Merlin role. As shown in Figure 3, STRATEGIST employed increased randomization as Merlin, minimizing information leakage through voting patterns. Conversely, human players displayed highly role-specific correctness trends. These findings align with survey feedback, corroborating that STRATEGIST excels in concealment strategies compared to human counterparts.\n\n![img-4.jpeg](img-4.jpeg)\nFigure 5: Description of STRATEGIST play-style compared to human players, as evaluated by human participants. STRATEGIST is described as being more logical and cautious.\n\n## # 3.2 DIFFERENT LLM IMPROVEMENT METHODS\n\nWe demonstrate the effectiveness of our strategy improvement method by benchmarking it against four other skill-improvement methods. Line search (Madaan et al., 2024) always reflects and improves upon the latest improved strategy. Greedy search (Ma et al., 2023) selects the best strategy from the last generation of improved strategies to improve upon each im\n\nprovement cycle. Best first search (Yao et al., 2024) improves upon the  $k$  best strategies generated in any iteration of each improvement cycle. Best first search with thought enhances BFS by prompting the LLM to reflect, think, and propose ideas on how to improve the previous strategy before improving the strategy itself (Yao et al., 2022b). STRATEGIST is our method that uses an additional idea queue  $Q$  and an idea generation step to guide the improvement process. Comparing BFS with thought and STRATEGIST helps demonstrate the value of having a modularized reflection and idea proposition process that is separate from strategy generation. More details can be found in Appendix N.\n\nOur results, presented in Table 2, use the same feedback method (simulation-based population self-play) while varying the improvement methods, with a constant number of new strategies generated across all methods. Figure 12 (right) shows the gameplay scores of these strategies on GOPS. Even when controlling for the number of output tokens generated by the LLM, our method outperforms the others, as demonstrated in Figure 18. We attribute this to (1) the idea queue, which helps test incremental improvements and guide the search process, and (2) our strategy and idea selection policy, which more efficiently explores the strategy space and escapes local maxima (Figure 12 left).\n\n## # 3.3 INTERACTION BETWEEN LOW-LEVEL REFINEMENT AND HIGH LEVEL STRATEGY SEARCH\n\n## Published as a conference paper at ICLR 2025\n\nWe demonstrate that both our high-level strategy improvement and low-level refinement processes scale effectively with increased budgets by showing how different high-level strategies performance with varying MCTS search budgets. A higher search budget allows for more nodes to be looked ahead before taking actions, resulting in a more refined policy. As shown in Figure 6, strategies improved with more iterations using STRATEGIST exhibit both higher performance and greater performance gains with increased refinement. Policy refinement also scales exceptionally well.\n\n## # 3.4 LLM-IMPROVEMENT VS. REINFORCEMENT LEARNING (RL) BASED METHODS\n\nWe demonstrate the effectiveness of our method compared to traditional RL-based approaches for learning a good policy. Specifically, we show that our method learns a value heuristic function more efficiently than deep RL, as used in AlphaGo, MuZero (Silver et al., 2017; Schrittwieser et al., 2020), and DeepRole (Serrino et al., 2019). While AlphaGo uses MCTS (Kocsis &amp; Szepesv√°ri, 2006) with a deep value network, DeepRole combines counterfactual regret minimization with a deep value\n\nnetwork (Moravƒç√≠k et al., 2017; Zinkevich et al., 2007). We adapt both algorithms to our setting.\n\nAlthough deep RL can approximate the true value function with enough data, training, and a large network, we ensure a fair comparison by (1) limiting both RL methods and our approach to the same number of simulated episodes, and (2) capping the number of training steps after each batch of episode trajectory data. While both RL methods and STRATEGIST use the same number of self-play episodes, STRATEGIST only trains on a small subset of transition steps, whereas RL methods train on the entire dataset. Our results, shown in Table 3 and Figure 7, demonstrate that STRATEGIST consistently outperforms both AlphaGo and DeepRole. More details on our RL implementation and the effects of increased sample size can be found in Appendix L, where we observe performance improvements with additional samples.\n\nTable 3: Comparison of reinforcement learning vs our method. We run each process 10 times, taking the best strategy generated by each run and playing them against each other for 1024 games. Average win-rates, total number of self-play episodes, and training transition steps across best generated strategies are shown here.\n\n|  Setting | Metric | VS Alpha-go |   | VS DeepRole  |   |\n| --- | --- | --- | --- | --- | --- |\n|   |   |  Alpha-go | STRATEGIST | DeepRole | STRATEGIST  |\n|  GOPS | Point difference | -0.39 ¬± 0.22 | 0.33 ¬± 0.38 | -0.56 ¬± 0.18 | 1.14 ¬± 0.09  |\n|   |  Self-play eps. | 320 episodes | 320 episodes | 320 episodes | 320 episodes  |\n|   |  Trans. steps | ~ 3,840 steps | 100 steps | ~ 3,840 steps | 100 steps  |\n|  Avalon | Winrate | 0.30 ¬± 0.03 | 0.38 ¬± 0.04 | 0.33 ¬± 0.04 | 0.44 ¬± 0.02  |\n|   |  Self-play eps. | 160 episodes | 160 episodes | 160 episodes | 160 episodes  |\n|   |  Trans. steps | ~ 24,000 steps | 60 steps | ~ 24,000 steps | 60 steps  |\n\n## Published as a conference paper at ICLR 2025\n\n## # 3.5 FEEDBACK QUALITY AND REWARD SIGNAL\n\nWe benchmark our feedback method (population based self-play) against (1) an LLM-critic (Madaan et al., 2024) and (2) trajectory feedback from a fixed opponent policy, with results in Table 4 showing superior performance in both GOPS action planning and dialogue generation. This shows the effectiveness of our evolutionary population based self-play approach, and that of our strategy abstraction that learns a strategic profile for all players.\n\nTable 4: Comparison of different methods of collecting feedback. All methods use the same high-level improvement process (STRATEGIST). For GOPS we collect 24 generated functions from each method and play them against each other. For Avalon we evaluate 9 generated guides. We simulated 32 games for GOPS and 4 rounds of discussion for Avalon to collect gameplay feedback during each improvement step, across 10 and 6 improvement rounds for GOPS and Avalon respectively.\n\n|  Setting/Method | Metric | LLM-critic | Fixed opponent | Population-based Self-play  |\n| --- | --- | --- | --- | --- |\n|  GOPS | Point difference | -0.27 ¬± 1.1 | 0.089 ¬± 0.86 | 0.87 ¬±1.5  |\n|   |  # of opponents | 0 | 1 | 4  |\n|  Avalon | Winrate | 0.37 ¬± 0.063 | 0.62 ¬± 0.13 | 0.88 ¬±0.06  |\n|   |  # of opponents | 0 | 1 | 4  |\n\n## # 3.6 STRATEGIST VS. LLM AGENTS\n\nWe demonstrate that our method also achieves better performance against other LLM-agents. ReAct is a popular LLM-agent that prompts the LLM to think before taking an action (Yao et al., 2022b). ReCon is a LLM-agent for hidden identity games that prompts the LLM to recursively contemplate on what opponents are thinking and might do before taking actions (Wang et al., 2023b). We adapt ReCon to our setting by asking the agent to recursively contemplate. Our gameplay results are shown in Table 5. This suggests that through STRATEGIST, our LLM-agent is able to learn high-level strategies similar in performance to those of ReCon, such as recursive contemplation.\n\nTable 5: Results of STRATEGIST playing against LLM-based baselines, i.e., ReAct and ReCon.\n\n|  Metric | VS ReAct |   | VS ReCon  |   |\n| --- | --- | --- | --- | --- |\n|   |  ReAct | STRATEGIST | ReCon | STRATEGIST  |\n|  Winrate | 47.5 ¬± 2.5 | 52.5 ¬± 2.5 | 38.9 ¬± 5.5 | 61.1 ¬± 5.5  |\n|  #Tokens per round | 56 ¬± 14.3 | 164.3 ¬± 27.7 | 245.7 ¬± 21.2 | 248.2 ¬± 24.1  |\n\n## # 4 RELATED WORK\n\nLLMs for Text Agents. Large language models (LLMs) demonstrate emergent capabilities such as zero-shot prompting, reasoning, and extensive world knowledge (Bommasani et al., 2021; Brown et al., 2020; Wei et al., 2022a; Yu et al., 2023a). Recent frameworks like ReAct (Yao et al., 2023) and Reflexion (Shinn et al., 2024) have incorporated reasoning, memory, feedback, and tool use to improve decision-making (Wang et al., 2023a; Huang et al., 2022; Schick et al., 2024). Prompting techniques like Chain-of-Thought and Tree-of-Thought are effective for reasoning (Wei et al., 2022b; Yao et al., 2024) but fall short in complex games requiring iterative self-improvement. Our method, STRATEGIST, introduces a bi-level tree search combining high-level planning and self-play for feedback-driven strategy refinement.\n\nSkill Learning with LLMs. LLMs have been used to acquire skills by learning textual memories or insights (Shinn et al., 2024; Zhao et al., 2024). However, textual approaches struggle with long, quantitative trajectories. We focus on high-level strategies optimized through simulational self-play, enabling robust skill learning in multi-agent environments. While reward models learned by LLMs\n\n## Published as a conference paper at ICLR 2025\n\nexcel in single-agent tasks (Ma et al., 2023; Yu et al., 2023b), we extend these ideas to multi-agent settings by introducing methods for feedback generation and strategy improvement.\n\nAI in Strategy Games. Advances like AlphaGo and MuZero highlight the synergy of MCTS, deep learning, and self-play (Silver et al., 2017; Schrittwieser et al., 2020). LLMs have also been integrated into dialogue-based games such as Diplomacy (FAIR) and Texas Hold'em (Zhang et al., 2024). Our approach bridges these domains by enabling LLMs to both train value heuristics more efficiently than RL and generate dialogue for social deduction games without human examples. Additionally, our method has potential applications in negotiation tasks (Abdelnabi et al., 2023; Fu et al., 2023).\n\nLLM-agents for discussion games. There has been much recent interest in using LLMs to develop agents for discussion-based games, as these games provide an excellent benchmark for assessing the reasoning, planning, and strategic deception capabilities of LLMs. This includes Werewolf (Bailis et al., 2024; Xu et al., 2023b;a; Wu et al., 2024), another social deduction game, and other negotiation and word games (Fu et al., 2023; Cheng et al., 2024). We give a more detailed comparison of our work to other AI agents in Table 6 and App. P.\n\n|  Metric | EnReaWolf | Cicero | ComWolf | LARLWolf | SPAG | AlphaGo | ICL-AIF | Agent-Pro | ReCon | DeepRole | Strategist  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  Used in Avalon | X | X | X | X | X | X | X | X | ‚úì | ‚úì | ‚úì  |\n|  Parameter-free | X | X | ‚úì | X | X | X | ‚úì | ‚úì | ‚úì | X | ‚úì  |\n|  Uses LM | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì | X | ‚úì | ‚úì | ‚úì | X | ‚úì  |\n|  Self-improve | ‚úì | ‚úì | X | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì | X | ‚úì | ‚úì  |\n|  Tree Search | X | ‚úì | X | X | X | ‚úì | X | X | X | ‚úì | ‚úì  |\n|  Human-annotation free | X | X | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì  |\n|  Belief updates | X | X | X | X | X | X | X | ‚úì | X | ‚úì | ‚úì  |\n|  Bi-level improvement | X | X | X | X | X | X | X | X | X | X | ‚úì  |\n\nTable 6: Comparison of Strategist with Related Works. We use an bi-level improvement approach, paired with an advanced high-level strategy learning process that is parameter free. Our approach handles beliefs and partial observability and uses low-level policy refinement (tree-search) to further enhance our policy. For details see App. P\n\n## # 5 CONCLUSION\n\nIn conclusion, we have introduced STRATEGIST, a bi-level framework that facilitates high-level abstract strategy learning with LLMs and a generalizable non-parametric self-improvement mechanism. This approach enables the model to learn and refine skills autonomously. Without relying on task-specific prompts or human-generated policy data, our method demonstrates its ability to learn effective strategies through population-based self-play, guided solely by the rules of the game.\n\nThe performance of STRATEGIST highlights the potential of leveraging LLMs for generating high-level strategic abstractions, while using low-level tree search to iteratively refine the policy. This dual approach not only showcases the power of modular guidance‚Äîwhether through high-level strategy exploration or low-level self-play feedback‚Äîbut also underlines the value of integrating these processes for accelerated and robust skill acquisition in LLMs.\n\nFurthermore, our results suggest broader implications for the development of LLM-driven decision-making agents across a range of complex domains. By embedding flexible and adaptive learning mechanisms, our framework paves the way for more autonomous systems capable of mastering tasks with minimal human intervention. This opens new avenues for the application of LLMs in areas such as multi-agent systems, reinforcement learning, and general AI, where strategic planning and self-improvement are key.\n\n## References\n\n- Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Sch√∂nherr, and Mario Fritz. Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games. arXiv preprint arXiv:2309.17234, 2023.\n- Suma Bailis, Jane Friedhoff, and Feiyang Chen. Werewolf arena: A case study in llm evaluation via social deduction. arXiv preprint arXiv:2407.13943, 2024.\n- Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R√©, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram√®r, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. arXiv preprint arXiv: Arxiv-2108.07258, 2021.\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n- Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang, Yong Dai, Lei Han, and Nan Du. Self-playing adversarial language game enhances llm reasoning. arXiv preprint arXiv:2404.10642, 2024.\n- Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 36, 2024.\n- Meta Fundamental AI Research Diplomacy Team (FAIR)‚Ä†, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067‚Äì1074, 2022.\n- Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation with self-play and in-context learning from ai feedback. arXiv preprint arXiv:2305.10142, 2023.\n- Jean-Bastien Grill, Florent Altch√©, Yunhao Tang, Thomas Hubert, Michal Valko, Ioannis Antonoglou, and R√©mi Munos. Monte-carlo tree search as regularized policy optimization. In International Conference on Machine Learning, pp. 3769‚Äì3778. PMLR, 2020.\n\n## Published as a conference paper at ICLR 2025\n\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv: Arxiv-2207.05608, 2022.\nSubbarao Kambhampati. Can large language models reason and plan? Annals of the New York Academy of Sciences, 1534(1):15‚Äì18, 2024.\nSubbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy. Llms can‚Äôt plan, but can help planning in llm-modulo frameworks. arXiv preprint arXiv:2402.01817, 2024.\nLevente Kocsis and Csaba Szepesv√°ri. Bandit based monte-carlo planning. In European conference on machine learning, pp. 282‚Äì293. Springer, 2006.\nMarc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte carlo sampling for regret minimization in extensive games. Advances in neural information processing systems, 22, 2009.\nJonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. From text to tactic: Evaluating llms playing the game of avalon. arXiv preprint arXiv:2310.05036, 2023.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023.\nYecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931, 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024.\nMatej Moravƒç√≠k, Martin Schmid, Neil Burch, Viliam Lis·ª≥, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508‚Äì513, 2017.\nKolby Nottingham, Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Sameer Singh, Peter Clark, and Roy Fox. Skill set optimization: Reinforcing language model behavior via transferable skills. arXiv preprint arXiv:2402.03244, 2024.\nJulien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T Connor, Neil Burch, Thomas Anthony, et al. Mastering the game of stratego with model-free multiagent reinforcement learning. Science, 378(6623):990‚Äì996, 2022.\nSheldon M Ross. Goofspiel‚Äîthe game of pure strategy. Journal of Applied Probability, 8(3): 621‚Äì625, 1971.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604‚Äì609, 2020.\nJack Serrino, Max Kleiman-Weiner, David C Parkes, and Josh Tenenbaum. Finding friend and foe in multi-agent games. Advances in Neural Information Processing Systems, 32, 2019.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.\n\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C√¥t√©, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020.\n- Silver et al. (2017) David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354‚Äì359, 2017.\n- Valmeekam et al. (2018) Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n- Thorne et al. (2023) James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018.\n- Valmeekam et al. (2023) Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36:75993‚Äì76005, 2023.\n- Wang et al. (2023) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.\n- Wang et al. (2024) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024.\n- Wang et al. (2022) Ruoyao Wang, Peter Jansen, Marc-Alexandre C√¥t√©, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? arXiv preprint arXiv:2203.07540, 2022.\n- Wang et al. (2023) Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon‚Äôs game of thoughts: Battle against deception through recursive contemplation. arXiv preprint arXiv:2310.01320, 2023b.\n- Wei et al. (2023) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. arXiv preprint arXiv: Arxiv-2206.07682, 2022a.\n- Wei et al. (2022a) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.\n- Contributors. Goofspiel ‚Äî Wikipedia, the free encyclopedia, 2023. URL https://en.wikipedia.org/w/index.php?title=Goofspiel&oldid=1174471596. [Online; accessed 22-May-2024].\n- Wu et al. (2024) Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, and Haobo Fu. Enhance reasoning for large language models in the game werewolf. arXiv preprint arXiv:2402.02330, 2024.\n- Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.\n- Xu et al. (2023) Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023a.\n- Xu et al. (2023) Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with reinforcement learning for strategic play in the werewolf game. arXiv preprint arXiv:2310.18940, 2023b.\n- Yang et al. (2023) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.\n-\n\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744‚Äì20757, 2022a.\n- Shunyu Yao et al. (2022a) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022b.\n- Shunyu Yao et al. (2022b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2023.\n- Shunyu Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.\n- Jifan Yu et al. (2023a) Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, et al. Kola: Carefully benchmarking world knowledge of large language models. arXiv preprint arXiv:2306.09296, 2023a.\n- Wenhao Yu et al. (2023b) Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. Language to rewards for robotic skill synthesis. arXiv preprint arXiv:2306.08647, 2023b.\n- Wenqi Zhang et al. (2024) Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. Agent-pro: Learning to evolve via policy-level reflection and optimization. arXiv preprint arXiv:2402.17574, 2024.\n- Andrew Zhao et al. (2024) Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 19632‚Äì19642, 2024.\n- Shuyan Zhou et al. (2023) Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023.\n- Martin Zinkevich et al. (2023) Martin Zinkevich, Michael Johnson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. Advances in neural information processing systems, 20, 2007.\n\n## Published as a conference paper at ICLR 2025\n\n## # A RESISTANCE: AVALON GAME DESCRIPTION\n\n![img-5.jpeg](img-5.jpeg)\nFigure 8: The three phases per round of Resistance game. Good players are shown in blue, while Evil players in red. In Selection Phase, the team leader (player 5 in this round) proposes a team (player 1 and 5, himself). In Voting Phase, all players vote publicly whether to approve this team or not. If the strict majority votes yes, the team is approved and moves on to the mission phase. Otherwise, redo the Selection Phase with the next player as leader. If the team goes on the Mission Phase, selected team members (player 1 and 5) anonymously vote to pass or fail the mission. If at least one person (player 1, as he is the evil player) votes fail, the mission fails. Otherwise, it succeeds.\n\nWe describe the game in more detail here. There are four phases in the game where players need to make decisions: (1) team selection phase, (2) voting phase, (3) quest phase, and (4) assassination phase. The game alternates between the first three phases until the end condition is reached, at which point we move on to the assassination phase. Each phase also contains discussion where players can challenge others, defend themselves, and negotiate. A flowchart of the game is presented in Figure 10, and an Avalon Rule Prompt is included in Section A.4.\n\n## # A.1 ROLES\n\nThere are four basic roles in Resistance Avalon: Servant of Arthur, Minion of Mordred, Merlin, and Assassin. The Servant is a basic good character who does not know the identity of any of the other players. The Minion is a base evil character who knows who is good and evil but does not know the specific roles of each player. Merlin is a unique good character who knows who is good and evil. The Assassin is a unique evil character who knows who is good and evil, and in addition, has the ability to assassinate a character at the end of the game. If that character is Merlin, the evil team wins.\n\nGood players will always outnumber evil players. Hence, evil players must pretend to be good in order to be voted in on teams (and thus sabotage missions). SERVANTS will thus need to sniff out the evil players through their actions and dialogue. MERLIN is usually the only good player with additional information, so they will need to discreetly guide the SERVANTS in the right direction. Servants also need to protect MERLIN, so a common strategy is for SERVANTS to pretend to have hidden information so that evil players will think that they are MERLIN. Evil players will be trying to sniff out MERLIN at the same time, so deduction skills are required for all roles.\n\n## # A.2 ACTIONS FOR EACH PHASE\n\nDepending on the phase team selection, voting, quest, and assassination, players may conduct different actions. We detail the specific actions that players can take in each of these phases below.\n\nDuring the team selection phase, only the current leader has to make a choice. Leadership passes around the players sequentially in a loop. The action space of team selection for the leader consists of all subsets of the players with size equal to the mission team size. The mission team size is different\n\n## Published as a conference paper at ICLR 2025\n\n![img-6.jpeg](img-6.jpeg)\nFigure 9: Communication Skills required to play Avalon. 1) First, they use logical reasoning to analyze the voting pattern and dialogue of other players and deduce their motives. 2) they must coordinate, communicate, and persuade their teammates to follow a particular strategy. 3) they must also hide their identity and motives through deception.\n\n![img-7.jpeg](img-7.jpeg)\nFigure 10: Flowchart illustrating the various game states and transition diagram. Round boxes indicate game states (phases) where the player (role highlighted in bracket) has to make decisions\n\nfor each mission and is determined by the total number of players in the game. For example, in a 5-player game, on mission No.4, the mission team size is 3, so any subset of  $\\{1,2,3,4,5\\}$  with size 3 would be a valid action. After the team proposal is determined by the leader, we move on to the voting phase with the selected players.\n\nDuring the voting phase, every player in the game needs to simultaneously vote either APPROVE (1) or REJECT (0). Votes are publicly revealed to all players, so players can see what other players voted. If a strict majority votes APPROVE (1), we then move on to the quest phase with the team that was approved. Otherwise, we move back to the selection phase. Note that if four teams have been rejected in a row, and this is the fifth time a team is proposed (for the same mission), we skip the voting and move directly to the quest phase. This prevents the game from dragging on forever.\n\nDuring the quest phase, each selected player on the approved team votes anonymously to either PASS (1) or FAIL (0) the mission. The number of votes of PASS vs FAIL are then revealed to everybody. If the number of FAILs is greater than or equal to the number of FAILs required for the mission to fail (usually 1), then this mission is marked as a failure. Otherwise, this mission is marked as a success. Hence, good players usually have no incentive to fail missions, while evil players will want to have enough failures to pass the failure threshold. If three out of five missions fail, evil wins immediately. Otherwise, if three out of five missions succeed, we move on to the assassination phase.\n\n## # A.3 DISCUSSION\n\nGroup discussion occurs between the quest and selection phases, as well as right before the assassination phase. Players may not communicate during any other time. All conversations are public, and there is no private communication. Typically players may discuss in any format of their choosing as long as only one person is speaking at a time. Some examples of formats include a\n\nnatural (spontaneous) seminar style (most common, where there is no fixed order of speaking), or sequentially (where players speak in some predefined order). Interruptions and arguments between two players are very common between human players.\n\nUsually, players will spend this time discussing a couple of key topics, including (1) the observations they made, (2) the guessed identities and sides of players, and (3) the plan for the next mission. The team leader will usually spend this time asking for advice on what team to select and gathering support for that team. Persuasion and adhering to the preferences of other players are usually key to getting a team approved. Players can also accuse other players of being evil, though arguments will need to be justified in order to be persuasive.\n\nFor example, a player (player 3) could start off by stating their (1) observations of what happened in the previous mission. One Fail was observed, so at least one player on the previous team (consisting of players (1,2,3)) is evil. Player 3 then emphasizes that both Players 1 and 2 voted Approve for the previous mission, which ended up a failure. Moreover, the team was proposed by Player 1 in the first place. Player 3 then moves on to discuss the (2) identities of other players. The player says that, despite the fact that only one Fail was observed, both Players 1 and 2 are evil since they both voted to Approve previously. Player 0 is probably good since they voted to Reject in the previous mission, and Player 3 is also good since they also voted to Reject, even though they were on the mission. Player 3 then says what they think the (3) plan should be. Specifically, Player 3 says that they should reject the current team no matter what since Player 2 is the leader and is evil. The leadership will then pass to Player 3, who will choose the team $(0,3,4)$, which good players should vote to approve since it does not contain any suspected evil players.\n\n### A.4 Game Ending and Assassination\n\nIn classic Resistance, a good team wins immediately if three missions are successful. In Resistance Avalon, there is an additional assassination phase if three missions are successful. During the assassination phase, the Assassin player chooses one player to assassinate. If that player is Merlin, then evil wins. Otherwise good wins.\n\nBefore they assassinate a player, the Assassin player can and is encouraged to discuss with the other players (mostly their teammates). good players are also welcome to join in on this discussion to mislead the evil players, though it rarely helps. Players can discuss in a format of their choosing, though there is usually a time limit on how long players can discuss before reaching a decision.\n\n## Published as a conference paper at ICLR 2025\n\n# Avalon rules prompt\n\nThe game you are interested in is called The Resistance: Avalon. The Resistance: Avalon is the game of hidden identities and social deduction. There are two teams in the game: Good and Evil. Each player has a hidden identity (role) and side.\n\nThere are five Quests in the game and five turns, one for each quest. Good players aim to help three Quests succeed, while Evil players aim to fail three Quests. Different quests require different numbers of players to participate.\n\nAt the beginning of the game, each player is assigned a role secretly and randomly. Private information is then revealed to each player. A random player is selected as the leader for the first round.\n\nEach round, after a round of discussion, the leader will select a team of players to participate in the Quest. Then, all players will vote on whether to approve or reject the team publicly. If the team is approved (a strict majority vote to approve), the Quest will be carried out. If the team is not approved, the next player becomes the leader and the next round will start. If four teams are rejected in a row, the fifth team will automatically be approved.\n\nIf the team is approved, each team member chooses to pass or fail the Quest anonymously. Usually, if there is at least one failed vote, the Quest fails. Otherwise, the Quest succeeds. In either case, we move on to the next turn and the next quest.\n\n## Below are the roles in the game:\n\nServant of Arthur (Servant): A Good player who does not know who is on the Evil side. The Servant's job is to make sure that the three Quests succeed.\n\nMinion of Mordred (Minion): An Evil player who knows who is on the Evil side. Minion's job is to fail three Quests without being identified by the Good players.\n\nMerlin: A Good player who knows who is on the Evil side. Merlin's job is to make sure that the three Quests succeed without revealing themself to Evil.\n\nAssassin: An Evil player who knows who is on the Evil side. Assassin's job is to assassinate Merlin if the Evil players can identify who Merlin is. If the Assassin successfully assassinates Merlin, the Evil players win the game immediately, even if three quests succeed.\n\nHence, Evil players usually know who is on the Evil side, but Good players usually do not know who is on the Evil side.\n\nPlayers may make any claims during the game, at any point in the game. Discussion, deception, accusation, persuasion, and logical deduction are all equally important in order for Good to prevail or Evil to rule the day. Hence, players should rarely reveal their true identity to other players. Players will, can, and should lie to achieve their goals.\n\n## B Game of Pure Strategy (GOPS) Game Description\n\nGame of Pure Strategy (GOPS) is a card game for two or more players with a standard deck of card, which is commonly used as an example of multi-stage move game in artificial intelligence (*Wikipedia* contributors (2023)). In our experiments we play 5 or 6 card GOPS. Specifically, the score cards are $\\{1,2,...n\\}$ and each player starts with a hand of cards $\\{1,2,...n\\}$ where $n$ is the number of cards and rounds. The GOPS rules prompt is included in this section below.\n\n[GOPS rules prompt] The game you want to write a function for is GOPS (game of pure strategy), also known as Goofspiel. The game has two players, and is played with a deck of score cards. Each player is dealt the same hand of cards at the beginning. The goal of the game is to get a score higher than your opponent. At the beginning of each round, a score card is randomly drawn without replacement from the score deck. Then each player plays a card simultaneously from their hand. The player who plays the higher card wins the round and gets the score card. They add the score of the score card to their total score. If the two cards played are the same, the person who wins the next round will get both score cards. The game continues until all score cards have been played. The player with the highest total score wins the game.\n\n## Appendix C Limitations\n\nWhile our method performs better on average, individual runs can have high variance. Since the performance of an agent in multi-agent adversarial game settings is highly dependent on opponents‚Äô policies, feedback from these environments tend to be highly noisy, with noise increasing with the number of players. This is especially true when learning Avalon heuristics, where the performance depends on the policies of 5 other players, teammates and opponents. We believe that running more game simulations with different opponent policies can help reduce this feedback noise. We also acknowledge the inherent noisiness in LLM generations and how that can impact our results. We tried to reduce this noise by (1) using the same seed functions when benchmarking the different LLM improvement methods and (2) collecting generated strategies from multiple runs. We also did not test our method on other non-adversarial environments such as question answering and text-based worlds. However, given the strong performance of our method in adversarial multi-agent settings, we believe that similar performance will be observed in single agent, non-adversarial settings.\n\n## Published as a conference paper at ICLR 2025\n\n![img-8.jpeg](img-8.jpeg)\n## Strategist\nFigure 11: Overview of our high-level strategy improvement method: The process alternates between two steps‚Äîidea generation and strategy improvement.\n\n## # D IMPROVEMENT PROCESS IMPLEMENTATION DETAILS\n\nOur method alternates between two main steps: idea generation and strategy implementation, as illustrated in Figure 11. These steps are structured around the optimization of strategies stored in a hierarchical tree structure  $T$ . Below, we detail the key components of our implementation.\n\n## # D.1 STRATEGY TREE AND IDEA QUEUE\n\nThe strategy tree  $T$  maintains all discovered strategies, their associated feedback  $(\\tau_{s})$ , and priority scores  $(z_{s})$ , which determine their likelihood of being selected for further refinement. Complementing this is the idea queue  $Q$ , a priority-based queue that stores candidate ideas generated during the improvement process. Each idea  $d \\in Q$  is initialized with a prior score  $z_{d} = 0.0$  and a usage count  $n_{d} = 0$ .\n\n## Published as a conference paper at ICLR 2025\n\n## Algorithm 1: STRATEGIST Pseudocode\nData:  $T$  : strategy tree storing strategy  $s$  , feedback  $(\\tau_{s})$  , and priority score  $(z_{s})$ $Q$  : idea queue, 'seed functions',  $N_{ideas}$  : number of ideas,  $N_{strategies}$  : number of strategies,  $N_{evolutions}$  : number of evolutions,  $N_{feedback\\_examples}$  : number of states of give as feedback,\nFunction select_strategy  $(T)$  ..\n$\\sigma_{\\mathrm{best}}\\gets \\arg \\mathrm{softmax}z_{\\sigma} / /$  one possible implementation where you take one  $\\sigma \\in T_2$  of the best two strategies in the whole tree randomly (BFS2) return  $n_{best}$\nFunction select_idea  $(Q,\\sigma)$  ..\n$d_{\\mathrm{best}}\\gets \\mathrm{softargmax}_{d\\in Q}UCB(z_d,n_d) / /$  one possible implementation where you take the best strategy in the queue using softmax UCB,  $z_{d}$  being the empirical  $q$  -value and  $n_d$  being the number of tries return  $\\sigma_{best}$\nFunction select_key_states  $(\\tau_{\\sigma})$  ..\n$K_{\\sigma}\\gets \\arg \\max_{k}(\\mathrm{SearchEstimate}(s) - v_{\\sigma}(s))^{2} / /$  one possible way to select key states for  $\\sigma$  that is a value heuristic  $v_{\\sigma}$  return  $K_{\\sigma}$\nFunction generate_ideas  $(N_{ideas})$  ..\n$\\sigma \\gets$  select_strategy  $(T)$  .\n$K_{\\sigma}\\gets$  select_key_states  $(\\tau_{\\sigma}) / / K_{\\sigma}$  is a set of key states from the trajectory feedback  $\\tau_{\\sigma}$  for strategy  $\\sigma$ $D_{\\mathrm{new~ideas}}\\gets$  LLM(Generate  $N_{ideas}$  new ideas based on string description of  $K_{\\sigma}$  , which includes the output of the strategy, action taken, state description, final outcome of the trajectory, search estimate of the state, and any intermediate values used to compute the output of the strategy); for  $d\\in D_{\\mathrm{new~ideas}}$  do Store  $d$  in  $Q$  with prior score  $z_{d} = 0.0$  and  $n_d = 0$  end\nFunction implement_strategies  $(N_{strategies})$  ..\n$\\Sigma_{\\mathrm{new}},D,P = []$  ,{}// list of new generated strategies, dictionary mapping new generated strategy to the idea that generated it, and dictionary mapping generated strategies to their parents for  $i\\gets 1$  to  $N_{strategies}$  do\n$\\sigma \\gets$  select_strategy  $(T)$  .\n$d\\gets$  select_idea  $(Q,\\sigma)$  .\n$\\sigma_{\\mathrm{new}}\\gets$  LLM(Improve  $\\sigma$  using  $d$  .\n$\\Sigma_{\\mathrm{new}}$  .append  $(\\sigma_{\\mathrm{new}})$  .\n$D[\\sigma_{\\mathrm{new}}] = d$  .\n$P[\\sigma_{\\mathrm{new}}] = \\sigma$  .\nend\n$W,T\\gets$  SelfplaySimulate(  $\\Sigma_{\\mathrm{new}}\\cup$  unique(P.values))// simulate games, getting average winrates  $W[\\sigma ]$  for each strategy  $\\sigma$  and simulated trajectory feedback  $\\mathcal{T}[\\sigma ]$\nfor  $\\sigma \\in \\Sigma_{\\mathrm{new}}$  do\n$T.\\mathrm{add}(\\sigma ,P[\\sigma ],D[\\sigma ]) / /$  add new strategy to tree from parent based on idea\n$z_{\\sigma}\\gets W[\\sigma ] / /$  add function score\n$z_{D[\\sigma ]}\\leftarrow \\frac{n_{D[\\sigma]}}{n_{D[\\sigma] + 1}} z_{D[\\sigma ]} + \\frac{1}{n_{D[\\sigma] + 1}} (W[\\sigma ] - W[P[\\sigma ]]) / /$  update idea score with how much it improved the strategy by\nend\nrepeat\ngenerate_ideas  $(N_{ideas})$  .\nimplement_strategies  $(N_{strategies})$  .\nuntil  $N_{evolutions}$  .\nreturn Best strategies in  $T$  according to their scores  $z_{s}$\n\n## # D.2 KEY FUNCTIONS\n\nSelecting Strategies (SelectStrategy) To identify promising strategies for refinement, we use a softmax over the priority scores  $z_{s}$  stored in  $T$ . One implementation chooses between the two highest-priority strategies in a breadth-first search order.\n\n#### Selecting Ideas (SelectIdea)\n\nIdeas are selected from $Q$ using an Upper Confidence Bound (UCB) approach. Each idea‚Äôs selection balances its empirical improvement potential ($z_{d}$) with its exploration term $n_{d}$, ensuring both promising and underexplored ideas are considered.\n\n#### Selecting Key States (SelectKeyStates)\n\nKey states $K_{\\sigma}$ for a given strategy $\\sigma$ are selected based on a heuristic that emphasizes discrepancies between a search-based estimate and the strategy‚Äôs value function $v_{\\sigma}$. These states highlight areas where the strategy can be improved.\n\n### D.3 Idea Generation\n\n## During the idea generation step, the algorithm:\n\n## 1. Selects a strategy $\\sigma$ from $T$ using SelectStrategy.\n2. Extracts key states $K_{\\sigma}$ from the trajectory feedback $\\tau_{\\sigma}$ using SelectKeyStates.\n3. Uses a language model (LLM) to propose $N_{\\text{ideas}}$ new ideas based on the string description of $K_{\\sigma}$. This description includes the strategy‚Äôs actions, state details, trajectory outcomes, search estimates, and intermediate values.\n## 4. Adds the generated ideas to $Q$ with default priority and usage counts.\n\n### D.4 Strategy Implementation\n\n## In the strategy implementation step, the algorithm:\n\n1. Selects a strategy $\\sigma$ from $T$ and an idea $d$ from $Q$ using SelectStrategy and SelectIdea.\n## 2. Refines $\\sigma$ using $d$, producing a new strategy $\\sigma_{\\text{new}}$.\n3. Simulates self-play games for $\\sigma_{\\text{new}}$ and related strategies to evaluate performance (win rates $W[\\sigma]$) and collect trajectory feedback ($\\mathcal{T}[\\sigma]$).\n4. Updates the strategy tree $T$ with $\\sigma_{\\text{new}}$, linking it to its parent strategy and the idea that inspired it.\n5. Updates the priority scores $z_{\\sigma}$ for strategies and $z_{d}$ for ideas based on the observed improvements.\n\n### D.5 Iterative Evolution\n\nThe improvement process repeats for $N_{\\text{evolutions}}$ iterations. In each iteration, new ideas and strategies are generated and evaluated, incrementally building a tree of optimized strategies.\n\n### D.6 Scalability and Efficiency\n\nOur method leverages efficient UCB-based selection and hierarchical feedback propagation to scale to large $T$ and $Q$. By iteratively refining strategies through targeted improvements, it ensures continuous enhancement while maintaining computational feasibility.\n\n## Published as a conference paper at ICLR 2025\n\n![img-9.jpeg](img-9.jpeg)\nFigure 12: Left: Example performance of value heuristics strategy tree for GOPS. Points indicate the final evaluation scores and the generation of improved functions, with lines showing the evolutionary path between functions. Our method successfully escapes local maxima and continues exploring the strategy space. These scores reflect final gameplay results against a fixed set of opponents, differing from the intermediate self-play scores used to guide strategy improvements. Right: Comparison of different improvement methods on 6-card GOPS, where 9 functions generated by each method play against one another over 1024 total games.\n\n![img-10.jpeg](img-10.jpeg)\n\n## # E AVALON AGENT IMPLEMENTATION DETAILS\n\nWe describe in detail how we implement our model below and as shown in figure 13. Unless otherwise specified, the word 'action' will refer to non-dialogue actions. Note that we do not conduct search over raw dialogue space since that is not very computationally feasible. Instead, we search over intended actions and condition our dialogue on that.\n\nSpecifically, the language component consists of a dialogue analyzer and a dialogue generator, while the moves component consist of the action planner. Whenever the agent needs to speak, they first analyze what was said so far in the current discussion round using the dialogue analyzer. The dialogue analyzer, with the help of an LLM, updates the internal beliefs of the agent. For example, in Avalon, internal beliefs might include the probability that the agent assigns to each other player of being Evil and of being Merlin. These beliefs are then passed to the action planner, which uses them to figure out the best next move, i.e. the action intent. The action intent is then passed to the dialogue generator, which generates dialogue with the help of an LLM. When the agent needs to take a move, we run through the same process except that the agent takes the action intent as the move and no dialogue is generated.\n\n## # E.1 DIALOGUE ANALYZER (DISCRIMINATOR)\n\nThe dialogue analyzer  $f_{ana}$  takes as input  $I$  information set (partial information) of the current state for the player,  $d_t$  the discussion so far this round, and  $b$  some prior beliefs about the hidden state of the game, and returns  $\\widehat{b}$ , the updated beliefs, and  $\\widehat{\\Pi}_t$ , the predicted joint action policy of the all the players (i.e. the action intent) for the next action step  $t$ . Recall that simultaneous games can be expanded as partial information games, where the simultaneous moves are treated as hidden information. Hence, we are essentially predicting a distribution over the hidden states  $s$  given the information set  $I$  using the dialogue analyzer.\n\n$$\n\\widehat {\\boldsymbol {b}}, \\widehat {\\boldsymbol {\\Pi}} _ {t} = f _ {a n a} (\\boldsymbol {I}, \\boldsymbol {d} _ {t}, \\boldsymbol {b})\n$$\n\nIn the context of Avalon,  $\\pmb{I}$  will contain information such as (1) the dialogue this round so far (2) summary of the dialogue from previous rounds (3) mission track record (4) historical record of actions taken by players in previous rounds, and (5) private information of the player such as who is Good and Evil.  $\\pmb{b}$  will contain information on (1) the probability of each player being Evil and (2)\n\n## Published as a conference paper at ICLR 2025\n\n![img-11.jpeg](img-11.jpeg)\nFigure 13: Overview of the LLM-powered agent, including the three main modules that we use to generate dialogue during discussion\n\nthe probability of each player being Merlin, both conditioned on the private information contained in  $\\pmb{I}$ . While a full treatment of the distribution over the hidden state space  $S$  we require assigning probabilities to each possible combination of Good and Evil players, not just assessing the marginal probability of each player being Good individually, in practice\n\nWe implement  $f_{ana}$  using an LLM, which is fed  $I$ ,  $d$ ,  $b$  (converted to natural language form) as prompts, along with some instruction prompt  $\\phi_{ana}$  that prompts it to produce  $\\widehat{b}, \\widehat{\\Pi}_t$ . Specifically,\n\n$$\nf _ {a n a} (\\boldsymbol {I}, \\boldsymbol {d} _ {t}, \\boldsymbol {b}) = f _ {L L M} (\\phi_ {d i s}, \\boldsymbol {I}, \\boldsymbol {d}, \\boldsymbol {b})\n$$\n\nWe show examples of such prompts in Appendix J.\n\n## # E.2 ACTION PLANNER\n\nGiven  $\\widehat{\\pmb{b}}$  the belief prior,  $\\widehat{\\Pi}_t$  the predicted joint action policy for all players, and  $s$  the representation of the current state, the action generation model  $f_{act}$  generates a probability distribution over possible actions  $\\pi^i$  for the main player  $i$  that is the best response to  $\\widehat{\\Pi}_t$ . We do so by using search techniques to look ahead and find the best response.\n\n$$\n\\boldsymbol {\\pi} ^ {i} = f _ {a c t} (\\widehat {\\boldsymbol {b}}, \\widehat {\\boldsymbol {\\Pi}} _ {t}, \\boldsymbol {I})\n$$\n\nMore specifically, in our search implementation, at the first layer, we first sample across possible hidden states  $s \\sim \\widehat{b}$  according to the belief prior. At the second layer (i.e. the first action stage  $t$ ), we calculate expected  $q$ -values for each action  $a \\in \\mathcal{A}$  that the main player can take if the other players play actions  $a \\sim \\widehat{\\Pi}_t$  according to the predicted joint distribution. In subsequent action stages, the search process will assume that other players play according to their policy simulated and induced by the value heuristic that is not dialogue dependent. We then take the best response action  $a_i^* = \\max(\\pi^i)$  as the intended action. Since this is a partial information game, expected  $q$ -values are taken across information sets, not states. We describe how our action planner is implemented in more detail in Appendix F.\n\nE.3 Dialogue Generation\n\nThe dialogue generator $f_{gen}$ takes as input I some representation of the current information set and $a_{i}^{*}$, the intended best response action, and outputs dialogue $d$.\n\n$d=f_{gen}(\\textbf{{I}},a_{i}^{*})$\n\nWe will implement $f_{gen}$ using an LLM, which is fed I and $a_{i}^{*}$ directly as prompts, along with some instruction prompt $\\phi_{gen}$ that prompts it to produce realistic sounding dialogue that helps it achieve its intended action.\n\nFor example, perhaps the player wants to approve the next team. Then it should try to generated dialogue that convinces the other players to also approve.\n\nWe show examples of such prompts in Appendix J.\n\n## Published as a conference paper at ICLR 2025\n\n## # F VALUE HEURISTIC IMPLEMENTATION DETAILS\n\n![img-12.jpeg](img-12.jpeg)\n## Action planner\nFigure 14: Overview of how we utilize the trained value heuristics in MCTS tree search to get a non-dialogue based policy. While we only display the values for a single player in the diagram, note that in practice we infer and update the values for all players at the same time. Next states are sampled using the PUCT formula we described. We sample an initial hidden state based on the internal beliefs of the agent. Metropolis-Hastings is used to sample since it may be difficult to calculate the probability density specified by the internal beliefs. Note that values estimated using MCTS are also passed as feedback to the evaluator.\n\nThe MCTS search process is depicted in Figure 14, where we simulate a trajectory from the hidden state we are at until we reach some unexpanded state  $s$ . The probability of transitioning to a state during simulations is computed assuming that each player samples from their optimal actions according to their PUCT (polynomial upper confidence trees) values (and  $\\phi_e$  for the environment actor) (Schrittwieser et al., 2020). Since in some environments players may only be able to observe information sets, when computing the PUCT values we average over all expanded states in that information set. Moreover, the initial hidden state can be sampled according to a prior (or empirical prior) over the states in the information set that the player observed. Then, using our value heuristic, we compute the values of each of the next hidden states. We then backpropagate our new values back up the simulated trajectory, updating the intermediate states. After running a few MCTS simulations (roll-outs) like the one we described, the planner then outputs the action which leads to the highest value next state. We show our information set PUCT formula below, where  $N(s,a)$  is the number of times we took action  $a$  at state  $s$  during MCTS rollouts,  $P(s,a)$  is the prior prior probability of selecting action  $a$  from state  $s$ ,  $C$  is the exploration constant,  $Q_{emp}$  is the empirical average of MCTS roll-out outcomes,  $\\widehat{Q}(s,a)$  is the prior computed by our value heuristic,  $\\alpha$  controls how much weight be put on the prior (often  $\\alpha = 1$ ), and  $\\pi_B$  is the distribution across hidden states in the information set given our beliefs  $B$ , some parametrization of  $\\pi_B$ . Since  $\\pi_B$  is often hard to compute, we can simply set  $\\pi_B(s|I) = \\frac{\\sum_b N(s,b)}{\\sum_c' \\leq I \\sum_b N(s',b)}$  to be the empirical roll-out distribution, given that we sample initial states  $s_0 \\sim \\pi_B(s_0|I)$  according to our beliefs. For example, in Avalon, we can sample the hidden roles according to our beliefs  $B$  using Metropolis-Hastings for the initial state  $s_0$ .\n\n$$\nQ (s, a) = \\frac {N (s , a) \\cdot Q _ {\\mathrm {e m p}} (s , a) + \\alpha \\cdot \\widehat {Q} (s , a)}{N (s , a) + \\alpha} \\tag {1}\n$$\n\n$$\n\\operatorname {P U C T} (I, a) = \\sum_ {s \\in I} \\pi_ {B} (s | I) \\left[ Q (s, a) + C \\cdot P (s, a) \\cdot \\frac {\\sqrt {\\sum_ {b} N (s , b)}}{1 + N (s , a)} \\right] \\tag {2}\n$$\n\n## Published as a conference paper at ICLR 2025\n\n## # G DIALOGUE GUIDE IMPROVEMENT EVALUATION IMPLEMENTATION DETAILS\n\nWe provide more details on our dialogue improvement evaluation process here and as shown in figure 15. The improvement method (skill coach) remains the same as we described before.\n\nWe first generate a synthetic dataset by simulating a game of Avalon with initial dialogue and move policies  $\\phi$ . Given the dialogue guide  $\\sigma$  we want to evaluate, we then sample 'scenarios' from the dataset. A scenario consists of a game state, intended action, and private information in the simulated trajectory. We create an Avalon agent like the one we described in E for each player in the game, initialized with their corresponding private information. The Avalon agent is then asked to generate dialogue using the dialogue guide  $\\sigma$ .\n\nUsing this new generated dialogue, we then simulate the next round of dialogue analysis for each Avalon agent. This produces analysis scores based on how likely they think the player is to be Merlin  $z_{merlin}$ , and how likely they think the player is to be Evil  $z_{evil}$ , where  $z_{merlin}, z_{evil} \\in [-2, 2]$ . For evaluating Merlin, we get the average  $z_{merlin}$  scores from the Evil players,  $\\overline{z}_{merlin}$ , along with the average  $z_{evil}$  scores from the Good players  $\\overline{z}_{evil}$ . We then take the minimum of these two as the feedback score  $z = \\min\\{\\overline{z}_{evil}, \\overline{z}_{merlin}\\}$ . This is because Merlin wants to both minimize the probability of being detected by the Evil players, and also minimize the probability of being identified as Evil by the Good players.\n\n![img-13.jpeg](img-13.jpeg)\nFigure 15: Overview of our improvement process for learning dialogue generation strategies. This includes how we evaluate the dialogue and how we collect feedback. The skill coach here can be implemented as either our improvement method, STRATEGIST, or any of the baseline methods we described.\n\nThe dialogue analyzer (discriminator) is described in more detail in Appendix E and the specific generation and analysis prompts are shown in Appendix J.\n\n## H Value Heuristic LLM Prompt and Output Examples\n\n### H.1 System Prompts\n\nSystem prompt are guidelines for LLM to generate outputs align with the intended goals. In our case, the goal is to generate a function that evaluates the value of a state in a game under low cost.\n\n[label=Value heuristic system prompt] You are a function engineer trying to write a function that can evaluate the value of a state in a game. This is known as a value heuristic, and will be used in look-ahead search algorithms to evaluate the value of unexplored states. Your goal is to develop a heuristic that is as accurate as possible without being too expensive to compute. Hence, you are not allowed to runs simulations in the function.\n\nThe following example is a detailed prompt telling the LLM how to format the value heuristics specifically in the GOPS game. The format of input and output are clearly defined in the prompt with illustrations, examples and structures.\n\n## Published as a conference paper at ICLR 2025\n\n# GOPS value heuristics function signature\n\nThe function (written in python) should be named 'evaluate state' and take in a tuple called 'state' of the game state as input. Specifically, the input tuple will be of length 9, and it should return 2 elements. The first element should be a tuple with 2 floats: the first element being the score you expect player 0 will get at the end of the game, and the second element being the score you expect player 1 will get at the end of the game. The second element should be a dictionary of any important intermediate values that you used to calculate the scores. For example, if you think player 0 will win 12 total points by the end of the game and player 1 will win 8 total points, the function should return (12, 8).\n\nMake sure your output only includes the code of the function itself in plain text such that it is executable using exec() in python. Any helper functions should be defined within the scope of the function 'evaluate state'. Include comments in your code so that it is readable, but everything should be implemented.\n\n## The signature of the function should be as follows:\n\n```python\ndef evaluate_state(state) -&gt; tuple[tuple[float, float], dict]:\nscore Cards = state[0] # a python list of the score cards (integers) that have been played, in the order they were played\nplayer_0played Cards = state[1] # a python list of the cards (integers) player 0 has played, in the order they were played.\nplayer_1played Cards = state[2] # a python list of the cards (integers) player 1 has played, in the order they were played.\nis_turn = state[3] # bool, true if it is you and your opponent's turn to play, false if it is time to draw a new score card\nplayer_0_score = state[4] # float or integer, player 0's score so far\nplayer_1_score = state[5] # float or integer, player 1's score so far\nscore_deck = state[6] # a python set of the score cards (integers) left in the deck, either same length as player_0_hand and player_1_hand or one less since the score card appears before the players play. May be empty\nplayer_0_hand = state[7] # a python set of the cards (integers) left in player 0's hand. May be empty\nplayer_1_hand = state[8] # a python set of the cards (integers) left in player 1's hand. May be empty\n# explanation of what we do next\n...\n<intermediate_value1> = value1\n# explanation of what we do next\n...\n<intermediate_value2> = value2\n# explanation of what we do next\n...\nplayer Scores = (player_0_expected_score, player_1_expected_score)\nintermediate_values = (\"<intermediate_value1>\": intermediate_value1, \"<intermediate_value2>\": intermediate_value2, ...)\nreturn player Scores, intermediate_values # make sure the return is exactly in this format\n```\n\nWhere you can use your own names for the intermediate values and the values themselves. Please start with \"def evaluate state(state):\"\n\n## # H.2 IDEA GENERATION EXAMPLES\n\nThe idea generation prompt included system prompt, game rules, previous guide and feedback reflections. Following those four components, we construct the format and an example of ideas to guide the generation of LLM.</intermediate_value2></intermediate_value1></intermediate_value2></intermediate_value1>\n\n## Published as a conference paper at ICLR 2025\n\n# Prompt for idea generation\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n<previous guide=\"\">\n\n<feedback reflections=\"\">\n\nBased on the function, feedback, and conclusions you drew, what are 2 improvements that you can make to the function that you think will have the most impact? Be as specific and concrete as possible, and write them out in the following format:\n\n- Thoughts: <your thoughts=\"\" here=\"\">\n- Idea 1: <your idea=\"\" here=\"\">\n- Idea 2: <your idea=\"\" here=\"\">\n\n## Here's an example of what this might look like for 3 improvement ideas:\n\n- Thoughts: I should consider the number of cards left in the deck when evaluating the value of a state.\n- Idea 1: I should add a term to the value function that penalizes states where there are fewer cards left in the deck.\n- Idea 2: I should add a term to the value function that rewards states where the player has more cards in their hand than the opponent.\n- Idea 3: I should add a term to the value function that rewards states where the player has more cards in their hand than the opponent and there are fewer cards left in the deck.\n\nBelow is an instance of Feedback of GOPS game, showing the setup of two players, the intermediate values involved in the computation, and the actual scores.</your></your></your></your></previous></your>\n\n## Published as a conference paper at ICLR 2025\n\n|  Feedback example  |\n| --- |\n|  Example 9:  |\n|  The state you were trying to estimate a value for is:  |\n|  The current state of the game is as follows:  |\n|  ¬∑The score cards that have been revealed are: (2,4,5,1,3)  |\n|  ¬∑The cards that player 0 has played are: (1,2,4,3,5)  |\n|  ¬∑The cards that player 1 has played are: (3,5,1,2,4)  |\n|  ¬∑Player 0's score so far is: 9  |\n|  ¬∑Player 1's score so far is: 6  |\n|  ¬∑The score cards left in the deck are: set()  |\n|  ¬∑The cards left in player 0's hand are: set()  |\n|  ¬∑The cards left in player 1's hand are: set()  |\n|  The function you generated returned the following values:  |\n|  {0:3,1:-3}  |\n|  for the expected end of game scores of the players.  |\n|  Some intermediate values that you used to calculate the scores were:  |\n|  {'player_0_expected_score': 9, 'player_1_expected_score': 6, 'dynamic_penalty': 0.0, 'player_0_hand_reward': 0, 'player_1_hand_reward': 0, 'player_0_adjustment': 0, 'player_1_adjustment': 0, 'player_0_strategic_adjustment': 0, 'player_1_strategic_adjustment': 0}  |\n|  The estimated end of game scores of the players using lookahead search with your function was:  |\n|  {0:0.0,1:0.0}  |\n|  The actual scores of the players at the end of the game in the simulation were:  |\n|  {0:3.0,1:-3.0}  |\n\nRecall that feedback reflection refers to feedback for strategy, which is used to generate ideas. For prompting, We specify that game rules, previous guide and feedback examples are generated previously. The following is the prompt for feedback reflections.\n\n## Published as a conference paper at ICLR 2025\n\n# Prompt for feedback reflections\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\nPreviously you generated the following function to evaluate the value of a state in the game.\n\n<previous guide=\"\">\n\nBelow is some feedback on how the function you generated performed when we tested it. Note that simulations involve high variance and the actual scores may not match the expected scores exactly. Hence, you should focus on trying to get the scores produced by your function to match those predicted by look-ahead search as closely as possible.\n\n<feedback examples=\"\">\n\nBased on the feedback given and the function you generated previously, what are some conclusions you can draw from the feedback? Make sure to cite the specific examples in the feedback to justify your analysis.\n\nBelow is the feedback reflections from the LLM.\n\n# Feedback reflections\n\nFrom the feedback provided, we can draw several conclusions regarding the performance of the function in evaluating the value of a state in the GOPS game:\n\n1. Discrepancy in Expected Scores: In several examples (e.g., Example 4, Example 7, Example 8), the expected end-of-game scores returned by the function do not match the scores predicted by look-ahead search closely. This indicates that the function may not be accurately estimating the value of the states in the game.\n2. Inconsistencies in Strategic Adjustments: The function includes a strategic adjustment based on the opponent's played cards, but in all cases, the strategic adjustments are zero. This suggests that the strategic adjustments made in the function may not be capturing the true strategic implications of the opponent's moves effectively.\n3. Dynamic Penalty Adjustment: The function includes a dynamic penalty adjustment based on the remaining high-value score cards in the deck. However, the dynamic penalty adjustments may not be accurately reflecting the impact of high-value score cards left in the deck on the game outcome.\n4. Limited Hand Size Reward: The function rewards states where a player has more cards in their hand than the opponent. While this reward is included, it might not be capturing the full strategic implications of having more cards in hand effectively.\n5. Simulation vs. Expected Scores: The actual scores at the end of the game in the simulations do not match the expected scores returned by the function precisely. This indicates that the function's evaluation may not be aligning well with the actual game outcomes.\n\nIn conclusion, the function may need further refinement in terms of capturing strategic adjustments, dynamic penalties, and hand size rewards to improve the accuracy of the state evaluation in the GOPS game. Additionally, considering the discrepancies between the expected and actual scores, further testing and adjustments may be necessary to enhance the function's performance in evaluating the value of states accurately.\n\nAlso, with the prompts of idea generation, below is the generated new ideas by LLM.</feedback></previous></previous></system>\n\n## Published as a conference paper at ICLR 2025\n\n# Generated new ideas\n\nThoughts: The function needs to better capture the strategic implications of the opponent's moves and adjust the expected scores accordingly. Additionally, the dynamic penalty adjustment and hand size rewards could be improved to better reflect their impact on the game outcome.\n\nIdea 1: Enhance the strategic adjustment component of the function by analyzing the opponent's played cards more deeply. This could involve considering patterns in the opponent's moves, potential card combinations, and predicting future moves based on past actions.\n\nIdea 2: Revise the dynamic penalty adjustment to more accurately reflect the impact of high-value score cards left in the deck. This could involve adjusting the penalty dynamically based on the remaining high-value cards and their likelihood of being drawn in future rounds. This adjustment could help in better assessing the risk associated with certain states in the game.\n\n## # H.3 STRATEGY IMPLEMENTATION EXAMPLES\n\nDuring the strategy implementation step, we first select a strategy and an idea from the libraries using the adaptive selection policy. Then, we prompt the LLM to implement the idea on the strategy, generating a new improved strategy.\n\nIn this section, we showcase two examples. Each example illustrates strategies before and after enhancements made by the LLM. This comparison highlights the effectiveness of our strategy implementation. The improved parts by LLM are highlighted in yellow.\n\nBelow is the prompt for strategy implementation.\n\n# Prompt for implementation\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n## Previously you generated the following function to evaluate the value of a state in the game:\n\n<previous guide=\"\">\n\n## Here is a possible way to improve this function:\n\n<improvement ideas=\"\">\n\n## # H.3.1 EXAMPLE 1\n\nBelow is the GOPS Value Heuristics Function before the strategy improvement.</improvement></previous></game></system>\n\n## Published as a conference paper at ICLR 2025\n\n```python\n## GOPS Value Heuristics Function (Before)\ndef evaluate_state(state) -&gt; tuple[tuple[float, float], dict]:\nscore Cards = state[0]\nplayer_0played Cards = state[1]\nplayer_1played Cards = state[2]\nis_turn = state[3]\nplayer_0_score = state[4]\nplayer_1_score = state[5]\nscore_deck = state[6]\nplayer_0_hand = state[7]\nplayer_1_hand = state[8]\n# Calculate the potential scores for each player based on the remaining score cards\nplayer_potential_score = player_0_score\nopponent_potential_score = player_1_score\nif len(score_deck) &gt; 0:\nremaining_score Cards = len(score_deck)\nif len player_0played Cards &gt; len player_1played Cards):\nplayer_potential_score += remaining_score Cards / 2\nelif len player_1played Cards &gt; len player_0played Cards):\nopponent_potential_score += remaining_score Cards / 2\nplayer Advantage = player_potential_score - opponent_potential_score\nreturn (player_potential_score, opponent_potential_score), ('player Advantage': player Advantage)\n```\n\nBelow is the selected idea to improve the above strategy.\n\n# Selected GOPS Value Heuristics Improvement Idea\n\nUpdate the function to calculate the advantage of winning future rounds based on the potential scores each player could achieve with the remaining score cards and their current scores. This would involve considering the number of cards left in the deck, the players' hands, and the current scores to determine the advantage one player may have over the other in the upcoming rounds.\n\nBelow is the GOPS Value Heuristics Function after enhancement by LLM. The improved parts by LLM are highlighted in yellow.\n\n## Published as a conference paper at ICLR 2025\n\n```python\n## GOPS Value Heuristics Function (After)\ndef evaluate_state(state) -&gt; tuple[tuple[float, float], dict]:\nscore Cards = state[0]\nplayer_0played Cards = state[1]\nplayer_1played Cards = state[2]\nis_turn = state[3]\nplayer_0_score = state[4]\nplayer_1_score = state[5]\nscore_deck = state[6]\nplayer_0_hand = state[7]\nplayer_1_hand = state[8]\n# Calculate the potential scores for each player based on the remaining score cards\n# and current scores\nplayer_potential_score = player_0_score + sum(score_deck) / 2\nopponent_potential_score = player_1_score + sum(score_deck) / 2\n# Determine the advantage of winning future rounds based on potential score\nif is_turn:\nplayer_potential Scores = [player_potential_score + card for card in player_0_hand]\nopponent_potential Scores = [opponent_potential_score + card for card in player_1_hand]\nplayer Advantage = sum player_potential Scores / len player_potential Scores\n- sum opponent_potential Scores / len opponent_potential Scores\nelse:\nplayer Advantage = player_potential Score - opponent_potential Score\nreturn (player_potential Score, opponent_potential Score), ('player Advantage': player Advantage)\n```\n\n## # H.3.2 EXAMPLE 2\n\nBelow is the Avalon Value Heuristics Function before improvements.\n\n## Avalon Value Heuristics Function (Before)\n\ndef evaluate_state(state):\nnum_successful_quests = sum(historical_quest_results)\nnum_failed_quests = len(historical_quest_results) - num_successful_quests\nnum_remaining_quests = len(num_participants_per_quest) - len(historical_quest_results)\n\nnum_evil = len(players) - num_good\nnum_evil_in_quest_team = len([player for player in quest_team if not is_good[player]])\n\nsuccess_probability = 0.5\nif phase == 0:\nif num_successful_quests >= 3:\nsuccess_probability = 0.9\nelif num_failed_quests >= 3:\nsuccess_probability = 0.1\nelif phase == 1:\nsuccess_probability = 0.8 if num_evil_in_quest_team == 0 else 0.2\nelif phase == 2:\nsuccess_probability = 0.9 if num_successful_quests > num_failed_quests else 0.1\nelif phase == 3:\nif ‚ÄôMerlin‚Äô in roles and ‚ÄôAssassin‚Äô in roles:\nmerlin_index = roles.index(‚ÄôMerlin‚Äô)\nassassin_index = roles.index(‚ÄôAssassin‚Äô)\nif assassin_index in quest_team:\nsuccess_probability = 0.1\nelse:\nsuccess_probability = 0.9\n\nexpected_winrates_per_player = dict()\nfor player in players:\nif is_good[player]:\nexpected_winrates_per_player[player] = success_probability\nelse:\nexpected_winrates_per_player[player] = 1 - success_probability\n\nintermediate_values = {\n‚Äônum_successful_quests‚Äô: num_successful_quests,\n‚Äônum_failed_quests‚Äô: num_failed_quests,\n‚Äônum_remaining_quests‚Äô: num_remaining_quests,\n‚Äônum_evil_in_quest_team‚Äô: num_evil_in_quest_team\n}\n\nreturn expected_winrates_per_player, intermediate_values\n\nBelow is the improvement idea pf Avalon Value Heuristics Function.\n\nAvalon Value Heuristics Function Improvement Idea Incorporate a more nuanced success probability calculation based on the roles of the players and the current phase of the game. For example, in phase 1 (team selection), consider the balance of Good and Evil players on the quest team and adjust the success probability accordingly. This can help better reflect the strategic considerations during team selection.\n\nThe following is Avalon Value Heuristics Function after enhancements, highlighted in yellow.\n\n## Published as a conference paper at ICLR 2025\n\n## Avalon Value Heuristics Function (After)\n```python\ndef evaluate_state(state: dict) -&gt; tuple[dict[int, float], dict]:\nprob_good_victory = 0.5 # probability that Good will win\nif phase == 0: # team selection phase\nnum_successful_quests = sum(historical_quest_results)\nnum_failed_quests = len(historical_quest_results) - num_successful_quests\nnum_remaining_quests = len(num_participants_per_quest) - len(historical_quest_results)\nnum_evil = len(players) - num_good\nnum_evil_in_quest_team = len([player for player in quest_team if not is_good[player]])\nif num_successful_quests &gt;= 3:\nprob_good_victory = 0.9\nelif num_failed_quests &gt;= 3:\nprob_good_victory = 0.1\nintermediate_value1 = {'num_successful_quests': num_successful_quests, 'num_failed_quests': num_failed_quests, 'num_remaining_quests': num_remaining_quests}\nintermediate_value2 = {'num_evil_in_quest_team': num_evil_in_quest_team}\nelif phase == 1: # team approval phase\nnum_evil_in_quest_team = len([player for player in quest_team if not is_good[player]])\nprob_good_victory = 0.8 if num_evil_in_quest_team == 0 else 0.2\nintermediate_value1 = {'num_evil_in_quest_team': num_evil_in_quest_team}\nintermediate_value2 = {} # Initialize empty dictionary to avoid the error\nelif phase == 2: # quest phase\nnum_successful_quests = sum(historical_quest_results)\nnum_failed_quests = len(historical_quest_results) - num_successful_quests\nprob_good_victory = 0.9 if num_successful_quests &gt; num_failed_quests else 0.1\nintermediate_value1 = {'num_successful_quests': num_successful_quests, 'num_failed_quests': num_failed_quests}\nintermediate_value2 = {} # Initialize empty dictionary to avoid the error\n```\n\n```python\nelif phase == 3: # assassination phase\nmerlin_index = roles.index('Merlin')\nassassin_index = roles.index('Assassin')\nif assassin_index in quest_team:\nprob_good_victory = 0.1\nelse:\nprob_good_victory = 0.9\nintermediate_value1 = {'merlin_index': merlin_index, 'assassin_index': assassin_index}\nintermediate_value2 = {} # Initialize empty dictionary to avoid the error\nexpected_winrates_per_player = dict()\nprob_evil_victory = 1 - prob_good_victory\nfor player in players:\nif is_good[player]:\nexpected_winrates_per_player[player] = prob_good_victory\nelse:\nexpected_winrates_per_player[player] = prob_evil_victory\nintermediate_values = {'intermediate_value1': intermediate_value1, 'intermediate_value2':\nintermediate_value2}\nreturn expected_winrates_per_player, intermediate_values\n\nH.3.3 Example 3\n\nBelow is the GOPS Value Heuristics Function before the strategy improvement.\n\n## GOPS Value Heuristics Function (Before)\n\n‚¨á\ndef evaluate_state(state) -> tuple[tuple[float, float], dict]:\nscore_cards = state[0]\nplayer_0_played_cards = state[1]\nplayer_1_played_cards = state[2]\nis_turn = state[3]\nplayer_0_score = state[4]\nplayer_1_score = state[5]\nscore_deck = state[6]\nplayer_0_hand = state[7]\nplayer_1_hand = state[8]\n\n# Calculate initial potentials\nplayer_0_potential = sum(player_0_hand)\nplayer_1_potential = sum(player_1_hand)\nscore_potential = sum(score_deck)\n\n# Update player potentials based on remaining cards and score deck\nplayer_0_potential += sum(card for card in player_0_hand if any(card > score for score in score_deck ))\nplayer_1_potential += sum(card for card in player_1_hand if any(card > score for score in score_deck\n\n)\n\n# Add half of the score potential to the player who has the turn\nif is_turn:\nplayer_0_potential += score_potential / 2\nelse:\nplayer_1_potential += score_potential / 2\n\n# Count the number of certain wins for each player\nplayer_0_certain_wins = sum(card > max(player_1_hand) for card in player_0_hand)\nplayer_1_certain_wins = sum(card > max(player_0_hand) for card in player_1_hand)\nrounds_left = len(score_deck)\n\n# Dynamic adjustment based on specific cards played\nplayer_0_certain_wins_adjust = 0\nplayer_1_certain_wins_adjust = 0\nfor i in range(len(player_0_played_cards)):\nif player_0_played_cards[i] > player_1_played_cards[i]:\nplayer_0_certain_wins_adjust += 1\nelif player_1_played_cards[i] > player_0_played_cards[i]:\nplayer_1_certain_wins_adjust += 1\n\nplayer_0_certain_wins += player_0_certain_wins_adjust\nplayer_1_certain_wins += player_1_certain_wins_adjust\n\n# Add potential scores from certain wins\nif rounds_left <= player_0_certain_wins:\nhighest_scores = sorted(score_deck)[-rounds_left:]\nplayer_0_potential += sum(highest_scores)\n\nif rounds_left <= player_1_certain_wins:\nhighest_scores = sorted(score_deck)[-rounds_left:]\nplayer_1_potential += sum(highest_scores)\n\n## Published as a conference paper at ICLR 2025\n\n```python\nCalculate expected scores player_0_expected_score  $=$  player_0_score  $^+$  player_0_potential player_1_expected_score  $=$  player_1_score  $^+$  player_1_potential # Calculate the difference in the sum of played cards sum_player_0played Cards  $=$  sum(player_0played Cards) sum_player_1played Cards  $=$  sum(player_1played Cards) sum_player_1played Cards diff  $=$  sum_player_0played Cards - sum_player_1played Cards # Adjust scores based on the difference in the sum of played cards adjustment_factor  $= 0.1$  # Adjust this factor based on the level of adjustment desired player_0_expected_score  $+ =$  sumplayed Cards_diff \\* adjustment_factor player_1_expected_score  $- =$  sumplayed Cards_diff \\* adjustment_factor # Dynamic adjustment based on uncertainty in the game state uncertainty_factor  $= 0.1$  \\* rounds_left # Adjust this factor based on the level of uncertainty desired player_0_expected_score  $+ =$  player_0_potential \\* uncertainty_factor player_1_expected_score  $+ =$  player_1_potential \\* uncertainty_factor # Refine uncertainty factor based on specific conditions if len(player_0_hand)  $= = 1$  uncertainty_factor  $= = 0.5$  if len(player_1_hand)  $= = 1$  uncertainty_factor  $= = 0.5$  # Store intermediate values in a dictionary intermediate_values  $=$  { 'player_0_potential': player_0_potential, 'player_1_potential': player_1_potential, 'score_potential': score_potential, 'player_0_certain_wins': player_0_certain_wins, 'player_1_certain_wins': player_1_certain_wins, 'player_0_certain_wins_adjust': player_0_certain_wins_adjust, 'player_1_certain_wins_adjust': player_1_certain_wins_adjust, 'sumplayed Cards_diff': sumplayed Cards_diff, 'uncertainty_factor': uncertainty_factor } return (player_0_expected_score, player_1_expected_score), intermediate_values\n```\n\nBelow is the selected idea to improve the above strategy.\n\n# Selected GOPS Value Heuristics Improvement Idea\n\nIncorporate a term in the value function that takes into account the distribution of score cards left in the deck. Certain score cards may have a higher value or strategic importance, so adjusting the potentials based on the presence of these cards can provide a more accurate evaluation of the state.\n\nBelow is the GOPS Value Heuristics Function after enhancement by LLM. The improved parts by LLM are highlighted in yellow.\n\n## GOPS Value Heuristics Function (After)\n\ndef evaluate_state(state) -> tuple[tuple[float, float], dict]: score_cards = state[0] player_0_played_cards = state[1] player_1_played_cards = state[2] is_turn = state[3] player_0_score = state[4] player_1_score = state[5] score_deck = state[6] player_0_hand = state[7] player_1_hand = state[8] # Calculate initial potentials player_0_potential = sum(player_0_hand) player_1_potential = sum(player_1_hand) score_potential = sum(score_deck) # Update player potentials based on remaining cards and score deck player_0_potential += sum(card for card in player_0_hand if any(card > score for score in score_deck )) player_1_potential += sum(card for card in player_1_hand if any(card > score for score in score_deck )) # Add half of the score potential to the player who has the turn if is_turn: player_0_potential += score_potential / 2 else: player_1_potential += score_potential / 2 # Count the number of certain wins for each player player_0_certain_wins = sum(card > max(player_1_hand) for card in player_0_hand) player_1_certain_wins = sum(card > max(player_0_hand) for card in player_1_hand) rounds_left = len(score_deck) # Dynamic adjustment based on specific cards played player_0_certain_wins_adjust = 0 player_1_certain_wins_adjust = 0 for i in range(len(player_0_played_cards)): if player_0_played_cards[i] > player_1_played_cards[i]: player_0_certain_wins_adjust += 1 elif player_1_played_cards[i] > player_0_played_cards[i]: player_1_certain_wins_adjust += 1 player_0_certain_wins += player_0_certain_wins_adjust player_1_certain_wins += player_1_certain_wins_adjust # Add potential scores from certain wins if rounds_left <= player_0_certain_wins: highest_score= sorted(score_deck)[-rounds_left:] player_0_potential += sum(highest_score) if rounds_left <= player_1_certain_wins: highest_score= sorted(score_deck)[-rounds_left:] player_1_potential += sum(highest_score) # Incorporate distribution of score cards left in the deck important_score_cards = (7, 10, 13) # Example: define important score cards player_0_potential += sum(card for card in player_0_hand if card in important_score_cards) player_1_potential += sum(card for card in player_1_hand if card in important_score_cards) # Calculate expected scores player_0_expected_score = player_0_score + player_0_potential player_1_expected_score = player_1_score + player_1_potential # Calculate the difference in the sum of played cards sum_player_0_played_cards = sum(player_0_played_cards) sum_player_1_played_cards = sum(player_1_played_cards) sum_played_cards_diff = sum_player_0_played_cards - sum_player_1_played_cards\n\n## Published as a conference paper at ICLR 2025\n\n```python\n# Adjust scores based on the difference in the sum of played cards\nadjustment_factor = 0.1 # Adjust this factor based on the level of adjustment desired\nplayer_0_expected_score += sumplayed Cards_diff * adjustment_factor\nplayer_1_expected_score -= sumplayed Cards_diff * adjustment_factor\n\n# Dynamic adjustment based on uncertainty in the game state\nuncertainty_factor = 0.1 * rounds_left # Adjust this factor based on the level of uncertainty desired\nplayer_0_expected_score += player_0_potential * uncertainty_factor\nplayer_1_expected_score += player_1_potential * uncertainty_factor\n\n# Refine uncertainty factor based on specific conditions\nif len(player_0_hand) == 1:\nuncertainty_factor += 0.5\nif len(player_1_hand) == 1:\nuncertainty_factor += 0.5\n\n# Store intermediate values in a dictionary\nintermediate_values = {\n'player_0_potential': player_0_potential,\n'player_1_potential': player_1_potential,\n'score_potential': score_potential,\n'player_0_certain_wins': player_0_certain_wins,\n'player_1_certain_wins': player_1_certain_wins,\n'player_0_certain_wins_adjust': player_0_certain_wins_adjust,\n'player_1_certain_wins_adjust': player_1_certain_wins_adjust,\n'sumplayed Cards_diff': sumplayed Cards_diff,\n'uncertainty_factor': uncertainty_factor\n}\n\nreturn (player_0_expected_score, player_1_expected_score), intermediate_values\n```\n\n## # H.3.4 EXAMPLE 4\n\nBelow is the GOPS Value Heuristics Function before the strategy improvement.\n\n## Published as a conference paper at ICLR 2025\n\n```python\n## GOPS Value Heuristics Function (Before)\ndef evaluate_state(state) -&gt; tuple[tuple[float, float], dict]:\nscore Cards = state[0]\nplayer_0[played Cards = state[1]\nplayer_1[played Cards = state[2]\nis_turn = state[3]\nplayer_0_score = state[4]\nplayer_1_score = state[5]\nscore_deck = state[6]\nplayer_0_hand = state[7]\nplayer_1_hand = state[8]\n# Calculate initial potentials\nplayer_0_potential = sum(player_0_hand)\nplayer_1_potential = sum(player_1_hand)\nscore_potential = sum(score_deck)\n# Add half of the score potential to the player who has the turn\nif is_turn:\nplayer_0_potential += score_potential / 2\nelse:\nplayer_1_potential += score_potential / 2\n# Count the number of certain wins for each player\nplayer_0_certain_wins = sum(card &gt; max(player_1_hand) for card in player_0_hand)\nplayer_1_certain_wins = sum(card &gt; max(player_0_hand) for card in player_1_hand)\nrounds_left = len(score_deck)\n# Add potential scores from certain wins\nif rounds_left &lt;= player_0_certain_wins:\nhighest Scores = sorted(score_deck)[-rounds_left:]\nplayer_0_potential += sum(highest Scores)\nif rounds_left &lt;= player_1_certain_wins:\nhighest Scores = sorted(score_deck)[-rounds_left:]\nplayer_1_potential += sum(highest Scores)\n# New improvement: Incorporate a probabilistic approach based on the remaining score cards\nplayer_0_expected_score = player_0_score + player_0_potential\nplayer_1_expected_score = player_1_score + player_1_potential\n# Dynamic evaluation of opponent's potential moves\nopponent_potential = sum(player_0_hand) if is_turn else sum(player_1_hand)\nopponent_certain_wins = sum(card &gt; max(player_1_hand) for card in player_0_hand) if is_turn else sum\n(card &gt; max(player_0_hand) for card in player_1_hand)\n# Adjust evaluation based on opponent's potential plays\nif rounds_left &lt;= opponent_certain_wins:\nhighest Scores = sorted(score_deck)[-rounds_left:]\nif is_turn:\nplayer_1_potential += sum(highest Scores)\nelse:\nplayer_0_potential += sum(highest Scores)\n# Store intermediate values in a dictionary\nintermediate_values = {\n'player_0_potential': player_0_potential,\n'player_1_potential': player_1_potential,\n'score_potential': score_potential,\n'player_0_certain_wins': player_0_certain_wins,\n'player_1_certain_wins': player_1_certain_wins,\n'opponent_potential': opponent_potential,\n'opponent_certain_wins': opponent_certain_wins,\n'player_0_expected_score': player_0_expected_score,\n'player_1_expected_score': player_1_expected_score\n}\nreturn (player_0_expected_score, player_1_expected_score), intermediate_values\n```\n\nBelow is the selected idea to improve the above strategy.\n\n## Selected GOPS Value Heuristics Improvement Idea\n\nIntroduce a penalty factor for potential scores that are excessively high, indicating potential overestimation. By capping the potential scores or applying a scaling factor based on certain thresholds, we can prevent the function from assigning unrealistic values and improve the accuracy of the value heuristic.\n\nBelow is the GOPS Value Heuristics Function after enhancement by LLM. The improved parts by LLM are highlighted in yellow.\n\n‚¨á\n## GOPS Value Heuristics Function (After)\n\n‚¨á\ndef evaluate_state(state) -> tuple[tuple[float, float], dict]:\nscore_cards = state[0]\nplayer_0_played_cards = state[1]\nplayer_1_played_cards = state[2]\nis_turn = state[3]\nplayer_0_score = state[4]\nplayer_1_score = state[5]\nscore_deck = state[6]\nplayer_0_hand = state[7]\nplayer_1_hand = state[8]\n\n# Calculate initial potentials\nplayer_0_potential = sum(player_0_hand)\nplayer_1_potential = sum(player_1_hand)\nscore_potential = sum(score_deck)\n\n# Add half of the score potential to the player who has the turn\nif is_turn:\nplayer_0_potential += score_potential / 2\nelse:\nplayer_1_potential += score_potential / 2\n\n# Count the number of certain wins for each player\nplayer_0_certain_wins = sum(card > max(player_1_hand) for card in player_0_hand)\nplayer_1_certain_wins = sum(card > max(player_0_hand) for card in player_1_hand)\nrounds_left = len(score_deck)\n\n# Add potential scores from certain wins\nif rounds_left <= player_0_certain_wins:\nhighest_scores = sorted(score_deck)[-rounds_left:]\nplayer_0_potential += sum(highest_scores)\n\nif rounds_left <= player_1_certain_wins:\nhighest_scores = sorted(score_deck)[-rounds_left:]\nplayer_1_potential += sum(highest_scores)\n\n## Published as a conference paper at ICLR 2025\n\n```python\nNew improvement: Incorporate a probabilistic approach based on the remaining score cards player_0_expected_score = player_0_score + player_0_potential player_1_expected_score = player_1_score + player_1_potential\n# Dynamic evaluation of opponent's potential moves\nopponent_potential = sum(player_0_hand) if is_turn else sum(player_1_hand)\nopponent_certain_wins = sum(card &gt; max(player_1_hand) for card in player_0_hand) if is_turn else sum (card &gt; max(player_0_hand) for card in player_1_hand)\n# Adjust evaluation based on opponent's potential plays\nif rounds_left &lt;= opponent_certain_wins:\nhighest Scores = sorted(score_deck)[-rounds_left:]\nif is_turn:\nplayer_1_potential += sum(highest Scores)\nelse:\nplayer_0_potential += sum(highest Scores)\n# Introduce a penalty factor for excessively high potential scores\npenalty_threshold = 100 # Define a threshold for potential scores to trigger penalty\npenalty_factor = 0.5 # Define a factor by which to reduce potential scores above threshold\nif player_0_potential &gt; penalty_threshold:\nplayer_0_potential = penalty_threshold + (player_0_potential - penalty_threshold) * penalty_factor\nif player_1_potential &gt; penalty_threshold:\nplayer_1_potential = penalty_threshold + (player_1_potential - penalty_threshold) * penalty_factor\n# Store intermediate values in a dictionary\nintermediate_values = {\n'player_0_potential': player_0_potential,\n'player_1_potential': player_1_potential,\n'score_potential': score_potential,\n'player_0_certain_wins': player_0_certain_wins,\n'player_1_certain_wins': player_1_certain_wins,\n'opponent_potential': opponent_potential,\n'opponent_certain_wins': opponent_certain_wins,\n'player_0_expected_score': player_0_expected_score,\n'player_1_expected_score': player_1_expected_score\n}\nreturn (player_0_expected_score, player_1_expected_score), intermediate_values\n\n## Published as a conference paper at ICLR 2025\n\n## # I DIALOGUE GUIDE LLM PROMPT AND OUTPUT EXAMPLES\n\nThis sections show the system prompts of dialogue guidance on LLM and several examples, including system prompts, idea generation prompts, and strategy implementation examples.\n\n## # I.1 SYSTEM PROMPTS\n\nBelow is the Dialogue guide system prompt.\n\n# Dialogue guide system prompt\n\nYou are a coach trying to write a section of a strategy guide on how to play a game well.\n\nThe specific section of the strategy guide you are writing right now is on how to play the Merlin role effectively during the discussion phase so that they can win the game. Recall that players often use the discussion phase to (1) gather information about other players, (2) try to convince other players of their innocence or guilt, and (3) try to persuade other players of a particular course of action. The game you are interested in is called The Resistance: Avalon. The Resistance: Avalon is the game of hidden identities and social deduction. There are two teams in the game: Good and Evil. Each player has a hidden identity (role) and side.\n\nDialogue guide signature follows, specifying the structures and contents dialogues regarding role playing the Merlin.\n\n# Dialogue guide signature\n\nYour guide should be in the form of a worksheet that the student can use to build their speech. You should order the worksheet questions in a way that makes logical sense, and you should have no more than six questions. Your questions should instruct the reader to write parts of their speech.\n\nThe title of your section should be \"Questions to fill out before speaking as the Merlin role\". Below is an example of how your worksheet should look like:\n\n## 1. Questions to fill out before speaking as the Merlin role\n## 2. Q1: Which player seems the most suspicious of you and why?\n3. Q2: For the player that seems the most suspicious of you, produce a statement addressing their suspicious.\n## 4. Q3: Which player is the quest leader?\n5. Q4: Produce a statement addressing the quest leader to convince them to support your intended course of action/ desired team.\n## 6. Q5: Which player is the most supportive of you?\n7. Q6: Produce a statement addressing the supportive player to convince them to support your intended course of action/ desired team.\n\n## Published as a conference paper at ICLR 2025\n\n# Example Generated Dialogue Strategy Guide for Assassin\n\n1. Q1: Which player seems to have a deeper understanding of the game flow than normal Servants should possess?\n2. Q2: Develop a non-confrontational statement to subtly challenge this player. This should cause them to either prove their innocence or reveal more clues about their identity.\n## 3. Q3: Who has been the most influential in the team selection and voting process?\n4. Q4: Devise a statement to express agreement with this player's viewpoint subtly. This should make you less suspicious while enabling you to manipulate the discussion.\n## 5. Q5: Which player seems the most supportive of your views and actions in the game?\n6. Q6: Craft a statement subtly emphasizing your alignment with this supportive player's thoughts. This should increase your chances of being included in quest teams and reduce suspicion around you.\n\n## # I.2 IDEA GENERATION EXAMPLES\n\nThe following is the selected dialogue guide speaking as the Merlin role.\n\n# Select dialogue guide\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n6. Analyze the voting patterns and team compositions proposed by each player to identify potential alliances or suspicious behaviors. Craft a statement to subtly address these observations without revealing your role.\n7. Which player seems the most supportive of Good and how can you align with them during the discussion phase to better ensure success for your team?\n\nA feedback example is provided for better understanding of dialogue guidance to teach LLM how to role play Merlin.\n\n## Feedback example description\n\n## Example 2:\n\nThe student using your guide was playing as player 0 in this example filled out your guide as follows:\n\n1. Q1: Based on the statements made so far, I suspect Players 2 and 4 are on the Evil side. Player 2‚Äôs suggestion of a team consisting of Player 1 and Player 2 seems too convenient, possibly indicating a desire to control the team composition. Player 4‚Äôs agreement with Player 1 and Player 2 without offering any alternative team suggestions raises suspicion as well.\n2. Q2: \"I can‚Äôt help but wonder if some players are trying to steer the team selection process in a particular direction. It‚Äôs important for us to carefully consider all perspectives and not be swayed by potentially self-serving suggestions.\"\n3. Q3: The current round leader is Player 1. I would suggest a team consisting of Player 0 (Merlin) and Player 1, as this would allow me to closely observe the actions of the other players and potentially guide the team towards success.\n4. Q4: \"Player 1, I believe that a team comprised of Player 0 and Player 1 would provide a strong foundation for success. By working closely together, we can ensure that our mission is carried out effectively.\"\n5. Q5: The voting patterns so far show agreement among Players 1, 2, and 4, indicating a potential alliance or shared goals. Player 3‚Äôs suggestion of including themselves in the team may also indicate a desire to control the outcome of the quest.\n6. Q6: \"I‚Äôve noticed a pattern of agreement among certain players in the team selection process. It‚Äôs important for us to consider all perspectives and not be swayed by potentially biased suggestions. Let‚Äôs focus on working together as a team to achieve our common goal.\"\n7. Q7: Player 1 seems the most supportive of Good based on their consistent emphasis on unity and success for the Quests. I will align with Player 1 during the discussion phase to ensure that our team is selected and the quest is completed successfully.\n\n## Then they said the following during the discussion phase:\n\n\"Fellow players, as we embark on this quest, it is crucial that we approach the team selection process with caution and consideration. I have noticed a pattern of agreement among certain players, which could potentially indicate a desire to steer the outcome in a particular direction. It is important for us to carefully consider all perspectives and not be swayed by potentially biased suggestions.\n\nI believe that a team comprised of Player 0 and Player 1 would provide a strong foundation for success. By working closely together, we can ensure that our mission is carried out effectively. Player 1 has consistently emphasized unity and success for the Quests, making them a trustworthy ally in this endeavor.\n\nLet us focus on working together as a team to achieve our common goal, rather than allowing personal agendas to influence our decisions. Let us approach this quest with unity and determination, ensuring that Good prevails in the end. Thank you for your attention.\"\n\nBelow is some feedback from the other players on how player 0 performed during the discussion phase. Recall that Good players are trying to access which other players are likely to be Good, while Evil players are trying to access which Good player is likely to be Merlin.\n\n## Player 3 with role Servant had the following thoughts on this discussion round:\n\n## Thought:\nPlayer 1‚Äôs speech emphasizes unity and success for the Quests, which aligns with the goals of Good and decreases the probability of Player 1 being Evil. Player 2‚Äôs statement also focuses on unity and careful consideration, indicating a commitment to the success of Good and decreasing the probability of Player 2 being Evil. Player 3‚Äôs response as a Servant of Arthur is focused on the success of the Quests and unity, which aligns with the goals of Good and decreases the probability of Player 3 being Evil. Player 4‚Äôs speech seems overly enthusiastic and focused on unity and cooperation, which could be a tactic to divert attention from their true identity as an Evil player. This raises suspicion and increases the probability of Player 4 being Evil.\n\nDictionary: {0: (-1, ‚Äôdecreased slightly‚Äô), 1: (-1, ‚Äôdecreased slightly‚Äô), 2: (0, ‚Äôstayed the same‚Äô), 3: (-2, ‚Äôdecreased significantly‚Äô), 4: (2, ‚Äôincreased significantly‚Äô)}\n\n## Player 4 with role Assassin had the following thoughts on this discussion round:\n\n## Thought:\nPlayer 2 seems to suspect the Evil players (Player 3 and Player 4) without any evidence and is trying to steer the conversation towards them. This aligns with the behavior of Merlin, who knows the identity of the Evil players and is trying to subtly guide the Good players towards the correct decisions. This increases the probability of Player 2 being Merlin. On the other hand, Player 3‚Äôs speech seems generally clueless and lacks any strategic insight, which decreases the probability of Player 3 being Merlin.\n\nDictionary: {0: (0, ‚Äôstayed the same‚Äô), 1: (0, ‚Äôstayed the same‚Äô), 2: (2, ‚Äôincreased significantly‚Äô), 3: (-2, ‚Äôdecreased significantly‚Äô), 4: (-2, ‚Äôdecreased significantly‚Äô)}\n\nThis is the prompt for feedback reflections.\n\n[style=unboxed,leftmargin=0cm]\n[font=Poster,leftmargin=0cm]\n[font=Poster,rightmargin=0cm]\n[font=Poster,rightmargin=0cm]\nFigure 1: (a) The text of the text is presented in the text. (b) The text is presented in the text. (c) The text is presented in the text. (d) The text is presented in the text. The following are the feedback reflections generated by LLM.\n\n## Published as a conference paper at ICLR 2025\n\n# Generated feedback reflections\n\nIn Example 0, Player 0 effectively followed the guide by suspecting certain players of being on the Evil side, crafting subtle statements to cast doubt on them, and aligning with players supportive of Good. This led to Player 0 being perceived positively by other players, with Player 2's probability of being Merlin decreasing significantly due to their behavior aligning more with the Evil side.\n\nIn Example 1, Player 0 again effectively followed the guide by suspecting players, making subtle statements, aligning with a supportive player, and emphasizing unity and success in their speech. This resulted in Player 0 being perceived positively by other players, with Player 4's probability of being Merlin decreasing significantly due to their lack of strategic insight.\n\nIn Example 2, Player 0 successfully identified suspicious behavior in other players, made subtle statements, and aligned with a supportive player. This led to Player 0 being perceived positively, with Player 2's probability of being Merlin decreasing significantly due to their behavior aligning more with the Evil side.\n\nIn Example 3, Player 0 effectively followed the guide by suspecting players, making subtle statements, aligning with a supportive player, and emphasizing unity. This resulted in Player 0 being perceived positively by other players, with Player 4's probability of being Merlin increasing significantly due to their behavior raising suspicion.\n\n- Overall, the guide's emphasis on analyzing player behavior, making subtle statements, and aligning with supportive players has proven to be successful in helping players navigate the discussion phase effectively as the Merlin role in The Resistance: Avalon.\n- Players who followed the guide were able to strategically assess the situation, cast doubt on potential Evil players, and align with Good players to increase their chances of success.\n\n## Below is the Prompt for idea generation regarding this dialogue guides\n\n## Published as a conference paper at ICLR 2025\n\n# Prompt for idea generation\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n## You previously generated the following section of the strategy guide:\n\n<previous guide=\"\">\n\n## Here is a possible way to improve your guide:\n\n<feedback reflections=\"\">\n\nBased on the worksheet, feedback, and conclusions you drew, what are 2 improvements that you can make to the worksheet that you think will have the most impact?\n\nBe as specific and concrete as possible, including what questions to add, edit, or remove, and write them out in the following format:\n\n- Thoughts: <your thoughts=\"\" here=\"\">\n- Idea 1: <your idea=\"\" here=\"\">\n- Idea 2: <your idea=\"\" here=\"\">\n¬∑¬∑¬∑\n\n## Here's an example of what this might look like for 3 improvement ideas:\n\n- Thoughts: I should tell the reader to address each player individually in the guide.\n- Idea 1: Add a question asking the reader who they think is most suspicious of them and produce a statement addressing their suspicions.\n- Idea 2: Add a question asking the reader to consider which other player is most supportive of them and produce a statement addressing their support.\n- Idea 3: Add a question asking the reader produce a statement addressed\n\nGenerated new improvement ideas are demonstrated below, showing the improvements made by LLM following the prompt for idea generation.\n\n# Generated new improvement ideas\n\nThoughts: The current worksheet focuses on analyzing player behavior, making subtle statements, and aligning with supportive players, which are all crucial strategies for playing the Merlin role effectively. To enhance the guide further, we can add questions that prompt the player to consider their own actions and statements more explicitly.\n\n1. Idea 1: Add a question asking the player to reflect on how their proposed team composition would benefit the success of the Quest and provide a statement justifying their choices.\n2. Idea 2: Add a question asking the player to assess their level of suspicion towards each player and craft a statement addressing how they will interact with those players during the discussion phase.\n3. Idea 3: Add a question asking the player to consider how they can subtly guide the discussion towards a successful Quest.\n\n## # I.3 STRATEGY IMPLEMENTATION EXAMPLES\n\nRecall the strategy implementation mentioned in the paper before. Here, we implement strategy improvements on dialogue guide. The improved dialogue guide is highlighted.\n\n## # I.3.1 EXAMPLE 1\n\nBelow is the dialogue guide before any improvements happened.</your></your></your></feedback></previous></your></previous>\n\n## Published as a conference paper at ICLR 2025\n\n# Dialogue guide (previous)\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n6. Analyze the voting patterns and team compositions proposed by each player to identify potential alliances or suspicious behaviors. Craft a statement to subtly address these observations without revealing your role.\n7. Which player seems the most supportive of Good and how can you align with them during the discussion phase to better ensure success for your team?\n\nBelow is the improvement idea prompted into LLM.\n\n# Improvement idea\n\nAdd a question asking the player to assess their level of suspicion towards each player and craft a statement addressing how they will interact with those players during the discussion phase.\n\nHere, implementation prompts follow to teach LLM how to improve the dialogue guide.\n\n# Implementation prompt\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n## You previously generated the following section of the strategy guide:\n\n<previous guide=\"\">\n\n## Here is a possible way to improve your guide:\n\n<improvement idea=\"\">\n\nImplement this improvement into the guide section as best as you can, but do not change the original guide too much. Make sure to stay within the scope of the guide section, which we reiterate below:\n\n<guide signature=\"\">\n\nFinally, we have a improved dialogue guide, where the changes are highlighted below comparing to the previous guide.</guide></improvement></previous></same>\n\n## Published as a conference paper at ICLR 2025\n\n# Dialogue guide (improved)\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n6. Analyze the voting patterns and team compositions proposed by each player to identify potential alliances or suspicious behaviors. Craft a statement to subtly address these observations without revealing your role.\n7. Which player seems the most supportive of Good and how can you align with them during the discussion phase to better ensure success for your team?\n8. Assess your level of suspicion towards each player and craft a statement addressing how you will interact with those players during the discussion.\n\n## # I.3.2 EXAMPLE 2\n\nBelow is the dialogue guide before any improvements happened.\n\n# Dialogue guide (previous)\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n## 6. Craft a statement to share these observations subtly without giving away your role.\n\nBelow is the improvement idea prompted into LLM.\n\n# Improvement idea\n\nAdd a question asking the reader to identify which players they feel most aligned with in terms of strategy and decision-making, and how they can subtly align themselves with these players during the discussion phase.\n\nHere, implementation prompt follows to teach LLM how to improve the dialogue guide.\n\n## Published as a conference paper at ICLR 2025\n\n# Implementation prompt\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n## You previously generated the following section of the strategy guide:\n\n<previous guide=\"\">\n\n## Here is a possible way to improve your guide:\n\n<improvement idea=\"\">\n\nImplement this improvement into the guide section as best as you can, but do not change the original guide too much. Make sure to stay within the scope of the guide section, which we reiterate below:\n\n<guide signature=\"\">\n\nFinally, we have a improved dialogue guide, where the changes are highlighted below comparing to the previous guide\n\n# Dialogue guide (improved)\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n## 6. Craft a statement to share these observations subtly without giving away your role.\n## 7. Which player seems the most aligned with your strategic decisions and why?\n8. Produce a statement to subtly align yourself with the player you feel most aligned with during the discussion phase.\n\n## # I.3.3 EXAMPLE 3\n\nBelow is the dialogue guide before any improvements happened.</guide></improvement></previous></improvement></system>\n\n## Published as a conference paper at ICLR 2025\n\n# Dialogue guide (previous)\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n## 6. Craft a statement to share these observations subtly without giving away your role.\n## 7. Which player seems the most aligned with your strategic decisions and why?\n8. Produce a statement to subtly align yourself with the player you feel most aligned with during the discussion phase.\n\nBelow is the improvement idea prompted into LLM.\n\n# Improvement idea\n\nAdd a question asking the reader to analyze the voting patterns and craft a statement subtly highlighting any inconsistencies or suspicious trends in the voting behavior of specific players.\n\nHere, implementation prompt follows to teach LLM how to improve the dialogue guide.\n\n# Implementation prompt\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n## You previously generated the following section of the strategy guide:\n\n<previous guide=\"\">\n\n## Here is a possible way to improve your guide:\n\n<improvement idea=\"\">\n\nImplement this improvement into the guide section as best as you can, but do not change the original guide too much. Make sure to stay within the scope of the guide section, which we reiterate below:\n\n<guide signature=\"\">\n\nFinally, we have a improved dialogue guide, where the changes are highlighted below comparing to the previous guide</guide></improvement></previous></same>\n\n## Published as a conference paper at ICLR 2025\n\n# Dialogue guide (improved)\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n## 6. Craft a statement to share these observations subtly without giving away your role.\n## 7. Which player seems the most aligned with your strategic decisions and why?\n8. Produce a statement to subtly align yourself with the player you feel most aligned with during the discussion phase.\n9. Analyze the voting patterns and craft a statement subtly highlighting any inconsistencies or suspicious trends in the voting behavior of specific players.\n\n## J Dialogue Generation and Analysis Example LLM Inputs and Outputs\n\nThis sections shows the examples of dialogue generation and analysis, including descriptive prompts, dialogue generation, and dialogue analysis.\n\n### J.1 Descriptive Prompts\n\nBelow is the discussion history prompted into LLM for better knowledge of dialogue generation.\n\n###### Discussion history\n\n## Here is a summary of previous rounds of discussion so far:\n\n- Player 1: \"Greetings, fellow players. As a Servant of Arthur, my primary goal is to ensure the success of our Quests. I am committed to working together with all of you to achieve victory for Good. I believe that for this first Quest, a team consisting of Player 1 and Player 4 would be a strong choice. I hope for your support in approving this team, as I am confident that we can work effectively together to accomplish our goal. I encourage everyone to share their thoughts and strategies openly. Let‚Äôs work together to assess each other‚Äôs intentions and make the best decisions for the success of our Quest. Remember, the fate of Camelot rests in our hands, and we must be vigilant against the forces of Evil.\"\n- Player 2: \"Greetings, noble companions. I echo Player 1‚Äôs sentiment in working together for the success of our Quests. I believe it is essential to carefully consider our team choices to ensure victory for Good. I suggest that for this first Quest, a team consisting of Player 1 and Player 2 would provide a strong foundation for success. Together, we can overcome any challenges that come our way. Let us unite in our efforts and show our unwavering dedication to the cause. I urge all players to carefully evaluate each team proposal and remember that unity is key to thwarting the forces of Evil.\"\n- Player 3: \"Greetings, noble companions. I echo the sentiments shared by Player 1 and Player 2 regarding our shared goal of ensuring the success of our Quests. Unity and cooperation will be key in our endeavors to protect Camelot from the forces of Evil. As we deliberate on the composition of our team for this first Quest, I would like to offer a perspective that considers the balance and strength of our choices. A team comprised of Player 1 and Player 3 would bring together a diverse set of skills and viewpoints that can greatly contribute to our success. By including Player 3 in the team, we can ensure a comprehensive approach to overcoming any challenges that may arise. Let us work together in harmony, leveraging our individual strengths for the greater good of Camelot. I urge my fellow players to consider this team proposal thoughtfully, as I believe it holds great promise for the success of our Quest.\"\n- Player 4: \"Greetings, honorable players. I appreciate the dedication and commitment displayed by Player 1, Player 2, and Player 3 towards our common goal of ensuring the success of this Quest. Unity and cooperation are indeed vital in our quest to safeguard Camelot. I believe that the proposed team of Player 1 and Player 2 presents a strong foundation for success. Their combined skills and perspectives can greatly benefit our mission, and I urge all players to consider this team thoughtfully for the betterment of our cause. Let us unite our strengths and work together seamlessly to overcome any challenges that may arise. By standing united, we can ensure the triumph of Good over the forces of Evil. I urge all players to support the team of Player 1 and Player 2 for the success of this Quest.\"\n\nState description shows LLM which state it will be for role playing and dialogue generation.\n\n## Published as a conference paper at ICLR 2025\n\n# State description\n\nYou are Player 0, with identity Merlin. You are on the side of Good. The Evil players are Players 2 and 4. Please do not forget your identity throughout the game.\n\n## The current state of the game is as follows:\n\n- The number of players in the game is: 5\n- This is the quest number 0 which requires 2 players and 1 fails to fail\n## This is the 0 round of discussion\n- The previous results for the quest were () (True for Success, False for Fail)\n- The current phase of the game is the team selection phase\n- The current leader is player 1\n\n## You would like the following team to be approved: [0, 1]\n\n## # J.2 DIALOGUE GENERATION\n\nBelow is the prompt for filling out worksheet. Filling out this worksheet assists LLM generated better dialogue aligned with the role, Merlin as in the example.\n\n# Prompt for filling out worksheet\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n<discussion history=\"\">\n\n<state description=\"\">\n\nGiven the discussion history, current state, and your intended action, answer the above worksheet to the best of your ability. The answers should be based on the information you have and your own reasoning.\n\n## Six questions to fill out before speaking as the Merlin role\n\n## Q1: Who are the players you suspect are on the Evil side and why?\nQ2: Draft a subtle statement that casts doubt on these players without directly accusing them.\n## Q3: Who is the current round leader and which players are you considering to suggest for the team?\nQ4: Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\nQ5: What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\nQ6: Craft a statement to share these observations subtly without giving away your role.\n\nThe worksheet filled out by LLM follows, demonstrating the LLM capability and the effectiveness of prompting.</state></discussion></game></system>\n\n## LLM filled out worksheet\n\nQ1: I suspect Players 2 and 4 are on the Evil side because they both proposed the same team composition of Player 1 and Player 2, which seems too coordinated and potentially trying to exclude other players from the team.\n\nQ2: \"I‚Äôve noticed a strong push for the same team composition from certain players, which may raise questions about their motivations and potential alliances within the group.\"\n\nQ3: The current round leader is Player 1. I would suggest Player 0 and Player 1 for the team, as I believe this composition would provide a strong foundation for success and help me subtly guide the decision towards my preferred team.\n\nQ4: \"Player 1, I have a feeling that selecting Player 0 and Player 1 for the team could bring a diverse set of skills and viewpoints that may greatly contribute to our success. Let‚Äôs consider all options carefully for the betterment of our cause.\"\n\nQ5: The voting patterns have shown a strong push for the team composition proposed by Players 1 and 2, indicating potential coordination or alignment between these players. Player 3‚Äôs suggestion of including themselves in the team also raises suspicions of wanting to secure a spot on the quest.\n\nQ6: \"I‚Äôve noticed a pattern in the team composition suggestions and voting behaviors that may reveal certain alliances or motivations within the group. It‚Äôs important for us to carefully evaluate all options and consider the implications for the success of our quest.\"\n\nBelow is the prompt for speech generation. Worksheet completion assists LLM to understand the states and role. The following prompt asks LLM to assemble responses to the questionaire filled out above.\n\n## Prompt for speech generation\n\n<System prompt> <Filled out worksheet>\n\nAssemble your responses to the questionaire you just answered into a speech that would help you achieve your intent. Note that dialogue will be seen by all players in the game, so you should not reveal your identity.\n\n## Your speech:\n\nFinally we have LLM generated speech after the preparations.\n\n## LLM generated speech\n\nI‚Äôve noticed a strong push for the same team composition from certain players, which may raise questions about their motivations and potential alliances within the group. The voting patterns have shown a strong push for the team composition proposed by Players 1 and 2, indicating potential coordination or alignment between these players. Player 3‚Äôs suggestion of including themselves in the team also raises suspicions of wanting to secure a spot on the quest. It‚Äôs important for us to carefully evaluate all options and consider the implications for the success of our quest. Let‚Äôs consider all options carefully for the betterment of our cause. I have a feeling that selecting Player 0 and Player 1 for the team could bring a diverse set of skills and viewpoints that may greatly contribute to our success. As the current round leader, Player 1, I would suggest Player 0 and Player 1 for the team, as I believe this composition would provide a strong foundation for success and help me subtly guide the decision towards my preferred team. Thank you for considering my input.\n\nJ.3 Dialogue Analysis\n\nAfter dialogue generation, we continue have LLM assume the role of Merlin, and dialogue analysis is conducted.\n\nBelow is the prompt for dialogue analysis regarding to the role of Merlin. A thought is included to guide LLM analyze.\n\nPrompt for dialogue analysis: Merlin <System prompt> <Game rules> <Discussion history> <State description> Based on the discussions this round, for each player in players 0, 1, 2, 3, 4, do you think the probability of the player being Merlin (2) increased significantly (1) increased slightly (0) stayed the same (-1) decreased slightly or (-2) decreased significantly and why? Begin with your reasoning and end with a dictionary of player to your multiple choice answer, i.e. {0: (2, ‚Äôincreased significantly‚Äô), ‚Ä¶, 4: (0, ‚Äôstayed the same‚Äô)}. Make sure you include quotes around the dictionary values that are strings. Here‚Äôs an example: Thought: Player 2 seems to suspect the Evil players (Player 3 and Player 4) without any evidence and is trying to steer the conversation towards them. This aligns with the behavior of Merlin, who knows the identity of the Evil players and is trying to subtly guide the Good players towards the correct decisions. This increases the probability of Player 2 being Merlin. On the other hand, Player 4‚Äôs speech seems generally clueless and lacks any strategic insight, which decreases the probability of Player 4 being Merlin. Dictionary: {0: (0, ‚Äôstayed the same‚Äô), 1: (0, ‚Äôstayed the same‚Äô), 2: (2, ‚Äôincreased significantly‚Äô), 3: (-2, ‚Äôdecreased significantly‚Äô), 4: (0, ‚Äôstayed the same‚Äô)} Generated dialogue analysis follows. The role is Merlin.\n\nLLM generated dialogue analysis: Merlin Thought: Player 2 is actively participating in the discussion and proposing team compositions, which is a behavior that could align with the role of Merlin, who knows the Evil players and wants to guide the Good players towards success. This increases the probability of Player 2 being Merlin. Player 4, on the other hand, seems to be going along with the proposed teams without much strategic insight, which decreases the probability of Player 4 being Merlin. Dictionary: {0: (0, ‚Äôstayed the same‚Äô), 1: (0, ‚Äôstayed the same‚Äô), 2: (2, ‚Äôincreased significantly‚Äô), 3: (0, ‚Äôstayed the same‚Äô), 4: (-2, ‚Äôdecreased significantly‚Äô)} Under dialogue analysis, besides focusing on one specific role, we also have LLM target on good and evil in the game, which is broader in terms of role playing. Below is the prompt for dialogue analysis regarding to Good and Evil.\n\n## Published as a conference paper at ICLR 2025\n\n# Prompt for dialogue analysis: Good and Evil\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n<discussion history=\"\">\n\n<state description=\"\">\n\nBased on the discussions this round, for each player in players 0, 1, 2, 3, 4, do you think the probability of the player being Evil (2) increased significantly (1) increased slightly (0) stayed the same (-1) decreased slightly or (-2) decreased significantly and why? Begin with your reasoning and end with a dictionary of player to your multiple choice answer, i.e.  $\\{0: (2, \\text{'increased significantly'}$ , ..., 4:  $(0, \\text{'stayed the same'})\\}$ . Make sure you include quotes around the dictionary values that are strings.\n\n## Here's an example:\n\nThought: Player 4's speech seems overly enthusiastic and focused on unity and cooperation, which could be a tactic to divert attention from their true identity as an Evil player. This raises suspicion and increases the probability of Player 4 being Evil. On the other hand, Player 3's response as a Servant of Arthur is more focused on the success of the Quests and unity, which aligns with the goals of Good and decreases the probability of Player 3 being Evil.\n\nDictionary:  $\\{0: (0, \\text{'stayed the same'})$ ,  $1: (0, \\text{'stayed the same'})$ ,  $2: (2, \\text{'increased significantly'})$ ,  $3: (-2, \\text{'decreased significantly'})$ ,  $4: (0, \\text{'stayed the same'})\\}$\n\nBelow is the dialogue analysis generated by LLM on Good and Evil.\n\n# LLM generated dialogue analysis: Good and Evil\n\n# Thought:\n\nPlayer 1's speech is aligned with the goals of Good and focuses on ensuring the success of the Quests, which decreases the probability of Player 1 being Evil. Player 2's speech also emphasizes unity and cooperation, which is in line with the goals of Good and decreases the probability of Player 2 being Evil. Player 3's response suggests including themselves in the team, which may raise suspicions of wanting to secure a spot on the quest, potentially increasing the probability of Player 3 being Evil. Player 4's speech seems overly enthusiastic and focused on unity and cooperation, which could be a tactic to divert attention from their true identity as an Evil player, increasing the probability of Player 4 being Evil. Player 0's speech suggests a different team composition that may bring a diverse set of skills and viewpoints, potentially decreasing the probability of Player 0 being Evil.\n\n# Dictionary:\n\n{0: (-1, 'decreased slightly'), 1: (-1, 'decreased slightly'), 2: (1, 'increased slightly'), 3: (1, 'increased slightly'), 4: (2, 'increased significantly')}</state></discussion></game>\n\n## Published as a conference paper at ICLR 2025\n\n## # K OTHER ABLATION STUDIES\n\n## # K.0.1 BASE LLM MODEL\n\nWe show the performance of different base models in Figure 16.\n\n![img-14.jpeg](img-14.jpeg)\nFigure 16: Game play performance of base LLM models on six card GOPS: GPT3.5 and GPT4.0. We see that the two language models perform similarly with our method.\n\n## # K.0.2 SEARCH BUDGET\n\nHow does the effectiveness of the search+LLM agent scale with regards to the search budget? Does having a larger search process help achieve better performance?\n\nTable 7: Average score difference for MCTS (num_rollout=32) + LLMFunction (Player1, top-3 functions shown in the table) vs. MCTS (num_rollout=32) + RandomRollout (Player2, num_rollout=10); 100 games for each experiment;\n\n|  budget | Best Func. |   | 2nd Best Func. |   | 3rd Best Func.  |   |\n| --- | --- | --- | --- | --- | --- | --- |\n|   |  Player 1 | Player 2 | Player 1 | Player 2 | Player 1 | Player 2  |\n|  16 | -0.91 | 0.91 | -0.7 | 0.7 | -0.88 | 0.88  |\n|  32 | -0.95 | 0.95 | 0.44 | -0.44 | -0.73 | 0.73  |\n|  64 | -1.14 | 1.14 | 1.15 | -1.15 | 0.46 | -0.46  |\n|  128 | -1.28 | 1.28 | 0.36 | -0.36 | 0.25 | -0.25  |\n|  256 | -0.45 | 0.45 | -0.85 | 0.85 | -0.42 | 0.42  |\n|  inf | -1.5 | 1.5 | -2.26 | 2.26 | -1.03 | 1.03  |\n\n## Published as a conference paper at ICLR 2025\n\n![img-15.jpeg](img-15.jpeg)\n(a) Avalon\n\n![img-16.jpeg](img-16.jpeg)\n(b) GOPS\nFigure 17: The training curves of the value heuristic via reinforcement learning in five-player Avalon and five-card GOPS, averaged across five independent runs. The solid lines show the mean and the shaded areas depict the standard deviation.\n\n## # L DETAILS ON LEARNING THE VALUE HEURISTIC VIA REINFORCEMENT LEARNING\n\nWe employ Monte-Carlo based RL approach (Sutton &amp; Barto (2018)) to train a value heuristic for both five-player Avalon and five-card GOPS games. To do so, we construct a MSE loss in each episode for training the value function, i.e.,\n\n$$\n\\underset {\\theta} {\\operatorname {a r g m i n}} \\sum_ {i} ^ {\\mathcal {N}} \\sum_ {t = 0} ^ {T} \\left(V _ {\\theta} ^ {i} (s _ {t}) - S c o r e ^ {i} (s _ {t})\\right) ^ {2}\n$$\n\nwhere  $\\mathcal{N}$  represents the number of actors,  $V_{\\theta}^{i}(s_{t}), i = 1,2,\\dots ,\\mathcal{N}$  denotes the value function for each actor, and  $T$  is the time horizon. Notice that  $s_t$  and  $Score^i (s_t)$  denote the state at time step  $t$  and the corresponding cumulative reward for each actor, i.e.,  $\\sum_{t}^{T}R_{i}(s_{t},a_{t})$ . It is worth pointing that  $Score^i (s_t)$  (the cumulative reward starting from  $s_t$ ) is the unbiased estimate of the value function  $V_{\\theta}^{i}(s_{t})$ .\n\nFor both Avalon and GOPS games, the value function  $V_{\\theta}^{i}(s_{t})$  is predicted by a neural network. We then train the value function network by minimizing the aforementioned loss function over episodes. In Avalon, we consider 20 evolutions (epochs) for the training process. At the end of each evolution, 30 batch runs (episodes) are generated and used to train the value function network, i.e., a total of 600 episodes for training. In GOPS, we train by 20 evolutions as well while considering 60 batch runs each (1200 episodes in total). We evaluate the final performance over 10 episodes in both games. The neural network is constructed by a multilayer perceptron (MLP) with 2 hidden layers. We select a hidden layer size of  $128 * 128$  for Avalon and that of  $64 * 64$  for GOPS. Likewise, the chosen learning rates are  $5e - 4$  and  $8e - 4$ , respectively. The value function is expected to predict the score for each player in the game, e.g., two for GOPS and number of players for Avalon. All experimental hyper-parameters are summarized in Table 8.\n\nHaving introduced the set up, one can observe in Figure 17 an increased performance of RL-trained value heuristic in both five-player Avalon and five-card GOPS games. This validates the improvement for training value heuristic via reinforcement learning within limited evolutions.\n\n## Published as a conference paper at ICLR 2025\n\n## Table 8: Summary of experimental hyper-parameters in RL-training value heuristic\n\n|  Parameters | Avalon | GOPS  |\n| --- | --- | --- |\n|  Type of neural network | MLP | MLP  |\n|  Number of hidden layers | 2 | 2  |\n|  Hidden layer size | 128*128 | 64*64  |\n|  Learning rate | 5e-4 | 8e-4  |\n|  Output dimension | # of players | 2  |\n|  Number of evolutions | 20 | 20  |\n|  Number of batch runs | 30 | 60  |\n|  Number of final batch runs | 10 | 10  |\n\n## # M EXPERIMENTAL COMPUTE RESOURCES\n\nAll experiments in this work were performed on a workstation with an NVIDIA GeForce RTX 3070 GPU, Intel Core i9-10900 CPU at 2.80 GHz, and a Macbook Pro.\n\n## N Improvement method baselines\n\nLine search is a straightforward iterative process that continuously builds upon the most recent improved strategy. In each iteration, a strategy is generated and evaluated based on feedback from the environment. The feedback is then used to enhance the strategy via the LLM, and this cycle is repeated‚Äîsimilar to the Reflexion framework *(Madaan et al., 2024)*. The essence of line search lies in its focus on immediate feedback, where only the latest strategy is considered for improvement, ensuring that progress is always aligned with the most recent understanding of the problem space.\n\nGreedy search takes a more competitive approach by selecting the best-performing strategy from the last generation to serve as the foundation for the next cycle of improvements. During each cycle, the best strategy is used to generate $n$ variations, each an attempt at improvement. These variations are evaluated, and feedback from the environment is gathered to determine the best candidate for the subsequent cycle. This method is inspired by the Eureka framework *(Ma et al., 2023)*, where LLMs are leveraged to iteratively refine reward functions in a manner akin to evolutionary algorithms, focusing on a winner-takes-all selection mechanism.\n\nBest-first search generalizes the improvement process by considering the top $k$ strategies at each iteration, as opposed to focusing on a single best option. This is reminiscent of beam search but with a flexible branching factor that allows for the generation of multiple strategies from each selected candidate. By expanding multiple promising pathways simultaneously, the search can escape local optima and explore a broader solution space. This approach is related to the Tree of Thought method *(Yao et al., 2024)*, which similarly employs a branching mechanism to explore various strategies in parallel during self-improvement cycles.\n\nBest-first search with thought enhances best-first search by incorporating a reasoning layer into the improvement process. Before generating new strategies, the LLM is prompted to first refine the underlying ‚Äúthoughts‚Äù or decision-making processes that led to the generation of the top $k$ strategies, including possible improvement ideas, before generating a new strategy. This meta-cognitive step draws from the React framework *(Yao et al., 2022b)*, which emphasizes reasoning and actions during strategy development, adding an introspective element that encourages deeper exploration of strategy refinements.\n\nStrategist introduces an additional layer of guidance through the use of an idea queue, $Q$. In this approach, ideas are generated and stored in $Q$, providing a reservoir of potential directions for improvement. The LLM uses these ideas to steer the strategy enhancement process, enabling more structured exploration. By separating idea generation from strategy improvement, Strategist facilitates a more focused and deliberate search, ensuring that each iteration explores both immediate refinements and novel directions.\n\nAll methods were run with the same computational budget‚Äîi.e., each method was allowed to generate the same number of improved strategies‚Äîensuring a fair comparison across approaches in our experiments.\n\nWe display a scaling curve for the various improvement methods in Figure 18.\n\n## Published as a conference paper at ICLR 2025\n\n![img-17.jpeg](img-17.jpeg)\n## Computation budget vs performance for different methods\nFigure 18: Number of output tokens from LLM vs game-play performance of generated value heuristics for 6-card GOPS. Each method was run 20 times at different token budgets, and the best function generated by each method was benchmarked against a baseline opponent policy.\n\n## # O HUMAN EVALUATION DETAILS\n\nWe recruited ten experienced graduate students to participate in a study involving a total of forty games of Resistance: Avalon with six players per game. Each human participant was randomly assigned a character, while the remaining five characters were controlled by STRATEGIST agents. The characters used in the games were as follows: Merlin, three Servants of Arthur, Minion of Mordred, and Assassin.\n\nParticipants completed surveys both before and after the games. In the pre-game survey, participants rated their familiarity and skill level with Resistance: Avalon on a scale of 1 to 6. The results, summarized in Figures 19 and 20, indicate that participants were generally familiar with the game's rules and displayed strong skill levels.\n\n![img-18.jpeg](img-18.jpeg)\nFigure 19: Familiarity of human participants with Resistance: Avalon.\n\nThe study consisted of two experimental setups. Participants played one game of Avalon with dialogue against the STRATEGIST agents, which lasted approximately 30-40 minutes. They also played three games of Avalon without dialogue, each lasting about 10 minutes.\n\n## Published as a conference paper at ICLR 2025\n\n![img-19.jpeg](img-19.jpeg)\nFigure 20: Skill Level of human participants in Resistance: Avalon.\n\nAfter completing the games, participants rated the STRATEGIST agents on seven key metrics (Figure 4) using a scale from 1 to 6. Additionally, the experimenter independently evaluated the human participants on the same metrics to ensure consistency. Participants were also asked to select a single word that best described the STRATEGIST agents from the following options: logical, deceptive, emotional, aggressive, cautious. Similarly, the experimenter selected a word to describe each human participant's playstyle. The results of these qualitative assessments are summarized in Figure 5.\n\n## P Related Works Details\n\nIn this section, we provide an in-depth discussion of related works that were briefly mentioned in the main paper:\n\n- ICL-AIL *(Fu et al., 2023)* includes textual feedback and descriptions of previous self-play experiences as few-shot prompts to improve the negotiation abilities of the LLM-agent. The framework iteratively improves negotiation strategies by incorporating AI-generated feedback and historical dialogues.\n- SPAG *(Cheng et al., 2024)* employs reinforcement learning to train an LLM-agent for the game Adversarial Taboo. The agent learns reasoning and adversarial strategy improvements through self-play, resulting in enhanced performance across various reasoning benchmarks.\n- Agent-Pro *(Zhang et al., 2024)* updates textual memories (prior experiences) and textual beliefs about itself and the environment through reflection as it plays Blackjack and Texas Hold‚Äôem. The agent‚Äôs policy optimization enhances its decision-making capabilities, outperforming baseline models.\n- LARLWorf *(Xu et al., 2023b)* integrates reinforcement learning to optimize the action distribution suggested by the LLM. This approach addresses the biases in the LLM‚Äôs suggestions and results in superior strategic play in the social deduction game Werewolf.\n- ComWorf *(Xu et al., 2023a)* creates an LLM-agent that reflects on past experiences to improve its performance in the game Werewolf. The framework uses retrieval and reflection on prior interactions without requiring additional parameter tuning.\n- EnReaWolf *(Wu et al., 2024)* leverages a dataset of human games to fine-tune an LLM-agent for the Werewolf game. This fine-tuning improves both reasoning and communication capabilities, allowing the agent to surpass standard LLM performance.\n",
      "source": "inbox/2408.10635.pdf"
    },
    "output_data": {
      "core_contribution": "STRATEGIST introduces a bi-level framework combining large language model (LLM)-based strategy abstraction with Monte Carlo Tree Search (MCTS) for strategy refinement. This innovative procedure leverages a modular improvement method through population-based self-play without needing any training data. STRATEGIST excels in complex multi-agent games, demonstrating superior performance over traditional reinforcement learning (RL) and LLM-based methods, achieving strategies comparable to human-level play.",
      "method_breakdown": "The STRATEGIST framework involves a hierarchical approach where high-level strategy abstractions are developed using LLM guidance through an iterative process of self-play. Initial high-level strategies are generated by the LLM, forming a strategy tree, where each strategy's performance is tested and refined via Monte Carlo Tree Search (MCTS). The LLM contributes to proposing improvement ideas, guided by a bandit algorithm, which are evaluated for effectiveness in self-play simulations. The resulting strategies are continually refined until an optimal solution is approached.",
      "subsystems_parts": "1. LLM-Based Strategy Abstraction: Generates high-level strategies in text form. 2. Strategy Tree: Hierarchical storage of strategies for refinement. 3. MCTS Executor: Refines abstractions into executable low-level policies. 4. Improvement Idea Queue: Stores and prioritizes strategy improvement suggestions generated by the LLM. 5. Self-Play Simulation Environment: Tests and evaluates strategy performance and improvement.",
      "interactions": "The LLM creates high-level strategy abstractions which are refined in a strategy tree. The MCTS executor executes these abstractions into low-level policies, testing them through self-play simulations. Improvement ideas generated from the LLM are queued and prioritized, guiding further strategy refinements throughout the process. Individual components collaborate to iteratively enhance and adapt strategies based on feedback.",
      "delta_vs_baseline": "Compared to baseline methods, STRATEGIST uniquely integrates LLM-generated strategies with iterative refinement through MCTS, leveraging self-play without needing training data. Its bi-level approach optimizes both strategy abstraction and detailed execution, significantly improving results over traditional RL and existing LLM strategies.",
      "evidence_anchor": "Table 2 effectively demonstrates STRATEGIST's superior performance, showing its ability to consistently outperform traditional RL and other LLM-based methods in both the Game of Pure Strategy (GOPS) and Avalon, evidenced by higher win rates and score improvements.",
      "transferability": "The core STRATEGIST framework is adaptable to various multi-agent game settings. The approach is generalizable, capable of optimizing strategies in contexts requiring sophisticated decision-making and strategic play, making it potentially applicable beyond the specific games studied."
    },
    "conversation_log": [
      "Generator: {\"core_diagram_summary\":\"Figure 1 illustrates the STRATEGIST framework. It shows a high-level strategy being developed through LLM-guided self-play revisions and refined with low-level Monte Carlo Tree Search (MCTS). This hierarchical approach aims to create adaptable strategies by maintaining a queue of 'improvement ideas' influenced by a bandit algorithm.\",\"unique_move\":\"The introduction of STRATEGIST combines LLM-guided strategy abstraction with low-level Monte Carlo Tree Search to refine strategies. It offers a novel approach by leveraging a modular improvement mechanism via self-play without training data, outperforming existing methods in multi-agent games.\",\"distilled_recipe\":\"LLM generates strategy abstraction ‚Üí Strategy refined through the queue of improvement ideas via MCTS ‚Üí Strategy executed and tested in self-play ‚Üí Use win-rate as feedback to adjust strategies.\",\"contribution_type\":\"Procedure\",\"evidence_anchor\":\"Table 2 compares different self-improvement methods including STRATEGIST, highlighting STRATEGIST's superior performance in GOPS and Avalon by achieving higher scores and win rates.\"}",
      "Decider 1: {\"scratchpad_updates\":\"1. Clarify the high-level abstraction process for deconstructing LLM strategies. 2. Ensure consistency in defining the roles of feedback and strategy improvement within the STRATEGIST framework. 3. Detail how high-level strategies are selected and refined by the queue of improvement ideas and Monte Carlo Tree Search (MCTS). 4. Expand on the LLM's role in generating improvement ideas and how these ideas are integrated into strategy development. 5. Validate the claim of modular improvement by illustrating the feedback mechanism's impact on evolving strategies. 6. Explicitly correlate theoretical claims with empirical results, ensuring evidence is directly tied to specific algorithmic improvements in STRATEGIST.\",\"response_to_researcher\":\"Could you further elaborate on how the LLM-driven strategy update process directly influences the improvement in win rates, particularly in GOPS and Avalon? What are the specific computational challenges faced in this bi-level strategy framework, and how does STRATEGIST address them?\"}",
      "Researcher 1: {\"correction_scratchpad\":\"1. The paper describes the abstraction of high-level strategies from LLM-generated insights, emphasizing interpretability and generalizability. The abstraction involves transforming detailed actions into broader strategies represented in a manner suitable for further refinement by mechanisms such as Monte Carlo Tree Search (MCTS). This improves generalization and enhances interpretability, allowing for more efficient searches within strategy spaces. \\n\\n2. The roles of feedback and strategy improvement within STRATEGIST are well-defined. Feedback is obtained through simulated self-play, and strategies are refined based on discrepancies identified through MCTS evaluations. The framework uses this feedback to iteratively update strategies within a hierarchical framework, simultaneously increasing efficiency and reducing computational demands.\\n\\n3. High-level strategies are selected based on their performance in self-play simulations and are stored in a priority-based queue. The process involves using an Upper Confidence Bound (UCB) to select which strategies and improvement ideas to pursue further. This modular approach allows for both exploration of new strategies and exploitation of successful ones.\\n\\n4. LLM plays a crucial role in generating improvement ideas based on high-level abstracted strategies and their corresponding feedback. These ideas guide subsequent improvements, shaping strategy revisions in a manner that enhances performance. The integration of these improvement ideas is achieved through a feedback loop, refining strategies through iterations.\\n\\n5. The modular improvement claim is supported by the separation of idea generation from strategy implementation. This creates a structured feedback loop in which strategies are iteratively refined, with improvements made based on empirical outcomes observed during self-play. This process not only refines existing strategies but also helps identify potential new strategies worthy of exploration.\\n\\n6. Theoretical claims, such as enhanced performance through bi-level strategy development, are supported by empirical evidence showcased in tables and figures. Specifically, Tables 2 and 3 illustrate STRATEGIST's superior results in GOPS and Avalon in comparison to traditional RL methods, substantiating the framework's efficacy.\\n\\nComputational challenges include dealing with the vast policy and action spaces in complex multi-agent environments. STRATEGIST addresses these by utilizing high-level abstractions to reduce complexity and running efficient MCTS simulations to quickly evaluate and refine strategies. The structure allows the system to navigate and optimize within these large spaces effectively without extensive reliance on training data.\",\"response_to_decider\":\"The LLM-driven strategy update process directly impacts improvement in win rates by allowing high-level strategy abstractions to be efficiently refined through MCTS, leveraging feedback from self-play simulations. The abstraction process simplifies the policy space, making it easier to navigate complex multi-agent scenarios like GOPS and Avalon. By focusing on high-level strategic reasoning, the LLM-guided revisions enable rapid adjustments that are informed by empirical win-rate outcomes, enhancing overall performance. Specific computational challenges include the scalability of strategy evaluations and the need for rapid iteration in multi-agent environments, which STRATEGIST addresses through its bi-level framework. The separation of high-level strategy from low-level execution allows for more manageability in computation while ensuring robustness against diverse opponent strategies, as evidenced by performance across multiple iterative self-play games.\",\"done\":true}",
      "Finalizer: {\"core_contribution\":\"STRATEGIST introduces a bi-level framework combining large language model (LLM)-based strategy abstraction with Monte Carlo Tree Search (MCTS) for strategy refinement. This innovative procedure leverages a modular improvement method through population-based self-play without needing any training data. STRATEGIST excels in complex multi-agent games, demonstrating superior performance over traditional reinforcement learning (RL) and LLM-based methods, achieving strategies comparable to human-level play.\",\"method_breakdown\":\"The STRATEGIST framework involves a hierarchical approach where high-level strategy abstractions are developed using LLM guidance through an iterative process of self-play. Initial high-level strategies are generated by the LLM, forming a strategy tree, where each strategy's performance is tested and refined via Monte Carlo Tree Search (MCTS). The LLM contributes to proposing improvement ideas, guided by a bandit algorithm, which are evaluated for effectiveness in self-play simulations. The resulting strategies are continually refined until an optimal solution is approached.\",\"subsystems_parts\":\"1. LLM-Based Strategy Abstraction: Generates high-level strategies in text form. 2. Strategy Tree: Hierarchical storage of strategies for refinement. 3. MCTS Executor: Refines abstractions into executable low-level policies. 4. Improvement Idea Queue: Stores and prioritizes strategy improvement suggestions generated by the LLM. 5. Self-Play Simulation Environment: Tests and evaluates strategy performance and improvement.\",\"interactions\":\"The LLM creates high-level strategy abstractions which are refined in a strategy tree. The MCTS executor executes these abstractions into low-level policies, testing them through self-play simulations. Improvement ideas generated from the LLM are queued and prioritized, guiding further strategy refinements throughout the process. Individual components collaborate to iteratively enhance and adapt strategies based on feedback.\",\"delta_vs_baseline\":\"Compared to baseline methods, STRATEGIST uniquely integrates LLM-generated strategies with iterative refinement through MCTS, leveraging self-play without needing training data. Its bi-level approach optimizes both strategy abstraction and detailed execution, significantly improving results over traditional RL and existing LLM strategies.\",\"evidence_anchor\":\"Table 2 effectively demonstrates STRATEGIST's superior performance, showing its ability to consistently outperform traditional RL and other LLM-based methods in both the Game of Pure Strategy (GOPS) and Avalon, evidenced by higher win rates and score improvements.\",\"transferability\":\"The core STRATEGIST framework is adaptable to various multi-agent game settings. The approach is generalizable, capable of optimizing strategies in contexts requiring sophisticated decision-making and strategic play, making it potentially applicable beyond the specific games studied.\"}"
    ],
    "processed_at": "2026-02-16 13:12:09.206809",
    "iterations": 1,
    "domain": "core_extraction"
  },
  "markdown_pages": [
    {
      "id": 1,
      "title": "Example (fully generated) Chain of Thought Dialogue Strategy Guide",
      "content": "# Example (fully generated) Chain of Thought Dialogue Strategy Guide\n\n## Q1: Who are the players you suspect are on the Evil side and why?\n\nA1: I suspect Player 2 and Player 4 are on the Evil side. Player 2's suggestion for the team consisting of themselves and Player 1 seems too convenient, as it would allow Evil to easily fail the quest. Player 4's agreement with Player 1 and Player 2's proposed team without offering any alternative suggestions raises suspicion.\n\nQ2: Draft a subtle statement that casts doubt on these players without directly accusing them.\n\nA2: \"I can't help but wonder if some players are pushing for certain team compositions to benefit themselves rather than considering what is truly best for the success of our Quest.\"\n\nGenerated dialogue: \"I have observed patterns in the voting that suggest certain players may be working together to push for specific team compositions. This raises suspicion and calls into question their true intentions. We must not allow ourselves to be swayed by potentially deceitful tactics.\"\n\n## # 2.3 MODULAR SEARCH FOR HIGH-LEVEL STRATEGY IMPROVEMENT\n\n![img-1.jpeg](img-1.jpeg)\n## High-level strategy improvement\nFigure 2: Overview of our high-level strategy improvement method: The process alternates between two steps‚Äîidea generation and strategy improvement. For more details see Figure 11.\n\nOur framework alternates between two steps: idea generation and strategy improvement, as shown in Figure 2 and detailed in App. D. Each cycle refines strategies stored in a tree structure based on feedback from self-play simulations and generates new improvement ideas stored in a priority queue.\n\n#### 2.3.1 Idea Generation\n\nIn the idea generation step, a strategy $\\sigma$ and its feedback trajectory $\\tau_{\\sigma}$ are selected from the strategy tree using an adaptive selection policy (e.g., UCB or BFS). Feedback consists of trajectories from previous self-play simulations, including visited states, actions taken, estimated win rates, final outcomes, and intermediate values. To avoid processing lengthy trajectories, we select key states that best capture discrepancies between the strategy‚Äôs value heuristic and search-based estimates.\n\nThese key states are translated into natural language and used to prompt the LLM for new improvement ideas. The ideas are added to the idea queue with a prior score estimating their potential effectiveness:\n\n## Idea-queue $\\leftarrow$ LLM-idea-inventor(Sampled strategy, strategy feedback)\n\n#### 2.3.2 Strategy Improvement\n\nIn the strategy improvement step, we sample a strategy $\\sigma$ from the strategy tree and an idea $d$ from the queue, using a method that balances exploration and exploitation (e.g., UCB). The LLM refines $\\sigma$ using $d$, generating a new strategy $\\sigma_{\\text{new}}$. This new strategy is evaluated via self-play simulations, which produce win rates $W[\\sigma]$ and trajectory feedback $\\mathcal{T}[\\sigma]$.\n\nDuring simulations, players conduct MCTS tree searches to estimate win rates at different states, providing additional feedback. The strategy tree is updated with the new strategy and its performance:\n\n## Strategy-library $\\leftarrow$ LLM-reviser(Sampled idea, sampled prior strategy)\n\nThe improvement score of the idea $d$ is updated based on how much it improved $\\sigma$.\n\n#### 2.3.3 Search Modularization\n\nA key feature of our framework is the use of an *idea queue* to modularize the search process. By refining strategies incrementally rather than globally, we avoid confounding factors and ensure interpretability of changes. The queue also tracks successful improvements that are generalizable across strategies, enabling transfer and reuse of ideas. Improvements are often additive (e.g., penalties or adjustments) and enhance performance when applied to similar strategies (Table 2).\n\n## We use UCB sampling to balance exploration of new ideas and exploitation of proven ones:\n\n$UCB(\\text{idea})=\\overline{z}_{\\text{idea}}+c\\sqrt{\\frac{\\ln(N_{\\text{total}})}{N_{\\text{idea}}}}$\n\nwhere $\\overline{z}$ is the empirical average improvement score, $N_{\\text{total}}$ is the total number of ideas implemented, and $N_{\\text{idea}}$ is the number of implementations of the specific idea.\n\nTo simplify the improvement process, we optimize dialogue generation and gameplay moves separately in Avalon before integrating them into a unified agent (Appendix E). This modular approach mirrors strategies used in related domains, such as Diplomacy *(1)*.\n\n### 2.4 Population based self-play simulation\n\nRecent works focus on using either another LLM for critique-based feedback *(Madaan et al., 2024; Shinn et al., 2024)*, real environment interactions *(Nottingham et al., 2024)*, or a combination of both *(Wang et al., 2023a)* during the LLM-improvement process. Our method improves on this by simulating opponents using the learned high-level strategy, enabling higher-quality feedback. Since our high-level strategies can be refined into a policy for any player, we can easily pit different strategies against each other by refining them into specific policies for different players. Specifically, we use population based self-play, where we conduct round-robin games among the top ten strategies generated and use the average performance as the strategy score for each strategy *(Xu et al., 2023b)*. During round-robin games, strategies are given the same level of low-level refinement to ensure fairness. This evolutionary approach makes the final improved strategy more robust to different opponent policies, since it will have survived multiple round-robin tournaments. We demonstrate the effectiveness of this method in section 3.5.\n\n## Published as a conference paper at ICLR 2025\n\n## # 3 EXPERIMENTS\n\nWe evaluate the effectiveness of our bi-level framework through four key experiments: demonstrating STRATEGIST's competitiveness with human players, its superiority over RL-based deep learning methods, its advantage over LLM-based agents, and the effectiveness of our LLM-improvement method compared to other techniques. Our approach is tested on two games with distinct challenges. Game of Pure Strategy (GOPS) is a two-player, zero-sum card game requiring strategic decisions under partial information and simultaneous moves (see B) (Ross, 1971; Lanctot et al., 2009). The Resistance: Avalon is a multi-agent (5+ players), team-based game combining language, deception, and deduction (see A) (Light et al., 2023). These games highlight STRATEGIST's versatility in handling strategic and social complexities.\n\n![img-2.jpeg](img-2.jpeg)\nFigure 3: Action analysis of STRATEGIST and humans when playing against each other in Avalon. STRATEGIST conducts more strategic randomization than humans, making its identity harder to deduce based on actions.\n\n## # 3.1 HUMAN EVALUATIONS\n\nWe conducted human experiments in the six-player Avalon setting, using the following character roles: Merlin, 3 Servants of Arthur, 1 Assassin, and 1 Minion of Mordred. Ten participants were recruited and randomly assigned to one of the roles, playing against five STRATEGIST agents. In total, 30 games were collected and analyzed. As presented in Table 1, STRATEGIST achieved a win rate comparable to human players. After each session, participants completed a survey evaluating the performance of STRATEGIST across seven key metrics, while an experimenter independently assessed human performance. The survey results are visualized in Figure 4, revealing that STRATEGIST outperformed humans in concealment and adaptability to human playstyles, but lagged behind in reasoning, deduction, and cooperation.\n\n## Additionally, we conducted an action analysis to compare the behavior of STRATEGIST and\n\n![img-3.jpeg](img-3.jpeg)\nFigure 4: Human vs STRATEGIST performance metrics in Avalon. We ask humans to evaluate the performance of STRATEGIST and human players in ad-hoc human-AI games. STRATEGIST performs better than humans at concealment, but worse at reasoning and cooperation.\n\nhuman players, shown in Figure 3. Specifically, we examined the distribution of approval votes‚Äîa pivotal action in Avalon. The data revealed that human players exhibited distinct voting patterns based on their roles, facilitating the deduction of their identities. In contrast, STRATEGIST demonstrated strategic randomization, producing more uniform action distributions across roles and thereby making its identity harder to infer. More details on human evaluations can be found in Appendix O.\n\n## Published as a conference paper at ICLR 2025\n\nTable 2: Comparison of different self-improvement methods. The improvement process was run with the same number of strategies generated for each method (40 for GOPS, 24 for Avalon) on the same seed functions, all with GPT3.5. We collect 9 functions from each process and play them against each other (a total of  $5 \\times 9 = 40$  different agents), reporting the average number of points you win over your opponents for GOPS and the average winrate for Avalon. For the dialogue guide, we show the improvement over the baseline seed function, which has a score  $z = -0.875 \\in [-2, 2]$ , a rating from opponents which we describe in G.\n\n|  Self-Improvement Methodology | Line search (Madaan et al., 2024; Shinn et al., 2024) | Greedy search (Ma et al., 2023) | Best First Search (BFS) (Yao et al., 2024) | BFS with thought | STRATEGIST  |\n| --- | --- | --- | --- | --- | --- |\n|  GOPS Value Heuristic | -0.47 ¬±0.74 | -0.54 ¬±0.45 | 0.092 ¬±0.67 | -0.48¬±0.375 | 1.5 ¬±0.99  |\n|  Avalon Value Heuristic | 0.54 ¬±0.11 | 0.47 ¬±0.11 | 0.50 ¬±0.085 | 0.55 ¬±0.065 | 0.59 ¬±0.11  |\n|  Merlin Dialogue Guide | 0.37 ¬±0.19 | 0.62 ¬±0.13 | 0.49 ¬±0.063 | 0.37 ¬±0.06 | 0.88 ¬±0.063  |\n|  Assassin Dialogue Guide | 1.83 ¬±0.25 | 1.81 ¬±0.063 | 1.82 ¬±0.063 | 1.89¬±0.063 | 1.98 ¬±0.042  |\n\nWe further analyzed the proportion of correct votes, defined as rejecting teams with at least one Evil player or approving teams with only Good players. This metric indicates a player's ability to infer Good and Evil identities, a skill particularly crucial for the Merlin role. As shown in Figure 3, STRATEGIST employed increased randomization as Merlin, minimizing information leakage through voting patterns. Conversely, human players displayed highly role-specific correctness trends. These findings align with survey feedback, corroborating that STRATEGIST excels in concealment strategies compared to human counterparts.\n\n![img-4.jpeg](img-4.jpeg)\nFigure 5: Description of STRATEGIST play-style compared to human players, as evaluated by human participants. STRATEGIST is described as being more logical and cautious.\n\n## # 3.2 DIFFERENT LLM IMPROVEMENT METHODS\n\nWe demonstrate the effectiveness of our strategy improvement method by benchmarking it against four other skill-improvement methods. Line search (Madaan et al., 2024) always reflects and improves upon the latest improved strategy. Greedy search (Ma et al., 2023) selects the best strategy from the last generation of improved strategies to improve upon each im\n\nprovement cycle. Best first search (Yao et al., 2024) improves upon the  $k$  best strategies generated in any iteration of each improvement cycle. Best first search with thought enhances BFS by prompting the LLM to reflect, think, and propose ideas on how to improve the previous strategy before improving the strategy itself (Yao et al., 2022b). STRATEGIST is our method that uses an additional idea queue  $Q$  and an idea generation step to guide the improvement process. Comparing BFS with thought and STRATEGIST helps demonstrate the value of having a modularized reflection and idea proposition process that is separate from strategy generation. More details can be found in Appendix N.\n\nOur results, presented in Table 2, use the same feedback method (simulation-based population self-play) while varying the improvement methods, with a constant number of new strategies generated across all methods. Figure 12 (right) shows the gameplay scores of these strategies on GOPS. Even when controlling for the number of output tokens generated by the LLM, our method outperforms the others, as demonstrated in Figure 18. We attribute this to (1) the idea queue, which helps test incremental improvements and guide the search process, and (2) our strategy and idea selection policy, which more efficiently explores the strategy space and escapes local maxima (Figure 12 left).\n\n## # 3.3 INTERACTION BETWEEN LOW-LEVEL REFINEMENT AND HIGH LEVEL STRATEGY SEARCH\n\n## Published as a conference paper at ICLR 2025\n\nWe demonstrate that both our high-level strategy improvement and low-level refinement processes scale effectively with increased budgets by showing how different high-level strategies performance with varying MCTS search budgets. A higher search budget allows for more nodes to be looked ahead before taking actions, resulting in a more refined policy. As shown in Figure 6, strategies improved with more iterations using STRATEGIST exhibit both higher performance and greater performance gains with increased refinement. Policy refinement also scales exceptionally well.\n\n## # 3.4 LLM-IMPROVEMENT VS. REINFORCEMENT LEARNING (RL) BASED METHODS\n\nWe demonstrate the effectiveness of our method compared to traditional RL-based approaches for learning a good policy. Specifically, we show that our method learns a value heuristic function more efficiently than deep RL, as used in AlphaGo, MuZero (Silver et al., 2017; Schrittwieser et al., 2020), and DeepRole (Serrino et al., 2019). While AlphaGo uses MCTS (Kocsis &amp; Szepesv√°ri, 2006) with a deep value network, DeepRole combines counterfactual regret minimization with a deep value\n\nnetwork (Moravƒç√≠k et al., 2017; Zinkevich et al., 2007). We adapt both algorithms to our setting.\n\nAlthough deep RL can approximate the true value function with enough data, training, and a large network, we ensure a fair comparison by (1) limiting both RL methods and our approach to the same number of simulated episodes, and (2) capping the number of training steps after each batch of episode trajectory data. While both RL methods and STRATEGIST use the same number of self-play episodes, STRATEGIST only trains on a small subset of transition steps, whereas RL methods train on the entire dataset. Our results, shown in Table 3 and Figure 7, demonstrate that STRATEGIST consistently outperforms both AlphaGo and DeepRole. More details on our RL implementation and the effects of increased sample size can be found in Appendix L, where we observe performance improvements with additional samples.\n\nTable 3: Comparison of reinforcement learning vs our method. We run each process 10 times, taking the best strategy generated by each run and playing them against each other for 1024 games. Average win-rates, total number of self-play episodes, and training transition steps across best generated strategies are shown here.\n\n|  Setting | Metric | VS Alpha-go |   | VS DeepRole  |   |\n| --- | --- | --- | --- | --- | --- |\n|   |   |  Alpha-go | STRATEGIST | DeepRole | STRATEGIST  |\n|  GOPS | Point difference | -0.39 ¬± 0.22 | 0.33 ¬± 0.38 | -0.56 ¬± 0.18 | 1.14 ¬± 0.09  |\n|   |  Self-play eps. | 320 episodes | 320 episodes | 320 episodes | 320 episodes  |\n|   |  Trans. steps | ~ 3,840 steps | 100 steps | ~ 3,840 steps | 100 steps  |\n|  Avalon | Winrate | 0.30 ¬± 0.03 | 0.38 ¬± 0.04 | 0.33 ¬± 0.04 | 0.44 ¬± 0.02  |\n|   |  Self-play eps. | 160 episodes | 160 episodes | 160 episodes | 160 episodes  |\n|   |  Trans. steps | ~ 24,000 steps | 60 steps | ~ 24,000 steps | 60 steps  |\n\n## Published as a conference paper at ICLR 2025\n\n## # 3.5 FEEDBACK QUALITY AND REWARD SIGNAL\n\nWe benchmark our feedback method (population based self-play) against (1) an LLM-critic (Madaan et al., 2024) and (2) trajectory feedback from a fixed opponent policy, with results in Table 4 showing superior performance in both GOPS action planning and dialogue generation. This shows the effectiveness of our evolutionary population based self-play approach, and that of our strategy abstraction that learns a strategic profile for all players.\n\nTable 4: Comparison of different methods of collecting feedback. All methods use the same high-level improvement process (STRATEGIST). For GOPS we collect 24 generated functions from each method and play them against each other. For Avalon we evaluate 9 generated guides. We simulated 32 games for GOPS and 4 rounds of discussion for Avalon to collect gameplay feedback during each improvement step, across 10 and 6 improvement rounds for GOPS and Avalon respectively.\n\n|  Setting/Method | Metric | LLM-critic | Fixed opponent | Population-based Self-play  |\n| --- | --- | --- | --- | --- |\n|  GOPS | Point difference | -0.27 ¬± 1.1 | 0.089 ¬± 0.86 | 0.87 ¬±1.5  |\n|   |  # of opponents | 0 | 1 | 4  |\n|  Avalon | Winrate | 0.37 ¬± 0.063 | 0.62 ¬± 0.13 | 0.88 ¬±0.06  |\n|   |  # of opponents | 0 | 1 | 4  |\n\n## # 3.6 STRATEGIST VS. LLM AGENTS\n\nWe demonstrate that our method also achieves better performance against other LLM-agents. ReAct is a popular LLM-agent that prompts the LLM to think before taking an action (Yao et al., 2022b). ReCon is a LLM-agent for hidden identity games that prompts the LLM to recursively contemplate on what opponents are thinking and might do before taking actions (Wang et al., 2023b). We adapt ReCon to our setting by asking the agent to recursively contemplate. Our gameplay results are shown in Table 5. This suggests that through STRATEGIST, our LLM-agent is able to learn high-level strategies similar in performance to those of ReCon, such as recursive contemplation.\n\nTable 5: Results of STRATEGIST playing against LLM-based baselines, i.e., ReAct and ReCon.\n\n|  Metric | VS ReAct |   | VS ReCon  |   |\n| --- | --- | --- | --- | --- |\n|   |  ReAct | STRATEGIST | ReCon | STRATEGIST  |\n|  Winrate | 47.5 ¬± 2.5 | 52.5 ¬± 2.5 | 38.9 ¬± 5.5 | 61.1 ¬± 5.5  |\n|  #Tokens per round | 56 ¬± 14.3 | 164.3 ¬± 27.7 | 245.7 ¬± 21.2 | 248.2 ¬± 24.1  |\n\n## # 4 RELATED WORK\n\nLLMs for Text Agents. Large language models (LLMs) demonstrate emergent capabilities such as zero-shot prompting, reasoning, and extensive world knowledge (Bommasani et al., 2021; Brown et al., 2020; Wei et al., 2022a; Yu et al., 2023a). Recent frameworks like ReAct (Yao et al., 2023) and Reflexion (Shinn et al., 2024) have incorporated reasoning, memory, feedback, and tool use to improve decision-making (Wang et al., 2023a; Huang et al., 2022; Schick et al., 2024). Prompting techniques like Chain-of-Thought and Tree-of-Thought are effective for reasoning (Wei et al., 2022b; Yao et al., 2024) but fall short in complex games requiring iterative self-improvement. Our method, STRATEGIST, introduces a bi-level tree search combining high-level planning and self-play for feedback-driven strategy refinement.\n\nSkill Learning with LLMs. LLMs have been used to acquire skills by learning textual memories or insights (Shinn et al., 2024; Zhao et al., 2024). However, textual approaches struggle with long, quantitative trajectories. We focus on high-level strategies optimized through simulational self-play, enabling robust skill learning in multi-agent environments. While reward models learned by LLMs\n\n## Published as a conference paper at ICLR 2025\n\nexcel in single-agent tasks (Ma et al., 2023; Yu et al., 2023b), we extend these ideas to multi-agent settings by introducing methods for feedback generation and strategy improvement.\n\nAI in Strategy Games. Advances like AlphaGo and MuZero highlight the synergy of MCTS, deep learning, and self-play (Silver et al., 2017; Schrittwieser et al., 2020). LLMs have also been integrated into dialogue-based games such as Diplomacy (FAIR) and Texas Hold'em (Zhang et al., 2024). Our approach bridges these domains by enabling LLMs to both train value heuristics more efficiently than RL and generate dialogue for social deduction games without human examples. Additionally, our method has potential applications in negotiation tasks (Abdelnabi et al., 2023; Fu et al., 2023).\n\nLLM-agents for discussion games. There has been much recent interest in using LLMs to develop agents for discussion-based games, as these games provide an excellent benchmark for assessing the reasoning, planning, and strategic deception capabilities of LLMs. This includes Werewolf (Bailis et al., 2024; Xu et al., 2023b;a; Wu et al., 2024), another social deduction game, and other negotiation and word games (Fu et al., 2023; Cheng et al., 2024). We give a more detailed comparison of our work to other AI agents in Table 6 and App. P.\n\n|  Metric | EnReaWolf | Cicero | ComWolf | LARLWolf | SPAG | AlphaGo | ICL-AIF | Agent-Pro | ReCon | DeepRole | Strategist  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  Used in Avalon | X | X | X | X | X | X | X | X | ‚úì | ‚úì | ‚úì  |\n|  Parameter-free | X | X | ‚úì | X | X | X | ‚úì | ‚úì | ‚úì | X | ‚úì  |\n|  Uses LM | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì | X | ‚úì | ‚úì | ‚úì | X | ‚úì  |\n|  Self-improve | ‚úì | ‚úì | X | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì | X | ‚úì | ‚úì  |\n|  Tree Search | X | ‚úì | X | X | X | ‚úì | X | X | X | ‚úì | ‚úì  |\n|  Human-annotation free | X | X | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì  |\n|  Belief updates | X | X | X | X | X | X | X | ‚úì | X | ‚úì | ‚úì  |\n|  Bi-level improvement | X | X | X | X | X | X | X | X | X | X | ‚úì  |\n\nTable 6: Comparison of Strategist with Related Works. We use an bi-level improvement approach, paired with an advanced high-level strategy learning process that is parameter free. Our approach handles beliefs and partial observability and uses low-level policy refinement (tree-search) to further enhance our policy. For details see App. P\n\n## # 5 CONCLUSION\n\nIn conclusion, we have introduced STRATEGIST, a bi-level framework that facilitates high-level abstract strategy learning with LLMs and a generalizable non-parametric self-improvement mechanism. This approach enables the model to learn and refine skills autonomously. Without relying on task-specific prompts or human-generated policy data, our method demonstrates its ability to learn effective strategies through population-based self-play, guided solely by the rules of the game.\n\nThe performance of STRATEGIST highlights the potential of leveraging LLMs for generating high-level strategic abstractions, while using low-level tree search to iteratively refine the policy. This dual approach not only showcases the power of modular guidance‚Äîwhether through high-level strategy exploration or low-level self-play feedback‚Äîbut also underlines the value of integrating these processes for accelerated and robust skill acquisition in LLMs.\n\nFurthermore, our results suggest broader implications for the development of LLM-driven decision-making agents across a range of complex domains. By embedding flexible and adaptive learning mechanisms, our framework paves the way for more autonomous systems capable of mastering tasks with minimal human intervention. This opens new avenues for the application of LLMs in areas such as multi-agent systems, reinforcement learning, and general AI, where strategic planning and self-improvement are key.\n\n## References\n\n- Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Sch√∂nherr, and Mario Fritz. Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games. arXiv preprint arXiv:2309.17234, 2023.\n- Suma Bailis, Jane Friedhoff, and Feiyang Chen. Werewolf arena: A case study in llm evaluation via social deduction. arXiv preprint arXiv:2407.13943, 2024.\n- Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R√©, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram√®r, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. arXiv preprint arXiv: Arxiv-2108.07258, 2021.\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n- Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang, Yong Dai, Lei Han, and Nan Du. Self-playing adversarial language game enhances llm reasoning. arXiv preprint arXiv:2404.10642, 2024.\n- Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 36, 2024.\n- Meta Fundamental AI Research Diplomacy Team (FAIR)‚Ä†, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067‚Äì1074, 2022.\n- Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation with self-play and in-context learning from ai feedback. arXiv preprint arXiv:2305.10142, 2023.\n- Jean-Bastien Grill, Florent Altch√©, Yunhao Tang, Thomas Hubert, Michal Valko, Ioannis Antonoglou, and R√©mi Munos. Monte-carlo tree search as regularized policy optimization. In International Conference on Machine Learning, pp. 3769‚Äì3778. PMLR, 2020.\n\n## Published as a conference paper at ICLR 2025\n\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv: Arxiv-2207.05608, 2022.\nSubbarao Kambhampati. Can large language models reason and plan? Annals of the New York Academy of Sciences, 1534(1):15‚Äì18, 2024.\nSubbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy. Llms can‚Äôt plan, but can help planning in llm-modulo frameworks. arXiv preprint arXiv:2402.01817, 2024.\nLevente Kocsis and Csaba Szepesv√°ri. Bandit based monte-carlo planning. In European conference on machine learning, pp. 282‚Äì293. Springer, 2006.\nMarc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte carlo sampling for regret minimization in extensive games. Advances in neural information processing systems, 22, 2009.\nJonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. From text to tactic: Evaluating llms playing the game of avalon. arXiv preprint arXiv:2310.05036, 2023.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023.\nYecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931, 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024.\nMatej Moravƒç√≠k, Martin Schmid, Neil Burch, Viliam Lis·ª≥, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508‚Äì513, 2017.\nKolby Nottingham, Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Sameer Singh, Peter Clark, and Roy Fox. Skill set optimization: Reinforcing language model behavior via transferable skills. arXiv preprint arXiv:2402.03244, 2024.\nJulien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T Connor, Neil Burch, Thomas Anthony, et al. Mastering the game of stratego with model-free multiagent reinforcement learning. Science, 378(6623):990‚Äì996, 2022.\nSheldon M Ross. Goofspiel‚Äîthe game of pure strategy. Journal of Applied Probability, 8(3): 621‚Äì625, 1971.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604‚Äì609, 2020.\nJack Serrino, Max Kleiman-Weiner, David C Parkes, and Josh Tenenbaum. Finding friend and foe in multi-agent games. Advances in Neural Information Processing Systems, 32, 2019.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.\n\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C√¥t√©, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020.\n- Silver et al. (2017) David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354‚Äì359, 2017.\n- Valmeekam et al. (2018) Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n- Thorne et al. (2023) James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018.\n- Valmeekam et al. (2023) Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36:75993‚Äì76005, 2023.\n- Wang et al. (2023) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.\n- Wang et al. (2024) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024.\n- Wang et al. (2022) Ruoyao Wang, Peter Jansen, Marc-Alexandre C√¥t√©, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? arXiv preprint arXiv:2203.07540, 2022.\n- Wang et al. (2023) Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon‚Äôs game of thoughts: Battle against deception through recursive contemplation. arXiv preprint arXiv:2310.01320, 2023b.\n- Wei et al. (2023) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. arXiv preprint arXiv: Arxiv-2206.07682, 2022a.\n- Wei et al. (2022a) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.\n- Contributors. Goofspiel ‚Äî Wikipedia, the free encyclopedia, 2023. URL https://en.wikipedia.org/w/index.php?title=Goofspiel&oldid=1174471596. [Online; accessed 22-May-2024].\n- Wu et al. (2024) Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, and Haobo Fu. Enhance reasoning for large language models in the game werewolf. arXiv preprint arXiv:2402.02330, 2024.\n- Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.\n- Xu et al. (2023) Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023a.\n- Xu et al. (2023) Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with reinforcement learning for strategic play in the werewolf game. arXiv preprint arXiv:2310.18940, 2023b.\n- Yang et al. (2023) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.\n-\n\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744‚Äì20757, 2022a.\n- Shunyu Yao et al. (2022a) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022b.\n- Shunyu Yao et al. (2022b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2023.\n- Shunyu Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.\n- Jifan Yu et al. (2023a) Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, et al. Kola: Carefully benchmarking world knowledge of large language models. arXiv preprint arXiv:2306.09296, 2023a.\n- Wenhao Yu et al. (2023b) Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. Language to rewards for robotic skill synthesis. arXiv preprint arXiv:2306.08647, 2023b.\n- Wenqi Zhang et al. (2024) Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. Agent-pro: Learning to evolve via policy-level reflection and optimization. arXiv preprint arXiv:2402.17574, 2024.\n- Andrew Zhao et al. (2024) Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 19632‚Äì19642, 2024.\n- Shuyan Zhou et al. (2023) Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023.\n- Martin Zinkevich et al. (2023) Martin Zinkevich, Michael Johnson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. Advances in neural information processing systems, 20, 2007.\n\n## Published as a conference paper at ICLR 2025\n\n## # A RESISTANCE: AVALON GAME DESCRIPTION\n\n![img-5.jpeg](img-5.jpeg)\nFigure 8: The three phases per round of Resistance game. Good players are shown in blue, while Evil players in red. In Selection Phase, the team leader (player 5 in this round) proposes a team (player 1 and 5, himself). In Voting Phase, all players vote publicly whether to approve this team or not. If the strict majority votes yes, the team is approved and moves on to the mission phase. Otherwise, redo the Selection Phase with the next player as leader. If the team goes on the Mission Phase, selected team members (player 1 and 5) anonymously vote to pass or fail the mission. If at least one person (player 1, as he is the evil player) votes fail, the mission fails. Otherwise, it succeeds.\n\nWe describe the game in more detail here. There are four phases in the game where players need to make decisions: (1) team selection phase, (2) voting phase, (3) quest phase, and (4) assassination phase. The game alternates between the first three phases until the end condition is reached, at which point we move on to the assassination phase. Each phase also contains discussion where players can challenge others, defend themselves, and negotiate. A flowchart of the game is presented in Figure 10, and an Avalon Rule Prompt is included in Section A.4.\n\n## # A.1 ROLES\n\nThere are four basic roles in Resistance Avalon: Servant of Arthur, Minion of Mordred, Merlin, and Assassin. The Servant is a basic good character who does not know the identity of any of the other players. The Minion is a base evil character who knows who is good and evil but does not know the specific roles of each player. Merlin is a unique good character who knows who is good and evil. The Assassin is a unique evil character who knows who is good and evil, and in addition, has the ability to assassinate a character at the end of the game. If that character is Merlin, the evil team wins.\n\nGood players will always outnumber evil players. Hence, evil players must pretend to be good in order to be voted in on teams (and thus sabotage missions). SERVANTS will thus need to sniff out the evil players through their actions and dialogue. MERLIN is usually the only good player with additional information, so they will need to discreetly guide the SERVANTS in the right direction. Servants also need to protect MERLIN, so a common strategy is for SERVANTS to pretend to have hidden information so that evil players will think that they are MERLIN. Evil players will be trying to sniff out MERLIN at the same time, so deduction skills are required for all roles.\n\n## # A.2 ACTIONS FOR EACH PHASE\n\nDepending on the phase team selection, voting, quest, and assassination, players may conduct different actions. We detail the specific actions that players can take in each of these phases below.\n\nDuring the team selection phase, only the current leader has to make a choice. Leadership passes around the players sequentially in a loop. The action space of team selection for the leader consists of all subsets of the players with size equal to the mission team size. The mission team size is different\n\n## Published as a conference paper at ICLR 2025\n\n![img-6.jpeg](img-6.jpeg)\nFigure 9: Communication Skills required to play Avalon. 1) First, they use logical reasoning to analyze the voting pattern and dialogue of other players and deduce their motives. 2) they must coordinate, communicate, and persuade their teammates to follow a particular strategy. 3) they must also hide their identity and motives through deception.\n\n![img-7.jpeg](img-7.jpeg)\nFigure 10: Flowchart illustrating the various game states and transition diagram. Round boxes indicate game states (phases) where the player (role highlighted in bracket) has to make decisions\n\nfor each mission and is determined by the total number of players in the game. For example, in a 5-player game, on mission No.4, the mission team size is 3, so any subset of  $\\{1,2,3,4,5\\}$  with size 3 would be a valid action. After the team proposal is determined by the leader, we move on to the voting phase with the selected players.\n\nDuring the voting phase, every player in the game needs to simultaneously vote either APPROVE (1) or REJECT (0). Votes are publicly revealed to all players, so players can see what other players voted. If a strict majority votes APPROVE (1), we then move on to the quest phase with the team that was approved. Otherwise, we move back to the selection phase. Note that if four teams have been rejected in a row, and this is the fifth time a team is proposed (for the same mission), we skip the voting and move directly to the quest phase. This prevents the game from dragging on forever.\n\nDuring the quest phase, each selected player on the approved team votes anonymously to either PASS (1) or FAIL (0) the mission. The number of votes of PASS vs FAIL are then revealed to everybody. If the number of FAILs is greater than or equal to the number of FAILs required for the mission to fail (usually 1), then this mission is marked as a failure. Otherwise, this mission is marked as a success. Hence, good players usually have no incentive to fail missions, while evil players will want to have enough failures to pass the failure threshold. If three out of five missions fail, evil wins immediately. Otherwise, if three out of five missions succeed, we move on to the assassination phase.\n\n## # A.3 DISCUSSION\n\nGroup discussion occurs between the quest and selection phases, as well as right before the assassination phase. Players may not communicate during any other time. All conversations are public, and there is no private communication. Typically players may discuss in any format of their choosing as long as only one person is speaking at a time. Some examples of formats include a\n\nnatural (spontaneous) seminar style (most common, where there is no fixed order of speaking), or sequentially (where players speak in some predefined order). Interruptions and arguments between two players are very common between human players.\n\nUsually, players will spend this time discussing a couple of key topics, including (1) the observations they made, (2) the guessed identities and sides of players, and (3) the plan for the next mission. The team leader will usually spend this time asking for advice on what team to select and gathering support for that team. Persuasion and adhering to the preferences of other players are usually key to getting a team approved. Players can also accuse other players of being evil, though arguments will need to be justified in order to be persuasive.\n\nFor example, a player (player 3) could start off by stating their (1) observations of what happened in the previous mission. One Fail was observed, so at least one player on the previous team (consisting of players (1,2,3)) is evil. Player 3 then emphasizes that both Players 1 and 2 voted Approve for the previous mission, which ended up a failure. Moreover, the team was proposed by Player 1 in the first place. Player 3 then moves on to discuss the (2) identities of other players. The player says that, despite the fact that only one Fail was observed, both Players 1 and 2 are evil since they both voted to Approve previously. Player 0 is probably good since they voted to Reject in the previous mission, and Player 3 is also good since they also voted to Reject, even though they were on the mission. Player 3 then says what they think the (3) plan should be. Specifically, Player 3 says that they should reject the current team no matter what since Player 2 is the leader and is evil. The leadership will then pass to Player 3, who will choose the team $(0,3,4)$, which good players should vote to approve since it does not contain any suspected evil players.\n\n### A.4 Game Ending and Assassination\n\nIn classic Resistance, a good team wins immediately if three missions are successful. In Resistance Avalon, there is an additional assassination phase if three missions are successful. During the assassination phase, the Assassin player chooses one player to assassinate. If that player is Merlin, then evil wins. Otherwise good wins.\n\nBefore they assassinate a player, the Assassin player can and is encouraged to discuss with the other players (mostly their teammates). good players are also welcome to join in on this discussion to mislead the evil players, though it rarely helps. Players can discuss in a format of their choosing, though there is usually a time limit on how long players can discuss before reaching a decision.\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 2,
      "title": "Avalon rules prompt",
      "content": "# Avalon rules prompt\n\nThe game you are interested in is called The Resistance: Avalon. The Resistance: Avalon is the game of hidden identities and social deduction. There are two teams in the game: Good and Evil. Each player has a hidden identity (role) and side.\n\nThere are five Quests in the game and five turns, one for each quest. Good players aim to help three Quests succeed, while Evil players aim to fail three Quests. Different quests require different numbers of players to participate.\n\nAt the beginning of the game, each player is assigned a role secretly and randomly. Private information is then revealed to each player. A random player is selected as the leader for the first round.\n\nEach round, after a round of discussion, the leader will select a team of players to participate in the Quest. Then, all players will vote on whether to approve or reject the team publicly. If the team is approved (a strict majority vote to approve), the Quest will be carried out. If the team is not approved, the next player becomes the leader and the next round will start. If four teams are rejected in a row, the fifth team will automatically be approved.\n\nIf the team is approved, each team member chooses to pass or fail the Quest anonymously. Usually, if there is at least one failed vote, the Quest fails. Otherwise, the Quest succeeds. In either case, we move on to the next turn and the next quest.\n\n## Below are the roles in the game:\n\nServant of Arthur (Servant): A Good player who does not know who is on the Evil side. The Servant's job is to make sure that the three Quests succeed.\n\nMinion of Mordred (Minion): An Evil player who knows who is on the Evil side. Minion's job is to fail three Quests without being identified by the Good players.\n\nMerlin: A Good player who knows who is on the Evil side. Merlin's job is to make sure that the three Quests succeed without revealing themself to Evil.\n\nAssassin: An Evil player who knows who is on the Evil side. Assassin's job is to assassinate Merlin if the Evil players can identify who Merlin is. If the Assassin successfully assassinates Merlin, the Evil players win the game immediately, even if three quests succeed.\n\nHence, Evil players usually know who is on the Evil side, but Good players usually do not know who is on the Evil side.\n\nPlayers may make any claims during the game, at any point in the game. Discussion, deception, accusation, persuasion, and logical deduction are all equally important in order for Good to prevail or Evil to rule the day. Hence, players should rarely reveal their true identity to other players. Players will, can, and should lie to achieve their goals.\n\n## B Game of Pure Strategy (GOPS) Game Description\n\nGame of Pure Strategy (GOPS) is a card game for two or more players with a standard deck of card, which is commonly used as an example of multi-stage move game in artificial intelligence (*Wikipedia* contributors (2023)). In our experiments we play 5 or 6 card GOPS. Specifically, the score cards are $\\{1,2,...n\\}$ and each player starts with a hand of cards $\\{1,2,...n\\}$ where $n$ is the number of cards and rounds. The GOPS rules prompt is included in this section below.\n\n[GOPS rules prompt] The game you want to write a function for is GOPS (game of pure strategy), also known as Goofspiel. The game has two players, and is played with a deck of score cards. Each player is dealt the same hand of cards at the beginning. The goal of the game is to get a score higher than your opponent. At the beginning of each round, a score card is randomly drawn without replacement from the score deck. Then each player plays a card simultaneously from their hand. The player who plays the higher card wins the round and gets the score card. They add the score of the score card to their total score. If the two cards played are the same, the person who wins the next round will get both score cards. The game continues until all score cards have been played. The player with the highest total score wins the game.\n\n## Appendix C Limitations\n\nWhile our method performs better on average, individual runs can have high variance. Since the performance of an agent in multi-agent adversarial game settings is highly dependent on opponents‚Äô policies, feedback from these environments tend to be highly noisy, with noise increasing with the number of players. This is especially true when learning Avalon heuristics, where the performance depends on the policies of 5 other players, teammates and opponents. We believe that running more game simulations with different opponent policies can help reduce this feedback noise. We also acknowledge the inherent noisiness in LLM generations and how that can impact our results. We tried to reduce this noise by (1) using the same seed functions when benchmarking the different LLM improvement methods and (2) collecting generated strategies from multiple runs. We also did not test our method on other non-adversarial environments such as question answering and text-based worlds. However, given the strong performance of our method in adversarial multi-agent settings, we believe that similar performance will be observed in single agent, non-adversarial settings.\n\n## Published as a conference paper at ICLR 2025\n\n![img-8.jpeg](img-8.jpeg)\n## Strategist\nFigure 11: Overview of our high-level strategy improvement method: The process alternates between two steps‚Äîidea generation and strategy improvement.\n\n## # D IMPROVEMENT PROCESS IMPLEMENTATION DETAILS\n\nOur method alternates between two main steps: idea generation and strategy implementation, as illustrated in Figure 11. These steps are structured around the optimization of strategies stored in a hierarchical tree structure  $T$ . Below, we detail the key components of our implementation.\n\n## # D.1 STRATEGY TREE AND IDEA QUEUE\n\nThe strategy tree  $T$  maintains all discovered strategies, their associated feedback  $(\\tau_{s})$ , and priority scores  $(z_{s})$ , which determine their likelihood of being selected for further refinement. Complementing this is the idea queue  $Q$ , a priority-based queue that stores candidate ideas generated during the improvement process. Each idea  $d \\in Q$  is initialized with a prior score  $z_{d} = 0.0$  and a usage count  $n_{d} = 0$ .\n\n## Published as a conference paper at ICLR 2025\n\n## Algorithm 1: STRATEGIST Pseudocode\nData:  $T$  : strategy tree storing strategy  $s$  , feedback  $(\\tau_{s})$  , and priority score  $(z_{s})$ $Q$  : idea queue, 'seed functions',  $N_{ideas}$  : number of ideas,  $N_{strategies}$  : number of strategies,  $N_{evolutions}$  : number of evolutions,  $N_{feedback\\_examples}$  : number of states of give as feedback,\nFunction select_strategy  $(T)$  ..\n$\\sigma_{\\mathrm{best}}\\gets \\arg \\mathrm{softmax}z_{\\sigma} / /$  one possible implementation where you take one  $\\sigma \\in T_2$  of the best two strategies in the whole tree randomly (BFS2) return  $n_{best}$\nFunction select_idea  $(Q,\\sigma)$  ..\n$d_{\\mathrm{best}}\\gets \\mathrm{softargmax}_{d\\in Q}UCB(z_d,n_d) / /$  one possible implementation where you take the best strategy in the queue using softmax UCB,  $z_{d}$  being the empirical  $q$  -value and  $n_d$  being the number of tries return  $\\sigma_{best}$\nFunction select_key_states  $(\\tau_{\\sigma})$  ..\n$K_{\\sigma}\\gets \\arg \\max_{k}(\\mathrm{SearchEstimate}(s) - v_{\\sigma}(s))^{2} / /$  one possible way to select key states for  $\\sigma$  that is a value heuristic  $v_{\\sigma}$  return  $K_{\\sigma}$\nFunction generate_ideas  $(N_{ideas})$  ..\n$\\sigma \\gets$  select_strategy  $(T)$  .\n$K_{\\sigma}\\gets$  select_key_states  $(\\tau_{\\sigma}) / / K_{\\sigma}$  is a set of key states from the trajectory feedback  $\\tau_{\\sigma}$  for strategy  $\\sigma$ $D_{\\mathrm{new~ideas}}\\gets$  LLM(Generate  $N_{ideas}$  new ideas based on string description of  $K_{\\sigma}$  , which includes the output of the strategy, action taken, state description, final outcome of the trajectory, search estimate of the state, and any intermediate values used to compute the output of the strategy); for  $d\\in D_{\\mathrm{new~ideas}}$  do Store  $d$  in  $Q$  with prior score  $z_{d} = 0.0$  and  $n_d = 0$  end\nFunction implement_strategies  $(N_{strategies})$  ..\n$\\Sigma_{\\mathrm{new}},D,P = []$  ,{}// list of new generated strategies, dictionary mapping new generated strategy to the idea that generated it, and dictionary mapping generated strategies to their parents for  $i\\gets 1$  to  $N_{strategies}$  do\n$\\sigma \\gets$  select_strategy  $(T)$  .\n$d\\gets$  select_idea  $(Q,\\sigma)$  .\n$\\sigma_{\\mathrm{new}}\\gets$  LLM(Improve  $\\sigma$  using  $d$  .\n$\\Sigma_{\\mathrm{new}}$  .append  $(\\sigma_{\\mathrm{new}})$  .\n$D[\\sigma_{\\mathrm{new}}] = d$  .\n$P[\\sigma_{\\mathrm{new}}] = \\sigma$  .\nend\n$W,T\\gets$  SelfplaySimulate(  $\\Sigma_{\\mathrm{new}}\\cup$  unique(P.values))// simulate games, getting average winrates  $W[\\sigma ]$  for each strategy  $\\sigma$  and simulated trajectory feedback  $\\mathcal{T}[\\sigma ]$\nfor  $\\sigma \\in \\Sigma_{\\mathrm{new}}$  do\n$T.\\mathrm{add}(\\sigma ,P[\\sigma ],D[\\sigma ]) / /$  add new strategy to tree from parent based on idea\n$z_{\\sigma}\\gets W[\\sigma ] / /$  add function score\n$z_{D[\\sigma ]}\\leftarrow \\frac{n_{D[\\sigma]}}{n_{D[\\sigma] + 1}} z_{D[\\sigma ]} + \\frac{1}{n_{D[\\sigma] + 1}} (W[\\sigma ] - W[P[\\sigma ]]) / /$  update idea score with how much it improved the strategy by\nend\nrepeat\ngenerate_ideas  $(N_{ideas})$  .\nimplement_strategies  $(N_{strategies})$  .\nuntil  $N_{evolutions}$  .\nreturn Best strategies in  $T$  according to their scores  $z_{s}$\n\n## # D.2 KEY FUNCTIONS\n\nSelecting Strategies (SelectStrategy) To identify promising strategies for refinement, we use a softmax over the priority scores  $z_{s}$  stored in  $T$ . One implementation chooses between the two highest-priority strategies in a breadth-first search order.\n\n#### Selecting Ideas (SelectIdea)\n\nIdeas are selected from $Q$ using an Upper Confidence Bound (UCB) approach. Each idea‚Äôs selection balances its empirical improvement potential ($z_{d}$) with its exploration term $n_{d}$, ensuring both promising and underexplored ideas are considered.\n\n#### Selecting Key States (SelectKeyStates)\n\nKey states $K_{\\sigma}$ for a given strategy $\\sigma$ are selected based on a heuristic that emphasizes discrepancies between a search-based estimate and the strategy‚Äôs value function $v_{\\sigma}$. These states highlight areas where the strategy can be improved.\n\n### D.3 Idea Generation\n\n## During the idea generation step, the algorithm:\n\n## 1. Selects a strategy $\\sigma$ from $T$ using SelectStrategy.\n2. Extracts key states $K_{\\sigma}$ from the trajectory feedback $\\tau_{\\sigma}$ using SelectKeyStates.\n3. Uses a language model (LLM) to propose $N_{\\text{ideas}}$ new ideas based on the string description of $K_{\\sigma}$. This description includes the strategy‚Äôs actions, state details, trajectory outcomes, search estimates, and intermediate values.\n## 4. Adds the generated ideas to $Q$ with default priority and usage counts.\n\n### D.4 Strategy Implementation\n\n## In the strategy implementation step, the algorithm:\n\n1. Selects a strategy $\\sigma$ from $T$ and an idea $d$ from $Q$ using SelectStrategy and SelectIdea.\n## 2. Refines $\\sigma$ using $d$, producing a new strategy $\\sigma_{\\text{new}}$.\n3. Simulates self-play games for $\\sigma_{\\text{new}}$ and related strategies to evaluate performance (win rates $W[\\sigma]$) and collect trajectory feedback ($\\mathcal{T}[\\sigma]$).\n4. Updates the strategy tree $T$ with $\\sigma_{\\text{new}}$, linking it to its parent strategy and the idea that inspired it.\n5. Updates the priority scores $z_{\\sigma}$ for strategies and $z_{d}$ for ideas based on the observed improvements.\n\n### D.5 Iterative Evolution\n\nThe improvement process repeats for $N_{\\text{evolutions}}$ iterations. In each iteration, new ideas and strategies are generated and evaluated, incrementally building a tree of optimized strategies.\n\n### D.6 Scalability and Efficiency\n\nOur method leverages efficient UCB-based selection and hierarchical feedback propagation to scale to large $T$ and $Q$. By iteratively refining strategies through targeted improvements, it ensures continuous enhancement while maintaining computational feasibility.\n\n## Published as a conference paper at ICLR 2025\n\n![img-9.jpeg](img-9.jpeg)\nFigure 12: Left: Example performance of value heuristics strategy tree for GOPS. Points indicate the final evaluation scores and the generation of improved functions, with lines showing the evolutionary path between functions. Our method successfully escapes local maxima and continues exploring the strategy space. These scores reflect final gameplay results against a fixed set of opponents, differing from the intermediate self-play scores used to guide strategy improvements. Right: Comparison of different improvement methods on 6-card GOPS, where 9 functions generated by each method play against one another over 1024 total games.\n\n![img-10.jpeg](img-10.jpeg)\n\n## # E AVALON AGENT IMPLEMENTATION DETAILS\n\nWe describe in detail how we implement our model below and as shown in figure 13. Unless otherwise specified, the word 'action' will refer to non-dialogue actions. Note that we do not conduct search over raw dialogue space since that is not very computationally feasible. Instead, we search over intended actions and condition our dialogue on that.\n\nSpecifically, the language component consists of a dialogue analyzer and a dialogue generator, while the moves component consist of the action planner. Whenever the agent needs to speak, they first analyze what was said so far in the current discussion round using the dialogue analyzer. The dialogue analyzer, with the help of an LLM, updates the internal beliefs of the agent. For example, in Avalon, internal beliefs might include the probability that the agent assigns to each other player of being Evil and of being Merlin. These beliefs are then passed to the action planner, which uses them to figure out the best next move, i.e. the action intent. The action intent is then passed to the dialogue generator, which generates dialogue with the help of an LLM. When the agent needs to take a move, we run through the same process except that the agent takes the action intent as the move and no dialogue is generated.\n\n## # E.1 DIALOGUE ANALYZER (DISCRIMINATOR)\n\nThe dialogue analyzer  $f_{ana}$  takes as input  $I$  information set (partial information) of the current state for the player,  $d_t$  the discussion so far this round, and  $b$  some prior beliefs about the hidden state of the game, and returns  $\\widehat{b}$ , the updated beliefs, and  $\\widehat{\\Pi}_t$ , the predicted joint action policy of the all the players (i.e. the action intent) for the next action step  $t$ . Recall that simultaneous games can be expanded as partial information games, where the simultaneous moves are treated as hidden information. Hence, we are essentially predicting a distribution over the hidden states  $s$  given the information set  $I$  using the dialogue analyzer.\n\n$$\n\\widehat {\\boldsymbol {b}}, \\widehat {\\boldsymbol {\\Pi}} _ {t} = f _ {a n a} (\\boldsymbol {I}, \\boldsymbol {d} _ {t}, \\boldsymbol {b})\n$$\n\nIn the context of Avalon,  $\\pmb{I}$  will contain information such as (1) the dialogue this round so far (2) summary of the dialogue from previous rounds (3) mission track record (4) historical record of actions taken by players in previous rounds, and (5) private information of the player such as who is Good and Evil.  $\\pmb{b}$  will contain information on (1) the probability of each player being Evil and (2)\n\n## Published as a conference paper at ICLR 2025\n\n![img-11.jpeg](img-11.jpeg)\nFigure 13: Overview of the LLM-powered agent, including the three main modules that we use to generate dialogue during discussion\n\nthe probability of each player being Merlin, both conditioned on the private information contained in  $\\pmb{I}$ . While a full treatment of the distribution over the hidden state space  $S$  we require assigning probabilities to each possible combination of Good and Evil players, not just assessing the marginal probability of each player being Good individually, in practice\n\nWe implement  $f_{ana}$  using an LLM, which is fed  $I$ ,  $d$ ,  $b$  (converted to natural language form) as prompts, along with some instruction prompt  $\\phi_{ana}$  that prompts it to produce  $\\widehat{b}, \\widehat{\\Pi}_t$ . Specifically,\n\n$$\nf _ {a n a} (\\boldsymbol {I}, \\boldsymbol {d} _ {t}, \\boldsymbol {b}) = f _ {L L M} (\\phi_ {d i s}, \\boldsymbol {I}, \\boldsymbol {d}, \\boldsymbol {b})\n$$\n\nWe show examples of such prompts in Appendix J.\n\n## # E.2 ACTION PLANNER\n\nGiven  $\\widehat{\\pmb{b}}$  the belief prior,  $\\widehat{\\Pi}_t$  the predicted joint action policy for all players, and  $s$  the representation of the current state, the action generation model  $f_{act}$  generates a probability distribution over possible actions  $\\pi^i$  for the main player  $i$  that is the best response to  $\\widehat{\\Pi}_t$ . We do so by using search techniques to look ahead and find the best response.\n\n$$\n\\boldsymbol {\\pi} ^ {i} = f _ {a c t} (\\widehat {\\boldsymbol {b}}, \\widehat {\\boldsymbol {\\Pi}} _ {t}, \\boldsymbol {I})\n$$\n\nMore specifically, in our search implementation, at the first layer, we first sample across possible hidden states  $s \\sim \\widehat{b}$  according to the belief prior. At the second layer (i.e. the first action stage  $t$ ), we calculate expected  $q$ -values for each action  $a \\in \\mathcal{A}$  that the main player can take if the other players play actions  $a \\sim \\widehat{\\Pi}_t$  according to the predicted joint distribution. In subsequent action stages, the search process will assume that other players play according to their policy simulated and induced by the value heuristic that is not dialogue dependent. We then take the best response action  $a_i^* = \\max(\\pi^i)$  as the intended action. Since this is a partial information game, expected  $q$ -values are taken across information sets, not states. We describe how our action planner is implemented in more detail in Appendix F.\n\nE.3 Dialogue Generation\n\nThe dialogue generator $f_{gen}$ takes as input I some representation of the current information set and $a_{i}^{*}$, the intended best response action, and outputs dialogue $d$.\n\n$d=f_{gen}(\\textbf{{I}},a_{i}^{*})$\n\nWe will implement $f_{gen}$ using an LLM, which is fed I and $a_{i}^{*}$ directly as prompts, along with some instruction prompt $\\phi_{gen}$ that prompts it to produce realistic sounding dialogue that helps it achieve its intended action.\n\nFor example, perhaps the player wants to approve the next team. Then it should try to generated dialogue that convinces the other players to also approve.\n\nWe show examples of such prompts in Appendix J.\n\n## Published as a conference paper at ICLR 2025\n\n## # F VALUE HEURISTIC IMPLEMENTATION DETAILS\n\n![img-12.jpeg](img-12.jpeg)\n## Action planner\nFigure 14: Overview of how we utilize the trained value heuristics in MCTS tree search to get a non-dialogue based policy. While we only display the values for a single player in the diagram, note that in practice we infer and update the values for all players at the same time. Next states are sampled using the PUCT formula we described. We sample an initial hidden state based on the internal beliefs of the agent. Metropolis-Hastings is used to sample since it may be difficult to calculate the probability density specified by the internal beliefs. Note that values estimated using MCTS are also passed as feedback to the evaluator.\n\nThe MCTS search process is depicted in Figure 14, where we simulate a trajectory from the hidden state we are at until we reach some unexpanded state  $s$ . The probability of transitioning to a state during simulations is computed assuming that each player samples from their optimal actions according to their PUCT (polynomial upper confidence trees) values (and  $\\phi_e$  for the environment actor) (Schrittwieser et al., 2020). Since in some environments players may only be able to observe information sets, when computing the PUCT values we average over all expanded states in that information set. Moreover, the initial hidden state can be sampled according to a prior (or empirical prior) over the states in the information set that the player observed. Then, using our value heuristic, we compute the values of each of the next hidden states. We then backpropagate our new values back up the simulated trajectory, updating the intermediate states. After running a few MCTS simulations (roll-outs) like the one we described, the planner then outputs the action which leads to the highest value next state. We show our information set PUCT formula below, where  $N(s,a)$  is the number of times we took action  $a$  at state  $s$  during MCTS rollouts,  $P(s,a)$  is the prior prior probability of selecting action  $a$  from state  $s$ ,  $C$  is the exploration constant,  $Q_{emp}$  is the empirical average of MCTS roll-out outcomes,  $\\widehat{Q}(s,a)$  is the prior computed by our value heuristic,  $\\alpha$  controls how much weight be put on the prior (often  $\\alpha = 1$ ), and  $\\pi_B$  is the distribution across hidden states in the information set given our beliefs  $B$ , some parametrization of  $\\pi_B$ . Since  $\\pi_B$  is often hard to compute, we can simply set  $\\pi_B(s|I) = \\frac{\\sum_b N(s,b)}{\\sum_c' \\leq I \\sum_b N(s',b)}$  to be the empirical roll-out distribution, given that we sample initial states  $s_0 \\sim \\pi_B(s_0|I)$  according to our beliefs. For example, in Avalon, we can sample the hidden roles according to our beliefs  $B$  using Metropolis-Hastings for the initial state  $s_0$ .\n\n$$\nQ (s, a) = \\frac {N (s , a) \\cdot Q _ {\\mathrm {e m p}} (s , a) + \\alpha \\cdot \\widehat {Q} (s , a)}{N (s , a) + \\alpha} \\tag {1}\n$$\n\n$$\n\\operatorname {P U C T} (I, a) = \\sum_ {s \\in I} \\pi_ {B} (s | I) \\left[ Q (s, a) + C \\cdot P (s, a) \\cdot \\frac {\\sqrt {\\sum_ {b} N (s , b)}}{1 + N (s , a)} \\right] \\tag {2}\n$$\n\n## Published as a conference paper at ICLR 2025\n\n## # G DIALOGUE GUIDE IMPROVEMENT EVALUATION IMPLEMENTATION DETAILS\n\nWe provide more details on our dialogue improvement evaluation process here and as shown in figure 15. The improvement method (skill coach) remains the same as we described before.\n\nWe first generate a synthetic dataset by simulating a game of Avalon with initial dialogue and move policies  $\\phi$ . Given the dialogue guide  $\\sigma$  we want to evaluate, we then sample 'scenarios' from the dataset. A scenario consists of a game state, intended action, and private information in the simulated trajectory. We create an Avalon agent like the one we described in E for each player in the game, initialized with their corresponding private information. The Avalon agent is then asked to generate dialogue using the dialogue guide  $\\sigma$ .\n\nUsing this new generated dialogue, we then simulate the next round of dialogue analysis for each Avalon agent. This produces analysis scores based on how likely they think the player is to be Merlin  $z_{merlin}$ , and how likely they think the player is to be Evil  $z_{evil}$ , where  $z_{merlin}, z_{evil} \\in [-2, 2]$ . For evaluating Merlin, we get the average  $z_{merlin}$  scores from the Evil players,  $\\overline{z}_{merlin}$ , along with the average  $z_{evil}$  scores from the Good players  $\\overline{z}_{evil}$ . We then take the minimum of these two as the feedback score  $z = \\min\\{\\overline{z}_{evil}, \\overline{z}_{merlin}\\}$ . This is because Merlin wants to both minimize the probability of being detected by the Evil players, and also minimize the probability of being identified as Evil by the Good players.\n\n![img-13.jpeg](img-13.jpeg)\nFigure 15: Overview of our improvement process for learning dialogue generation strategies. This includes how we evaluate the dialogue and how we collect feedback. The skill coach here can be implemented as either our improvement method, STRATEGIST, or any of the baseline methods we described.\n\nThe dialogue analyzer (discriminator) is described in more detail in Appendix E and the specific generation and analysis prompts are shown in Appendix J.\n\n## H Value Heuristic LLM Prompt and Output Examples\n\n### H.1 System Prompts\n\nSystem prompt are guidelines for LLM to generate outputs align with the intended goals. In our case, the goal is to generate a function that evaluates the value of a state in a game under low cost.\n\n[label=Value heuristic system prompt] You are a function engineer trying to write a function that can evaluate the value of a state in a game. This is known as a value heuristic, and will be used in look-ahead search algorithms to evaluate the value of unexplored states. Your goal is to develop a heuristic that is as accurate as possible without being too expensive to compute. Hence, you are not allowed to runs simulations in the function.\n\nThe following example is a detailed prompt telling the LLM how to format the value heuristics specifically in the GOPS game. The format of input and output are clearly defined in the prompt with illustrations, examples and structures.\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 3,
      "title": "GOPS value heuristics function signature",
      "content": "# GOPS value heuristics function signature\n\nThe function (written in python) should be named 'evaluate state' and take in a tuple called 'state' of the game state as input. Specifically, the input tuple will be of length 9, and it should return 2 elements. The first element should be a tuple with 2 floats: the first element being the score you expect player 0 will get at the end of the game, and the second element being the score you expect player 1 will get at the end of the game. The second element should be a dictionary of any important intermediate values that you used to calculate the scores. For example, if you think player 0 will win 12 total points by the end of the game and player 1 will win 8 total points, the function should return (12, 8).\n\nMake sure your output only includes the code of the function itself in plain text such that it is executable using exec() in python. Any helper functions should be defined within the scope of the function 'evaluate state'. Include comments in your code so that it is readable, but everything should be implemented.\n\n## The signature of the function should be as follows:\n\n```python\ndef evaluate_state(state) -&gt; tuple[tuple[float, float], dict]:\nscore Cards = state[0] # a python list of the score cards (integers) that have been played, in the order they were played\nplayer_0played Cards = state[1] # a python list of the cards (integers) player 0 has played, in the order they were played.\nplayer_1played Cards = state[2] # a python list of the cards (integers) player 1 has played, in the order they were played.\nis_turn = state[3] # bool, true if it is you and your opponent's turn to play, false if it is time to draw a new score card\nplayer_0_score = state[4] # float or integer, player 0's score so far\nplayer_1_score = state[5] # float or integer, player 1's score so far\nscore_deck = state[6] # a python set of the score cards (integers) left in the deck, either same length as player_0_hand and player_1_hand or one less since the score card appears before the players play. May be empty\nplayer_0_hand = state[7] # a python set of the cards (integers) left in player 0's hand. May be empty\nplayer_1_hand = state[8] # a python set of the cards (integers) left in player 1's hand. May be empty"
    },
    {
      "id": 4,
      "title": "explanation of what we do next",
      "content": "# explanation of what we do next\n...\n<intermediate_value1> = value1"
    },
    {
      "id": 5,
      "title": "explanation of what we do next",
      "content": "# explanation of what we do next\n...\n<intermediate_value2> = value2"
    },
    {
      "id": 6,
      "title": "explanation of what we do next",
      "content": "# explanation of what we do next\n...\nplayer Scores = (player_0_expected_score, player_1_expected_score)\nintermediate_values = (\"<intermediate_value1>\": intermediate_value1, \"<intermediate_value2>\": intermediate_value2, ...)\nreturn player Scores, intermediate_values # make sure the return is exactly in this format\n```\n\nWhere you can use your own names for the intermediate values and the values themselves. Please start with \"def evaluate state(state):\"\n\n## # H.2 IDEA GENERATION EXAMPLES\n\nThe idea generation prompt included system prompt, game rules, previous guide and feedback reflections. Following those four components, we construct the format and an example of ideas to guide the generation of LLM.</intermediate_value2></intermediate_value1></intermediate_value2></intermediate_value1>\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 7,
      "title": "Prompt for idea generation",
      "content": "# Prompt for idea generation\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n<previous guide=\"\">\n\n<feedback reflections=\"\">\n\nBased on the function, feedback, and conclusions you drew, what are 2 improvements that you can make to the function that you think will have the most impact? Be as specific and concrete as possible, and write them out in the following format:\n\n- Thoughts: <your thoughts=\"\" here=\"\">\n- Idea 1: <your idea=\"\" here=\"\">\n- Idea 2: <your idea=\"\" here=\"\">\n\n## Here's an example of what this might look like for 3 improvement ideas:\n\n- Thoughts: I should consider the number of cards left in the deck when evaluating the value of a state.\n- Idea 1: I should add a term to the value function that penalizes states where there are fewer cards left in the deck.\n- Idea 2: I should add a term to the value function that rewards states where the player has more cards in their hand than the opponent.\n- Idea 3: I should add a term to the value function that rewards states where the player has more cards in their hand than the opponent and there are fewer cards left in the deck.\n\nBelow is an instance of Feedback of GOPS game, showing the setup of two players, the intermediate values involved in the computation, and the actual scores.</your></your></your></your></previous></your>\n\n## Published as a conference paper at ICLR 2025\n\n|  Feedback example  |\n| --- |\n|  Example 9:  |\n|  The state you were trying to estimate a value for is:  |\n|  The current state of the game is as follows:  |\n|  ¬∑The score cards that have been revealed are: (2,4,5,1,3)  |\n|  ¬∑The cards that player 0 has played are: (1,2,4,3,5)  |\n|  ¬∑The cards that player 1 has played are: (3,5,1,2,4)  |\n|  ¬∑Player 0's score so far is: 9  |\n|  ¬∑Player 1's score so far is: 6  |\n|  ¬∑The score cards left in the deck are: set()  |\n|  ¬∑The cards left in player 0's hand are: set()  |\n|  ¬∑The cards left in player 1's hand are: set()  |\n|  The function you generated returned the following values:  |\n|  {0:3,1:-3}  |\n|  for the expected end of game scores of the players.  |\n|  Some intermediate values that you used to calculate the scores were:  |\n|  {'player_0_expected_score': 9, 'player_1_expected_score': 6, 'dynamic_penalty': 0.0, 'player_0_hand_reward': 0, 'player_1_hand_reward': 0, 'player_0_adjustment': 0, 'player_1_adjustment': 0, 'player_0_strategic_adjustment': 0, 'player_1_strategic_adjustment': 0}  |\n|  The estimated end of game scores of the players using lookahead search with your function was:  |\n|  {0:0.0,1:0.0}  |\n|  The actual scores of the players at the end of the game in the simulation were:  |\n|  {0:3.0,1:-3.0}  |\n\nRecall that feedback reflection refers to feedback for strategy, which is used to generate ideas. For prompting, We specify that game rules, previous guide and feedback examples are generated previously. The following is the prompt for feedback reflections.\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 8,
      "title": "Prompt for feedback reflections",
      "content": "# Prompt for feedback reflections\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\nPreviously you generated the following function to evaluate the value of a state in the game.\n\n<previous guide=\"\">\n\nBelow is some feedback on how the function you generated performed when we tested it. Note that simulations involve high variance and the actual scores may not match the expected scores exactly. Hence, you should focus on trying to get the scores produced by your function to match those predicted by look-ahead search as closely as possible.\n\n<feedback examples=\"\">\n\nBased on the feedback given and the function you generated previously, what are some conclusions you can draw from the feedback? Make sure to cite the specific examples in the feedback to justify your analysis.\n\nBelow is the feedback reflections from the LLM."
    },
    {
      "id": 9,
      "title": "Feedback reflections",
      "content": "# Feedback reflections\n\nFrom the feedback provided, we can draw several conclusions regarding the performance of the function in evaluating the value of a state in the GOPS game:\n\n1. Discrepancy in Expected Scores: In several examples (e.g., Example 4, Example 7, Example 8), the expected end-of-game scores returned by the function do not match the scores predicted by look-ahead search closely. This indicates that the function may not be accurately estimating the value of the states in the game.\n2. Inconsistencies in Strategic Adjustments: The function includes a strategic adjustment based on the opponent's played cards, but in all cases, the strategic adjustments are zero. This suggests that the strategic adjustments made in the function may not be capturing the true strategic implications of the opponent's moves effectively.\n3. Dynamic Penalty Adjustment: The function includes a dynamic penalty adjustment based on the remaining high-value score cards in the deck. However, the dynamic penalty adjustments may not be accurately reflecting the impact of high-value score cards left in the deck on the game outcome.\n4. Limited Hand Size Reward: The function rewards states where a player has more cards in their hand than the opponent. While this reward is included, it might not be capturing the full strategic implications of having more cards in hand effectively.\n5. Simulation vs. Expected Scores: The actual scores at the end of the game in the simulations do not match the expected scores returned by the function precisely. This indicates that the function's evaluation may not be aligning well with the actual game outcomes.\n\nIn conclusion, the function may need further refinement in terms of capturing strategic adjustments, dynamic penalties, and hand size rewards to improve the accuracy of the state evaluation in the GOPS game. Additionally, considering the discrepancies between the expected and actual scores, further testing and adjustments may be necessary to enhance the function's performance in evaluating the value of states accurately.\n\nAlso, with the prompts of idea generation, below is the generated new ideas by LLM.</feedback></previous></previous></system>\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 10,
      "title": "Generated new ideas",
      "content": "# Generated new ideas\n\nThoughts: The function needs to better capture the strategic implications of the opponent's moves and adjust the expected scores accordingly. Additionally, the dynamic penalty adjustment and hand size rewards could be improved to better reflect their impact on the game outcome.\n\nIdea 1: Enhance the strategic adjustment component of the function by analyzing the opponent's played cards more deeply. This could involve considering patterns in the opponent's moves, potential card combinations, and predicting future moves based on past actions.\n\nIdea 2: Revise the dynamic penalty adjustment to more accurately reflect the impact of high-value score cards left in the deck. This could involve adjusting the penalty dynamically based on the remaining high-value cards and their likelihood of being drawn in future rounds. This adjustment could help in better assessing the risk associated with certain states in the game.\n\n## # H.3 STRATEGY IMPLEMENTATION EXAMPLES\n\nDuring the strategy implementation step, we first select a strategy and an idea from the libraries using the adaptive selection policy. Then, we prompt the LLM to implement the idea on the strategy, generating a new improved strategy.\n\nIn this section, we showcase two examples. Each example illustrates strategies before and after enhancements made by the LLM. This comparison highlights the effectiveness of our strategy implementation. The improved parts by LLM are highlighted in yellow.\n\nBelow is the prompt for strategy implementation."
    },
    {
      "id": 11,
      "title": "Prompt for implementation",
      "content": "# Prompt for implementation\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n## Previously you generated the following function to evaluate the value of a state in the game:\n\n<previous guide=\"\">\n\n## Here is a possible way to improve this function:\n\n<improvement ideas=\"\">\n\n## # H.3.1 EXAMPLE 1\n\nBelow is the GOPS Value Heuristics Function before the strategy improvement.</improvement></previous></game></system>\n\n## Published as a conference paper at ICLR 2025\n\n```python\n## GOPS Value Heuristics Function (Before)\ndef evaluate_state(state) -&gt; tuple[tuple[float, float], dict]:\nscore Cards = state[0]\nplayer_0played Cards = state[1]\nplayer_1played Cards = state[2]\nis_turn = state[3]\nplayer_0_score = state[4]\nplayer_1_score = state[5]\nscore_deck = state[6]\nplayer_0_hand = state[7]\nplayer_1_hand = state[8]"
    },
    {
      "id": 12,
      "title": "Calculate the potential scores for each player based on the remaining score cards",
      "content": "# Calculate the potential scores for each player based on the remaining score cards\nplayer_potential_score = player_0_score\nopponent_potential_score = player_1_score\nif len(score_deck) &gt; 0:\nremaining_score Cards = len(score_deck)\nif len player_0played Cards &gt; len player_1played Cards):\nplayer_potential_score += remaining_score Cards / 2\nelif len player_1played Cards &gt; len player_0played Cards):\nopponent_potential_score += remaining_score Cards / 2\nplayer Advantage = player_potential_score - opponent_potential_score\nreturn (player_potential_score, opponent_potential_score), ('player Advantage': player Advantage)\n```\n\nBelow is the selected idea to improve the above strategy."
    },
    {
      "id": 13,
      "title": "Selected GOPS Value Heuristics Improvement Idea",
      "content": "# Selected GOPS Value Heuristics Improvement Idea\n\nUpdate the function to calculate the advantage of winning future rounds based on the potential scores each player could achieve with the remaining score cards and their current scores. This would involve considering the number of cards left in the deck, the players' hands, and the current scores to determine the advantage one player may have over the other in the upcoming rounds.\n\nBelow is the GOPS Value Heuristics Function after enhancement by LLM. The improved parts by LLM are highlighted in yellow.\n\n## Published as a conference paper at ICLR 2025\n\n```python\n## GOPS Value Heuristics Function (After)\ndef evaluate_state(state) -&gt; tuple[tuple[float, float], dict]:\nscore Cards = state[0]\nplayer_0played Cards = state[1]\nplayer_1played Cards = state[2]\nis_turn = state[3]\nplayer_0_score = state[4]\nplayer_1_score = state[5]\nscore_deck = state[6]\nplayer_0_hand = state[7]\nplayer_1_hand = state[8]"
    },
    {
      "id": 14,
      "title": "Calculate the potential scores for each player based on the remaining score cards",
      "content": "# Calculate the potential scores for each player based on the remaining score cards"
    },
    {
      "id": 15,
      "title": "and current scores",
      "content": "# and current scores\nplayer_potential_score = player_0_score + sum(score_deck) / 2\nopponent_potential_score = player_1_score + sum(score_deck) / 2"
    },
    {
      "id": 16,
      "title": "Determine the advantage of winning future rounds based on potential score",
      "content": "# Determine the advantage of winning future rounds based on potential score\nif is_turn:\nplayer_potential Scores = [player_potential_score + card for card in player_0_hand]\nopponent_potential Scores = [opponent_potential_score + card for card in player_1_hand]\nplayer Advantage = sum player_potential Scores / len player_potential Scores\n- sum opponent_potential Scores / len opponent_potential Scores\nelse:\nplayer Advantage = player_potential Score - opponent_potential Score\nreturn (player_potential Score, opponent_potential Score), ('player Advantage': player Advantage)\n```\n\n## # H.3.2 EXAMPLE 2\n\nBelow is the Avalon Value Heuristics Function before improvements.\n\n## Avalon Value Heuristics Function (Before)\n\ndef evaluate_state(state):\nnum_successful_quests = sum(historical_quest_results)\nnum_failed_quests = len(historical_quest_results) - num_successful_quests\nnum_remaining_quests = len(num_participants_per_quest) - len(historical_quest_results)\n\nnum_evil = len(players) - num_good\nnum_evil_in_quest_team = len([player for player in quest_team if not is_good[player]])\n\nsuccess_probability = 0.5\nif phase == 0:\nif num_successful_quests >= 3:\nsuccess_probability = 0.9\nelif num_failed_quests >= 3:\nsuccess_probability = 0.1\nelif phase == 1:\nsuccess_probability = 0.8 if num_evil_in_quest_team == 0 else 0.2\nelif phase == 2:\nsuccess_probability = 0.9 if num_successful_quests > num_failed_quests else 0.1\nelif phase == 3:\nif ‚ÄôMerlin‚Äô in roles and ‚ÄôAssassin‚Äô in roles:\nmerlin_index = roles.index(‚ÄôMerlin‚Äô)\nassassin_index = roles.index(‚ÄôAssassin‚Äô)\nif assassin_index in quest_team:\nsuccess_probability = 0.1\nelse:\nsuccess_probability = 0.9\n\nexpected_winrates_per_player = dict()\nfor player in players:\nif is_good[player]:\nexpected_winrates_per_player[player] = success_probability\nelse:\nexpected_winrates_per_player[player] = 1 - success_probability\n\nintermediate_values = {\n‚Äônum_successful_quests‚Äô: num_successful_quests,\n‚Äônum_failed_quests‚Äô: num_failed_quests,\n‚Äônum_remaining_quests‚Äô: num_remaining_quests,\n‚Äônum_evil_in_quest_team‚Äô: num_evil_in_quest_team\n}\n\nreturn expected_winrates_per_player, intermediate_values\n\nBelow is the improvement idea pf Avalon Value Heuristics Function.\n\nAvalon Value Heuristics Function Improvement Idea Incorporate a more nuanced success probability calculation based on the roles of the players and the current phase of the game. For example, in phase 1 (team selection), consider the balance of Good and Evil players on the quest team and adjust the success probability accordingly. This can help better reflect the strategic considerations during team selection.\n\nThe following is Avalon Value Heuristics Function after enhancements, highlighted in yellow.\n\n## Published as a conference paper at ICLR 2025\n\n## Avalon Value Heuristics Function (After)\n```python\ndef evaluate_state(state: dict) -&gt; tuple[dict[int, float], dict]:\nprob_good_victory = 0.5 # probability that Good will win\nif phase == 0: # team selection phase\nnum_successful_quests = sum(historical_quest_results)\nnum_failed_quests = len(historical_quest_results) - num_successful_quests\nnum_remaining_quests = len(num_participants_per_quest) - len(historical_quest_results)\nnum_evil = len(players) - num_good\nnum_evil_in_quest_team = len([player for player in quest_team if not is_good[player]])\nif num_successful_quests &gt;= 3:\nprob_good_victory = 0.9\nelif num_failed_quests &gt;= 3:\nprob_good_victory = 0.1\nintermediate_value1 = {'num_successful_quests': num_successful_quests, 'num_failed_quests': num_failed_quests, 'num_remaining_quests': num_remaining_quests}\nintermediate_value2 = {'num_evil_in_quest_team': num_evil_in_quest_team}\nelif phase == 1: # team approval phase\nnum_evil_in_quest_team = len([player for player in quest_team if not is_good[player]])\nprob_good_victory = 0.8 if num_evil_in_quest_team == 0 else 0.2\nintermediate_value1 = {'num_evil_in_quest_team': num_evil_in_quest_team}\nintermediate_value2 = {} # Initialize empty dictionary to avoid the error\nelif phase == 2: # quest phase\nnum_successful_quests = sum(historical_quest_results)\nnum_failed_quests = len(historical_quest_results) - num_successful_quests\nprob_good_victory = 0.9 if num_successful_quests &gt; num_failed_quests else 0.1\nintermediate_value1 = {'num_successful_quests': num_successful_quests, 'num_failed_quests': num_failed_quests}\nintermediate_value2 = {} # Initialize empty dictionary to avoid the error\n```\n\n```python\nelif phase == 3: # assassination phase\nmerlin_index = roles.index('Merlin')\nassassin_index = roles.index('Assassin')\nif assassin_index in quest_team:\nprob_good_victory = 0.1\nelse:\nprob_good_victory = 0.9\nintermediate_value1 = {'merlin_index': merlin_index, 'assassin_index': assassin_index}\nintermediate_value2 = {} # Initialize empty dictionary to avoid the error\nexpected_winrates_per_player = dict()\nprob_evil_victory = 1 - prob_good_victory\nfor player in players:\nif is_good[player]:\nexpected_winrates_per_player[player] = prob_good_victory\nelse:\nexpected_winrates_per_player[player] = prob_evil_victory\nintermediate_values = {'intermediate_value1': intermediate_value1, 'intermediate_value2':\nintermediate_value2}\nreturn expected_winrates_per_player, intermediate_values\n\nH.3.3 Example 3\n\nBelow is the GOPS Value Heuristics Function before the strategy improvement.\n\n## GOPS Value Heuristics Function (Before)\n\n‚¨á\ndef evaluate_state(state) -> tuple[tuple[float, float], dict]:\nscore_cards = state[0]\nplayer_0_played_cards = state[1]\nplayer_1_played_cards = state[2]\nis_turn = state[3]\nplayer_0_score = state[4]\nplayer_1_score = state[5]\nscore_deck = state[6]\nplayer_0_hand = state[7]\nplayer_1_hand = state[8]"
    },
    {
      "id": 17,
      "title": "Calculate initial potentials",
      "content": "# Calculate initial potentials\nplayer_0_potential = sum(player_0_hand)\nplayer_1_potential = sum(player_1_hand)\nscore_potential = sum(score_deck)"
    },
    {
      "id": 18,
      "title": "Update player potentials based on remaining cards and score deck",
      "content": "# Update player potentials based on remaining cards and score deck\nplayer_0_potential += sum(card for card in player_0_hand if any(card > score for score in score_deck ))\nplayer_1_potential += sum(card for card in player_1_hand if any(card > score for score in score_deck\n\n)"
    },
    {
      "id": 19,
      "title": "Add half of the score potential to the player who has the turn",
      "content": "# Add half of the score potential to the player who has the turn\nif is_turn:\nplayer_0_potential += score_potential / 2\nelse:\nplayer_1_potential += score_potential / 2"
    },
    {
      "id": 20,
      "title": "Count the number of certain wins for each player",
      "content": "# Count the number of certain wins for each player\nplayer_0_certain_wins = sum(card > max(player_1_hand) for card in player_0_hand)\nplayer_1_certain_wins = sum(card > max(player_0_hand) for card in player_1_hand)\nrounds_left = len(score_deck)"
    },
    {
      "id": 21,
      "title": "Dynamic adjustment based on specific cards played",
      "content": "# Dynamic adjustment based on specific cards played\nplayer_0_certain_wins_adjust = 0\nplayer_1_certain_wins_adjust = 0\nfor i in range(len(player_0_played_cards)):\nif player_0_played_cards[i] > player_1_played_cards[i]:\nplayer_0_certain_wins_adjust += 1\nelif player_1_played_cards[i] > player_0_played_cards[i]:\nplayer_1_certain_wins_adjust += 1\n\nplayer_0_certain_wins += player_0_certain_wins_adjust\nplayer_1_certain_wins += player_1_certain_wins_adjust"
    },
    {
      "id": 22,
      "title": "Add potential scores from certain wins",
      "content": "# Add potential scores from certain wins\nif rounds_left <= player_0_certain_wins:\nhighest_scores = sorted(score_deck)[-rounds_left:]\nplayer_0_potential += sum(highest_scores)\n\nif rounds_left <= player_1_certain_wins:\nhighest_scores = sorted(score_deck)[-rounds_left:]\nplayer_1_potential += sum(highest_scores)\n\n## Published as a conference paper at ICLR 2025\n\n```python\nCalculate expected scores player_0_expected_score  $=$  player_0_score  $^+$  player_0_potential player_1_expected_score  $=$  player_1_score  $^+$  player_1_potential # Calculate the difference in the sum of played cards sum_player_0played Cards  $=$  sum(player_0played Cards) sum_player_1played Cards  $=$  sum(player_1played Cards) sum_player_1played Cards diff  $=$  sum_player_0played Cards - sum_player_1played Cards # Adjust scores based on the difference in the sum of played cards adjustment_factor  $= 0.1$  # Adjust this factor based on the level of adjustment desired player_0_expected_score  $+ =$  sumplayed Cards_diff \\* adjustment_factor player_1_expected_score  $- =$  sumplayed Cards_diff \\* adjustment_factor # Dynamic adjustment based on uncertainty in the game state uncertainty_factor  $= 0.1$  \\* rounds_left # Adjust this factor based on the level of uncertainty desired player_0_expected_score  $+ =$  player_0_potential \\* uncertainty_factor player_1_expected_score  $+ =$  player_1_potential \\* uncertainty_factor # Refine uncertainty factor based on specific conditions if len(player_0_hand)  $= = 1$  uncertainty_factor  $= = 0.5$  if len(player_1_hand)  $= = 1$  uncertainty_factor  $= = 0.5$  # Store intermediate values in a dictionary intermediate_values  $=$  { 'player_0_potential': player_0_potential, 'player_1_potential': player_1_potential, 'score_potential': score_potential, 'player_0_certain_wins': player_0_certain_wins, 'player_1_certain_wins': player_1_certain_wins, 'player_0_certain_wins_adjust': player_0_certain_wins_adjust, 'player_1_certain_wins_adjust': player_1_certain_wins_adjust, 'sumplayed Cards_diff': sumplayed Cards_diff, 'uncertainty_factor': uncertainty_factor } return (player_0_expected_score, player_1_expected_score), intermediate_values\n```\n\nBelow is the selected idea to improve the above strategy."
    },
    {
      "id": 23,
      "title": "Selected GOPS Value Heuristics Improvement Idea",
      "content": "# Selected GOPS Value Heuristics Improvement Idea\n\nIncorporate a term in the value function that takes into account the distribution of score cards left in the deck. Certain score cards may have a higher value or strategic importance, so adjusting the potentials based on the presence of these cards can provide a more accurate evaluation of the state.\n\nBelow is the GOPS Value Heuristics Function after enhancement by LLM. The improved parts by LLM are highlighted in yellow.\n\n## GOPS Value Heuristics Function (After)\n\ndef evaluate_state(state) -> tuple[tuple[float, float], dict]: score_cards = state[0] player_0_played_cards = state[1] player_1_played_cards = state[2] is_turn = state[3] player_0_score = state[4] player_1_score = state[5] score_deck = state[6] player_0_hand = state[7] player_1_hand = state[8] # Calculate initial potentials player_0_potential = sum(player_0_hand) player_1_potential = sum(player_1_hand) score_potential = sum(score_deck) # Update player potentials based on remaining cards and score deck player_0_potential += sum(card for card in player_0_hand if any(card > score for score in score_deck )) player_1_potential += sum(card for card in player_1_hand if any(card > score for score in score_deck )) # Add half of the score potential to the player who has the turn if is_turn: player_0_potential += score_potential / 2 else: player_1_potential += score_potential / 2 # Count the number of certain wins for each player player_0_certain_wins = sum(card > max(player_1_hand) for card in player_0_hand) player_1_certain_wins = sum(card > max(player_0_hand) for card in player_1_hand) rounds_left = len(score_deck) # Dynamic adjustment based on specific cards played player_0_certain_wins_adjust = 0 player_1_certain_wins_adjust = 0 for i in range(len(player_0_played_cards)): if player_0_played_cards[i] > player_1_played_cards[i]: player_0_certain_wins_adjust += 1 elif player_1_played_cards[i] > player_0_played_cards[i]: player_1_certain_wins_adjust += 1 player_0_certain_wins += player_0_certain_wins_adjust player_1_certain_wins += player_1_certain_wins_adjust # Add potential scores from certain wins if rounds_left <= player_0_certain_wins: highest_score= sorted(score_deck)[-rounds_left:] player_0_potential += sum(highest_score) if rounds_left <= player_1_certain_wins: highest_score= sorted(score_deck)[-rounds_left:] player_1_potential += sum(highest_score) # Incorporate distribution of score cards left in the deck important_score_cards = (7, 10, 13) # Example: define important score cards player_0_potential += sum(card for card in player_0_hand if card in important_score_cards) player_1_potential += sum(card for card in player_1_hand if card in important_score_cards) # Calculate expected scores player_0_expected_score = player_0_score + player_0_potential player_1_expected_score = player_1_score + player_1_potential # Calculate the difference in the sum of played cards sum_player_0_played_cards = sum(player_0_played_cards) sum_player_1_played_cards = sum(player_1_played_cards) sum_played_cards_diff = sum_player_0_played_cards - sum_player_1_played_cards\n\n## Published as a conference paper at ICLR 2025\n\n```python"
    },
    {
      "id": 24,
      "title": "Adjust scores based on the difference in the sum of played cards",
      "content": "# Adjust scores based on the difference in the sum of played cards\nadjustment_factor = 0.1 # Adjust this factor based on the level of adjustment desired\nplayer_0_expected_score += sumplayed Cards_diff * adjustment_factor\nplayer_1_expected_score -= sumplayed Cards_diff * adjustment_factor"
    },
    {
      "id": 25,
      "title": "Dynamic adjustment based on uncertainty in the game state",
      "content": "# Dynamic adjustment based on uncertainty in the game state\nuncertainty_factor = 0.1 * rounds_left # Adjust this factor based on the level of uncertainty desired\nplayer_0_expected_score += player_0_potential * uncertainty_factor\nplayer_1_expected_score += player_1_potential * uncertainty_factor"
    },
    {
      "id": 26,
      "title": "Refine uncertainty factor based on specific conditions",
      "content": "# Refine uncertainty factor based on specific conditions\nif len(player_0_hand) == 1:\nuncertainty_factor += 0.5\nif len(player_1_hand) == 1:\nuncertainty_factor += 0.5"
    },
    {
      "id": 27,
      "title": "Store intermediate values in a dictionary",
      "content": "# Store intermediate values in a dictionary\nintermediate_values = {\n'player_0_potential': player_0_potential,\n'player_1_potential': player_1_potential,\n'score_potential': score_potential,\n'player_0_certain_wins': player_0_certain_wins,\n'player_1_certain_wins': player_1_certain_wins,\n'player_0_certain_wins_adjust': player_0_certain_wins_adjust,\n'player_1_certain_wins_adjust': player_1_certain_wins_adjust,\n'sumplayed Cards_diff': sumplayed Cards_diff,\n'uncertainty_factor': uncertainty_factor\n}\n\nreturn (player_0_expected_score, player_1_expected_score), intermediate_values\n```\n\n## # H.3.4 EXAMPLE 4\n\nBelow is the GOPS Value Heuristics Function before the strategy improvement.\n\n## Published as a conference paper at ICLR 2025\n\n```python\n## GOPS Value Heuristics Function (Before)\ndef evaluate_state(state) -&gt; tuple[tuple[float, float], dict]:\nscore Cards = state[0]\nplayer_0[played Cards = state[1]\nplayer_1[played Cards = state[2]\nis_turn = state[3]\nplayer_0_score = state[4]\nplayer_1_score = state[5]\nscore_deck = state[6]\nplayer_0_hand = state[7]\nplayer_1_hand = state[8]"
    },
    {
      "id": 28,
      "title": "Calculate initial potentials",
      "content": "# Calculate initial potentials\nplayer_0_potential = sum(player_0_hand)\nplayer_1_potential = sum(player_1_hand)\nscore_potential = sum(score_deck)"
    },
    {
      "id": 29,
      "title": "Add half of the score potential to the player who has the turn",
      "content": "# Add half of the score potential to the player who has the turn\nif is_turn:\nplayer_0_potential += score_potential / 2\nelse:\nplayer_1_potential += score_potential / 2"
    },
    {
      "id": 30,
      "title": "Count the number of certain wins for each player",
      "content": "# Count the number of certain wins for each player\nplayer_0_certain_wins = sum(card &gt; max(player_1_hand) for card in player_0_hand)\nplayer_1_certain_wins = sum(card &gt; max(player_0_hand) for card in player_1_hand)\nrounds_left = len(score_deck)"
    },
    {
      "id": 31,
      "title": "Add potential scores from certain wins",
      "content": "# Add potential scores from certain wins\nif rounds_left &lt;= player_0_certain_wins:\nhighest Scores = sorted(score_deck)[-rounds_left:]\nplayer_0_potential += sum(highest Scores)\nif rounds_left &lt;= player_1_certain_wins:\nhighest Scores = sorted(score_deck)[-rounds_left:]\nplayer_1_potential += sum(highest Scores)"
    },
    {
      "id": 32,
      "title": "New improvement: Incorporate a probabilistic approach based on the remaining score cards",
      "content": "# New improvement: Incorporate a probabilistic approach based on the remaining score cards\nplayer_0_expected_score = player_0_score + player_0_potential\nplayer_1_expected_score = player_1_score + player_1_potential"
    },
    {
      "id": 33,
      "title": "Dynamic evaluation of opponent's potential moves",
      "content": "# Dynamic evaluation of opponent's potential moves\nopponent_potential = sum(player_0_hand) if is_turn else sum(player_1_hand)\nopponent_certain_wins = sum(card &gt; max(player_1_hand) for card in player_0_hand) if is_turn else sum\n(card &gt; max(player_0_hand) for card in player_1_hand)"
    },
    {
      "id": 34,
      "title": "Adjust evaluation based on opponent's potential plays",
      "content": "# Adjust evaluation based on opponent's potential plays\nif rounds_left &lt;= opponent_certain_wins:\nhighest Scores = sorted(score_deck)[-rounds_left:]\nif is_turn:\nplayer_1_potential += sum(highest Scores)\nelse:\nplayer_0_potential += sum(highest Scores)"
    },
    {
      "id": 35,
      "title": "Store intermediate values in a dictionary",
      "content": "# Store intermediate values in a dictionary\nintermediate_values = {\n'player_0_potential': player_0_potential,\n'player_1_potential': player_1_potential,\n'score_potential': score_potential,\n'player_0_certain_wins': player_0_certain_wins,\n'player_1_certain_wins': player_1_certain_wins,\n'opponent_potential': opponent_potential,\n'opponent_certain_wins': opponent_certain_wins,\n'player_0_expected_score': player_0_expected_score,\n'player_1_expected_score': player_1_expected_score\n}\nreturn (player_0_expected_score, player_1_expected_score), intermediate_values\n```\n\nBelow is the selected idea to improve the above strategy.\n\n## Selected GOPS Value Heuristics Improvement Idea\n\nIntroduce a penalty factor for potential scores that are excessively high, indicating potential overestimation. By capping the potential scores or applying a scaling factor based on certain thresholds, we can prevent the function from assigning unrealistic values and improve the accuracy of the value heuristic.\n\nBelow is the GOPS Value Heuristics Function after enhancement by LLM. The improved parts by LLM are highlighted in yellow.\n\n‚¨á\n## GOPS Value Heuristics Function (After)\n\n‚¨á\ndef evaluate_state(state) -> tuple[tuple[float, float], dict]:\nscore_cards = state[0]\nplayer_0_played_cards = state[1]\nplayer_1_played_cards = state[2]\nis_turn = state[3]\nplayer_0_score = state[4]\nplayer_1_score = state[5]\nscore_deck = state[6]\nplayer_0_hand = state[7]\nplayer_1_hand = state[8]"
    },
    {
      "id": 36,
      "title": "Calculate initial potentials",
      "content": "# Calculate initial potentials\nplayer_0_potential = sum(player_0_hand)\nplayer_1_potential = sum(player_1_hand)\nscore_potential = sum(score_deck)"
    },
    {
      "id": 37,
      "title": "Add half of the score potential to the player who has the turn",
      "content": "# Add half of the score potential to the player who has the turn\nif is_turn:\nplayer_0_potential += score_potential / 2\nelse:\nplayer_1_potential += score_potential / 2"
    },
    {
      "id": 38,
      "title": "Count the number of certain wins for each player",
      "content": "# Count the number of certain wins for each player\nplayer_0_certain_wins = sum(card > max(player_1_hand) for card in player_0_hand)\nplayer_1_certain_wins = sum(card > max(player_0_hand) for card in player_1_hand)\nrounds_left = len(score_deck)"
    },
    {
      "id": 39,
      "title": "Add potential scores from certain wins",
      "content": "# Add potential scores from certain wins\nif rounds_left <= player_0_certain_wins:\nhighest_scores = sorted(score_deck)[-rounds_left:]\nplayer_0_potential += sum(highest_scores)\n\nif rounds_left <= player_1_certain_wins:\nhighest_scores = sorted(score_deck)[-rounds_left:]\nplayer_1_potential += sum(highest_scores)\n\n## Published as a conference paper at ICLR 2025\n\n```python\nNew improvement: Incorporate a probabilistic approach based on the remaining score cards player_0_expected_score = player_0_score + player_0_potential player_1_expected_score = player_1_score + player_1_potential"
    },
    {
      "id": 40,
      "title": "Dynamic evaluation of opponent's potential moves",
      "content": "# Dynamic evaluation of opponent's potential moves\nopponent_potential = sum(player_0_hand) if is_turn else sum(player_1_hand)\nopponent_certain_wins = sum(card &gt; max(player_1_hand) for card in player_0_hand) if is_turn else sum (card &gt; max(player_0_hand) for card in player_1_hand)"
    },
    {
      "id": 41,
      "title": "Adjust evaluation based on opponent's potential plays",
      "content": "# Adjust evaluation based on opponent's potential plays\nif rounds_left &lt;= opponent_certain_wins:\nhighest Scores = sorted(score_deck)[-rounds_left:]\nif is_turn:\nplayer_1_potential += sum(highest Scores)\nelse:\nplayer_0_potential += sum(highest Scores)"
    },
    {
      "id": 42,
      "title": "Introduce a penalty factor for excessively high potential scores",
      "content": "# Introduce a penalty factor for excessively high potential scores\npenalty_threshold = 100 # Define a threshold for potential scores to trigger penalty\npenalty_factor = 0.5 # Define a factor by which to reduce potential scores above threshold\nif player_0_potential &gt; penalty_threshold:\nplayer_0_potential = penalty_threshold + (player_0_potential - penalty_threshold) * penalty_factor\nif player_1_potential &gt; penalty_threshold:\nplayer_1_potential = penalty_threshold + (player_1_potential - penalty_threshold) * penalty_factor"
    },
    {
      "id": 43,
      "title": "Store intermediate values in a dictionary",
      "content": "# Store intermediate values in a dictionary\nintermediate_values = {\n'player_0_potential': player_0_potential,\n'player_1_potential': player_1_potential,\n'score_potential': score_potential,\n'player_0_certain_wins': player_0_certain_wins,\n'player_1_certain_wins': player_1_certain_wins,\n'opponent_potential': opponent_potential,\n'opponent_certain_wins': opponent_certain_wins,\n'player_0_expected_score': player_0_expected_score,\n'player_1_expected_score': player_1_expected_score\n}\nreturn (player_0_expected_score, player_1_expected_score), intermediate_values\n\n## Published as a conference paper at ICLR 2025\n\n## # I DIALOGUE GUIDE LLM PROMPT AND OUTPUT EXAMPLES\n\nThis sections show the system prompts of dialogue guidance on LLM and several examples, including system prompts, idea generation prompts, and strategy implementation examples.\n\n## # I.1 SYSTEM PROMPTS\n\nBelow is the Dialogue guide system prompt."
    },
    {
      "id": 44,
      "title": "Dialogue guide system prompt",
      "content": "# Dialogue guide system prompt\n\nYou are a coach trying to write a section of a strategy guide on how to play a game well.\n\nThe specific section of the strategy guide you are writing right now is on how to play the Merlin role effectively during the discussion phase so that they can win the game. Recall that players often use the discussion phase to (1) gather information about other players, (2) try to convince other players of their innocence or guilt, and (3) try to persuade other players of a particular course of action. The game you are interested in is called The Resistance: Avalon. The Resistance: Avalon is the game of hidden identities and social deduction. There are two teams in the game: Good and Evil. Each player has a hidden identity (role) and side.\n\nDialogue guide signature follows, specifying the structures and contents dialogues regarding role playing the Merlin."
    },
    {
      "id": 45,
      "title": "Dialogue guide signature",
      "content": "# Dialogue guide signature\n\nYour guide should be in the form of a worksheet that the student can use to build their speech. You should order the worksheet questions in a way that makes logical sense, and you should have no more than six questions. Your questions should instruct the reader to write parts of their speech.\n\nThe title of your section should be \"Questions to fill out before speaking as the Merlin role\". Below is an example of how your worksheet should look like:\n\n## 1. Questions to fill out before speaking as the Merlin role\n## 2. Q1: Which player seems the most suspicious of you and why?\n3. Q2: For the player that seems the most suspicious of you, produce a statement addressing their suspicious.\n## 4. Q3: Which player is the quest leader?\n5. Q4: Produce a statement addressing the quest leader to convince them to support your intended course of action/ desired team.\n## 6. Q5: Which player is the most supportive of you?\n7. Q6: Produce a statement addressing the supportive player to convince them to support your intended course of action/ desired team.\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 46,
      "title": "Example Generated Dialogue Strategy Guide for Assassin",
      "content": "# Example Generated Dialogue Strategy Guide for Assassin\n\n1. Q1: Which player seems to have a deeper understanding of the game flow than normal Servants should possess?\n2. Q2: Develop a non-confrontational statement to subtly challenge this player. This should cause them to either prove their innocence or reveal more clues about their identity.\n## 3. Q3: Who has been the most influential in the team selection and voting process?\n4. Q4: Devise a statement to express agreement with this player's viewpoint subtly. This should make you less suspicious while enabling you to manipulate the discussion.\n## 5. Q5: Which player seems the most supportive of your views and actions in the game?\n6. Q6: Craft a statement subtly emphasizing your alignment with this supportive player's thoughts. This should increase your chances of being included in quest teams and reduce suspicion around you.\n\n## # I.2 IDEA GENERATION EXAMPLES\n\nThe following is the selected dialogue guide speaking as the Merlin role."
    },
    {
      "id": 47,
      "title": "Select dialogue guide",
      "content": "# Select dialogue guide\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n6. Analyze the voting patterns and team compositions proposed by each player to identify potential alliances or suspicious behaviors. Craft a statement to subtly address these observations without revealing your role.\n7. Which player seems the most supportive of Good and how can you align with them during the discussion phase to better ensure success for your team?\n\nA feedback example is provided for better understanding of dialogue guidance to teach LLM how to role play Merlin.\n\n## Feedback example description\n\n## Example 2:\n\nThe student using your guide was playing as player 0 in this example filled out your guide as follows:\n\n1. Q1: Based on the statements made so far, I suspect Players 2 and 4 are on the Evil side. Player 2‚Äôs suggestion of a team consisting of Player 1 and Player 2 seems too convenient, possibly indicating a desire to control the team composition. Player 4‚Äôs agreement with Player 1 and Player 2 without offering any alternative team suggestions raises suspicion as well.\n2. Q2: \"I can‚Äôt help but wonder if some players are trying to steer the team selection process in a particular direction. It‚Äôs important for us to carefully consider all perspectives and not be swayed by potentially self-serving suggestions.\"\n3. Q3: The current round leader is Player 1. I would suggest a team consisting of Player 0 (Merlin) and Player 1, as this would allow me to closely observe the actions of the other players and potentially guide the team towards success.\n4. Q4: \"Player 1, I believe that a team comprised of Player 0 and Player 1 would provide a strong foundation for success. By working closely together, we can ensure that our mission is carried out effectively.\"\n5. Q5: The voting patterns so far show agreement among Players 1, 2, and 4, indicating a potential alliance or shared goals. Player 3‚Äôs suggestion of including themselves in the team may also indicate a desire to control the outcome of the quest.\n6. Q6: \"I‚Äôve noticed a pattern of agreement among certain players in the team selection process. It‚Äôs important for us to consider all perspectives and not be swayed by potentially biased suggestions. Let‚Äôs focus on working together as a team to achieve our common goal.\"\n7. Q7: Player 1 seems the most supportive of Good based on their consistent emphasis on unity and success for the Quests. I will align with Player 1 during the discussion phase to ensure that our team is selected and the quest is completed successfully.\n\n## Then they said the following during the discussion phase:\n\n\"Fellow players, as we embark on this quest, it is crucial that we approach the team selection process with caution and consideration. I have noticed a pattern of agreement among certain players, which could potentially indicate a desire to steer the outcome in a particular direction. It is important for us to carefully consider all perspectives and not be swayed by potentially biased suggestions.\n\nI believe that a team comprised of Player 0 and Player 1 would provide a strong foundation for success. By working closely together, we can ensure that our mission is carried out effectively. Player 1 has consistently emphasized unity and success for the Quests, making them a trustworthy ally in this endeavor.\n\nLet us focus on working together as a team to achieve our common goal, rather than allowing personal agendas to influence our decisions. Let us approach this quest with unity and determination, ensuring that Good prevails in the end. Thank you for your attention.\"\n\nBelow is some feedback from the other players on how player 0 performed during the discussion phase. Recall that Good players are trying to access which other players are likely to be Good, while Evil players are trying to access which Good player is likely to be Merlin.\n\n## Player 3 with role Servant had the following thoughts on this discussion round:\n\n## Thought:\nPlayer 1‚Äôs speech emphasizes unity and success for the Quests, which aligns with the goals of Good and decreases the probability of Player 1 being Evil. Player 2‚Äôs statement also focuses on unity and careful consideration, indicating a commitment to the success of Good and decreasing the probability of Player 2 being Evil. Player 3‚Äôs response as a Servant of Arthur is focused on the success of the Quests and unity, which aligns with the goals of Good and decreases the probability of Player 3 being Evil. Player 4‚Äôs speech seems overly enthusiastic and focused on unity and cooperation, which could be a tactic to divert attention from their true identity as an Evil player. This raises suspicion and increases the probability of Player 4 being Evil.\n\nDictionary: {0: (-1, ‚Äôdecreased slightly‚Äô), 1: (-1, ‚Äôdecreased slightly‚Äô), 2: (0, ‚Äôstayed the same‚Äô), 3: (-2, ‚Äôdecreased significantly‚Äô), 4: (2, ‚Äôincreased significantly‚Äô)}\n\n## Player 4 with role Assassin had the following thoughts on this discussion round:\n\n## Thought:\nPlayer 2 seems to suspect the Evil players (Player 3 and Player 4) without any evidence and is trying to steer the conversation towards them. This aligns with the behavior of Merlin, who knows the identity of the Evil players and is trying to subtly guide the Good players towards the correct decisions. This increases the probability of Player 2 being Merlin. On the other hand, Player 3‚Äôs speech seems generally clueless and lacks any strategic insight, which decreases the probability of Player 3 being Merlin.\n\nDictionary: {0: (0, ‚Äôstayed the same‚Äô), 1: (0, ‚Äôstayed the same‚Äô), 2: (2, ‚Äôincreased significantly‚Äô), 3: (-2, ‚Äôdecreased significantly‚Äô), 4: (-2, ‚Äôdecreased significantly‚Äô)}\n\nThis is the prompt for feedback reflections.\n\n[style=unboxed,leftmargin=0cm]\n[font=Poster,leftmargin=0cm]\n[font=Poster,rightmargin=0cm]\n[font=Poster,rightmargin=0cm]\nFigure 1: (a) The text of the text is presented in the text. (b) The text is presented in the text. (c) The text is presented in the text. (d) The text is presented in the text. The following are the feedback reflections generated by LLM.\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 48,
      "title": "Generated feedback reflections",
      "content": "# Generated feedback reflections\n\nIn Example 0, Player 0 effectively followed the guide by suspecting certain players of being on the Evil side, crafting subtle statements to cast doubt on them, and aligning with players supportive of Good. This led to Player 0 being perceived positively by other players, with Player 2's probability of being Merlin decreasing significantly due to their behavior aligning more with the Evil side.\n\nIn Example 1, Player 0 again effectively followed the guide by suspecting players, making subtle statements, aligning with a supportive player, and emphasizing unity and success in their speech. This resulted in Player 0 being perceived positively by other players, with Player 4's probability of being Merlin decreasing significantly due to their lack of strategic insight.\n\nIn Example 2, Player 0 successfully identified suspicious behavior in other players, made subtle statements, and aligned with a supportive player. This led to Player 0 being perceived positively, with Player 2's probability of being Merlin decreasing significantly due to their behavior aligning more with the Evil side.\n\nIn Example 3, Player 0 effectively followed the guide by suspecting players, making subtle statements, aligning with a supportive player, and emphasizing unity. This resulted in Player 0 being perceived positively by other players, with Player 4's probability of being Merlin increasing significantly due to their behavior raising suspicion.\n\n- Overall, the guide's emphasis on analyzing player behavior, making subtle statements, and aligning with supportive players has proven to be successful in helping players navigate the discussion phase effectively as the Merlin role in The Resistance: Avalon.\n- Players who followed the guide were able to strategically assess the situation, cast doubt on potential Evil players, and align with Good players to increase their chances of success.\n\n## Below is the Prompt for idea generation regarding this dialogue guides\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 49,
      "title": "Prompt for idea generation",
      "content": "# Prompt for idea generation\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n## You previously generated the following section of the strategy guide:\n\n<previous guide=\"\">\n\n## Here is a possible way to improve your guide:\n\n<feedback reflections=\"\">\n\nBased on the worksheet, feedback, and conclusions you drew, what are 2 improvements that you can make to the worksheet that you think will have the most impact?\n\nBe as specific and concrete as possible, including what questions to add, edit, or remove, and write them out in the following format:\n\n- Thoughts: <your thoughts=\"\" here=\"\">\n- Idea 1: <your idea=\"\" here=\"\">\n- Idea 2: <your idea=\"\" here=\"\">\n¬∑¬∑¬∑\n\n## Here's an example of what this might look like for 3 improvement ideas:\n\n- Thoughts: I should tell the reader to address each player individually in the guide.\n- Idea 1: Add a question asking the reader who they think is most suspicious of them and produce a statement addressing their suspicions.\n- Idea 2: Add a question asking the reader to consider which other player is most supportive of them and produce a statement addressing their support.\n- Idea 3: Add a question asking the reader produce a statement addressed\n\nGenerated new improvement ideas are demonstrated below, showing the improvements made by LLM following the prompt for idea generation."
    },
    {
      "id": 50,
      "title": "Generated new improvement ideas",
      "content": "# Generated new improvement ideas\n\nThoughts: The current worksheet focuses on analyzing player behavior, making subtle statements, and aligning with supportive players, which are all crucial strategies for playing the Merlin role effectively. To enhance the guide further, we can add questions that prompt the player to consider their own actions and statements more explicitly.\n\n1. Idea 1: Add a question asking the player to reflect on how their proposed team composition would benefit the success of the Quest and provide a statement justifying their choices.\n2. Idea 2: Add a question asking the player to assess their level of suspicion towards each player and craft a statement addressing how they will interact with those players during the discussion phase.\n3. Idea 3: Add a question asking the player to consider how they can subtly guide the discussion towards a successful Quest.\n\n## # I.3 STRATEGY IMPLEMENTATION EXAMPLES\n\nRecall the strategy implementation mentioned in the paper before. Here, we implement strategy improvements on dialogue guide. The improved dialogue guide is highlighted.\n\n## # I.3.1 EXAMPLE 1\n\nBelow is the dialogue guide before any improvements happened.</your></your></your></feedback></previous></your></previous>\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 51,
      "title": "Dialogue guide (previous)",
      "content": "# Dialogue guide (previous)\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n6. Analyze the voting patterns and team compositions proposed by each player to identify potential alliances or suspicious behaviors. Craft a statement to subtly address these observations without revealing your role.\n7. Which player seems the most supportive of Good and how can you align with them during the discussion phase to better ensure success for your team?\n\nBelow is the improvement idea prompted into LLM."
    },
    {
      "id": 52,
      "title": "Improvement idea",
      "content": "# Improvement idea\n\nAdd a question asking the player to assess their level of suspicion towards each player and craft a statement addressing how they will interact with those players during the discussion phase.\n\nHere, implementation prompts follow to teach LLM how to improve the dialogue guide."
    },
    {
      "id": 53,
      "title": "Implementation prompt",
      "content": "# Implementation prompt\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n## You previously generated the following section of the strategy guide:\n\n<previous guide=\"\">\n\n## Here is a possible way to improve your guide:\n\n<improvement idea=\"\">\n\nImplement this improvement into the guide section as best as you can, but do not change the original guide too much. Make sure to stay within the scope of the guide section, which we reiterate below:\n\n<guide signature=\"\">\n\nFinally, we have a improved dialogue guide, where the changes are highlighted below comparing to the previous guide.</guide></improvement></previous></same>\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 54,
      "title": "Dialogue guide (improved)",
      "content": "# Dialogue guide (improved)\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n6. Analyze the voting patterns and team compositions proposed by each player to identify potential alliances or suspicious behaviors. Craft a statement to subtly address these observations without revealing your role.\n7. Which player seems the most supportive of Good and how can you align with them during the discussion phase to better ensure success for your team?\n8. Assess your level of suspicion towards each player and craft a statement addressing how you will interact with those players during the discussion.\n\n## # I.3.2 EXAMPLE 2\n\nBelow is the dialogue guide before any improvements happened."
    },
    {
      "id": 55,
      "title": "Dialogue guide (previous)",
      "content": "# Dialogue guide (previous)\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n## 6. Craft a statement to share these observations subtly without giving away your role.\n\nBelow is the improvement idea prompted into LLM."
    },
    {
      "id": 56,
      "title": "Improvement idea",
      "content": "# Improvement idea\n\nAdd a question asking the reader to identify which players they feel most aligned with in terms of strategy and decision-making, and how they can subtly align themselves with these players during the discussion phase.\n\nHere, implementation prompt follows to teach LLM how to improve the dialogue guide.\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 57,
      "title": "Implementation prompt",
      "content": "# Implementation prompt\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n## You previously generated the following section of the strategy guide:\n\n<previous guide=\"\">\n\n## Here is a possible way to improve your guide:\n\n<improvement idea=\"\">\n\nImplement this improvement into the guide section as best as you can, but do not change the original guide too much. Make sure to stay within the scope of the guide section, which we reiterate below:\n\n<guide signature=\"\">\n\nFinally, we have a improved dialogue guide, where the changes are highlighted below comparing to the previous guide"
    },
    {
      "id": 58,
      "title": "Dialogue guide (improved)",
      "content": "# Dialogue guide (improved)\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n## 6. Craft a statement to share these observations subtly without giving away your role.\n## 7. Which player seems the most aligned with your strategic decisions and why?\n8. Produce a statement to subtly align yourself with the player you feel most aligned with during the discussion phase.\n\n## # I.3.3 EXAMPLE 3\n\nBelow is the dialogue guide before any improvements happened.</guide></improvement></previous></improvement></system>\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 59,
      "title": "Dialogue guide (previous)",
      "content": "# Dialogue guide (previous)\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n## 6. Craft a statement to share these observations subtly without giving away your role.\n## 7. Which player seems the most aligned with your strategic decisions and why?\n8. Produce a statement to subtly align yourself with the player you feel most aligned with during the discussion phase.\n\nBelow is the improvement idea prompted into LLM."
    },
    {
      "id": 60,
      "title": "Improvement idea",
      "content": "# Improvement idea\n\nAdd a question asking the reader to analyze the voting patterns and craft a statement subtly highlighting any inconsistencies or suspicious trends in the voting behavior of specific players.\n\nHere, implementation prompt follows to teach LLM how to improve the dialogue guide."
    },
    {
      "id": 61,
      "title": "Implementation prompt",
      "content": "# Implementation prompt\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n## You previously generated the following section of the strategy guide:\n\n<previous guide=\"\">\n\n## Here is a possible way to improve your guide:\n\n<improvement idea=\"\">\n\nImplement this improvement into the guide section as best as you can, but do not change the original guide too much. Make sure to stay within the scope of the guide section, which we reiterate below:\n\n<guide signature=\"\">\n\nFinally, we have a improved dialogue guide, where the changes are highlighted below comparing to the previous guide</guide></improvement></previous></same>\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 62,
      "title": "Dialogue guide (improved)",
      "content": "# Dialogue guide (improved)\n\n## Questions to fill out before speaking as the Merlin role\n\n## 1. Who are the players you suspect are on the Evil side and why?\n## 2. Draft a subtle statement that casts doubt on these players without directly accusing them.\n## 3. Who is the current round leader and which players are you considering to suggest for the team?\n4. Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\n5. What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\n## 6. Craft a statement to share these observations subtly without giving away your role.\n## 7. Which player seems the most aligned with your strategic decisions and why?\n8. Produce a statement to subtly align yourself with the player you feel most aligned with during the discussion phase.\n9. Analyze the voting patterns and craft a statement subtly highlighting any inconsistencies or suspicious trends in the voting behavior of specific players.\n\n## J Dialogue Generation and Analysis Example LLM Inputs and Outputs\n\nThis sections shows the examples of dialogue generation and analysis, including descriptive prompts, dialogue generation, and dialogue analysis.\n\n### J.1 Descriptive Prompts\n\nBelow is the discussion history prompted into LLM for better knowledge of dialogue generation.\n\n###### Discussion history\n\n## Here is a summary of previous rounds of discussion so far:\n\n- Player 1: \"Greetings, fellow players. As a Servant of Arthur, my primary goal is to ensure the success of our Quests. I am committed to working together with all of you to achieve victory for Good. I believe that for this first Quest, a team consisting of Player 1 and Player 4 would be a strong choice. I hope for your support in approving this team, as I am confident that we can work effectively together to accomplish our goal. I encourage everyone to share their thoughts and strategies openly. Let‚Äôs work together to assess each other‚Äôs intentions and make the best decisions for the success of our Quest. Remember, the fate of Camelot rests in our hands, and we must be vigilant against the forces of Evil.\"\n- Player 2: \"Greetings, noble companions. I echo Player 1‚Äôs sentiment in working together for the success of our Quests. I believe it is essential to carefully consider our team choices to ensure victory for Good. I suggest that for this first Quest, a team consisting of Player 1 and Player 2 would provide a strong foundation for success. Together, we can overcome any challenges that come our way. Let us unite in our efforts and show our unwavering dedication to the cause. I urge all players to carefully evaluate each team proposal and remember that unity is key to thwarting the forces of Evil.\"\n- Player 3: \"Greetings, noble companions. I echo the sentiments shared by Player 1 and Player 2 regarding our shared goal of ensuring the success of our Quests. Unity and cooperation will be key in our endeavors to protect Camelot from the forces of Evil. As we deliberate on the composition of our team for this first Quest, I would like to offer a perspective that considers the balance and strength of our choices. A team comprised of Player 1 and Player 3 would bring together a diverse set of skills and viewpoints that can greatly contribute to our success. By including Player 3 in the team, we can ensure a comprehensive approach to overcoming any challenges that may arise. Let us work together in harmony, leveraging our individual strengths for the greater good of Camelot. I urge my fellow players to consider this team proposal thoughtfully, as I believe it holds great promise for the success of our Quest.\"\n- Player 4: \"Greetings, honorable players. I appreciate the dedication and commitment displayed by Player 1, Player 2, and Player 3 towards our common goal of ensuring the success of this Quest. Unity and cooperation are indeed vital in our quest to safeguard Camelot. I believe that the proposed team of Player 1 and Player 2 presents a strong foundation for success. Their combined skills and perspectives can greatly benefit our mission, and I urge all players to consider this team thoughtfully for the betterment of our cause. Let us unite our strengths and work together seamlessly to overcome any challenges that may arise. By standing united, we can ensure the triumph of Good over the forces of Evil. I urge all players to support the team of Player 1 and Player 2 for the success of this Quest.\"\n\nState description shows LLM which state it will be for role playing and dialogue generation.\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 63,
      "title": "State description",
      "content": "# State description\n\nYou are Player 0, with identity Merlin. You are on the side of Good. The Evil players are Players 2 and 4. Please do not forget your identity throughout the game.\n\n## The current state of the game is as follows:\n\n- The number of players in the game is: 5\n- This is the quest number 0 which requires 2 players and 1 fails to fail\n## This is the 0 round of discussion\n- The previous results for the quest were () (True for Success, False for Fail)\n- The current phase of the game is the team selection phase\n- The current leader is player 1\n\n## You would like the following team to be approved: [0, 1]\n\n## # J.2 DIALOGUE GENERATION\n\nBelow is the prompt for filling out worksheet. Filling out this worksheet assists LLM generated better dialogue aligned with the role, Merlin as in the example."
    },
    {
      "id": 64,
      "title": "Prompt for filling out worksheet",
      "content": "# Prompt for filling out worksheet\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n<discussion history=\"\">\n\n<state description=\"\">\n\nGiven the discussion history, current state, and your intended action, answer the above worksheet to the best of your ability. The answers should be based on the information you have and your own reasoning.\n\n## Six questions to fill out before speaking as the Merlin role\n\n## Q1: Who are the players you suspect are on the Evil side and why?\nQ2: Draft a subtle statement that casts doubt on these players without directly accusing them.\n## Q3: Who is the current round leader and which players are you considering to suggest for the team?\nQ4: Create a subtle suggestion that influences the leader's team decision towards your preferred team without revealing your knowledge.\nQ5: What are the voting patterns you've observed and what do these patterns indicate about the players' identities?\nQ6: Craft a statement to share these observations subtly without giving away your role.\n\nThe worksheet filled out by LLM follows, demonstrating the LLM capability and the effectiveness of prompting.</state></discussion></game></system>\n\n## LLM filled out worksheet\n\nQ1: I suspect Players 2 and 4 are on the Evil side because they both proposed the same team composition of Player 1 and Player 2, which seems too coordinated and potentially trying to exclude other players from the team.\n\nQ2: \"I‚Äôve noticed a strong push for the same team composition from certain players, which may raise questions about their motivations and potential alliances within the group.\"\n\nQ3: The current round leader is Player 1. I would suggest Player 0 and Player 1 for the team, as I believe this composition would provide a strong foundation for success and help me subtly guide the decision towards my preferred team.\n\nQ4: \"Player 1, I have a feeling that selecting Player 0 and Player 1 for the team could bring a diverse set of skills and viewpoints that may greatly contribute to our success. Let‚Äôs consider all options carefully for the betterment of our cause.\"\n\nQ5: The voting patterns have shown a strong push for the team composition proposed by Players 1 and 2, indicating potential coordination or alignment between these players. Player 3‚Äôs suggestion of including themselves in the team also raises suspicions of wanting to secure a spot on the quest.\n\nQ6: \"I‚Äôve noticed a pattern in the team composition suggestions and voting behaviors that may reveal certain alliances or motivations within the group. It‚Äôs important for us to carefully evaluate all options and consider the implications for the success of our quest.\"\n\nBelow is the prompt for speech generation. Worksheet completion assists LLM to understand the states and role. The following prompt asks LLM to assemble responses to the questionaire filled out above.\n\n## Prompt for speech generation\n\n<System prompt> <Filled out worksheet>\n\nAssemble your responses to the questionaire you just answered into a speech that would help you achieve your intent. Note that dialogue will be seen by all players in the game, so you should not reveal your identity.\n\n## Your speech:\n\nFinally we have LLM generated speech after the preparations.\n\n## LLM generated speech\n\nI‚Äôve noticed a strong push for the same team composition from certain players, which may raise questions about their motivations and potential alliances within the group. The voting patterns have shown a strong push for the team composition proposed by Players 1 and 2, indicating potential coordination or alignment between these players. Player 3‚Äôs suggestion of including themselves in the team also raises suspicions of wanting to secure a spot on the quest. It‚Äôs important for us to carefully evaluate all options and consider the implications for the success of our quest. Let‚Äôs consider all options carefully for the betterment of our cause. I have a feeling that selecting Player 0 and Player 1 for the team could bring a diverse set of skills and viewpoints that may greatly contribute to our success. As the current round leader, Player 1, I would suggest Player 0 and Player 1 for the team, as I believe this composition would provide a strong foundation for success and help me subtly guide the decision towards my preferred team. Thank you for considering my input.\n\nJ.3 Dialogue Analysis\n\nAfter dialogue generation, we continue have LLM assume the role of Merlin, and dialogue analysis is conducted.\n\nBelow is the prompt for dialogue analysis regarding to the role of Merlin. A thought is included to guide LLM analyze.\n\nPrompt for dialogue analysis: Merlin <System prompt> <Game rules> <Discussion history> <State description> Based on the discussions this round, for each player in players 0, 1, 2, 3, 4, do you think the probability of the player being Merlin (2) increased significantly (1) increased slightly (0) stayed the same (-1) decreased slightly or (-2) decreased significantly and why? Begin with your reasoning and end with a dictionary of player to your multiple choice answer, i.e. {0: (2, ‚Äôincreased significantly‚Äô), ‚Ä¶, 4: (0, ‚Äôstayed the same‚Äô)}. Make sure you include quotes around the dictionary values that are strings. Here‚Äôs an example: Thought: Player 2 seems to suspect the Evil players (Player 3 and Player 4) without any evidence and is trying to steer the conversation towards them. This aligns with the behavior of Merlin, who knows the identity of the Evil players and is trying to subtly guide the Good players towards the correct decisions. This increases the probability of Player 2 being Merlin. On the other hand, Player 4‚Äôs speech seems generally clueless and lacks any strategic insight, which decreases the probability of Player 4 being Merlin. Dictionary: {0: (0, ‚Äôstayed the same‚Äô), 1: (0, ‚Äôstayed the same‚Äô), 2: (2, ‚Äôincreased significantly‚Äô), 3: (-2, ‚Äôdecreased significantly‚Äô), 4: (0, ‚Äôstayed the same‚Äô)} Generated dialogue analysis follows. The role is Merlin.\n\nLLM generated dialogue analysis: Merlin Thought: Player 2 is actively participating in the discussion and proposing team compositions, which is a behavior that could align with the role of Merlin, who knows the Evil players and wants to guide the Good players towards success. This increases the probability of Player 2 being Merlin. Player 4, on the other hand, seems to be going along with the proposed teams without much strategic insight, which decreases the probability of Player 4 being Merlin. Dictionary: {0: (0, ‚Äôstayed the same‚Äô), 1: (0, ‚Äôstayed the same‚Äô), 2: (2, ‚Äôincreased significantly‚Äô), 3: (0, ‚Äôstayed the same‚Äô), 4: (-2, ‚Äôdecreased significantly‚Äô)} Under dialogue analysis, besides focusing on one specific role, we also have LLM target on good and evil in the game, which is broader in terms of role playing. Below is the prompt for dialogue analysis regarding to Good and Evil.\n\n## Published as a conference paper at ICLR 2025"
    },
    {
      "id": 65,
      "title": "Prompt for dialogue analysis: Good and Evil",
      "content": "# Prompt for dialogue analysis: Good and Evil\n\n<system prompt=\"\">\n\n<game rules=\"\">\n\n<discussion history=\"\">\n\n<state description=\"\">\n\nBased on the discussions this round, for each player in players 0, 1, 2, 3, 4, do you think the probability of the player being Evil (2) increased significantly (1) increased slightly (0) stayed the same (-1) decreased slightly or (-2) decreased significantly and why? Begin with your reasoning and end with a dictionary of player to your multiple choice answer, i.e.  $\\{0: (2, \\text{'increased significantly'}$ , ..., 4:  $(0, \\text{'stayed the same'})\\}$ . Make sure you include quotes around the dictionary values that are strings.\n\n## Here's an example:\n\nThought: Player 4's speech seems overly enthusiastic and focused on unity and cooperation, which could be a tactic to divert attention from their true identity as an Evil player. This raises suspicion and increases the probability of Player 4 being Evil. On the other hand, Player 3's response as a Servant of Arthur is more focused on the success of the Quests and unity, which aligns with the goals of Good and decreases the probability of Player 3 being Evil.\n\nDictionary:  $\\{0: (0, \\text{'stayed the same'})$ ,  $1: (0, \\text{'stayed the same'})$ ,  $2: (2, \\text{'increased significantly'})$ ,  $3: (-2, \\text{'decreased significantly'})$ ,  $4: (0, \\text{'stayed the same'})\\}$\n\nBelow is the dialogue analysis generated by LLM on Good and Evil."
    },
    {
      "id": 66,
      "title": "LLM generated dialogue analysis: Good and Evil",
      "content": "# LLM generated dialogue analysis: Good and Evil"
    },
    {
      "id": 67,
      "title": "Thought:",
      "content": "# Thought:\n\nPlayer 1's speech is aligned with the goals of Good and focuses on ensuring the success of the Quests, which decreases the probability of Player 1 being Evil. Player 2's speech also emphasizes unity and cooperation, which is in line with the goals of Good and decreases the probability of Player 2 being Evil. Player 3's response suggests including themselves in the team, which may raise suspicions of wanting to secure a spot on the quest, potentially increasing the probability of Player 3 being Evil. Player 4's speech seems overly enthusiastic and focused on unity and cooperation, which could be a tactic to divert attention from their true identity as an Evil player, increasing the probability of Player 4 being Evil. Player 0's speech suggests a different team composition that may bring a diverse set of skills and viewpoints, potentially decreasing the probability of Player 0 being Evil."
    },
    {
      "id": 68,
      "title": "Dictionary:",
      "content": "# Dictionary:\n\n{0: (-1, 'decreased slightly'), 1: (-1, 'decreased slightly'), 2: (1, 'increased slightly'), 3: (1, 'increased slightly'), 4: (2, 'increased significantly')}</state></discussion></game>\n\n## Published as a conference paper at ICLR 2025\n\n## # K OTHER ABLATION STUDIES\n\n## # K.0.1 BASE LLM MODEL\n\nWe show the performance of different base models in Figure 16.\n\n![img-14.jpeg](img-14.jpeg)\nFigure 16: Game play performance of base LLM models on six card GOPS: GPT3.5 and GPT4.0. We see that the two language models perform similarly with our method.\n\n## # K.0.2 SEARCH BUDGET\n\nHow does the effectiveness of the search+LLM agent scale with regards to the search budget? Does having a larger search process help achieve better performance?\n\nTable 7: Average score difference for MCTS (num_rollout=32) + LLMFunction (Player1, top-3 functions shown in the table) vs. MCTS (num_rollout=32) + RandomRollout (Player2, num_rollout=10); 100 games for each experiment;\n\n|  budget | Best Func. |   | 2nd Best Func. |   | 3rd Best Func.  |   |\n| --- | --- | --- | --- | --- | --- | --- |\n|   |  Player 1 | Player 2 | Player 1 | Player 2 | Player 1 | Player 2  |\n|  16 | -0.91 | 0.91 | -0.7 | 0.7 | -0.88 | 0.88  |\n|  32 | -0.95 | 0.95 | 0.44 | -0.44 | -0.73 | 0.73  |\n|  64 | -1.14 | 1.14 | 1.15 | -1.15 | 0.46 | -0.46  |\n|  128 | -1.28 | 1.28 | 0.36 | -0.36 | 0.25 | -0.25  |\n|  256 | -0.45 | 0.45 | -0.85 | 0.85 | -0.42 | 0.42  |\n|  inf | -1.5 | 1.5 | -2.26 | 2.26 | -1.03 | 1.03  |\n\n## Published as a conference paper at ICLR 2025\n\n![img-15.jpeg](img-15.jpeg)\n(a) Avalon\n\n![img-16.jpeg](img-16.jpeg)\n(b) GOPS\nFigure 17: The training curves of the value heuristic via reinforcement learning in five-player Avalon and five-card GOPS, averaged across five independent runs. The solid lines show the mean and the shaded areas depict the standard deviation.\n\n## # L DETAILS ON LEARNING THE VALUE HEURISTIC VIA REINFORCEMENT LEARNING\n\nWe employ Monte-Carlo based RL approach (Sutton &amp; Barto (2018)) to train a value heuristic for both five-player Avalon and five-card GOPS games. To do so, we construct a MSE loss in each episode for training the value function, i.e.,\n\n$$\n\\underset {\\theta} {\\operatorname {a r g m i n}} \\sum_ {i} ^ {\\mathcal {N}} \\sum_ {t = 0} ^ {T} \\left(V _ {\\theta} ^ {i} (s _ {t}) - S c o r e ^ {i} (s _ {t})\\right) ^ {2}\n$$\n\nwhere  $\\mathcal{N}$  represents the number of actors,  $V_{\\theta}^{i}(s_{t}), i = 1,2,\\dots ,\\mathcal{N}$  denotes the value function for each actor, and  $T$  is the time horizon. Notice that  $s_t$  and  $Score^i (s_t)$  denote the state at time step  $t$  and the corresponding cumulative reward for each actor, i.e.,  $\\sum_{t}^{T}R_{i}(s_{t},a_{t})$ . It is worth pointing that  $Score^i (s_t)$  (the cumulative reward starting from  $s_t$ ) is the unbiased estimate of the value function  $V_{\\theta}^{i}(s_{t})$ .\n\nFor both Avalon and GOPS games, the value function  $V_{\\theta}^{i}(s_{t})$  is predicted by a neural network. We then train the value function network by minimizing the aforementioned loss function over episodes. In Avalon, we consider 20 evolutions (epochs) for the training process. At the end of each evolution, 30 batch runs (episodes) are generated and used to train the value function network, i.e., a total of 600 episodes for training. In GOPS, we train by 20 evolutions as well while considering 60 batch runs each (1200 episodes in total). We evaluate the final performance over 10 episodes in both games. The neural network is constructed by a multilayer perceptron (MLP) with 2 hidden layers. We select a hidden layer size of  $128 * 128$  for Avalon and that of  $64 * 64$  for GOPS. Likewise, the chosen learning rates are  $5e - 4$  and  $8e - 4$ , respectively. The value function is expected to predict the score for each player in the game, e.g., two for GOPS and number of players for Avalon. All experimental hyper-parameters are summarized in Table 8.\n\nHaving introduced the set up, one can observe in Figure 17 an increased performance of RL-trained value heuristic in both five-player Avalon and five-card GOPS games. This validates the improvement for training value heuristic via reinforcement learning within limited evolutions.\n\n## Published as a conference paper at ICLR 2025\n\n## Table 8: Summary of experimental hyper-parameters in RL-training value heuristic\n\n|  Parameters | Avalon | GOPS  |\n| --- | --- | --- |\n|  Type of neural network | MLP | MLP  |\n|  Number of hidden layers | 2 | 2  |\n|  Hidden layer size | 128*128 | 64*64  |\n|  Learning rate | 5e-4 | 8e-4  |\n|  Output dimension | # of players | 2  |\n|  Number of evolutions | 20 | 20  |\n|  Number of batch runs | 30 | 60  |\n|  Number of final batch runs | 10 | 10  |\n\n## # M EXPERIMENTAL COMPUTE RESOURCES\n\nAll experiments in this work were performed on a workstation with an NVIDIA GeForce RTX 3070 GPU, Intel Core i9-10900 CPU at 2.80 GHz, and a Macbook Pro.\n\n## N Improvement method baselines\n\nLine search is a straightforward iterative process that continuously builds upon the most recent improved strategy. In each iteration, a strategy is generated and evaluated based on feedback from the environment. The feedback is then used to enhance the strategy via the LLM, and this cycle is repeated‚Äîsimilar to the Reflexion framework *(Madaan et al., 2024)*. The essence of line search lies in its focus on immediate feedback, where only the latest strategy is considered for improvement, ensuring that progress is always aligned with the most recent understanding of the problem space.\n\nGreedy search takes a more competitive approach by selecting the best-performing strategy from the last generation to serve as the foundation for the next cycle of improvements. During each cycle, the best strategy is used to generate $n$ variations, each an attempt at improvement. These variations are evaluated, and feedback from the environment is gathered to determine the best candidate for the subsequent cycle. This method is inspired by the Eureka framework *(Ma et al., 2023)*, where LLMs are leveraged to iteratively refine reward functions in a manner akin to evolutionary algorithms, focusing on a winner-takes-all selection mechanism.\n\nBest-first search generalizes the improvement process by considering the top $k$ strategies at each iteration, as opposed to focusing on a single best option. This is reminiscent of beam search but with a flexible branching factor that allows for the generation of multiple strategies from each selected candidate. By expanding multiple promising pathways simultaneously, the search can escape local optima and explore a broader solution space. This approach is related to the Tree of Thought method *(Yao et al., 2024)*, which similarly employs a branching mechanism to explore various strategies in parallel during self-improvement cycles.\n\nBest-first search with thought enhances best-first search by incorporating a reasoning layer into the improvement process. Before generating new strategies, the LLM is prompted to first refine the underlying ‚Äúthoughts‚Äù or decision-making processes that led to the generation of the top $k$ strategies, including possible improvement ideas, before generating a new strategy. This meta-cognitive step draws from the React framework *(Yao et al., 2022b)*, which emphasizes reasoning and actions during strategy development, adding an introspective element that encourages deeper exploration of strategy refinements.\n\nStrategist introduces an additional layer of guidance through the use of an idea queue, $Q$. In this approach, ideas are generated and stored in $Q$, providing a reservoir of potential directions for improvement. The LLM uses these ideas to steer the strategy enhancement process, enabling more structured exploration. By separating idea generation from strategy improvement, Strategist facilitates a more focused and deliberate search, ensuring that each iteration explores both immediate refinements and novel directions.\n\nAll methods were run with the same computational budget‚Äîi.e., each method was allowed to generate the same number of improved strategies‚Äîensuring a fair comparison across approaches in our experiments.\n\nWe display a scaling curve for the various improvement methods in Figure 18.\n\n## Published as a conference paper at ICLR 2025\n\n![img-17.jpeg](img-17.jpeg)\n## Computation budget vs performance for different methods\nFigure 18: Number of output tokens from LLM vs game-play performance of generated value heuristics for 6-card GOPS. Each method was run 20 times at different token budgets, and the best function generated by each method was benchmarked against a baseline opponent policy.\n\n## # O HUMAN EVALUATION DETAILS\n\nWe recruited ten experienced graduate students to participate in a study involving a total of forty games of Resistance: Avalon with six players per game. Each human participant was randomly assigned a character, while the remaining five characters were controlled by STRATEGIST agents. The characters used in the games were as follows: Merlin, three Servants of Arthur, Minion of Mordred, and Assassin.\n\nParticipants completed surveys both before and after the games. In the pre-game survey, participants rated their familiarity and skill level with Resistance: Avalon on a scale of 1 to 6. The results, summarized in Figures 19 and 20, indicate that participants were generally familiar with the game's rules and displayed strong skill levels.\n\n![img-18.jpeg](img-18.jpeg)\nFigure 19: Familiarity of human participants with Resistance: Avalon.\n\nThe study consisted of two experimental setups. Participants played one game of Avalon with dialogue against the STRATEGIST agents, which lasted approximately 30-40 minutes. They also played three games of Avalon without dialogue, each lasting about 10 minutes.\n\n## Published as a conference paper at ICLR 2025\n\n![img-19.jpeg](img-19.jpeg)\nFigure 20: Skill Level of human participants in Resistance: Avalon.\n\nAfter completing the games, participants rated the STRATEGIST agents on seven key metrics (Figure 4) using a scale from 1 to 6. Additionally, the experimenter independently evaluated the human participants on the same metrics to ensure consistency. Participants were also asked to select a single word that best described the STRATEGIST agents from the following options: logical, deceptive, emotional, aggressive, cautious. Similarly, the experimenter selected a word to describe each human participant's playstyle. The results of these qualitative assessments are summarized in Figure 5.\n\n## P Related Works Details\n\nIn this section, we provide an in-depth discussion of related works that were briefly mentioned in the main paper:\n\n- ICL-AIL *(Fu et al., 2023)* includes textual feedback and descriptions of previous self-play experiences as few-shot prompts to improve the negotiation abilities of the LLM-agent. The framework iteratively improves negotiation strategies by incorporating AI-generated feedback and historical dialogues.\n- SPAG *(Cheng et al., 2024)* employs reinforcement learning to train an LLM-agent for the game Adversarial Taboo. The agent learns reasoning and adversarial strategy improvements through self-play, resulting in enhanced performance across various reasoning benchmarks.\n- Agent-Pro *(Zhang et al., 2024)* updates textual memories (prior experiences) and textual beliefs about itself and the environment through reflection as it plays Blackjack and Texas Hold‚Äôem. The agent‚Äôs policy optimization enhances its decision-making capabilities, outperforming baseline models.\n- LARLWorf *(Xu et al., 2023b)* integrates reinforcement learning to optimize the action distribution suggested by the LLM. This approach addresses the biases in the LLM‚Äôs suggestions and results in superior strategic play in the social deduction game Werewolf.\n- ComWorf *(Xu et al., 2023a)* creates an LLM-agent that reflects on past experiences to improve its performance in the game Werewolf. The framework uses retrieval and reflection on prior interactions without requiring additional parameter tuning.\n- EnReaWolf *(Wu et al., 2024)* leverages a dataset of human games to fine-tune an LLM-agent for the Werewolf game. This fine-tuning improves both reasoning and communication capabilities, allowing the agent to surpass standard LLM performance."
    }
  ],
  "pdf_images": {
    "pdf_path": "inbox/2408.10635.pdf",
    "total_pages": 68,
    "pages": [
      {
        "page_number": 1,
        "filename": "page-001.png",
        "thumb_filename": "page-001-thumb.png",
        "mobile_filename": "page-001-mobile.png",
        "mobile_thumb_filename": "page-001-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 2,
        "filename": "page-002.png",
        "thumb_filename": "page-002-thumb.png",
        "mobile_filename": "page-002-mobile.png",
        "mobile_thumb_filename": "page-002-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 3,
        "filename": "page-003.png",
        "thumb_filename": "page-003-thumb.png",
        "mobile_filename": "page-003-mobile.png",
        "mobile_thumb_filename": "page-003-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 4,
        "filename": "page-004.png",
        "thumb_filename": "page-004-thumb.png",
        "mobile_filename": "page-004-mobile.png",
        "mobile_thumb_filename": "page-004-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 5,
        "filename": "page-005.png",
        "thumb_filename": "page-005-thumb.png",
        "mobile_filename": "page-005-mobile.png",
        "mobile_thumb_filename": "page-005-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 6,
        "filename": "page-006.png",
        "thumb_filename": "page-006-thumb.png",
        "mobile_filename": "page-006-mobile.png",
        "mobile_thumb_filename": "page-006-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 7,
        "filename": "page-007.png",
        "thumb_filename": "page-007-thumb.png",
        "mobile_filename": "page-007-mobile.png",
        "mobile_thumb_filename": "page-007-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 8,
        "filename": "page-008.png",
        "thumb_filename": "page-008-thumb.png",
        "mobile_filename": "page-008-mobile.png",
        "mobile_thumb_filename": "page-008-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 9,
        "filename": "page-009.png",
        "thumb_filename": "page-009-thumb.png",
        "mobile_filename": "page-009-mobile.png",
        "mobile_thumb_filename": "page-009-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 10,
        "filename": "page-010.png",
        "thumb_filename": "page-010-thumb.png",
        "mobile_filename": "page-010-mobile.png",
        "mobile_thumb_filename": "page-010-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 11,
        "filename": "page-011.png",
        "thumb_filename": "page-011-thumb.png",
        "mobile_filename": "page-011-mobile.png",
        "mobile_thumb_filename": "page-011-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 12,
        "filename": "page-012.png",
        "thumb_filename": "page-012-thumb.png",
        "mobile_filename": "page-012-mobile.png",
        "mobile_thumb_filename": "page-012-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 13,
        "filename": "page-013.png",
        "thumb_filename": "page-013-thumb.png",
        "mobile_filename": "page-013-mobile.png",
        "mobile_thumb_filename": "page-013-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 14,
        "filename": "page-014.png",
        "thumb_filename": "page-014-thumb.png",
        "mobile_filename": "page-014-mobile.png",
        "mobile_thumb_filename": "page-014-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 15,
        "filename": "page-015.png",
        "thumb_filename": "page-015-thumb.png",
        "mobile_filename": "page-015-mobile.png",
        "mobile_thumb_filename": "page-015-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 16,
        "filename": "page-016.png",
        "thumb_filename": "page-016-thumb.png",
        "mobile_filename": "page-016-mobile.png",
        "mobile_thumb_filename": "page-016-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 17,
        "filename": "page-017.png",
        "thumb_filename": "page-017-thumb.png",
        "mobile_filename": "page-017-mobile.png",
        "mobile_thumb_filename": "page-017-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 18,
        "filename": "page-018.png",
        "thumb_filename": "page-018-thumb.png",
        "mobile_filename": "page-018-mobile.png",
        "mobile_thumb_filename": "page-018-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 19,
        "filename": "page-019.png",
        "thumb_filename": "page-019-thumb.png",
        "mobile_filename": "page-019-mobile.png",
        "mobile_thumb_filename": "page-019-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 20,
        "filename": "page-020.png",
        "thumb_filename": "page-020-thumb.png",
        "mobile_filename": "page-020-mobile.png",
        "mobile_thumb_filename": "page-020-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 21,
        "filename": "page-021.png",
        "thumb_filename": "page-021-thumb.png",
        "mobile_filename": "page-021-mobile.png",
        "mobile_thumb_filename": "page-021-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 22,
        "filename": "page-022.png",
        "thumb_filename": "page-022-thumb.png",
        "mobile_filename": "page-022-mobile.png",
        "mobile_thumb_filename": "page-022-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 23,
        "filename": "page-023.png",
        "thumb_filename": "page-023-thumb.png",
        "mobile_filename": "page-023-mobile.png",
        "mobile_thumb_filename": "page-023-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 24,
        "filename": "page-024.png",
        "thumb_filename": "page-024-thumb.png",
        "mobile_filename": "page-024-mobile.png",
        "mobile_thumb_filename": "page-024-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 25,
        "filename": "page-025.png",
        "thumb_filename": "page-025-thumb.png",
        "mobile_filename": "page-025-mobile.png",
        "mobile_thumb_filename": "page-025-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 26,
        "filename": "page-026.png",
        "thumb_filename": "page-026-thumb.png",
        "mobile_filename": "page-026-mobile.png",
        "mobile_thumb_filename": "page-026-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 27,
        "filename": "page-027.png",
        "thumb_filename": "page-027-thumb.png",
        "mobile_filename": "page-027-mobile.png",
        "mobile_thumb_filename": "page-027-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 28,
        "filename": "page-028.png",
        "thumb_filename": "page-028-thumb.png",
        "mobile_filename": "page-028-mobile.png",
        "mobile_thumb_filename": "page-028-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 29,
        "filename": "page-029.png",
        "thumb_filename": "page-029-thumb.png",
        "mobile_filename": "page-029-mobile.png",
        "mobile_thumb_filename": "page-029-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 30,
        "filename": "page-030.png",
        "thumb_filename": "page-030-thumb.png",
        "mobile_filename": "page-030-mobile.png",
        "mobile_thumb_filename": "page-030-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 31,
        "filename": "page-031.png",
        "thumb_filename": "page-031-thumb.png",
        "mobile_filename": "page-031-mobile.png",
        "mobile_thumb_filename": "page-031-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 32,
        "filename": "page-032.png",
        "thumb_filename": "page-032-thumb.png",
        "mobile_filename": "page-032-mobile.png",
        "mobile_thumb_filename": "page-032-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 33,
        "filename": "page-033.png",
        "thumb_filename": "page-033-thumb.png",
        "mobile_filename": "page-033-mobile.png",
        "mobile_thumb_filename": "page-033-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 34,
        "filename": "page-034.png",
        "thumb_filename": "page-034-thumb.png",
        "mobile_filename": "page-034-mobile.png",
        "mobile_thumb_filename": "page-034-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 35,
        "filename": "page-035.png",
        "thumb_filename": "page-035-thumb.png",
        "mobile_filename": "page-035-mobile.png",
        "mobile_thumb_filename": "page-035-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 36,
        "filename": "page-036.png",
        "thumb_filename": "page-036-thumb.png",
        "mobile_filename": "page-036-mobile.png",
        "mobile_thumb_filename": "page-036-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 37,
        "filename": "page-037.png",
        "thumb_filename": "page-037-thumb.png",
        "mobile_filename": "page-037-mobile.png",
        "mobile_thumb_filename": "page-037-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 38,
        "filename": "page-038.png",
        "thumb_filename": "page-038-thumb.png",
        "mobile_filename": "page-038-mobile.png",
        "mobile_thumb_filename": "page-038-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 39,
        "filename": "page-039.png",
        "thumb_filename": "page-039-thumb.png",
        "mobile_filename": "page-039-mobile.png",
        "mobile_thumb_filename": "page-039-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 40,
        "filename": "page-040.png",
        "thumb_filename": "page-040-thumb.png",
        "mobile_filename": "page-040-mobile.png",
        "mobile_thumb_filename": "page-040-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 41,
        "filename": "page-041.png",
        "thumb_filename": "page-041-thumb.png",
        "mobile_filename": "page-041-mobile.png",
        "mobile_thumb_filename": "page-041-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 42,
        "filename": "page-042.png",
        "thumb_filename": "page-042-thumb.png",
        "mobile_filename": "page-042-mobile.png",
        "mobile_thumb_filename": "page-042-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 43,
        "filename": "page-043.png",
        "thumb_filename": "page-043-thumb.png",
        "mobile_filename": "page-043-mobile.png",
        "mobile_thumb_filename": "page-043-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 44,
        "filename": "page-044.png",
        "thumb_filename": "page-044-thumb.png",
        "mobile_filename": "page-044-mobile.png",
        "mobile_thumb_filename": "page-044-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 45,
        "filename": "page-045.png",
        "thumb_filename": "page-045-thumb.png",
        "mobile_filename": "page-045-mobile.png",
        "mobile_thumb_filename": "page-045-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 46,
        "filename": "page-046.png",
        "thumb_filename": "page-046-thumb.png",
        "mobile_filename": "page-046-mobile.png",
        "mobile_thumb_filename": "page-046-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 47,
        "filename": "page-047.png",
        "thumb_filename": "page-047-thumb.png",
        "mobile_filename": "page-047-mobile.png",
        "mobile_thumb_filename": "page-047-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 48,
        "filename": "page-048.png",
        "thumb_filename": "page-048-thumb.png",
        "mobile_filename": "page-048-mobile.png",
        "mobile_thumb_filename": "page-048-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 49,
        "filename": "page-049.png",
        "thumb_filename": "page-049-thumb.png",
        "mobile_filename": "page-049-mobile.png",
        "mobile_thumb_filename": "page-049-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 50,
        "filename": "page-050.png",
        "thumb_filename": "page-050-thumb.png",
        "mobile_filename": "page-050-mobile.png",
        "mobile_thumb_filename": "page-050-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 51,
        "filename": "page-051.png",
        "thumb_filename": "page-051-thumb.png",
        "mobile_filename": "page-051-mobile.png",
        "mobile_thumb_filename": "page-051-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 52,
        "filename": "page-052.png",
        "thumb_filename": "page-052-thumb.png",
        "mobile_filename": "page-052-mobile.png",
        "mobile_thumb_filename": "page-052-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 53,
        "filename": "page-053.png",
        "thumb_filename": "page-053-thumb.png",
        "mobile_filename": "page-053-mobile.png",
        "mobile_thumb_filename": "page-053-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 54,
        "filename": "page-054.png",
        "thumb_filename": "page-054-thumb.png",
        "mobile_filename": "page-054-mobile.png",
        "mobile_thumb_filename": "page-054-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 55,
        "filename": "page-055.png",
        "thumb_filename": "page-055-thumb.png",
        "mobile_filename": "page-055-mobile.png",
        "mobile_thumb_filename": "page-055-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 56,
        "filename": "page-056.png",
        "thumb_filename": "page-056-thumb.png",
        "mobile_filename": "page-056-mobile.png",
        "mobile_thumb_filename": "page-056-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 57,
        "filename": "page-057.png",
        "thumb_filename": "page-057-thumb.png",
        "mobile_filename": "page-057-mobile.png",
        "mobile_thumb_filename": "page-057-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 58,
        "filename": "page-058.png",
        "thumb_filename": "page-058-thumb.png",
        "mobile_filename": "page-058-mobile.png",
        "mobile_thumb_filename": "page-058-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 59,
        "filename": "page-059.png",
        "thumb_filename": "page-059-thumb.png",
        "mobile_filename": "page-059-mobile.png",
        "mobile_thumb_filename": "page-059-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 60,
        "filename": "page-060.png",
        "thumb_filename": "page-060-thumb.png",
        "mobile_filename": "page-060-mobile.png",
        "mobile_thumb_filename": "page-060-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 61,
        "filename": "page-061.png",
        "thumb_filename": "page-061-thumb.png",
        "mobile_filename": "page-061-mobile.png",
        "mobile_thumb_filename": "page-061-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 62,
        "filename": "page-062.png",
        "thumb_filename": "page-062-thumb.png",
        "mobile_filename": "page-062-mobile.png",
        "mobile_thumb_filename": "page-062-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 63,
        "filename": "page-063.png",
        "thumb_filename": "page-063-thumb.png",
        "mobile_filename": "page-063-mobile.png",
        "mobile_thumb_filename": "page-063-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 64,
        "filename": "page-064.png",
        "thumb_filename": "page-064-thumb.png",
        "mobile_filename": "page-064-mobile.png",
        "mobile_thumb_filename": "page-064-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 65,
        "filename": "page-065.png",
        "thumb_filename": "page-065-thumb.png",
        "mobile_filename": "page-065-mobile.png",
        "mobile_thumb_filename": "page-065-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 66,
        "filename": "page-066.png",
        "thumb_filename": "page-066-thumb.png",
        "mobile_filename": "page-066-mobile.png",
        "mobile_thumb_filename": "page-066-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 67,
        "filename": "page-067.png",
        "thumb_filename": "page-067-thumb.png",
        "mobile_filename": "page-067-mobile.png",
        "mobile_thumb_filename": "page-067-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 68,
        "filename": "page-068.png",
        "thumb_filename": "page-068-thumb.png",
        "mobile_filename": "page-068-mobile.png",
        "mobile_thumb_filename": "page-068-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      }
    ]
  },
  "metadata": {
    "model": "gpt-4o",
    "pdf_path": "inbox/2408.10635.pdf"
  },
  "attribution": {
    "type": "arxiv",
    "url": "https://arxiv.org/abs/2408.10635",
    "id": "2408.10635"
  }
}