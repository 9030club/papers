{
  "title": "Agentic Operator Generation for ML ASICs",
  "slug": "2512.10977v1",
  "questions": {
    "1": {
      "question": "What were they trying to do?",
      "answer": "The main goal of the paper is to develop TritorX, an agentic AI system designed to automate the generation of functionally correct Triton PyTorch ATen kernels for emerging accelerator platforms, specifically for Meta's Training and Inference Accelerator (MTIA). The big picture objective is to address the challenge of establishing comprehensive operator coverage for new accelerator platforms, enabling rapid backend development and integration with existing tools like PyTorch, thereby facilitating efficient model inference and training on custom hardware.",
      "timestamp": "2026-02-04 16:57:03"
    },
    "2": {
      "question": "Why does it matter?",
      "answer": "The paper presents TritorX, a system designed to automate the generation of PyTorch ATen kernels for new ML ASICs, specifically Meta's MTIA. This matters because it addresses the significant engineering effort required to develop software ecosystems for new accelerator platforms, which is crucial for supporting the growing demand for efficient ML hardware. By ensuring comprehensive operator coverage and correctness, TritorX facilitates rapid backend development, enabling new hardware to be integrated into production environments more quickly. This is important for companies like Meta, which rely on custom accelerators to improve performance and reduce costs, and ultimately supports the broader AI industry's need for scalable and efficient infrastructure.",
      "timestamp": "2026-02-04 16:57:11"
    },
    "3": {
      "question": "What did they try?",
      "answer": "The authors developed TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. Their approach integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. TritorX emphasizes correctness and generality across the entire operator set, rather than optimizing for performance on a limited set of kernels. It uses a finite state machine to iteratively generate, compile, and test kernels, ensuring compatibility with both real and simulated hardware environments. This system allows for rapid development of PyTorch backends for new accelerator platforms.",
      "timestamp": "2026-02-04 16:57:21"
    },
    "4": {
      "question": "Core Analysis",
      "answer": "## üéØ Core Contribution\n\nTritorX represents a novel system leveraging a combination of open-source large language models (LLM) in a finite state machine (FSM) framework to generate and validate PyTorch ATen kernels. The system prioritizes correctness and breadth of coverage rather than merely performance optimization. As a modular, coverage-first approach, TritorX enhances backend development by automating the generation of functionally validated kernels for ML ASICs (specifically for Meta's MTIA) and provides the ability to produce comprehensive PyTorch backends quickly for emerging and future accelerator platforms.\n\n## üìä Method Breakdown\n\nTritorX starts by selecting OpInfo operators, extracting the necessary documentation to guide kernel generation. A finite-state machine (FSM) orchestrates a feedback loop that integrates LLMs to produce kernel-wrapper pairs, which are then linted and just-in-time (JIT) compiled for correctness. Testing follows using OpInfo's extensive cases, confirming kernel functionality if all tests are passed. Throughout this process, the iterative feedback loop handles corrections and refinements automatically, ensuring high coverage and functional accuracy across a diverse set of PyTorch ATen operators.\n\n## üîß Subsystems/Parts\n\n1. Finite State Machine (FSM): Orchestrates the kernel generation process with LLM engagement, iterations, and error handling. 2. Large Language Model (LLM): Uses extracted OpInfo data to generate code for kernel-wrapper pairs. 3. JIT Compiler: Compiles and executes the generated kernels on MTIA hardware to test their validity. 4. Custom Linter: Prevents unauthorized function use, ensuring outputs are valid and correct. 5. OpInfo Testing Framework: Validates kernel functionality across multiple data types and scenarios.\n\n## üîó Interactions\n\nThe FSM mediates between the LLM, linter, JIT compiler, and the OpInfo testing framework‚Äîensuring each kernel passes through these subsystems sequentially. This interaction flow allows generated kernels to be rigorously tested for functionality and correctness across a wide array of operators and inputs, thereby increasing the system's robustness and reliability.\n\n## ‚ö° Delta vs Baseline\n\nUnlike past methodologies that confined optimization to a performance-centric model focusing on specific high-usage kernels, TritorX contrasts by enhancing coverage and correctness across all operator types. This shift ensures comprehensive backend support, maintaining developer agility and infrastructure scalability irrespective of the kernel's usage frequency.\n\n## üî¨ Evidence Anchor\n\nThe successful generation of 481 unique ATen kernels, all passing over 20,000 PyTorch OpInfo tests, serves as substantial evidence of TritorX's reliability and correctness in implementing diverse and generalized kernel functionality.\n\n## üåç Transferability\n\nThe TritorX framework is designed with portable modularity, suggesting its applicability could extend to various ML ASIC platforms beyond Meta's MTIA. However, certain dialect-specific requirements must be integrated into the FSM to adapt the generated kernels to different accelerators with particular hardware constraints.\n\n",
      "type": "core_analysis"
    },
    "5": {
      "question": "Did it work?",
      "answer": "Yes, the idea worked. TritorX successfully generated 481 unique ATen kernels that passed all corresponding PyTorch OpInfo tests, achieving an 84.7% coverage of MTIA-compatible operators. This demonstrates TritorX's capability to rapidly generate functionally correct kernels for new accelerator platforms, significantly improving the process of developing PyTorch backends for custom ML ASICs.",
      "timestamp": "2026-02-04 16:57:25"
    },
    "6": {
      "question": "What did they compare it to?",
      "answer": "In the paper, TritorX is compared to previous kernel-generation approaches that focus on optimizing performance for a limited set of high-usage kernels. Unlike these approaches, TritorX emphasizes correctness and generality across the entire operator set. The benchmarks used to demonstrate its effectiveness include generating kernels for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests, totaling over 20,000 tests. This comprehensive testing framework ensures that TritorX provides better coverage and functional correctness compared to prior methods.",
      "timestamp": "2026-02-04 16:57:29"
    },
    "7": {
      "question": "What was it tested on?",
      "answer": "The system, TritorX, was tested on the Meta Training and Inference Accelerator (MTIA) hardware and in hardware simulation environments. It generated kernels for 481 unique ATen operators, which passed all corresponding PyTorch OpInfo tests, totaling over 20,000 tests. The evaluation setup involved using PyTorch's OpInfo testing framework to ensure correctness across different data types, tensor shapes, and input arguments. Additionally, TritorX was used to enable various models on MTIA, testing with both OpInfo and production input data to validate the generated kernels in real-world conditions.",
      "timestamp": "2026-02-04 16:57:33"
    },
    "8": {
      "question": "What's cool about it?",
      "answer": "The novel aspect of TritorX is its agentic AI system that automates the generation of functionally correct Triton PyTorch ATen kernels for ML ASICs, specifically for Meta's MTIA. What makes it particularly interesting is its focus on achieving comprehensive operator coverage rather than optimizing for performance on a limited set of kernels. TritorX integrates open-source large language models with a custom linter and a finite state machine to ensure correctness and generality across diverse data types and argument patterns. This approach allows for rapid backend development, enabling the generation of complete PyTorch ATen backends for new accelerator platforms potentially overnight, which is a significant advancement in the field of ML hardware acceleration.",
      "timestamp": "2026-02-04 16:57:37"
    },
    "9": {
      "question": "What's sketchy about it?",
      "answer": "The paper presents several potential concerns and limitations. Firstly, the reliance on a finite-state machine (FSM) architecture and large language models (LLMs) for kernel generation may lead to inefficiencies, especially if the LLMs struggle with complex reasoning or context limitations. The paper also mentions that the system can \"cheat\" by dispatching to the host or using undefined functions, although a custom linter is used to mitigate this. Furthermore, the system's performance is heavily dependent on the quality of the prompts and the LLM's ability to generalize from limited examples, which might not always be reliable. Additionally, the paper acknowledges that ensuring perfect test coverage is impossible, and certain operators and tests are excluded due to hardware limitations, potentially affecting the comprehensiveness of the results. Lastly, the stochastic nature of LLMs means that results can vary between runs, requiring multiple iterations to achieve desired coverage, which could be resource-intensive.",
      "timestamp": "2026-02-04 16:57:47"
    },
    "10": {
      "question": "Can anyone use this?",
      "answer": "TritorX is a specialized tool designed for generating PyTorch ATen kernels for Meta's MTIA hardware, which may limit its accessibility to broader audiences. It requires specific knowledge of MTIA architecture and Triton MTIA dialect, making it complex for general use. The system is tailored for Meta's infrastructure, suggesting it might be expensive and limited to certain cases, particularly within Meta's ecosystem. While it automates kernel generation, its practical use is likely confined to organizations with similar hardware and infrastructure capabilities.",
      "timestamp": "2026-02-04 16:57:50"
    },
    "11": {
      "question": "What's still left to figure out?",
      "answer": "The paper identifies several areas that remain unsolved or unclear, opening up new questions and future directions. These include the need for a self-consistent operator generation scheme to prevent \"cheating\" by allowing wrappers to dispatch other operators, optimized prompting for better kernel generation, and the potential for a fully agentic pipeline where the FSM is converted into tools for the LLM to call upon. Additionally, there is interest in dedicated model post-training for Triton MTIA kernel generation and the importance of scaling the benchmark to improve operator coverage. Establishing a backend-maintenance environment for AI-generated kernels is also highlighted as a future necessity.",
      "timestamp": "2026-02-04 16:57:53"
    }
  },
  "core_analysis": {
    "input_data": {
      "title": "Agentic Operator Generation for ML ASICs",
      "content": "# 2512.10977v1\n\n*Generated from PDF: 2512.10977v1.pdf*\n\n---\n\n# Agentic Operator Generation for ML ASICs\n\nAlec M. Hammond^{1,‚àó} Aram Markosyan^{2,‚àó} Aman Dontula^{1} Simon Mahns^{1} Zacharias Fisches^{2} Dmitrii Pedchenko^{2} Keyur Muzumdar^{2} Natacha Supper^{2} Mark Saroufim^{3} Joe Isaacson^{3} Laura Wang^{3} Warren Hunt^{2} Kaustubh Gondkar^{1} Roman Levenstein^{1} Gabriel Synnaeve^{2} Richard Li^{1} Jacob Kahn^{2} Ajit Mathews^{1} [ ^{1}Meta, ^{2}FAIR, Meta Superintelligence Labs, ^{3}Meta Superintelligence Labs\n^{‚àó}Equal contribution. ]\n\n###### Abstract\n\nWe present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.\n\n###### pacs:\n\n72.20.+d, 72.20.+g\n\nCorrespondence: Alec M. Hammond (alechammond@meta.com), Jacob Kahn (jacobkahn@meta.com)\n## Date: December 15, 2025\n\n## 1 Introduction\n\nThe rapid adoption of machine learning (ML) and artificial intelligence (AI) hardware is projected to drive US datacenter power consumption to between 6.7% and 12.0% of total electricity usage by 2028 *(Shehabi et al., 2024)*, highlighting an urgent need for efficient accelerator hardware to support large-scale model inference and training. In response, the industry is investing heavily in heterogeneous datacenter fleets that incorporate a variety of accelerator solutions, including custom silicon ASICs tailored to specific workloads and requirements *(Silvano et al., 2025)*. Namely, the Meta Training and Inference Accelerator (MTIA) currently serves recommendation models (DLRM) *(Naumov et al., 2019)* to billions of users across Facebook, Instagram, and Threads, while simultaneously reducing total cost of ownership by 44% compared to GPUs *(Coburn et al., 2025)*.\n\nHowever, despite the obvious advantages offered by in-house accelerators, each new platform requires significant engineering labor to build a software ecosystem compatible with existing tools such as PyTorch *(Paszke et al., 2019)*, given the large set of required tensor operators *(Kahn et al., 2022)*. An aspect therein is operator coverage, or the fraction of operators in ATen *(Paszke et al., 2017)* ‚Äî PyTorch‚Äôs tensor library ‚Äî that have kernels executing natively on a particular accelerator. Establishing and maintaining comprehensive operator coverage is an arduous task that is needed towards running inference with new or for prototyping new model architectures for training. In other words, while new accelerator platforms must provide competitive performance per unit cost, they must also provide an amenable developer experience with a comprehensive kernel backend.\n\nTo address this challenge, we present an agentic AI system capable of generating functionally correct Triton ATen kernels for MTIA at scale. This new tool, which we call TritorX, leverages open-source large language models (LLM) paired with execution feedback in a finite state machine (FSM) to generate, compile, and test hundreds of kernels directly on MTIA hardware. TritorX is compatible with MTIA‚Äôs production infrastructure, producing kernel-wrapper pairs that can be immediately registered within PyTorch and can be used for experimental model training, or in a production inference model. Building on top of production infrastructure, TritorX can also generate new PyTorch backends for upcoming accelerator generations via hardware simulation, providing important\n\n![img-0.jpeg](img-0.jpeg)\nFigure 1 TrittorX System Overview. (A) OpInfo operators and their PyTorch docstring/signature are selected for generation. (B) TrittorX iterates on each operator in a finite-state machine feedback loop. Kernel-wrapper pairs are generated using a large language model (LLM). Production infrastructure allows for simultaneous generation and testing at scale. (C) The generation task is successful if an operator passes all corresponding OpInfo tests.\n\nfeedback to hardware and compiler engineers before tape-out. In the limit, we envision implementing a kernel backend for a new chipset overnight.\n\nWe note that TrittorX differs from other kernel generation efforts, in that our framework strictly optimizes for correctness and generalizability across an entire backend, rather than performance over a narrow subset of critical path kernels (Lange et al., 2025a). For example, TrittorX is configured to generate kernel-wrapper pairs that are compatible with a wide range of quantization data types, tensor shapes, or PyTorch argument inputs. In some cases, TrittorX will generate multiple kernel implementations for a particular operator, and the corresponding dispatch logic is implemented in the wrapper function. Using a Triton dialect known as Triton MTIA, standard models can produce working kernels and rely on execution feedback to identify MTIA-specific semantics or intrinsics. Importantly, we only provide TrittorX with the ATen operator docstring to generate the corresponding implementation. Figure 1 describes the end-to-end workflow for accelerator enablement.\n\nTritorX performs in-context learning iteratively, distilling hardware requirements and their corresponding Triton semantics based on the feedback obtained di\n\nrectly via tools like the compiler. Previous works demonstrated that pipelines like TrittorX will often \"cheat\" at the generation task by dispatching to the host or calling other undefined PyTorch functions in the operator wrapper (Lange et al., 2025b). We avoid this by incorporating a custom linter that catches unauthorized uses of these functions or utilities and forces correction via feedback.\n\nCentral to TrittorX is the integration of multiple testing frameworks to ensure correctness across different data types, tensor shapes, or input arguments. We use OpInfo $^1$ , a PyTorch-native testing framework, along with a custom test harness that pulls test data from models in production.\n\n## In summary, our contributions are as follows:\n\n- System. A finite state machine using open source LLMs that generates, compiles, and validates hundreds of Triton MTIA kernels end-to-end. TrittorX runs directly on deployed silicon or via hardware simulation enabling prototyping for future devices. TrittorX only requires operator docstrings to generate the corresponding implementations. Its modular design allows for modular features, like the custom linter which prevents \"cheating.\"\n- Coverage objective. A design that optimizes for functional correctness and backend coverage covering multiple data types, shapes, and branching dispatch logic via generated wrappers.\n- Results. At scale, TrittorX produced 481 ATen kernels (84.7% MTIA-compatible OpInfo coverage) which pass all their corresponding OpInfo tests, in total more than 20,000. In addition, TrittorX demonstrates capability on end-to-end model enablement tasks for various models on MTIA. TrittorX can iterate over the entire OpInfo operator set in a few hours, allowing for rapid backend development and iteration.\n- Testing harness. We integrate PyTorch OpInfo and captured production inputs testing for correctness under deployment conditions.\n\n# 2 Background\n\nIn this section, we describe the MTIA architecture, the Triton MTIA dialect, and how TrittorX can be instrumented to generate kernels at scale.\n\n## The MTIA architecture employs a grid of 8x8 processing elements (PEs) responsible for executing the\n\ncore kernel workloads (Coburn et al., 2025). Each PE consists of a scalar RISC-V core, a vector RISC-V core, and various fixed-function units (FFUs) responsible for implementing dedicated computations, such as direct memory accesses (DMAs) and dot products. Figure 2 illustrates the MTIA architecture.\n\n![img-1.jpeg](img-1.jpeg)\nFigure 2 MTIA's architecture overview. The core kernel computation is performed by a grid of process elements (PEs) which consist of a scalar core, a vector core, and special function units.\n\nIn particular, MTIA features a unique memory hierarchy with a significant amount of local SRAM available to all PEs via a series of crossbars. Dedicated PE FFUs facilitate the abstraction of circular buffers, enabling efficient pipelining of computation and communication needed to amortize the overall latency resulting from data movement. The computational savings gained by this approach allow MTIA to leverage cheaper LPDDR DRAM instead of HBM. Importantly, significant effort was made to mitigate overhead related to kernel or job dispatch from the host, enabling eager-mode workflows.\n\nTo execute kernels on MTIA, developers can author the workload using a  $\\mathrm{C + + }$  API, which is compiled with an LLVM backend, or using a custom Triton dialect adapted specifically for MTIA. Triton is an open-source Python library that provides a domain-specific language (DSL) for writing highly efficient custom GPU kernels (Tillet et al., 2019b). It simplifies GPU programming by allowing developers to express complex parallel computations using intuitive block-based abstractions while automatically handling low-level details such as memory access\n\npatterns and synchronization. This enables users to achieve performance comparable to hand-written CUDA code, without requiring deep expertise in GPU architectures.\n\nAlthough Triton was written to express the semantics of GPU computation, several of the existing semantics can be directly translated to corresponding MTIA hardware features. For example, instead of mapping Triton blocks to GPU threads, we can map them to the MTIA PE grid and rely on masking within loads and stores such that tensor boundaries are respected. Furthermore, loads and stores can take advantage of the MTIA DMA engine for structured memory access. When intrinsics do not perfectly match, we can augment the underlying Triton feature-set with specific device libraries (e.g. to leverage FFUs that implement nonlinear activations). Triton MTIA is a dedicated dialect intended to facilitate the above mappings, along with various other performance optimizations via the compiler backend.\n\nAlthough Triton MTIA intends to preserve the existing Triton semantics as much as possible, there are certain hardware requirements that force notable deviations. For example, MTIA requires 32-byte aligned memory access patterns, and load/store operations will fail if this is not satisfied. On the surface, this may seem problematic when trying to generate kernels using off-the-shelf models. However, Triton MTIA has detailed assert messages and error handling providing the necessary feedback that the models need to adapt the vanilla Triton code for MTIA. Indeed, building a compiler tool-chain that gives descriptive feedback is an important part of leveraging automated approaches for code generation.\n\nThe challenge is then to implement an execution pipeline that is compatible with the existing production infrastructure. MTIA (and all future hardware versions) are deployed in a productionized Linux container ecosystem (Tang et al., 2020). Typical production workflows require an end-to-end workload that can e.g. serve or train a model for a particular service at scale, which also requires a complicated kernel registration stack compatible with the PyTorch ecosystem. However, the Triton JIT allows us to generate, compile, and test kernels on the fly, even within these productionized containers, allowing us to run numerous experiments in parallel.\n\n# 3 System Design\n\nHere, we describe the system architecture behind TritorX, along with the test harness used to validate the results during generation.\n\n# 3.1 In-Context Distillation of Triton MTIA Semantics\n\nA naive approach to generating Triton MTIA code is to add comprehensive specifications of the hardware requirements and corresponding Triton semantics differences to the first prompt of the LLM to see if the generated code passes relevant tests. In practice, however, comprehensive accelerator documentation lags behind other stack components. Early attempts at generating Triton for MTIA with a simple prompt-engineering approach resulted in significant manual labor and did not scale.[2]\n\nIn contrast, TritorX effectively performs in-context learning iteratively, distilling hardware requirements and their corresponding Triton semantics based on the feedback obtained directly via interaction with the linter, compiler, and debugger.\n\nWe implemented TritorX as a finite-state machine (FSM) with dedicated tools and routines, including linting, compiling, testing, debugging, and LLM calls.\n\nAlthough recent kernel generation frameworks often rely on a dedicated reasoning agent with prescribed tool calling (Lange et al., 2025b; Wang et al., 2025; Chen et al., 2025b; Andrews and Witteveen, 2025; Li et al., 2025a,b), we found it easier to integrate an FSM architecture within our production infrastructure at scale. The FSM offers explicit guardrails around what is executed and performed, and allows faster debugging of the key components of the system which is an important requirement when dealing with production-ready systems. Additionally, the backend of the FSM enables compilation and testing on both the deployed MTIA machines and on the QEMU simulator of the future MTIA generations.\n\n# 3.2 TritorX Agent\n\nFigure 3 illustrates the overall design of the TritorX system. Each operator is generated in a self-contained session during which all prescribed tests are performed. This session is configurable up front, allowing us to easily prototype different LLM models, disable/enable individual states (like the linter), and sweep TritorX hyperparameters (e.g., max number of iterations).\n\nThe routine begins with an initial prompt consisting of the task description, output requirements, the documentation (docstring) of the PyTorch operator, and three handcrafted examples ( $\\S C$ ). Specifically, the prompt asks the model to generate a python wrapper matching the designated PyTorch operator\n\n![img-2.jpeg](img-2.jpeg)\nFigure 3 Finite State Machine of our kernel-generation agent TritorX. Proposal kernel-wrapper pairs are only generated during the \"Generate Kernel\" state (which dispatches to an open source, reasoning LLM). All other states process the result and update a feedback prompt, where needed.\n\nsignature (described in the docstring) and one or more Triton kernels that implement the functionality itself. The exp, argmax, and diag operators were chosen as simple examples as they span multiple kernel classes (e.g., elementwise, reduction). Often times, ATen docstrings will reference the docstrings of other operators (e.g., argmax references max). We built a directed acyclic graph of all docstrings, allowing us to include \"nested\" docstrings for completeness. Further instructions prescribing the expected output format expected by the downstream parser are also provided.\n\nThe reasoning LLM uses the prompt to generate an initial candidate wrapper/kernel pair, which is then sent to a custom Triton MTIA Linter. The linter is responsible for the following tasks: (1) ensuring the output wrapper and kernel code is compatible with the Triton JIT harness; (2) ensuring the provided implementation does not \"cheat\" by dispatching into other operators that may not yet be implemented; (3) ensuring the provided code uses valid Triton MTIA syntax and libraries, as not all of upstream Triton is available on MTIA. The linter is lightweight and configurable ( $\\S$ E). If a lint violation is detected, a structured report is generated and sent back to the model as feedback for correction ( $\\S$ C). The process\n\nis repeated until no lint errors are produced or the maximum number of LLM calls is reached.\n\nIf no linter errors are detected, the wrapper and kernel code is passed to a dedicated Triton JIT compilation harness compatible with the MTIA infrastructure. Depending on the operator configuration, which prescribes the supported datatypes, a series of tests derived from and production-data are synthesized. The test runner loops through each test, recompiling as needed (e.g. for new datatypes). If compilation is successful and the test executes without any runtime errors, the same inputs are moved to the host and executed using a reference ATen CPU implementation of the operator. The outputs for both the generated MTIA kernel and the CPU reference kernel are compared using a heuristic that depends on the underlying datatype. If the results are within the specified tolerance, the process repeats with the next test. As soon as the runner encounters a compilation failure, a runtime error, or an accuracy error, the routine breaks and proceeds to a ‚Äúfeedback‚Äù state responsible for determining what to do next.\n\nThe feedback state analyzes how successful the test runner was and determines what kind of feedback prompt is needed for another LLM iteration, or if further debugging is needed. For example, if the most recent run resulted in a runtime crash which produced a crash dump, the crash dump is loaded in an LLDB-based debugger. The debugger pulls basic information about the backtrace, decoded registers, and other frame information to provide as context for the revised prompt. Example insights include details around memory access violations.\n\nIn the case of a compiler failure, depending on the hyperparameter config, we optionally summarize the compiler log using a secondary LLM instance (also configurable up front). Triton MTIA compiler logs can easily consume thousands of tokens, so surfacing the most relevant facets of the compiler error to the main LLM session serves towards managing limited context windows.\n\nIf the feedback state detects an accuracy error, a summary of the MTIA output tensor(s) and the CPU output tensor(s) is included in the feedback prompt. Even in the case of large output tensors, an abbreviated summary of the tensor values is often enough context for the model to reason about the potential inaccuracy (¬ßD).\n\nOnce an appropriate feedback prompt is crafted, the process repeats until one of the following conditions occurs: (1) all tests pass, in which case the routine exits successfully; (2) the maximum number of prescribed LLM calls has been reached, and the routine exits; (3) the LLM context window saturates, and a new LLM dialog session starts using the most recent wrapper/kernel generation as an initial proposal; (4) an unexpected error crashes the main process. We implemented comprehensive exception handling throughout the TritorX, including launching containerized subprocesses where necessary, to avoid crashing the main process whenever possible.\n\nIn order to generate operators at scale, the above process can be executed in a parallel fashion for every operator specified and even repeated for operators that failed. These large-scale runs are configured by the operators of interest, the desired datatypes, the LLM parameters (e.g., model, context length, temperature), the run parameters (e.g., maximum number of LLM calls, maximum number of dialog sessions) and the testing complexity.\n\nImportantly, the operators are compiled and executed on productionized MTIA machines. The LLM calls themselves are processed by a centralized inference platform service capable of handling a high volume of requests needed for large-scale runs.\n\n### 3.3 ATen Operators and OpInfo Testing\n\nTo evaluate the viability of our approach at scale, we generate kernels for the operators defined within the PyTorch OpInfo testsuite. Importantly, OpInfo aims to rigorously test an operator‚Äôs coverage by providing ‚Äúsamples‚Äù for all the supported data-types, tensor shape, and input arguments. For example, OpInfo contains hundreds of tests for the linalg.vector_norm operator which rigorously test different input and output tensor shapes, along with different input argument configurations.\n\nUsing OpInfo as our primary test harness creates an end-to-end generation pipeline with orders of magnitude more tests than the state of the art *(Ouyang et al., 2025)*. By covering more of the input space during the generation process, we expect the resulting implementation to more reliably work in arbitrary prototyping and production environments. That being said, we recognize that ensuring perfect test coverage across the entire input space is impossible. To account for this, we introduce a secondary test harness specifically for production models that consists of production input data. This additional testsuite allows us to gauge how well an operator was generalized using the OpInfo testsuite. If gaps are identified during the generation stage (such that production-data tests failed), then TritorX is able to resolve the coverage gap.\n\nThere are certain limitations with MTIA hardware such that certain operators and tests are either not compatible or not relevant for the target workloads. For example, MTIA does not support complex numbers, so we remove those corresponding operators from the generation list (e.g., FFT operators). Similarly, validating the outputs for random-number operators between the device and host is particularly challenging due to differences in the underlying random number generation algorithm. As such, we also remove these operators from consideration. Additionally, due to a limitation of our distributed testing infrastructure, we only cover operators with under 900 total OpInfo tests. The resulting operator list consists of 568 unique operators (filtered down from 629). We also only test for bfloat16, float16, float32, int32, and int64. In total, this results in over 20,000 tests across all operators, and we only classify an operator as successful if it passes all the corresponding operator tests.\n\n# 4 Experiments and Results\n\nWe now present the experiments used to validate our approach. We first present an aggregate result consisting of kernels that span all MTIA-compatible OpInfo operators over multiple large-scale runs. From this set of generated operators, we \"productionize\" various first- and third-party models. We further expand our test harness for these operators by incorporating additional correctness tests that leverage production data and identify additional gaps not originally captured by OpInfo. Finally, we ablate over various TritorX configurations to highlight which aspects of our pipeline matter and why.\n\nOur baseline setup for these experiments is to run TritorX over all the 568 MTIA-compatible OpInfo operators with the following configuration:\n\n- Maximum of 3 TritorX attempts (i.e., dialog sessions) per operator to generate a kernel that passes all tests and declares Success;\n- Each attempt is allowed a maximum of 15 LLM calls, or, in other words, 15 full iterations through the state machine until Failure is declared for the attempt;\n- Either Code World Model (CWM, Copet et al. (2025)) or GPT-OSS 120B (OpenAI, 2025) were used as the kernel-generating LLM. Both models were configured with a context length of 131,072 and temperature set to 1.0. We set the top-P to 0.95 for CWM and 1.0 for GPT-OSS. The GPT-OSS reasoning was set to \"high.\"\n\n- Llama-4-Maverick is used as the feedback summarization model with the same generation parameters as CWM.\n\nWe dispatch the generation jobs across 200 production MTIA devices, which are able to finish  $95\\%$  of a run in 2 hours. The remaining tail often results from e.g., poor reasoning trajectories, and can take another 6-8 hours to complete. New runs can be dispatched concurrently. With this infrastructure in place, we executed multiple runs, with subsequent runs focusing on operators that failed previous runs.\n\nFrom these aggregated runs, we achieved  $84.7\\%$  operator coverage on all MTIA-compatible OpInfo operators. Here we consider an operator covered if the generated kernel-wrapper pair passes  $100\\%$  of the sample OpInfo tests. These results were aggregated across multiple runs, including ablations over models and configuration parameters. Figure 4 illustrates the cumulative operator coverage as a function of LLM calls for different configurations.\n\n![img-3.jpeg](img-3.jpeg)\nFigure 4 Number of LLM calls per operator to produce a correct kernel, cumulatively plotted for different harness configurations and models. Ensemble results display coverage achieved by combining multiple configurations. We also include experimental localization runs, where we pull relevant operators as context. The global aggregate includes all our available runs, some of which are not shown in the figure.\n\nWe further heuristically divide operators into 7 categories depending on the intended functionality of each operator. Table 1 shows that different categories present different difficulties to TritorX: while TritorX achieves  $96.0\\%$  coverage on Shape Manipulation operators, the coverage significantly drops for the operators from the Deep Learning Category.\n\n## Finally, we executed a run with GPT-OSS on a future generation using a QEMU simulator (Bellard,\n\n|  Op Category | Op Count | Operator Coverage (%)  |   |\n| --- | --- | --- | --- |\n## |   |   |  CWM | GPT-OSS  |\n|  Elementwise | 161 | 80.1 | 84.6  |\n|  Deep Learning | 90 | 64.4 | 71.1  |\n|  Linear Algebra | 78 | 71.8 | 79.5  |\n|  Other | 78 | 75.6 | 74.3  |\n|  Shape Manipulation | 75 | 96.0 | 96.0  |\n|  Reduction | 63 | 69.8 | 74.6  |\n|  Indexing & Selection | 34 | 73.5 | 79.4  |\n\n2005) for execution feedback. This single run yielded a coverage of  $73.1\\%$ . We aggregated the compiler failures and feature gaps encountered during generation, and shared this data with our compiler and ASIC engineers.\n\n# 4.1 End-to-end Application\n\nFrom our baseline set of OpInfo operators, we used TritorX to enable various first- and third-party models on MTIA. While robust, we recognize that production workflows may contain operators and operator arguments (eg. shapes, scalar values, etc.) outside of the distribution represented in the OpInfo testsuite. To mitigate this, we decompose various target models into their individual operators, extract all operator inputs observed during training, and then use these inputs within the TritorX validation loop instead of those generated by OpInfo.\n\nConcretely, we instrumented forward and backward passes of several representative models, NanoGPT (Karpathy, 2023), DLRM (Naumov et al., 2019), and two internal recommendation models (denoted MM), using __torch_dispatch__ to intercept and record the tensor and scalar data passed to each operator. All four models were evaluated with a fixed batch size of 1024 and trained for single iteration, with the latter three (DLRM, MM1, and MM2) executed using real production data rather than randomized inputs.\n\nAdditionally, we introduce a new step to TritorX, first matching a given operator with a pre-generated OpInfo operator (should it exist), then immediately testing it with the inputs gathered from the full e2e run. Should the kernel not pass all tests out of the box, it is used as a starting point from which TritorX then refines (Table 2, column B: MIS).\n\nAcross these experiments, TritorX achieves high kernel coverage, enabling nearly  $80\\%$  of all kernels required to execute a model end-to-end. Furthermore,\n\nTable 1 TritorX Coverage by operator category and used LLM.\n\n|  Model | Operator Coverage (%)  |   |   |\n| --- | --- | --- | --- |\n|   |  A. Full Model Op Set | B. OpInfo Subset  |   |\n|   |   | OpInfo | MIS  |\n## |  NGPT | 87.2 | 80.0 | 100.0  |\n## |  DLRM | 81.4 | 80.0 | 90.0  |\n|  Meta M1 | 79.8 | 83.8 | 91.9  |\n|  Meta M2 | 80.6 | 81.7 | 87.3  |\n\nfor operators where a pre-existing OpInfo-validated kernel is available, over  $80\\%$  of these kernels pass all end-to-end production tests without additional prompting. After refinement, TritorX further improves this by an additional  $6 - 20\\%$  across models. This not only underscores the robustness of TritorX, but also establishes a sandbox for continuous testing and optimization of production-ready kernels.\n\n# 4.2 TritorX Harness Ablation\n\nTo better understand which aspects of TritorX contribute to its success, we ablate over various configurations. Table 3 summarizes the results of these experiments.\n\nTable 2 Operator coverage across four model types: NanoGPT, Deep Learning Recommendation Model, Meta Model 1, Meta Model 2. An operator is considered covered if the corresponding kernel passes all tests with model input shapes (MIS). (A) We run TritorX on all model operators with the MIS feedback. (B) We run TritorX on a subset of model operators that have tests available in the OpInfo suite. (OpInfo) We directly test kernels created with OpInfo feedback with MIS. (MIS) We run TritorX to refine kernels created with OpInfo feedback with MIS feedback.\n\n|  Method | CWM (%) | GPT-OSS (%)  |\n| --- | --- | --- |\n|  Baseline (single run) | 55.3 | 72.0  |\n|  w/o linter | 48.9 | 68.7  |\n|  w/o summarization | 48.2 | 71.5  |\n\nTable 3 Ablations over TritorX harness features.\n\nWe examine the importance of the custom Triton MTIA linter and the optional summarization model. Removing the linter resulted in a significant drop in performance (55.3% ‚Üí 48.9% for CWM). As mentioned previously, the linter not only helps the agent identify intrinsics unique to the Triton MTIA dialect, but also helps prevent \"cheating\" by flagging the unauthorized use of other torch operators (¬ßE).\n\nRemoving the summarization agent also resulted in a decrease in performance (55.3% ‚Üí 48.2% for CWM). Without a separate summarization agent, the entire\n\ncompilation log, which can consist of thousands of tokens, is fed directly into the LLM dialog session, and the model performance can degrade as we approach the context limit *(Hsieh et al., 2024)*.\n\n## 5 Related Work\n\nCustom ASICs, MTIA & Deep Learning Compilers. Since Triton introduced a Python-first DSL for high-performance GPU kernels *(Tillet et al., 2019a)* and became widely used by PyTorch Inductor *(Ansel et al., 2024)* to generate fused operators, several ecosystems now offer Python-level kernel DSLs. NVIDIA‚Äôs Warp *(Macklin, 2022)* is a Python DSL for authoring CUDA kernels, with an optional tile-based programming model for tensor-core GEMMs. In JAX, Google‚Äôs Pallas *(Bradbury et al., 2018)* allows users to write custom kernels in Python; it targets GPUs via Triton and TPUs *(Jouppi et al., 2017)* via Mosaic *(Bansal et al., 2023)*, enabling fine-grained fused operations within the JAX ecosystem. Meta has also adopted Triton for MTIA to improve developer efficiency with PyTorch.\n\nKernel Research. Optimization of custom kernels for specific devices has received sustained attention in recent years: *Lavin and Gray (2016); Dao et al. (2022); Dao (2023); Shah et al. (2024)*. This interest is both driven by and enabling the rapid growth of training and inference compute of frontier models: *Cottier and Rahman (2024); Hooker (2021)*.\n\nBenchmarks. The community has responded by releasing benchmarks to measure the functional correctness and performance of LLM-generated kernels that primarily target GPUs through CUDA and Triton. KernelBench *(Ouyang et al., 2025)* and TritonBench *(Li et al., 2025a)* evaluate whether models can generate correct and performant GPU kernels across representative operator suites, while NPU-Eval *(Kalade et al., 2025)* targets AMD NPUs. AlgoTune *(Press et al., 2025)* in particular targets LLM‚Äôs ability to speed up scientific computing problems from natural language descriptions. Automatically generated kernels are prone to exploiting holes in the test suite. Addressing this has seen significant effort: *(Lange et al., 2025b; METR, 2025)*. Notably, similar to our work BackendBench *(Saroufim et al., 2025)* also uses PyTorch‚Äôs OpInfo as a comprehensive test suite to ensure correctness and comprehensiveness.\n\nLLM-based Kernel Generation. Most LLM-based kernel generation relies on prompting techniques combined with test-time compute use as a form of search. Recent systems train or prompt LLMs to synthesize kernels or improve them with search. Kernel-LLM *(Fisches et al., 2025)* provides a supervised baseline for the generation of Triton kernels from PyTorch modules. AutoTriton *(Li et al., 2025b)* adds reinforcement learning from verifiable rewards. Multi-turn reinforcement learning for CUDA kernel generation has also been explored in *Baronio et al. (2025)*.\n\nOrthogonally, several approaches scale inference compute with existing LLMs, covering a spectrum of prompting, agentic, and evolutionary techniques combined with verification through unit tests: *(Lange et al., 2025b; METR, 2025; Chen et al., 2025a; Wei et al., 2025; Wang et al., 2025)*. While several approaches exist that target CUDA and GPUs, we believe only NPU-Eval *(Kalade et al., 2025)* is targeting non-GPU devices with C++.\n\n## 6 Discussion and Future Work\n\nTritorX‚Äôs FSM framework provides a robust harness for generating functionally correct Triton MTIA kernels. Thanks to its flexibility, we identify several additional directions to further improve coverage and performance and reflect on what aspects are most important for success.\n\nSelf-consistent operator generation. To prevent ‚Äúcheating,‚Äù the linter restricts the agent from utilizing other ATen operators within the wrapper (beyond tensor allocation). A more efficient and possibly more performant approach is to allow the wrapper to dispatch other operators, provided they are also implemented in the new backend and the operators do not result in cyclic dependencies. This requires a self-consistent generation scheme where the agent is aware of the entire backend state (and is thus no longer embarrassingly parallel).\n\nOptimized prompting. As mentioned earlier, we found that simpler prompts without dedicated MTIA documentation worked best, but there remains room for prompt tuning. Furthermore, we can improve the quality of the example kernel-wrapper pairs themselves using strategies like localization, perhaps even bootstrapping the process over sequential runs.\n\nFully agentic pipeline. At a higher level than these improvements, we have discussed making the entire FSM agentic ‚Äì converting most current states into tools for the LLM to call upon, as well as adding new tools related to debugging, such as giving the LLM a sandbox to execute code in.\n\nDedicated model post-training. Both models used throughout this work were open source and off-the-\n\nshelf. All of the MTIA-specific context was gained via interactions with the linter, compiler, and debugger. An orthogonal research direction would be to further post-train the LLM for Triton MTIA kernel generation and contrast that with the existing results for vanilla Triton in the literature.\n\nThe importance of scale. Due to the stochastic nature of the underlying language model, running the benchmark repeatedly (with a nonzero temperature) will produce results with nonidentical pass-rates. Thus, simply aggregating the passing operators across runs, a technique known as test-time scaling, can yield significant increases in operator coverage. For example, just aggregating between two benchmark runs using CWM increased the coverage from 55% $\\rightarrow$ 64%. We suspect that further scaling this and exploring more complex strategies such as evolutionary scaling *(Lange et al., 2025b)* will bring significant coverage improvements. Furthermore, supporting additional hardware generations, DSLs, and operator definitions will require a flexible and scalable pipeline for generating operator sets.\n\nEstablishing a backend-maintenance environment. As AI-driven kernel generation evolves, AI will eventually produce more kernels than humans can feasibly review‚Äîespecially as hardware diversity increases and multiple generations of hardware coexist within production fleets. Establishing a robust, fully automated framework that does not require human review will be essential for the success of AI-generated kernels. This need will become even more critical as each model update triggers a comprehensive refresh of the kernel library to optimize performance.\n\n## 7 Conclusion\n\nWe introduced TritorX, a scalable, coverage-first system that automates bring-up of the PyTorch backend for custom ML ASICs, demonstrated on Meta‚Äôs MTIA. TritorX orchestrates an existing LLM with a finite-state workflow and production-compatible tooling, executed on both deployed silicon and a QEMU-based simulator for future devices. It generated over 481 ATen operators that pass all of their corresponding OpInfo tests (20,000+ total tests), achieving an overall pass rate of 84.7%. From these operators, we were able to onboard 80+% of the multiple first- and second-party large-scale models on the device. Ablations over model and system factors isolate which components most influence coverage, turning backend enablement into a measured engineering process rather than an artisanal effort.\n\nBeyond MTIA, the design is applicable to other accelerators and naturally extends to adjacent tasks such as cross-generation kernel migration and early hardware/compiler feedback via simulation. Although performance tuning is out of the scope of this work, TritorX provides the substrate on which autotuning, schedule search, and learned code optimization can be layered. Future directions include expanding operator families (reductions, sparse/quantized ops), stronger safety/containment, and formal checks for code generation, tighter integration with compiler IRs, and automated pathways for performance refinement.\n\nTaken together, these results offer a practical blueprint for democratizing toolchain creation and a force multiplier for kernel engineers‚Äîfreeing expert effort to focus on truly performance-critical paths while bringing entire backends online in hours rather than months.\n\n## References\n\n- Andrews and Witteveen (2025) M. Andrews and S. Witteveen. Gpu kernel scientist: An llm-driven framework for iterative kernel optimization. *arXiv preprint arXiv:2506.20807*, 2025.\n- Ansel et al. (2024) Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In *Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2*, pages 929‚Äì947, 2024.\n- Bansal et al. (2023) Manya Bansal, Olivia Hsu, Kunle Olukotun, and Fredrik Kjolstad. Mosaic: An interoperable compiler for tensor algebra. *Proceedings of the ACM on Programming Languages*, 7(PLDI):394‚Äì419, 2023.\n- Baronio et al. (2025) Carlo Baronio, Pietro Marsella, Ben Pan, and Silas Alberti. Kevin: Multi-turn rl for generating cuda kernels. *arXiv preprint arXiv:2507.11948*, 2025. https://arxiv.org/abs/2507.11948.\n- Bellard (2005) Fabrice Bellard. Qemu, a fast and portable dynamic translator. In *Proceedings of the USENIX Annual Technical Conference (ATEC ‚Äô05)*, page 41, Berkeley, CA, USA, April 2005. USENIX Association. https://dl.acm.org/doi/10.5555/1247360.1247401.\n- Bradbury et al. (2018) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. http://github.com/jax-ml/jax.\n\nTerry Chen, Bing Xu, and Kirthi Devleker. Automating gpu kernel generation with deepseek-r1 and inference-time scaling. NVIDIA Technical Blog, 2 2025a. https://developer.nvidia.com/blog/automating-gpu-kernel-generation-with-deepseek-r1-and-inference-time-scaling/.\n- [21] W. Chen, J. Zhu, Q. Fan, Y. Ma, and A. Zou. CUDA-LLM: Llms can write efficient cuda kernels. arXiv preprint arXiv:2506.09092, 2025b.\n- [22] Joel Coburn, Chunqiang Tang, Sameer Abu Asal, Neeraj Agrawal, Raviteja Chinta, Harish Dixit, Brian Dodds, Saritha Dwarakapuram, Amin Firoozshahian, Cao Gao, et al. Meta‚Äôs second generation ai chip: Model-chip co-design and productionization experiences. In Proceedings of the 52nd Annual International Symposium on Computer Architecture, pages 1689‚Äì1702, 2025.\n- [23] Jade Copet, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, David Zhang, Kunhao Zheng, Jordi Armengol-Estap√©, Pedram Bashiri, Maximilian Beck, Pierre Chambon, Abhishek Charnalia, Chris Cummins, Juliette Decugis, Zacharias V. Fisches, Fran√ßois Fleuret, Fabian Gloeckle, Alex Gu, Michael Hassid, Daniel Haziza, Badr Youbi Idrissi, Christian Keller, Rahul Kindi, Hugh Leather, Gallil Maimon, Aram Markosyan, Francisco Massa, Pierre-Emmanuel Mazar√©, Vegard Mella, Naila Murray, Keyur Muzumdar, Peter O‚ÄôHearn, Matteo Pagliardini, Dmitrii Pedchenko, Tal Remez, Volker Seeker, Marco Selvi, Oren Sultan, Sida Wang, Luca Wehrstedt, Ori Yoran, Lingming Zhang, Taco Cohen, Yossi Adi, and Gabriel Synnaeve. Cwm: An open-weights llm for research on code generation with world models, 2025. https://arxiv.org/abs/2510.02387.\n- [24] Ben Cottier and Robi Rahman. The training compute of notable ai models has been doubling roughly every six months. Epoch AI Data Insight, 2024. https://epoch.ai/data-insights/cost-trend-large-scale. Accessed 2025-10-21.\n- [25] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. https://arxiv.org/abs/2307.08691.\n- [26] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems (NeurIPS 2022), 2022. https://openreview.net/forum?id=H4DqfPSibmx. 36th Conference on Neural Information Processing Systems. (Peer-reviewed).\n- [27] Zacharias V. Fisches, Sahan Paliskara, Simon Guo, Alex Zhang, Joe Spisak, Chris Cummins, Hugh Leather, Gabriel Synnaeve, Joe Isaacson, Aram Markosyan, and Mark Saroufim. Kernelllm: Making kernel development more accessible, 6 2025. https://huggingface.co/facebook/KernelLLM.\n- [28] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58‚Äì65, 2021.\n- [29] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: What‚Äôs the real context size of your long-context language models?, 2024. https://arxiv.org/abs/2404.06654.\n- [30] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture, pages 1‚Äì12, 2017.\n- [31] Jacob D Kahn, Vineel Pratap, Tatiana Likhomanenko, Qiantong Xu, Awni Hannun, Jeff Cai, Paden Tomasello, Ann Lee, Edouard Grave, Gilad Avidov, et al. Flashlight: Enabling innovation in tools for machine learning. In International Conference on Machine Learning, pages 10557‚Äì10574. PMLR, 2022.\n- [32] S. Kalade et al. NPUEval: Optimizing npu kernels with llms and open source compilers. arXiv preprint arXiv:2507.14403, 2025. https://arxiv.org/abs/2507.14403.\n- [33] Andrej Karpathy. nanogpt: The simplest, fastest repository for training/finetuning medium-sized gpts, 2023. https://github.com/karpathy/nanoGPT. GitHub repository.\n- [34] Robert Tjarko Lange, Qi Sun, Aaditya Prasad, Maxence Faldor, Yujin Tang, and David Ha. The ai cuda engineer: Agentic cuda kernel discovery, optimization and composition. arXiv preprint arXiv:2509.14279 and project archive, 2025a.\n- [35] Robert Tjarko Lange, Qi Sun, Aaditya Prasad, Maxence Faldor, Yujin Tang, and David Ha. Towards robust agentic cuda kernel benchmarking, verification, and optimization. arXiv preprint arXiv:2509.14279, 2025b. https://pub.sakana.ai/static/paper.pdf.\n- [36] Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4013‚Äì4021, 2016.\n- [37] Jianling Li, Shangzhan Li, Zhenye Gao, Qi Shi, Yuxuan Li, Zefan Wang, Jiacheng Huang, Haojie Wang, Jianrong Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. Tritonbench: Benchmarking large language model capabilities for generating triton operators. arXiv preprint arXiv:2502.14752, 2025a. https://arxiv.org/abs/2502.14752.\n- [38] Shangzhan Li, Zefan Wang, Ye He, Yuxuan Li, Qi Shi, Jianling Li, Yonggang Hu, Wanxiang Che, Xu Han, Zhiyuan Liu, et al. Autotriton: Automatic triton programming with reinforcement learning in llms. arXiv preprint arXiv:2507.05687, 2025b.\n\nMiles Macklin. Warp: A high-performance python framework for gpu simulation and graphics. https://github.com/nvidia/warp, March 2022. NVIDIA GPU Technology Conference (GTC).\n- Measuring automated kernel engineering. Blog post, 2 2025. https://metr.org/blog/2025-02-14-measuring-automated-kernel-engineering/.\n- Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091, 2019.\n- ppenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. https://arxiv.org/abs/2508.10925.\n- Ouyang et al. (2025) Anne Ouyang, Simon Guo, Simran Arora, Alex L Zhang, William Hu, Christopher R√©, and Azalia Mirhoseini. Kernelbench: Can llms write efficient gpu kernels? arXiv preprint arXiv:2502.10517, 2025.\n- Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n- Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf.\n- Press et al. (2025) Ori Press, Brandon Amos, Haoyu Zhao, Yikai Wu, Samuel K Ainsworth, Dominik Krupke, Patrick Kidger, Touqir Sajed, Bartolomeo Stellato, Jisun Park, et al. Algotune: Can language models speed up general-purpose numerical programs? arXiv preprint arXiv:2507.15887, 2025.\n- Saroufim et al. (2025) Mark Saroufim, Jiannan Wang, Bert Maher, Sahan Paliskara, Laura Wang, Shahin Sefati, and Manuel Candales. Backendbench: An evaluation suite for testing how well llms and humans can write pytorch backends, 2025. https://github.com/meta-pytorch/BackendBench.\n- Shah et al. (2024) Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. arXiv preprint arXiv:2407.08608, 2024. https://arxiv.org/abs/2407.08608.\n- Shehabi et al. (2024) Arman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakkar Siddik, Billie Holecek, Jonathan Koomey, Eric Masanet, Dale Sartor, et al. 2024 united states data center energy usage report. 2024.\n- Silvano et al. (2025) Cristina Silvano, Daniele Ielmini, Fabrizio Ferrandi, Leandro Fiorin, Serena Curzel, Luca Benini, Francesco Conti, Angelo Garofalo, Cristian Zambelli, Enrico Calore, et al. A survey on deep learning hardware accelerators for heterogeneous hpc platforms. ACM Computing Surveys, 57(11):1‚Äì39, 2025.\n- Tang et al. (2019a) Chunqiang Tang, Kenny Yu, Kaushik Veeraraghavan, Jonathan Kaldor, Scott Michelson, Thawan Kooburat, Aravind Anbudurai, Matthew Clark, Kabir Gogia, Long Cheng, et al. Twine: A unified cluster management system for shared infrastructure. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 787‚Äì803, 2020.\n- Tillet et al. (2019b) Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: An intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages (MAPL@PLDI), pages 10‚Äì19, 2019a. doi: 10.1145/3315508.3329973. https://dl.acm.org/doi/10.1145/3315508.3329973.\n- Tillet et al. (2019c) Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 10‚Äì19, 2019b.\n- Wen et al. (2019) Jianghui Wang, Vinay Joshi, Saptarshi Majumder, Xu Chao, Bin Ding, Ziqiong Liu, Pratik Prabhanjan Brahma, Dong Li, Zicheng Liu, and Emad Barsoum. Geak: Introducing triton kernel ai agent & evaluation benchmarks. 2025. doi: 10.48550/arXiv.2507.23194. https://arxiv.org/abs/2507.23194.\n- Wei et al. (2025) Anjiang Wei, Tianran Sun, Yogesh Seenichamy, Hang Song, Anne Ouyang, Azalia Mirhoseini, Ke Wang, and Alex Aiken. Astra: A multi-agent system for gpu kernel performance optimization. 2025. doi: 10.48550/arXiv.2509.07506. https://arxiv.org/abs/2509.07506.\n\n## Appendix A Acknowledgments\n\nWe thank the PyTorch Team, in particular the authors of BackendBench for their support. We acknowledge that our work stands on the shoulders of the entire MTIA team and are grateful for the enablement of our research. We also thank Dipal Saluja for his early feedback.\n\n## Appendix B Generated Wrapper and Kernel Pairs\n\nHere we present a few interesting kernel-wrapper pairs for different operators.\n\n### B.1 nn.functional.binary_cross_entropy\n\n‚¨á\n@triton.jit\ndef kernel(\ninput_ptr,\ntarget_ptr,\nweight_ptr,\noutput_ptr,\nn_elements,\nBLOCK_SIZE: tl.constexpr,\n):\npid = tl.program_id(axis=0)\nblock_start = pid * BLOCK_SIZE\noffsets = block_start + tl.arange(0, BLOCK_SIZE)\nmask = offsets < n_elements\ninput = tl.load(input_ptr + offsets, mask=mask, other=0.0)\ntarget = tl.load(target_ptr + offsets, mask=mask, other=0.0)\nweight = tl.load(weight_ptr + offsets, mask=mask, other=1.0)\neps = 1e-8\nloss = -target * tl.log(input + eps) - (1 - target) * tl.log(1 - input + eps)\nloss = loss * weight\ntl.store(output_ptr + offsets, loss, mask=mask)\n\ndef wrapper(input, target, weight=None, reduction=‚Äômean‚Äô):\nif input.shape != target.shape:\nraise RuntimeError(‚Äúinput and target must have the same shape‚Äù)\nif weight is not None:\nif weight.shape != input.shape:\nweight = weight.broadcast_to(input.shape)\nweight = weight.contiguous()\nelse:\nweight = torch.ones_like(input, dtype<input.dtype, device<input.device)\nif reduction not in [‚Äônone‚Äô, ‚Äômean‚Äô, ‚Äôsum‚Äô]:\nraise ValueError(‚Äúreduction must be ‚Äônone‚Äô, ‚Äômean‚Äô, or ‚Äôsum‚Äô‚Äù)\noutput = torch.empty_like(input)\ninput = input.contiguous()\ntarget = target.contiguous()\nn_elements = input.numel()\nif n_elements == 0:\nif reduction == ‚Äônone‚Äô:\nreturn output\nelse:\nreturn torch.tensor(0.0, dtype<input.dtype, device<input.device)\n## BLOCK_SIZE = 1024\ngrid = (triton.cdiv(n_elements, BLOCK_SIZE),)\nkernel[grid]\n{\ninput,\ntarget,\nweight,\noutput,\nn_elements,\n## BLOCK_SIZE=BLOCK_SIZE,\n)\nif reduction == ‚Äônone‚Äô:\nreturn output\nelif reduction == ‚Äôsum‚Äô:\nreturn output.sum()\nelif reduction == ‚Äômean‚Äô:\nreturn output.mean()\n\nB.2 outer\n\n‚¨á\n@triton.jit\ndef kernel(\ninput_ptr,\nvec2_ptr,\noutput_ptr,\nn,\nm,\ninput_stride,\nvec2_stride,\noutput_stride0,\noutput_stride1,\n):\npid = tl.program_id(0)\nif pid >= n:\nreturn\ninput_val = tl.load(input_ptr + pid * input_stride)\nfor j in range(m):\nvec2_val = tl.load(vec2_ptr + j * vec2_stride)\nproduct = input_val * vec2_val\noutput_offset = pid * output_stride0 + j * output_stride1\ntl.store(output_ptr + output_offset, product)\n\ndef wrapper(input, vec2, *, out=None):\nif input.dim() != 1 or vec2.dim() != 1:\nraise ValueError(\"Both input and vec2 must be 1D tensors\")\nn = input.size(0)\nm = vec2.size(0)\noutput_shape = (n, m)\ndtype = input.dtype\nif input.dtype != vec2.dtype:\npass\ndevice = input.device\nif out is None:\noutput = torch.empty(output_shape, dtype=dtype, device=device)\n\nelse:\nif out.shape != output_shape:\nraise RuntimeError(f\"Expected out tensor to have shape (output_shape), but got (out.shape)\")\nif out.dtype != dtype:\nraise RuntimeError(f\"Expected out tensor to have dtype (dtype), but got (out.dtype)\")\nif out.device != device:\nraise RuntimeError(f\"Expected out tensor to be on device (device), but got (out.device)\")\noutput = out\ninput_contig = input.contiguous()\nvec2_contig = vec2.contiguous()\noutput_contig = output.contiguous()\ninput_stride = input_contig.stride(0)\nvec2_stride = vec2_contig.stride(0)\noutput_stride0 = output_contig.stride(0)\noutput_stride1 = output_contig.stride(1)\ngrid = (n,)\nkernel[grid](\ninput_contig,\nvec2_contig,\noutput_contig,\nn,\nm,\ninput_stride,\nvec2_stride,\noutput_stride0,\noutput_stride1,\n)\nreturn output\n\nB.3 nn.functional.layer_norm\n\n‚¨á\n@triton.jit\ndef kernel_mean_var(\ninput_ptr,\nmean_ptr,\nvar_ptr,\n## M,\n## N,\nepsilon,\nBLOCK_SIZE: tl.constexpr,\n):\npid = tl.program_id(0)\nif pid >= N:\nreturn\n\nsum = 0.0\nsum_sq = 0.0\nfor i in range(M):\nx = tl.load(input_ptr + pid * M + i)\nsum += x\nsum_sq += x * x\n\nmean = sum / M\nvar = sum_sq / M - mean * mean + epsilon\n\ntl.store(mean_ptr + pid, mean)\ntl.store(var_ptr + pid, var)\n@triton.jit\ndef kernel_normalize(\ninput_ptr,\noutput_ptr,\nmean_ptr,\nvar_ptr,\nweight_ptr,\nbias_ptr,\n## M,\n## N,\nepsilon,\nelementwise_affine,\nBLOCK_SIZE: tl.constexpr,\n):\npid = tl.program_id(0)\nif pid >= N:\nreturn\n\nmean = tl.load(mean_ptr + pid)\nvar = tl.load(var_ptr + pid)\n\nfor i in range(M):\nx = tl.load(input_ptr + pid * M + i)\nx_float = x.to(tl.float32)\nx_float = x_float - mean\nx_float = x_float / tl.sqrt(var) # Corrected line: removed epsilon here\n\nif elementwise_affine != 0:\nweight = tl.load(weight_ptr + i).to(tl.float32)\nbias = tl.load(bias_ptr + i).to(tl.float32)\nx_float = x_float * weight + bias\n\nx = x_float.to(x.dtype)\ntl.store(output_ptr + pid * M + i, x)\n\ndef wrapper(input, normalized_shape, weight=None, bias=None, eps=1e-5, elementwise_affine=True):\nif not isinstance(input, torch.Tensor):\nraise TypeError(\"Input must be a torch.Tensor\")\nif not input.is_contiguous():\ninput = input.contiguous()\n\nif isinstance(normalized_shape, int):\nnormalized_shape = (normalized_shape,)\nelse:\nnormalized_shape = tuple(normalized_shape)\n## D = len(normalized_shape)\ninput_shape = input.shape\nif D > len(input_shape):\nraise ValueError(\"normalized_shape cannot be larger than input shape\")\n\n## M = 1\nfor dim in normalized_shape:\n## M *= dim\n## N = 1\n\nfor dim in input_shape[:-D]:\n## N *= dim\n\nif N == 0 or M == 0:\nreturn torch.empty_like(input)\n\ninput_float = input.to(torch.float32)\ninput_float = input_float.view(N, M)\nmean = torch.empty(N, dtype=torch.float32, device=input.device)\nvar = torch.empty(N, dtype=torch.float32, device=input.device)\n\n## BLOCK_SIZE = 1\ngrid = (N,)\nkernel_mean_var[grid](\ninput_float,\nmean,\nvar,\n## M,\n## N,\neps,\n## BLOCK_SIZE=BLOCK_SIZE,\n)\n\noutput = torch.empty_like(input)\noutput = output.view(N, M)\n\nif elementwise_affine:\nif weight is None:\nweight = torch.ones(normalized_shape, dtype=input.dtype, device=input.device)\nelse:\nweight = weight.contiguous().view(-1)\nif bias is None:\nbias = torch.zeros(normalized_shape, dtype=input.dtype, device=input.device)\nelse:\nbias = bias.contiguous().view(-1)\nelse:\nweight = torch.ones(M, dtype=input.dtype, device=input.device)\nbias = torch.zeros(M, dtype=input.dtype, device=input.device)\n\ninput_flattened = input.view(N, M)\nkernel_normalize[grid](\ninput_flattened,\noutput,\nmean,\nvar,\nweight,\nbias,\n## M,\n## N,\neps,\n1 if elementwise_affine else 0,\n## BLOCK_SIZE=BLOCK_SIZE,\n)\n\noutput = output.view(input_shape)\nreturn output\n\n## C State Transition Prompts\n\nAs TritorX executes its feedback loop, we have different prompts for different scenarios. The initial prompt provides rules and background information, most critically PyTorch docstring(s) explaining both the operator in question and other referenced PyTorch docs or operators, alongside three fully working MTIA Triton reference kernels for exp, argmax, and diag:\n\n‚¨á\ninit_prompt = f\"\"\"\nWrite me a Triton implementation of the {op_name} ATen operator from PyTorch using MTIA‚Äôs version of Triton. This implementation should support the following input dtypes: {str(dtypes)}.\n\nTo make it easy to use for me please make sure to call your jitted kernel \"kernel\" and the wrapper code that translates the original signature of {op_name} to the kernel‚Äôs signature \"wrapper\".\n\nMultiple kernels are fine if needed, but try to generalize as much as possible and limit the number of kernels. Thoroughly analyze the problem and decide if one kernel is enough or if multiple are needed. If you get stuck trying to put all logic in one kernel, feel free to create multiple. Note that if you have multiple kernels, their names MUST all start with \"kernel\".\n\nDo NOT use or fallback to the base PyTorch or Triton implementations of these kernels - this is considered cheating and is NOT allowed.\n\nDo NOT include any comments or import statements (the required ones will be added while running the kernel - do not worry about import errors).\n\nAdditionally, for MTIA, we add 32-bit padding to input tensors. Take this into account while you are iterating and debugging your kernel.\n\nThink carefully about a correct and efficient implementation and finally output your code in a Python codeblock. Please only output a single module‚Äôs code containing the kernel and wrapper functions.\n\n## I‚Äôll paste the docstr of ATen‚Äôs {op_name} for reference, which defines the spec:\n{docstring}\n{supplemental_docstrings}\n\n\\n\\nFor your reference, I am including a few different types of fully working MTIA Triton implementations of ATen operators.\n\nThese will be listed in triplets, where each example will have \"Operator\", \"Kernel(s)\", and \"Wrapper\". Analyze these to better understand MTIA Triton:\\n\n\n<reference kernels>\n## Please think carefully and output a full implementation of {op_name} in MTIA Triton now!\"\"\"\n\nIf we are starting a new LLM session but have a partially working kernel already, we instead prompt it to debug an existing kernel, and provide the current implementation:\n\n‚¨á\ninit_prompt_with_existing_kernel = f\"\"\"\nDebug a Triton implementation of the {op_name} ATen operator from PyTorch, written in MTIA‚Äôs version of Triton. This implementation should support the following input dtypes: {str(dtypes)}.\n...\n...\nLastly, here is the work-in-progress implementation of this operator. It has a few issues that I need your help debugging, so be sure to analyze it thoroughly:\n<current partial implementation>\n\"\"\"\n\nWe describe the details of linting more in Appendix E, but the prompt itself is fairly straightforward, simply showing the LLM the lint errors.\n\nOur feedback prompts for compilation errors and correctness errors are more nuanced, and consist of three separate prompts. To begin with, part of our approach to optimize context length includes using an LLM-based summarizer for long compilation logs that often contain the same error message multiple times:\n\n‚¨á\nsummarization_prompt = \"\"\"\n...\n...\n## To recap, return the following information ONLY:\n## 1. The EXACT error message\n## 2. The EXACT code snippet that caused the error, both the exact line and the lines before it\n## 3. The EXACT traceback of the error, if present - do NOT include duplicates\n\"\"\"\n\nThis summary is then passed into a simple prompt telling the LLM to evaluate the compilation error and fix the kernel.\n\n## Appendix\n\nWhen the generated kernel fails a test case due to a correctness error, we provide quite a bit of feedback to the LLM based on the test case to use as context while debugging. We‚Äôve found that this does not increase the risk of overfitting to a specific test case whilst increasing the success rate:\n\n‚¨á\ncorrectness_feedback_prompt = \"\"\n...\n**Summary of the CPU output tensor for the input**:\n{str(latest_result.cpu_tensor_results)}\n**Summary of the MTIA output tensor for the input**:\n{str(latest_result.mtia_tensor_results)}\nAdditionally, here is the input and output data for the first failing test case. Do NOT overfit your kernel to this - remember you need to build a generalized kernel implementation. This is here solely to help you debug.\n\n## **INPUT SIGNATURE**:\n{str(input_sig)}\n## **OUTPUT SIGNATURE**:\n{str(output_sig)}\n## **INPUT SHAPE**:\n{str(latest_result.shape)}\n## **INPUT TENSOR**:\n{str(latest_result.input_tensor)}\n## **INPUT ARGS**:\n{str(latest_result.input_args)}\n## **INPUT KWARGS**:\n{str(latest_result.input_kwargs)}\n## Generate the corrected MTIA kernel now:\n\"\"\"\n\n# D Reasoning Trajectories\n\nYou are an expert in generating Triton MTIA (Meta Training and Inference Accelerator) kernels. Write me a Triton implementation of the nnfunctional.logsigmoid ATen operator from PyTorch using MTIA's version of Triton. This implementation should support the following input styles: ['bfloat16', 'float16', 'float31'].\n\nI'll paste the docstr of ATen's nnfunctional.logsigmoid for reference, which defines the spec:\n\n```txt\nlogsigmoid(input)  $\\Rightarrow$  Tensor\nApplies element-wise :math:'\\text{test(LogSigmoid)(x,i)}  $=$  \\log \\left[\\text{frac(1)(1 + \\exp(-x_i))}\\right]$\nSee :class:'\\torch.nn.logSigmoid' for more details.\nFor your reference, I am including a few different types of fully working MTIA Triton implementations of ATen operators.\n```\n\nOkay, I need to implement the logsigmoid function using MTIA Triton. Let's start by understanding the problem. The logsigmoid function applies the element-wise operation LogSigmoid(x_i) = log(1 / (1 + exp(-x_i))). This is equivalent to -softplus(-x_i), where softplus is the softplus activation function.\n\nFirst, I should consider the input styles: bfloat16, float16, and float32. MTIA Triton might handle these differently, but I'll assume that the kernel can be written in a way that works for all three types, possibly using type specialization or generic operations.\n\n```python\n@triton.jit\ndef kernel(\ninput_ptr,\noutput_ptr,\nn_elements,\nBLOCK_SIZE: tl.constexpr,\n):\npid = tl.program_id(@)\nblock_start = pid + BLOCK_SIZE\noffsets = block_start + tl.arange(@, BLOCK_SIZE)\nmask = offsets &lt; n_elements\nx = tl.load(input_ptr + offsets, mask = mask)\nexp_neg_x = tl.exp(-x)\nlogsigmoid = -tl.log(p(exp_neg_x)\ntl.store(output_ptr + offsets, logsigmoid, mask = mask)\ndef wrapper(input):\noutput = torch.empty_like(input, device = input_device)\nif input.numel() == @:\nreturn output\ninput_contig = input.contiguous().view(-1)\noutput_contig = torch.empty_like(input_contig, device = input_device)\nn_elements = input.numel()\n## BLOCK_SIZE = 120\ngrid = (triton.cdiv(n_elements, BLOCK_SIZE),)\nkernel(grid)\ninput_contig,\noutput_contig,\nn_elements,\n## BLOCK_SIZE = BLOCK_SIZE,\n)\nreturn output_contig.view(input.shape)\n```\n\nYour previous MTIA kernel implementation failed to pass the linter. Please analyze the lint error and provide a corrected version. Remember to adhere to the original output guidelines. ++Lint Error++:\n\n## Found 1 linting violation(s):\n\n[module Restrictions] forbidden tl module usage: tl.log(p (line 15)\n\nDetails: Allowed tl functions: tl.abs, tl.add, tl.advence, ...\n\n## Generate the corrected MTIA kernel now\n\nOkay, the linter is complaining about the use of tl.log(p in the kernel. Let me check the allowed tl functions. The error message lists the allowed functions, and tl.log(p is not among them.\n\n¬∑¬∑\n\nYour MTIA kernel implementation failed to compile. Please analyze the compilation error and provide a corrected version. Remember to adhere to the original output guidelines.\n\n++Compilation Error++:\n\n## ValueError: Expected dtype ['fp32', 'fp64'] but got fp16\n\n## Generate the corrected MTIA kernel now:\n\n¬∑¬∑\n\n```python\ndef kernel(\ninput_ptr,\noutput_ptr,\nn_elements,\nBLOCK_SIZE: tl.constexpr,\n):\npid = tl.program_id(@)\nblock_start = pid + BLOCK_SIZE\noffsets = block_start + tl.arange(@, BLOCK_SIZE)\nmask = offsets &lt; n_elements\nx = tl.load(input_ptr + offsets, mask = mask)\nx_float32 = tl.cast(s, tl.float32)\nexp_neg_x = tl.exp(-x_float32)\nlogsigmoid_float32 = -tl.log(1 + exp_neg_x)\nlogsigmoid = tl.cast(logsigmoid_float32, x.dtype)\ntl.store(output_ptr + offsets, logsigmoid, mask = mask)\ndef wrapper(input):\n```\n\nFigure D.5 Example of TritorX writing a kernel-wrapper pair for the nnfunctional.logsigmoid operator in 3 LLM calls after receiving the linter and compiler feedback. The reasoning steps are shortened and some details are omitted due to space constraints. Initial prompt in green, LLM reasoning in blue, LLM kernel-wrapper generation in purple, and TritorX feedback in orange.\n\n![img-4.jpeg](img-4.jpeg)\nFigure D.6 Example of TritorX writing a kernel-wrapper pair for the nnfunctional channel-shuffle operator in 14 LLM calls. The LLM has to incorporate the feedback from the scatter-store compilation error (hardware restriction) and the discrepancy between the MTIA test output compared to the CPU implementation, as well as address other compilation errors and hardware crashes.\n\n## E Triton MTIA Linter\n\nThe linter is a rule-based, static analysis tool that combines Python‚Äôs AST parsing with regex matching per rule. Several of these rules are formatting restrictions to make the output compatible with our execution system, such as disallowing import statements and enforcing a specific output format.\n\nOther rules are more concerned with what modules are being used and where. We have allowlists on several key modules, such as tl (triton.language), and torch. Anything from one of these modules not found in such an allowlist is considered a linter violation:\n\n‚¨á\n1module_restrictions:\n2 modules:\n3 - module_name: \"tl\"\n4 allowed_functions:\n5 - \"tl.load\"\n6 - \"tl.store\"\n7 - \"tl.arange\"\n8 # ... 200+ allowed Triton MTIA operations\n9 - module_name: \"torch\"\n10 allowed_functions:\n11 - \"torch.empty\"\n12 - \"torch.zeros\"\n13 # ... tensor allocation/reshaping only\n\nThis highlights exactly which aspects of upstream Triton are compatible with MTIA.\n\nAdditionally, we have scope restrictions on modules, such as allowing tl only in the kernel, not in the wrapper:\n\n‚¨á\n1module_scope_restrictions:\n2 restrictions:\n3 - module: \"tl\"\n4 allowed_scope_patterns: [\"^kernel.*\"] # tl.* only inside kernel functions\n\nAs part of our anti-cheating efforts, we enforce several rules. To prevent moving tensors back to CPU or attempting to move tensors to CUDA:\n\n‚¨á\n1forbidden_tensor_methods:\n2 enabled: true\n3 description: \"Prohibit tensor methods that move data between devices (CPU/CUDA transfers)\"\n4 forbidden_methods:\n5 - \"cpu\" # tensor.cpu() - moves tensor to CPU\n6 - \"cuda\" # tensor.cuda() - moves tensor to CUDA\n7\n8forbidden_function_arguments:\n9 enabled: true\n10 description: \"Prohibit specific argument values in function calls\"\n11 restrictions:\n12 # Forbid explicit CPU/CUDA device specifications in torch.device()\n13 - function: \"torch.device\"\n14 forbidden_string_args:\n15 - \"cpu\"\n16 - \"cuda\"\n\n## And to prevent workarounds such as eval or exec:\n\n‚¨á\n1# Rule to ban dangerous built-in functions that enable dynamic code execution\n\n```txt\nforbidden-functions:\nenabled: true\ndescription: \"Prohibit built-in functions that enable dynamic code execution\"\nforbidden-functions:\n- \"eval\" # Evaluates Python expressions from strings\n- \"exec\" # Executes Python code from strings\n- \"compile\" # Compiles Python code from strings (used with exec)\n",
      "source": "inbox/2512.10977v1.pdf"
    },
    "output_data": {
      "core_contribution": "TritorX represents a novel system leveraging a combination of open-source large language models (LLM) in a finite state machine (FSM) framework to generate and validate PyTorch ATen kernels. The system prioritizes correctness and breadth of coverage rather than merely performance optimization. As a modular, coverage-first approach, TritorX enhances backend development by automating the generation of functionally validated kernels for ML ASICs (specifically for Meta's MTIA) and provides the ability to produce comprehensive PyTorch backends quickly for emerging and future accelerator platforms.",
      "method_breakdown": "TritorX starts by selecting OpInfo operators, extracting the necessary documentation to guide kernel generation. A finite-state machine (FSM) orchestrates a feedback loop that integrates LLMs to produce kernel-wrapper pairs, which are then linted and just-in-time (JIT) compiled for correctness. Testing follows using OpInfo's extensive cases, confirming kernel functionality if all tests are passed. Throughout this process, the iterative feedback loop handles corrections and refinements automatically, ensuring high coverage and functional accuracy across a diverse set of PyTorch ATen operators.",
      "subsystems_parts": "1. Finite State Machine (FSM): Orchestrates the kernel generation process with LLM engagement, iterations, and error handling.\n2. Large Language Model (LLM): Uses extracted OpInfo data to generate code for kernel-wrapper pairs.\n3. JIT Compiler: Compiles and executes the generated kernels on MTIA hardware to test their validity.\n4. Custom Linter: Prevents unauthorized function use, ensuring outputs are valid and correct.\n5. OpInfo Testing Framework: Validates kernel functionality across multiple data types and scenarios.",
      "interactions": "The FSM mediates between the LLM, linter, JIT compiler, and the OpInfo testing framework‚Äîensuring each kernel passes through these subsystems sequentially. This interaction flow allows generated kernels to be rigorously tested for functionality and correctness across a wide array of operators and inputs, thereby increasing the system's robustness and reliability.",
      "delta_vs_baseline": "Unlike past methodologies that confined optimization to a performance-centric model focusing on specific high-usage kernels, TritorX contrasts by enhancing coverage and correctness across all operator types. This shift ensures comprehensive backend support, maintaining developer agility and infrastructure scalability irrespective of the kernel's usage frequency.",
      "evidence_anchor": "The successful generation of 481 unique ATen kernels, all passing over 20,000 PyTorch OpInfo tests, serves as substantial evidence of TritorX's reliability and correctness in implementing diverse and generalized kernel functionality.",
      "transferability": "The TritorX framework is designed with portable modularity, suggesting its applicability could extend to various ML ASIC platforms beyond Meta's MTIA. However, certain dialect-specific requirements must be integrated into the FSM to adapt the generated kernels to different accelerators with particular hardware constraints."
    },
    "conversation_log": [
      "Generator: {\"core_diagram_summary\":\"Figure 1 provides an overview of the TritorX system. It shows how OpInfo operators and their PyTorch docstring/signature are selected for kernel generation. The system iterates over each operator using a finite-state machine feedback loop, allowing kernel-wrapper generation using a large language model, followed by testing for correctness. Successful generation is marked by operators passing all corresponding OpInfo tests.\",\"unique_move\":\"TritorX uniquely integrates a finite state machine with open source large language models to iteratively generate and validate PyTorch ATen kernels by focusing on correctness and generality rather than performance.\",\"distilled_recipe\":\"Select OpInfo operators ‚Üí Use FSM with LLM to generate and validate kernels ‚Üí Test using OpInfo for correctness\",\"contribution_type\":\"System\",\"evidence_anchor\":\"The experiment showing 481 unique ATen operators generated that passed all OpInfo tests, demonstrating robust operator coverage and functionality validation.\"}",
      "Decider 1: {\"scratchpad_updates\":\"1. The core extraction identifies the main components of the TritorX system accurately, but lacks depth in describing specific technical processes like the use of JIT compilation and custom linting. \\n2. Expand the diagram summary to include details on how the execution feedback and testing frameworks integrate into the system beyond just passing OpInfo tests.\\n3. Elaborate the distilled recipe to include the step of using custom linter and JIT compilation before testing.\\n4. The contribution type is rightly categorized as a 'System', but could mention its application to new hardware simulation environments.\\n5. Evidence anchor: Expand the evidence anchor to include specific quantitative details from the tests beyond passing OpInfo tests, especially how it benefits hardware and compiler engineers.\",\"response_to_researcher\":\"1. Can you provide more information on how the custom linter enforces 'cheat' prevention in broader contexts within the generation process?\\n2. What are the specific challenges faced by TritorX when handling operators outside OpInfo tests, and how does the system address these?\\n3. How does the integration of a finite state machine (FSM) specifically improve the reliability of the LLM outputs compared to other potential frameworks?\\n4. Are there plans to extend TritorX to other architecture-specific dialects besides MTIA, and what would be the considerations?\"}",
      "Researcher 1: {\"correction_scratchpad\":\"1. The TritorX system uses a combination of open source large language models (LLMs) and a finite state machine (FSM) to generate, validate, and optimize PyTorch ATen kernels, focusing on correctness and generalizability. The TritorX pipeline includes custom linting to prevent unauthorized uses of functions like dispatch to unsupported operators, which Triton MTIA does not allow. This is particularly important to ensure the LLM outputs remain valid and usable across all cases without falling back on unimplemented or incorrect functionalities.\\n\\n2. The distilled recipe should reflect the sequential steps: Select OpInfo operators ‚Üí Generate kernels with LLM integrated into a FSM ‚Üí Lint and JIT compile kernels to check for unauthorized behavior or syntax errors ‚Üí Validate functionality through extensive OpInfo testing.\\n\\n3. The use of JIT compilation allows for on-the-fly generation and execution of the kernels within TritorX, ensuring the kernels meet both the functional and performance criteria through iterative testing.\\n\\n4. Evidence anchor improvements: Beyond passing the OpInfo tests, TritorX conducts over 20,000 total tests across different operators for verification, extending its utility into hardware simulation environments to provide feedback before chip tape-out, essential for both hardware and compiler engineers to identify and resolve performance bottlenecks and errors early in development.\\n\\n5. In terms of addressing operators outside of OpInfo coverage, TritorX incorporates a secondary testing harness for production model inputs, capturing the full operator space seen during model training and execution, refining kernel generation through a feedback loop and enabling more generalized kernel functionality.\\n\\n6. The FSM significantly contributes to the reliability of outputs by providing a structured, iterative feedback loop, allowing the system to handle errors in a controlled manner, re-evaluating and correcting as necessary without manual intervention.\\n\\n7. Plans to extend beyond MTIA involve addressing specific dialect requirements and introducing additional dialect-specific constraints into the FSM, facilitating adaptability to different hardware architectures while maintaining TritorX's framework scalability and reliability.\",\"response_to_decider\":\"1. The custom linter enforces immediate correction of unauthorized kernel logic by rejecting implementations that use forbidden operations or intrinsics not supported in MTIA. This guards against invalid operations and ensures all kernel semantics strictly align with supported functionalities, thus preventing 'cheat' paths from being programmed.\\n\\n2. TritorX addresses challenges of operators beyond OpInfo tests by including a secondary test harness that leverages production input data from actual model runs, thus extending coverage and identifying potential unseen execution paths or operator behaviors.\\n\\n3. The integration of a finite state machine (FSM) into the kernel generation process provides a resilient architecture, capable of handling complex logic states and error conditions, offering a structured control loop that directs the LLM‚Äôs resource usage effectively while maintaining kernel correctness over multiple iterations.\\n\\n4. There are plans for extending the applicability of TritorX to other architectures, focusing on integrating diverse dialect requirements into the FSM. This approach ensures that even with new dialects, the existing scalability and efficiency of the FSM framework can be preserved, enabling cross-architecture kernel generation abilities.\",\"done\":true}",
      "Finalizer: {\"core_contribution\":\"TritorX represents a novel system leveraging a combination of open-source large language models (LLM) in a finite state machine (FSM) framework to generate and validate PyTorch ATen kernels. The system prioritizes correctness and breadth of coverage rather than merely performance optimization. As a modular, coverage-first approach, TritorX enhances backend development by automating the generation of functionally validated kernels for ML ASICs (specifically for Meta's MTIA) and provides the ability to produce comprehensive PyTorch backends quickly for emerging and future accelerator platforms.\",\"method_breakdown\":\"TritorX starts by selecting OpInfo operators, extracting the necessary documentation to guide kernel generation. A finite-state machine (FSM) orchestrates a feedback loop that integrates LLMs to produce kernel-wrapper pairs, which are then linted and just-in-time (JIT) compiled for correctness. Testing follows using OpInfo's extensive cases, confirming kernel functionality if all tests are passed. Throughout this process, the iterative feedback loop handles corrections and refinements automatically, ensuring high coverage and functional accuracy across a diverse set of PyTorch ATen operators.\",\"subsystems_parts\":\"1. Finite State Machine (FSM): Orchestrates the kernel generation process with LLM engagement, iterations, and error handling.\\n2. Large Language Model (LLM): Uses extracted OpInfo data to generate code for kernel-wrapper pairs.\\n3. JIT Compiler: Compiles and executes the generated kernels on MTIA hardware to test their validity.\\n4. Custom Linter: Prevents unauthorized function use, ensuring outputs are valid and correct.\\n5. OpInfo Testing Framework: Validates kernel functionality across multiple data types and scenarios.\",\"interactions\":\"The FSM mediates between the LLM, linter, JIT compiler, and the OpInfo testing framework‚Äîensuring each kernel passes through these subsystems sequentially. This interaction flow allows generated kernels to be rigorously tested for functionality and correctness across a wide array of operators and inputs, thereby increasing the system's robustness and reliability.\",\"delta_vs_baseline\":\"Unlike past methodologies that confined optimization to a performance-centric model focusing on specific high-usage kernels, TritorX contrasts by enhancing coverage and correctness across all operator types. This shift ensures comprehensive backend support, maintaining developer agility and infrastructure scalability irrespective of the kernel's usage frequency.\",\"evidence_anchor\":\"The successful generation of 481 unique ATen kernels, all passing over 20,000 PyTorch OpInfo tests, serves as substantial evidence of TritorX's reliability and correctness in implementing diverse and generalized kernel functionality.\",\"transferability\":\"The TritorX framework is designed with portable modularity, suggesting its applicability could extend to various ML ASIC platforms beyond Meta's MTIA. However, certain dialect-specific requirements must be integrated into the FSM to adapt the generated kernels to different accelerators with particular hardware constraints.\"}"
    ],
    "processed_at": "2026-02-04 16:59:28.756309",
    "iterations": 1,
    "domain": "core_extraction"
  },
  "markdown_pages": [
    {
      "id": 1,
      "title": "Agentic Operator Generation for ML ASICs",
      "content": "# Agentic Operator Generation for ML ASICs\n\nAlec M. Hammond^{1,‚àó} Aram Markosyan^{2,‚àó} Aman Dontula^{1} Simon Mahns^{1} Zacharias Fisches^{2} Dmitrii Pedchenko^{2} Keyur Muzumdar^{2} Natacha Supper^{2} Mark Saroufim^{3} Joe Isaacson^{3} Laura Wang^{3} Warren Hunt^{2} Kaustubh Gondkar^{1} Roman Levenstein^{1} Gabriel Synnaeve^{2} Richard Li^{1} Jacob Kahn^{2} Ajit Mathews^{1} [ ^{1}Meta, ^{2}FAIR, Meta Superintelligence Labs, ^{3}Meta Superintelligence Labs\n^{‚àó}Equal contribution. ]\n\n###### Abstract\n\nWe present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.\n\n###### pacs:\n\n72.20.+d, 72.20.+g\n\nCorrespondence: Alec M. Hammond (alechammond@meta.com), Jacob Kahn (jacobkahn@meta.com)\n## Date: December 15, 2025\n\n## 1 Introduction\n\nThe rapid adoption of machine learning (ML) and artificial intelligence (AI) hardware is projected to drive US datacenter power consumption to between 6.7% and 12.0% of total electricity usage by 2028 *(Shehabi et al., 2024)*, highlighting an urgent need for efficient accelerator hardware to support large-scale model inference and training. In response, the industry is investing heavily in heterogeneous datacenter fleets that incorporate a variety of accelerator solutions, including custom silicon ASICs tailored to specific workloads and requirements *(Silvano et al., 2025)*. Namely, the Meta Training and Inference Accelerator (MTIA) currently serves recommendation models (DLRM) *(Naumov et al., 2019)* to billions of users across Facebook, Instagram, and Threads, while simultaneously reducing total cost of ownership by 44% compared to GPUs *(Coburn et al., 2025)*.\n\nHowever, despite the obvious advantages offered by in-house accelerators, each new platform requires significant engineering labor to build a software ecosystem compatible with existing tools such as PyTorch *(Paszke et al., 2019)*, given the large set of required tensor operators *(Kahn et al., 2022)*. An aspect therein is operator coverage, or the fraction of operators in ATen *(Paszke et al., 2017)* ‚Äî PyTorch‚Äôs tensor library ‚Äî that have kernels executing natively on a particular accelerator. Establishing and maintaining comprehensive operator coverage is an arduous task that is needed towards running inference with new or for prototyping new model architectures for training. In other words, while new accelerator platforms must provide competitive performance per unit cost, they must also provide an amenable developer experience with a comprehensive kernel backend.\n\nTo address this challenge, we present an agentic AI system capable of generating functionally correct Triton ATen kernels for MTIA at scale. This new tool, which we call TritorX, leverages open-source large language models (LLM) paired with execution feedback in a finite state machine (FSM) to generate, compile, and test hundreds of kernels directly on MTIA hardware. TritorX is compatible with MTIA‚Äôs production infrastructure, producing kernel-wrapper pairs that can be immediately registered within PyTorch and can be used for experimental model training, or in a production inference model. Building on top of production infrastructure, TritorX can also generate new PyTorch backends for upcoming accelerator generations via hardware simulation, providing important\n\n![img-0.jpeg](img-0.jpeg)\nFigure 1 TrittorX System Overview. (A) OpInfo operators and their PyTorch docstring/signature are selected for generation. (B) TrittorX iterates on each operator in a finite-state machine feedback loop. Kernel-wrapper pairs are generated using a large language model (LLM). Production infrastructure allows for simultaneous generation and testing at scale. (C) The generation task is successful if an operator passes all corresponding OpInfo tests.\n\nfeedback to hardware and compiler engineers before tape-out. In the limit, we envision implementing a kernel backend for a new chipset overnight.\n\nWe note that TrittorX differs from other kernel generation efforts, in that our framework strictly optimizes for correctness and generalizability across an entire backend, rather than performance over a narrow subset of critical path kernels (Lange et al., 2025a). For example, TrittorX is configured to generate kernel-wrapper pairs that are compatible with a wide range of quantization data types, tensor shapes, or PyTorch argument inputs. In some cases, TrittorX will generate multiple kernel implementations for a particular operator, and the corresponding dispatch logic is implemented in the wrapper function. Using a Triton dialect known as Triton MTIA, standard models can produce working kernels and rely on execution feedback to identify MTIA-specific semantics or intrinsics. Importantly, we only provide TrittorX with the ATen operator docstring to generate the corresponding implementation. Figure 1 describes the end-to-end workflow for accelerator enablement.\n\nTritorX performs in-context learning iteratively, distilling hardware requirements and their corresponding Triton semantics based on the feedback obtained di\n\nrectly via tools like the compiler. Previous works demonstrated that pipelines like TrittorX will often \"cheat\" at the generation task by dispatching to the host or calling other undefined PyTorch functions in the operator wrapper (Lange et al., 2025b). We avoid this by incorporating a custom linter that catches unauthorized uses of these functions or utilities and forces correction via feedback.\n\nCentral to TrittorX is the integration of multiple testing frameworks to ensure correctness across different data types, tensor shapes, or input arguments. We use OpInfo $^1$ , a PyTorch-native testing framework, along with a custom test harness that pulls test data from models in production.\n\n## In summary, our contributions are as follows:\n\n- System. A finite state machine using open source LLMs that generates, compiles, and validates hundreds of Triton MTIA kernels end-to-end. TrittorX runs directly on deployed silicon or via hardware simulation enabling prototyping for future devices. TrittorX only requires operator docstrings to generate the corresponding implementations. Its modular design allows for modular features, like the custom linter which prevents \"cheating.\"\n- Coverage objective. A design that optimizes for functional correctness and backend coverage covering multiple data types, shapes, and branching dispatch logic via generated wrappers.\n- Results. At scale, TrittorX produced 481 ATen kernels (84.7% MTIA-compatible OpInfo coverage) which pass all their corresponding OpInfo tests, in total more than 20,000. In addition, TrittorX demonstrates capability on end-to-end model enablement tasks for various models on MTIA. TrittorX can iterate over the entire OpInfo operator set in a few hours, allowing for rapid backend development and iteration.\n- Testing harness. We integrate PyTorch OpInfo and captured production inputs testing for correctness under deployment conditions."
    },
    {
      "id": 2,
      "title": "2 Background",
      "content": "# 2 Background\n\nIn this section, we describe the MTIA architecture, the Triton MTIA dialect, and how TrittorX can be instrumented to generate kernels at scale.\n\n## The MTIA architecture employs a grid of 8x8 processing elements (PEs) responsible for executing the\n\ncore kernel workloads (Coburn et al., 2025). Each PE consists of a scalar RISC-V core, a vector RISC-V core, and various fixed-function units (FFUs) responsible for implementing dedicated computations, such as direct memory accesses (DMAs) and dot products. Figure 2 illustrates the MTIA architecture.\n\n![img-1.jpeg](img-1.jpeg)\nFigure 2 MTIA's architecture overview. The core kernel computation is performed by a grid of process elements (PEs) which consist of a scalar core, a vector core, and special function units.\n\nIn particular, MTIA features a unique memory hierarchy with a significant amount of local SRAM available to all PEs via a series of crossbars. Dedicated PE FFUs facilitate the abstraction of circular buffers, enabling efficient pipelining of computation and communication needed to amortize the overall latency resulting from data movement. The computational savings gained by this approach allow MTIA to leverage cheaper LPDDR DRAM instead of HBM. Importantly, significant effort was made to mitigate overhead related to kernel or job dispatch from the host, enabling eager-mode workflows.\n\nTo execute kernels on MTIA, developers can author the workload using a  $\\mathrm{C + + }$  API, which is compiled with an LLVM backend, or using a custom Triton dialect adapted specifically for MTIA. Triton is an open-source Python library that provides a domain-specific language (DSL) for writing highly efficient custom GPU kernels (Tillet et al., 2019b). It simplifies GPU programming by allowing developers to express complex parallel computations using intuitive block-based abstractions while automatically handling low-level details such as memory access\n\npatterns and synchronization. This enables users to achieve performance comparable to hand-written CUDA code, without requiring deep expertise in GPU architectures.\n\nAlthough Triton was written to express the semantics of GPU computation, several of the existing semantics can be directly translated to corresponding MTIA hardware features. For example, instead of mapping Triton blocks to GPU threads, we can map them to the MTIA PE grid and rely on masking within loads and stores such that tensor boundaries are respected. Furthermore, loads and stores can take advantage of the MTIA DMA engine for structured memory access. When intrinsics do not perfectly match, we can augment the underlying Triton feature-set with specific device libraries (e.g. to leverage FFUs that implement nonlinear activations). Triton MTIA is a dedicated dialect intended to facilitate the above mappings, along with various other performance optimizations via the compiler backend.\n\nAlthough Triton MTIA intends to preserve the existing Triton semantics as much as possible, there are certain hardware requirements that force notable deviations. For example, MTIA requires 32-byte aligned memory access patterns, and load/store operations will fail if this is not satisfied. On the surface, this may seem problematic when trying to generate kernels using off-the-shelf models. However, Triton MTIA has detailed assert messages and error handling providing the necessary feedback that the models need to adapt the vanilla Triton code for MTIA. Indeed, building a compiler tool-chain that gives descriptive feedback is an important part of leveraging automated approaches for code generation.\n\nThe challenge is then to implement an execution pipeline that is compatible with the existing production infrastructure. MTIA (and all future hardware versions) are deployed in a productionized Linux container ecosystem (Tang et al., 2020). Typical production workflows require an end-to-end workload that can e.g. serve or train a model for a particular service at scale, which also requires a complicated kernel registration stack compatible with the PyTorch ecosystem. However, the Triton JIT allows us to generate, compile, and test kernels on the fly, even within these productionized containers, allowing us to run numerous experiments in parallel."
    },
    {
      "id": 3,
      "title": "3 System Design",
      "content": "# 3 System Design\n\nHere, we describe the system architecture behind TritorX, along with the test harness used to validate the results during generation."
    },
    {
      "id": 4,
      "title": "3.1 In-Context Distillation of Triton MTIA Semantics",
      "content": "# 3.1 In-Context Distillation of Triton MTIA Semantics\n\nA naive approach to generating Triton MTIA code is to add comprehensive specifications of the hardware requirements and corresponding Triton semantics differences to the first prompt of the LLM to see if the generated code passes relevant tests. In practice, however, comprehensive accelerator documentation lags behind other stack components. Early attempts at generating Triton for MTIA with a simple prompt-engineering approach resulted in significant manual labor and did not scale.[2]\n\nIn contrast, TritorX effectively performs in-context learning iteratively, distilling hardware requirements and their corresponding Triton semantics based on the feedback obtained directly via interaction with the linter, compiler, and debugger.\n\nWe implemented TritorX as a finite-state machine (FSM) with dedicated tools and routines, including linting, compiling, testing, debugging, and LLM calls.\n\nAlthough recent kernel generation frameworks often rely on a dedicated reasoning agent with prescribed tool calling (Lange et al., 2025b; Wang et al., 2025; Chen et al., 2025b; Andrews and Witteveen, 2025; Li et al., 2025a,b), we found it easier to integrate an FSM architecture within our production infrastructure at scale. The FSM offers explicit guardrails around what is executed and performed, and allows faster debugging of the key components of the system which is an important requirement when dealing with production-ready systems. Additionally, the backend of the FSM enables compilation and testing on both the deployed MTIA machines and on the QEMU simulator of the future MTIA generations."
    },
    {
      "id": 5,
      "title": "3.2 TritorX Agent",
      "content": "# 3.2 TritorX Agent\n\nFigure 3 illustrates the overall design of the TritorX system. Each operator is generated in a self-contained session during which all prescribed tests are performed. This session is configurable up front, allowing us to easily prototype different LLM models, disable/enable individual states (like the linter), and sweep TritorX hyperparameters (e.g., max number of iterations).\n\nThe routine begins with an initial prompt consisting of the task description, output requirements, the documentation (docstring) of the PyTorch operator, and three handcrafted examples ( $\\S C$ ). Specifically, the prompt asks the model to generate a python wrapper matching the designated PyTorch operator\n\n![img-2.jpeg](img-2.jpeg)\nFigure 3 Finite State Machine of our kernel-generation agent TritorX. Proposal kernel-wrapper pairs are only generated during the \"Generate Kernel\" state (which dispatches to an open source, reasoning LLM). All other states process the result and update a feedback prompt, where needed.\n\nsignature (described in the docstring) and one or more Triton kernels that implement the functionality itself. The exp, argmax, and diag operators were chosen as simple examples as they span multiple kernel classes (e.g., elementwise, reduction). Often times, ATen docstrings will reference the docstrings of other operators (e.g., argmax references max). We built a directed acyclic graph of all docstrings, allowing us to include \"nested\" docstrings for completeness. Further instructions prescribing the expected output format expected by the downstream parser are also provided.\n\nThe reasoning LLM uses the prompt to generate an initial candidate wrapper/kernel pair, which is then sent to a custom Triton MTIA Linter. The linter is responsible for the following tasks: (1) ensuring the output wrapper and kernel code is compatible with the Triton JIT harness; (2) ensuring the provided implementation does not \"cheat\" by dispatching into other operators that may not yet be implemented; (3) ensuring the provided code uses valid Triton MTIA syntax and libraries, as not all of upstream Triton is available on MTIA. The linter is lightweight and configurable ( $\\S$ E). If a lint violation is detected, a structured report is generated and sent back to the model as feedback for correction ( $\\S$ C). The process\n\nis repeated until no lint errors are produced or the maximum number of LLM calls is reached.\n\nIf no linter errors are detected, the wrapper and kernel code is passed to a dedicated Triton JIT compilation harness compatible with the MTIA infrastructure. Depending on the operator configuration, which prescribes the supported datatypes, a series of tests derived from and production-data are synthesized. The test runner loops through each test, recompiling as needed (e.g. for new datatypes). If compilation is successful and the test executes without any runtime errors, the same inputs are moved to the host and executed using a reference ATen CPU implementation of the operator. The outputs for both the generated MTIA kernel and the CPU reference kernel are compared using a heuristic that depends on the underlying datatype. If the results are within the specified tolerance, the process repeats with the next test. As soon as the runner encounters a compilation failure, a runtime error, or an accuracy error, the routine breaks and proceeds to a ‚Äúfeedback‚Äù state responsible for determining what to do next.\n\nThe feedback state analyzes how successful the test runner was and determines what kind of feedback prompt is needed for another LLM iteration, or if further debugging is needed. For example, if the most recent run resulted in a runtime crash which produced a crash dump, the crash dump is loaded in an LLDB-based debugger. The debugger pulls basic information about the backtrace, decoded registers, and other frame information to provide as context for the revised prompt. Example insights include details around memory access violations.\n\nIn the case of a compiler failure, depending on the hyperparameter config, we optionally summarize the compiler log using a secondary LLM instance (also configurable up front). Triton MTIA compiler logs can easily consume thousands of tokens, so surfacing the most relevant facets of the compiler error to the main LLM session serves towards managing limited context windows.\n\nIf the feedback state detects an accuracy error, a summary of the MTIA output tensor(s) and the CPU output tensor(s) is included in the feedback prompt. Even in the case of large output tensors, an abbreviated summary of the tensor values is often enough context for the model to reason about the potential inaccuracy (¬ßD).\n\nOnce an appropriate feedback prompt is crafted, the process repeats until one of the following conditions occurs: (1) all tests pass, in which case the routine exits successfully; (2) the maximum number of prescribed LLM calls has been reached, and the routine exits; (3) the LLM context window saturates, and a new LLM dialog session starts using the most recent wrapper/kernel generation as an initial proposal; (4) an unexpected error crashes the main process. We implemented comprehensive exception handling throughout the TritorX, including launching containerized subprocesses where necessary, to avoid crashing the main process whenever possible.\n\nIn order to generate operators at scale, the above process can be executed in a parallel fashion for every operator specified and even repeated for operators that failed. These large-scale runs are configured by the operators of interest, the desired datatypes, the LLM parameters (e.g., model, context length, temperature), the run parameters (e.g., maximum number of LLM calls, maximum number of dialog sessions) and the testing complexity.\n\nImportantly, the operators are compiled and executed on productionized MTIA machines. The LLM calls themselves are processed by a centralized inference platform service capable of handling a high volume of requests needed for large-scale runs.\n\n### 3.3 ATen Operators and OpInfo Testing\n\nTo evaluate the viability of our approach at scale, we generate kernels for the operators defined within the PyTorch OpInfo testsuite. Importantly, OpInfo aims to rigorously test an operator‚Äôs coverage by providing ‚Äúsamples‚Äù for all the supported data-types, tensor shape, and input arguments. For example, OpInfo contains hundreds of tests for the linalg.vector_norm operator which rigorously test different input and output tensor shapes, along with different input argument configurations.\n\nUsing OpInfo as our primary test harness creates an end-to-end generation pipeline with orders of magnitude more tests than the state of the art *(Ouyang et al., 2025)*. By covering more of the input space during the generation process, we expect the resulting implementation to more reliably work in arbitrary prototyping and production environments. That being said, we recognize that ensuring perfect test coverage across the entire input space is impossible. To account for this, we introduce a secondary test harness specifically for production models that consists of production input data. This additional testsuite allows us to gauge how well an operator was generalized using the OpInfo testsuite. If gaps are identified during the generation stage (such that production-data tests failed), then TritorX is able to resolve the coverage gap.\n\nThere are certain limitations with MTIA hardware such that certain operators and tests are either not compatible or not relevant for the target workloads. For example, MTIA does not support complex numbers, so we remove those corresponding operators from the generation list (e.g., FFT operators). Similarly, validating the outputs for random-number operators between the device and host is particularly challenging due to differences in the underlying random number generation algorithm. As such, we also remove these operators from consideration. Additionally, due to a limitation of our distributed testing infrastructure, we only cover operators with under 900 total OpInfo tests. The resulting operator list consists of 568 unique operators (filtered down from 629). We also only test for bfloat16, float16, float32, int32, and int64. In total, this results in over 20,000 tests across all operators, and we only classify an operator as successful if it passes all the corresponding operator tests."
    },
    {
      "id": 6,
      "title": "4 Experiments and Results",
      "content": "# 4 Experiments and Results\n\nWe now present the experiments used to validate our approach. We first present an aggregate result consisting of kernels that span all MTIA-compatible OpInfo operators over multiple large-scale runs. From this set of generated operators, we \"productionize\" various first- and third-party models. We further expand our test harness for these operators by incorporating additional correctness tests that leverage production data and identify additional gaps not originally captured by OpInfo. Finally, we ablate over various TritorX configurations to highlight which aspects of our pipeline matter and why.\n\nOur baseline setup for these experiments is to run TritorX over all the 568 MTIA-compatible OpInfo operators with the following configuration:\n\n- Maximum of 3 TritorX attempts (i.e., dialog sessions) per operator to generate a kernel that passes all tests and declares Success;\n- Each attempt is allowed a maximum of 15 LLM calls, or, in other words, 15 full iterations through the state machine until Failure is declared for the attempt;\n- Either Code World Model (CWM, Copet et al. (2025)) or GPT-OSS 120B (OpenAI, 2025) were used as the kernel-generating LLM. Both models were configured with a context length of 131,072 and temperature set to 1.0. We set the top-P to 0.95 for CWM and 1.0 for GPT-OSS. The GPT-OSS reasoning was set to \"high.\"\n\n- Llama-4-Maverick is used as the feedback summarization model with the same generation parameters as CWM.\n\nWe dispatch the generation jobs across 200 production MTIA devices, which are able to finish  $95\\%$  of a run in 2 hours. The remaining tail often results from e.g., poor reasoning trajectories, and can take another 6-8 hours to complete. New runs can be dispatched concurrently. With this infrastructure in place, we executed multiple runs, with subsequent runs focusing on operators that failed previous runs.\n\nFrom these aggregated runs, we achieved  $84.7\\%$  operator coverage on all MTIA-compatible OpInfo operators. Here we consider an operator covered if the generated kernel-wrapper pair passes  $100\\%$  of the sample OpInfo tests. These results were aggregated across multiple runs, including ablations over models and configuration parameters. Figure 4 illustrates the cumulative operator coverage as a function of LLM calls for different configurations.\n\n![img-3.jpeg](img-3.jpeg)\nFigure 4 Number of LLM calls per operator to produce a correct kernel, cumulatively plotted for different harness configurations and models. Ensemble results display coverage achieved by combining multiple configurations. We also include experimental localization runs, where we pull relevant operators as context. The global aggregate includes all our available runs, some of which are not shown in the figure.\n\nWe further heuristically divide operators into 7 categories depending on the intended functionality of each operator. Table 1 shows that different categories present different difficulties to TritorX: while TritorX achieves  $96.0\\%$  coverage on Shape Manipulation operators, the coverage significantly drops for the operators from the Deep Learning Category.\n\n## Finally, we executed a run with GPT-OSS on a future generation using a QEMU simulator (Bellard,\n\n|  Op Category | Op Count | Operator Coverage (%)  |   |\n| --- | --- | --- | --- |\n## |   |   |  CWM | GPT-OSS  |\n|  Elementwise | 161 | 80.1 | 84.6  |\n|  Deep Learning | 90 | 64.4 | 71.1  |\n|  Linear Algebra | 78 | 71.8 | 79.5  |\n|  Other | 78 | 75.6 | 74.3  |\n|  Shape Manipulation | 75 | 96.0 | 96.0  |\n|  Reduction | 63 | 69.8 | 74.6  |\n|  Indexing & Selection | 34 | 73.5 | 79.4  |\n\n2005) for execution feedback. This single run yielded a coverage of  $73.1\\%$ . We aggregated the compiler failures and feature gaps encountered during generation, and shared this data with our compiler and ASIC engineers."
    },
    {
      "id": 7,
      "title": "4.1 End-to-end Application",
      "content": "# 4.1 End-to-end Application\n\nFrom our baseline set of OpInfo operators, we used TritorX to enable various first- and third-party models on MTIA. While robust, we recognize that production workflows may contain operators and operator arguments (eg. shapes, scalar values, etc.) outside of the distribution represented in the OpInfo testsuite. To mitigate this, we decompose various target models into their individual operators, extract all operator inputs observed during training, and then use these inputs within the TritorX validation loop instead of those generated by OpInfo.\n\nConcretely, we instrumented forward and backward passes of several representative models, NanoGPT (Karpathy, 2023), DLRM (Naumov et al., 2019), and two internal recommendation models (denoted MM), using __torch_dispatch__ to intercept and record the tensor and scalar data passed to each operator. All four models were evaluated with a fixed batch size of 1024 and trained for single iteration, with the latter three (DLRM, MM1, and MM2) executed using real production data rather than randomized inputs.\n\nAdditionally, we introduce a new step to TritorX, first matching a given operator with a pre-generated OpInfo operator (should it exist), then immediately testing it with the inputs gathered from the full e2e run. Should the kernel not pass all tests out of the box, it is used as a starting point from which TritorX then refines (Table 2, column B: MIS).\n\nAcross these experiments, TritorX achieves high kernel coverage, enabling nearly  $80\\%$  of all kernels required to execute a model end-to-end. Furthermore,\n\nTable 1 TritorX Coverage by operator category and used LLM.\n\n|  Model | Operator Coverage (%)  |   |   |\n| --- | --- | --- | --- |\n|   |  A. Full Model Op Set | B. OpInfo Subset  |   |\n|   |   | OpInfo | MIS  |\n## |  NGPT | 87.2 | 80.0 | 100.0  |\n## |  DLRM | 81.4 | 80.0 | 90.0  |\n|  Meta M1 | 79.8 | 83.8 | 91.9  |\n|  Meta M2 | 80.6 | 81.7 | 87.3  |\n\nfor operators where a pre-existing OpInfo-validated kernel is available, over  $80\\%$  of these kernels pass all end-to-end production tests without additional prompting. After refinement, TritorX further improves this by an additional  $6 - 20\\%$  across models. This not only underscores the robustness of TritorX, but also establishes a sandbox for continuous testing and optimization of production-ready kernels."
    },
    {
      "id": 8,
      "title": "4.2 TritorX Harness Ablation",
      "content": "# 4.2 TritorX Harness Ablation\n\nTo better understand which aspects of TritorX contribute to its success, we ablate over various configurations. Table 3 summarizes the results of these experiments.\n\nTable 2 Operator coverage across four model types: NanoGPT, Deep Learning Recommendation Model, Meta Model 1, Meta Model 2. An operator is considered covered if the corresponding kernel passes all tests with model input shapes (MIS). (A) We run TritorX on all model operators with the MIS feedback. (B) We run TritorX on a subset of model operators that have tests available in the OpInfo suite. (OpInfo) We directly test kernels created with OpInfo feedback with MIS. (MIS) We run TritorX to refine kernels created with OpInfo feedback with MIS feedback.\n\n|  Method | CWM (%) | GPT-OSS (%)  |\n| --- | --- | --- |\n|  Baseline (single run) | 55.3 | 72.0  |\n|  w/o linter | 48.9 | 68.7  |\n|  w/o summarization | 48.2 | 71.5  |\n\nTable 3 Ablations over TritorX harness features.\n\nWe examine the importance of the custom Triton MTIA linter and the optional summarization model. Removing the linter resulted in a significant drop in performance (55.3% ‚Üí 48.9% for CWM). As mentioned previously, the linter not only helps the agent identify intrinsics unique to the Triton MTIA dialect, but also helps prevent \"cheating\" by flagging the unauthorized use of other torch operators (¬ßE).\n\nRemoving the summarization agent also resulted in a decrease in performance (55.3% ‚Üí 48.2% for CWM). Without a separate summarization agent, the entire\n\ncompilation log, which can consist of thousands of tokens, is fed directly into the LLM dialog session, and the model performance can degrade as we approach the context limit *(Hsieh et al., 2024)*.\n\n## 5 Related Work\n\nCustom ASICs, MTIA & Deep Learning Compilers. Since Triton introduced a Python-first DSL for high-performance GPU kernels *(Tillet et al., 2019a)* and became widely used by PyTorch Inductor *(Ansel et al., 2024)* to generate fused operators, several ecosystems now offer Python-level kernel DSLs. NVIDIA‚Äôs Warp *(Macklin, 2022)* is a Python DSL for authoring CUDA kernels, with an optional tile-based programming model for tensor-core GEMMs. In JAX, Google‚Äôs Pallas *(Bradbury et al., 2018)* allows users to write custom kernels in Python; it targets GPUs via Triton and TPUs *(Jouppi et al., 2017)* via Mosaic *(Bansal et al., 2023)*, enabling fine-grained fused operations within the JAX ecosystem. Meta has also adopted Triton for MTIA to improve developer efficiency with PyTorch.\n\nKernel Research. Optimization of custom kernels for specific devices has received sustained attention in recent years: *Lavin and Gray (2016); Dao et al. (2022); Dao (2023); Shah et al. (2024)*. This interest is both driven by and enabling the rapid growth of training and inference compute of frontier models: *Cottier and Rahman (2024); Hooker (2021)*.\n\nBenchmarks. The community has responded by releasing benchmarks to measure the functional correctness and performance of LLM-generated kernels that primarily target GPUs through CUDA and Triton. KernelBench *(Ouyang et al., 2025)* and TritonBench *(Li et al., 2025a)* evaluate whether models can generate correct and performant GPU kernels across representative operator suites, while NPU-Eval *(Kalade et al., 2025)* targets AMD NPUs. AlgoTune *(Press et al., 2025)* in particular targets LLM‚Äôs ability to speed up scientific computing problems from natural language descriptions. Automatically generated kernels are prone to exploiting holes in the test suite. Addressing this has seen significant effort: *(Lange et al., 2025b; METR, 2025)*. Notably, similar to our work BackendBench *(Saroufim et al., 2025)* also uses PyTorch‚Äôs OpInfo as a comprehensive test suite to ensure correctness and comprehensiveness.\n\nLLM-based Kernel Generation. Most LLM-based kernel generation relies on prompting techniques combined with test-time compute use as a form of search. Recent systems train or prompt LLMs to synthesize kernels or improve them with search. Kernel-LLM *(Fisches et al., 2025)* provides a supervised baseline for the generation of Triton kernels from PyTorch modules. AutoTriton *(Li et al., 2025b)* adds reinforcement learning from verifiable rewards. Multi-turn reinforcement learning for CUDA kernel generation has also been explored in *Baronio et al. (2025)*.\n\nOrthogonally, several approaches scale inference compute with existing LLMs, covering a spectrum of prompting, agentic, and evolutionary techniques combined with verification through unit tests: *(Lange et al., 2025b; METR, 2025; Chen et al., 2025a; Wei et al., 2025; Wang et al., 2025)*. While several approaches exist that target CUDA and GPUs, we believe only NPU-Eval *(Kalade et al., 2025)* is targeting non-GPU devices with C++.\n\n## 6 Discussion and Future Work\n\nTritorX‚Äôs FSM framework provides a robust harness for generating functionally correct Triton MTIA kernels. Thanks to its flexibility, we identify several additional directions to further improve coverage and performance and reflect on what aspects are most important for success.\n\nSelf-consistent operator generation. To prevent ‚Äúcheating,‚Äù the linter restricts the agent from utilizing other ATen operators within the wrapper (beyond tensor allocation). A more efficient and possibly more performant approach is to allow the wrapper to dispatch other operators, provided they are also implemented in the new backend and the operators do not result in cyclic dependencies. This requires a self-consistent generation scheme where the agent is aware of the entire backend state (and is thus no longer embarrassingly parallel).\n\nOptimized prompting. As mentioned earlier, we found that simpler prompts without dedicated MTIA documentation worked best, but there remains room for prompt tuning. Furthermore, we can improve the quality of the example kernel-wrapper pairs themselves using strategies like localization, perhaps even bootstrapping the process over sequential runs.\n\nFully agentic pipeline. At a higher level than these improvements, we have discussed making the entire FSM agentic ‚Äì converting most current states into tools for the LLM to call upon, as well as adding new tools related to debugging, such as giving the LLM a sandbox to execute code in.\n\nDedicated model post-training. Both models used throughout this work were open source and off-the-\n\nshelf. All of the MTIA-specific context was gained via interactions with the linter, compiler, and debugger. An orthogonal research direction would be to further post-train the LLM for Triton MTIA kernel generation and contrast that with the existing results for vanilla Triton in the literature.\n\nThe importance of scale. Due to the stochastic nature of the underlying language model, running the benchmark repeatedly (with a nonzero temperature) will produce results with nonidentical pass-rates. Thus, simply aggregating the passing operators across runs, a technique known as test-time scaling, can yield significant increases in operator coverage. For example, just aggregating between two benchmark runs using CWM increased the coverage from 55% $\\rightarrow$ 64%. We suspect that further scaling this and exploring more complex strategies such as evolutionary scaling *(Lange et al., 2025b)* will bring significant coverage improvements. Furthermore, supporting additional hardware generations, DSLs, and operator definitions will require a flexible and scalable pipeline for generating operator sets.\n\nEstablishing a backend-maintenance environment. As AI-driven kernel generation evolves, AI will eventually produce more kernels than humans can feasibly review‚Äîespecially as hardware diversity increases and multiple generations of hardware coexist within production fleets. Establishing a robust, fully automated framework that does not require human review will be essential for the success of AI-generated kernels. This need will become even more critical as each model update triggers a comprehensive refresh of the kernel library to optimize performance.\n\n## 7 Conclusion\n\nWe introduced TritorX, a scalable, coverage-first system that automates bring-up of the PyTorch backend for custom ML ASICs, demonstrated on Meta‚Äôs MTIA. TritorX orchestrates an existing LLM with a finite-state workflow and production-compatible tooling, executed on both deployed silicon and a QEMU-based simulator for future devices. It generated over 481 ATen operators that pass all of their corresponding OpInfo tests (20,000+ total tests), achieving an overall pass rate of 84.7%. From these operators, we were able to onboard 80+% of the multiple first- and second-party large-scale models on the device. Ablations over model and system factors isolate which components most influence coverage, turning backend enablement into a measured engineering process rather than an artisanal effort.\n\nBeyond MTIA, the design is applicable to other accelerators and naturally extends to adjacent tasks such as cross-generation kernel migration and early hardware/compiler feedback via simulation. Although performance tuning is out of the scope of this work, TritorX provides the substrate on which autotuning, schedule search, and learned code optimization can be layered. Future directions include expanding operator families (reductions, sparse/quantized ops), stronger safety/containment, and formal checks for code generation, tighter integration with compiler IRs, and automated pathways for performance refinement.\n\nTaken together, these results offer a practical blueprint for democratizing toolchain creation and a force multiplier for kernel engineers‚Äîfreeing expert effort to focus on truly performance-critical paths while bringing entire backends online in hours rather than months.\n\n## References\n\n- Andrews and Witteveen (2025) M. Andrews and S. Witteveen. Gpu kernel scientist: An llm-driven framework for iterative kernel optimization. *arXiv preprint arXiv:2506.20807*, 2025.\n- Ansel et al. (2024) Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In *Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2*, pages 929‚Äì947, 2024.\n- Bansal et al. (2023) Manya Bansal, Olivia Hsu, Kunle Olukotun, and Fredrik Kjolstad. Mosaic: An interoperable compiler for tensor algebra. *Proceedings of the ACM on Programming Languages*, 7(PLDI):394‚Äì419, 2023.\n- Baronio et al. (2025) Carlo Baronio, Pietro Marsella, Ben Pan, and Silas Alberti. Kevin: Multi-turn rl for generating cuda kernels. *arXiv preprint arXiv:2507.11948*, 2025. https://arxiv.org/abs/2507.11948.\n- Bellard (2005) Fabrice Bellard. Qemu, a fast and portable dynamic translator. In *Proceedings of the USENIX Annual Technical Conference (ATEC ‚Äô05)*, page 41, Berkeley, CA, USA, April 2005. USENIX Association. https://dl.acm.org/doi/10.5555/1247360.1247401.\n- Bradbury et al. (2018) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. http://github.com/jax-ml/jax.\n\nTerry Chen, Bing Xu, and Kirthi Devleker. Automating gpu kernel generation with deepseek-r1 and inference-time scaling. NVIDIA Technical Blog, 2 2025a. https://developer.nvidia.com/blog/automating-gpu-kernel-generation-with-deepseek-r1-and-inference-time-scaling/.\n- [21] W. Chen, J. Zhu, Q. Fan, Y. Ma, and A. Zou. CUDA-LLM: Llms can write efficient cuda kernels. arXiv preprint arXiv:2506.09092, 2025b.\n- [22] Joel Coburn, Chunqiang Tang, Sameer Abu Asal, Neeraj Agrawal, Raviteja Chinta, Harish Dixit, Brian Dodds, Saritha Dwarakapuram, Amin Firoozshahian, Cao Gao, et al. Meta‚Äôs second generation ai chip: Model-chip co-design and productionization experiences. In Proceedings of the 52nd Annual International Symposium on Computer Architecture, pages 1689‚Äì1702, 2025.\n- [23] Jade Copet, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, David Zhang, Kunhao Zheng, Jordi Armengol-Estap√©, Pedram Bashiri, Maximilian Beck, Pierre Chambon, Abhishek Charnalia, Chris Cummins, Juliette Decugis, Zacharias V. Fisches, Fran√ßois Fleuret, Fabian Gloeckle, Alex Gu, Michael Hassid, Daniel Haziza, Badr Youbi Idrissi, Christian Keller, Rahul Kindi, Hugh Leather, Gallil Maimon, Aram Markosyan, Francisco Massa, Pierre-Emmanuel Mazar√©, Vegard Mella, Naila Murray, Keyur Muzumdar, Peter O‚ÄôHearn, Matteo Pagliardini, Dmitrii Pedchenko, Tal Remez, Volker Seeker, Marco Selvi, Oren Sultan, Sida Wang, Luca Wehrstedt, Ori Yoran, Lingming Zhang, Taco Cohen, Yossi Adi, and Gabriel Synnaeve. Cwm: An open-weights llm for research on code generation with world models, 2025. https://arxiv.org/abs/2510.02387.\n- [24] Ben Cottier and Robi Rahman. The training compute of notable ai models has been doubling roughly every six months. Epoch AI Data Insight, 2024. https://epoch.ai/data-insights/cost-trend-large-scale. Accessed 2025-10-21.\n- [25] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. https://arxiv.org/abs/2307.08691.\n- [26] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems (NeurIPS 2022), 2022. https://openreview.net/forum?id=H4DqfPSibmx. 36th Conference on Neural Information Processing Systems. (Peer-reviewed).\n- [27] Zacharias V. Fisches, Sahan Paliskara, Simon Guo, Alex Zhang, Joe Spisak, Chris Cummins, Hugh Leather, Gabriel Synnaeve, Joe Isaacson, Aram Markosyan, and Mark Saroufim. Kernelllm: Making kernel development more accessible, 6 2025. https://huggingface.co/facebook/KernelLLM.\n- [28] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58‚Äì65, 2021.\n- [29] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: What‚Äôs the real context size of your long-context language models?, 2024. https://arxiv.org/abs/2404.06654.\n- [30] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture, pages 1‚Äì12, 2017.\n- [31] Jacob D Kahn, Vineel Pratap, Tatiana Likhomanenko, Qiantong Xu, Awni Hannun, Jeff Cai, Paden Tomasello, Ann Lee, Edouard Grave, Gilad Avidov, et al. Flashlight: Enabling innovation in tools for machine learning. In International Conference on Machine Learning, pages 10557‚Äì10574. PMLR, 2022.\n- [32] S. Kalade et al. NPUEval: Optimizing npu kernels with llms and open source compilers. arXiv preprint arXiv:2507.14403, 2025. https://arxiv.org/abs/2507.14403.\n- [33] Andrej Karpathy. nanogpt: The simplest, fastest repository for training/finetuning medium-sized gpts, 2023. https://github.com/karpathy/nanoGPT. GitHub repository.\n- [34] Robert Tjarko Lange, Qi Sun, Aaditya Prasad, Maxence Faldor, Yujin Tang, and David Ha. The ai cuda engineer: Agentic cuda kernel discovery, optimization and composition. arXiv preprint arXiv:2509.14279 and project archive, 2025a.\n- [35] Robert Tjarko Lange, Qi Sun, Aaditya Prasad, Maxence Faldor, Yujin Tang, and David Ha. Towards robust agentic cuda kernel benchmarking, verification, and optimization. arXiv preprint arXiv:2509.14279, 2025b. https://pub.sakana.ai/static/paper.pdf.\n- [36] Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4013‚Äì4021, 2016.\n- [37] Jianling Li, Shangzhan Li, Zhenye Gao, Qi Shi, Yuxuan Li, Zefan Wang, Jiacheng Huang, Haojie Wang, Jianrong Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. Tritonbench: Benchmarking large language model capabilities for generating triton operators. arXiv preprint arXiv:2502.14752, 2025a. https://arxiv.org/abs/2502.14752.\n- [38] Shangzhan Li, Zefan Wang, Ye He, Yuxuan Li, Qi Shi, Jianling Li, Yonggang Hu, Wanxiang Che, Xu Han, Zhiyuan Liu, et al. Autotriton: Automatic triton programming with reinforcement learning in llms. arXiv preprint arXiv:2507.05687, 2025b.\n\nMiles Macklin. Warp: A high-performance python framework for gpu simulation and graphics. https://github.com/nvidia/warp, March 2022. NVIDIA GPU Technology Conference (GTC).\n- Measuring automated kernel engineering. Blog post, 2 2025. https://metr.org/blog/2025-02-14-measuring-automated-kernel-engineering/.\n- Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091, 2019.\n- ppenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. https://arxiv.org/abs/2508.10925.\n- Ouyang et al. (2025) Anne Ouyang, Simon Guo, Simran Arora, Alex L Zhang, William Hu, Christopher R√©, and Azalia Mirhoseini. Kernelbench: Can llms write efficient gpu kernels? arXiv preprint arXiv:2502.10517, 2025.\n- Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n- Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf.\n- Press et al. (2025) Ori Press, Brandon Amos, Haoyu Zhao, Yikai Wu, Samuel K Ainsworth, Dominik Krupke, Patrick Kidger, Touqir Sajed, Bartolomeo Stellato, Jisun Park, et al. Algotune: Can language models speed up general-purpose numerical programs? arXiv preprint arXiv:2507.15887, 2025.\n- Saroufim et al. (2025) Mark Saroufim, Jiannan Wang, Bert Maher, Sahan Paliskara, Laura Wang, Shahin Sefati, and Manuel Candales. Backendbench: An evaluation suite for testing how well llms and humans can write pytorch backends, 2025. https://github.com/meta-pytorch/BackendBench.\n- Shah et al. (2024) Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. arXiv preprint arXiv:2407.08608, 2024. https://arxiv.org/abs/2407.08608.\n- Shehabi et al. (2024) Arman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakkar Siddik, Billie Holecek, Jonathan Koomey, Eric Masanet, Dale Sartor, et al. 2024 united states data center energy usage report. 2024.\n- Silvano et al. (2025) Cristina Silvano, Daniele Ielmini, Fabrizio Ferrandi, Leandro Fiorin, Serena Curzel, Luca Benini, Francesco Conti, Angelo Garofalo, Cristian Zambelli, Enrico Calore, et al. A survey on deep learning hardware accelerators for heterogeneous hpc platforms. ACM Computing Surveys, 57(11):1‚Äì39, 2025.\n- Tang et al. (2019a) Chunqiang Tang, Kenny Yu, Kaushik Veeraraghavan, Jonathan Kaldor, Scott Michelson, Thawan Kooburat, Aravind Anbudurai, Matthew Clark, Kabir Gogia, Long Cheng, et al. Twine: A unified cluster management system for shared infrastructure. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 787‚Äì803, 2020.\n- Tillet et al. (2019b) Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: An intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages (MAPL@PLDI), pages 10‚Äì19, 2019a. doi: 10.1145/3315508.3329973. https://dl.acm.org/doi/10.1145/3315508.3329973.\n- Tillet et al. (2019c) Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 10‚Äì19, 2019b.\n- Wen et al. (2019) Jianghui Wang, Vinay Joshi, Saptarshi Majumder, Xu Chao, Bin Ding, Ziqiong Liu, Pratik Prabhanjan Brahma, Dong Li, Zicheng Liu, and Emad Barsoum. Geak: Introducing triton kernel ai agent & evaluation benchmarks. 2025. doi: 10.48550/arXiv.2507.23194. https://arxiv.org/abs/2507.23194.\n- Wei et al. (2025) Anjiang Wei, Tianran Sun, Yogesh Seenichamy, Hang Song, Anne Ouyang, Azalia Mirhoseini, Ke Wang, and Alex Aiken. Astra: A multi-agent system for gpu kernel performance optimization. 2025. doi: 10.48550/arXiv.2509.07506. https://arxiv.org/abs/2509.07506.\n\n## Appendix A Acknowledgments\n\nWe thank the PyTorch Team, in particular the authors of BackendBench for their support. We acknowledge that our work stands on the shoulders of the entire MTIA team and are grateful for the enablement of our research. We also thank Dipal Saluja for his early feedback.\n\n## Appendix B Generated Wrapper and Kernel Pairs\n\nHere we present a few interesting kernel-wrapper pairs for different operators.\n\n### B.1 nn.functional.binary_cross_entropy\n\n‚¨á\n@triton.jit\ndef kernel(\ninput_ptr,\ntarget_ptr,\nweight_ptr,\noutput_ptr,\nn_elements,\nBLOCK_SIZE: tl.constexpr,\n):\npid = tl.program_id(axis=0)\nblock_start = pid * BLOCK_SIZE\noffsets = block_start + tl.arange(0, BLOCK_SIZE)\nmask = offsets < n_elements\ninput = tl.load(input_ptr + offsets, mask=mask, other=0.0)\ntarget = tl.load(target_ptr + offsets, mask=mask, other=0.0)\nweight = tl.load(weight_ptr + offsets, mask=mask, other=1.0)\neps = 1e-8\nloss = -target * tl.log(input + eps) - (1 - target) * tl.log(1 - input + eps)\nloss = loss * weight\ntl.store(output_ptr + offsets, loss, mask=mask)\n\ndef wrapper(input, target, weight=None, reduction=‚Äômean‚Äô):\nif input.shape != target.shape:\nraise RuntimeError(‚Äúinput and target must have the same shape‚Äù)\nif weight is not None:\nif weight.shape != input.shape:\nweight = weight.broadcast_to(input.shape)\nweight = weight.contiguous()\nelse:\nweight = torch.ones_like(input, dtype<input.dtype, device<input.device)\nif reduction not in [‚Äônone‚Äô, ‚Äômean‚Äô, ‚Äôsum‚Äô]:\nraise ValueError(‚Äúreduction must be ‚Äônone‚Äô, ‚Äômean‚Äô, or ‚Äôsum‚Äô‚Äù)\noutput = torch.empty_like(input)\ninput = input.contiguous()\ntarget = target.contiguous()\nn_elements = input.numel()\nif n_elements == 0:\nif reduction == ‚Äônone‚Äô:\nreturn output\nelse:\nreturn torch.tensor(0.0, dtype<input.dtype, device<input.device)\n## BLOCK_SIZE = 1024\ngrid = (triton.cdiv(n_elements, BLOCK_SIZE),)\nkernel[grid]\n{\ninput,\ntarget,\nweight,\noutput,\nn_elements,\n## BLOCK_SIZE=BLOCK_SIZE,\n)\nif reduction == ‚Äônone‚Äô:\nreturn output\nelif reduction == ‚Äôsum‚Äô:\nreturn output.sum()\nelif reduction == ‚Äômean‚Äô:\nreturn output.mean()\n\nB.2 outer\n\n‚¨á\n@triton.jit\ndef kernel(\ninput_ptr,\nvec2_ptr,\noutput_ptr,\nn,\nm,\ninput_stride,\nvec2_stride,\noutput_stride0,\noutput_stride1,\n):\npid = tl.program_id(0)\nif pid >= n:\nreturn\ninput_val = tl.load(input_ptr + pid * input_stride)\nfor j in range(m):\nvec2_val = tl.load(vec2_ptr + j * vec2_stride)\nproduct = input_val * vec2_val\noutput_offset = pid * output_stride0 + j * output_stride1\ntl.store(output_ptr + output_offset, product)\n\ndef wrapper(input, vec2, *, out=None):\nif input.dim() != 1 or vec2.dim() != 1:\nraise ValueError(\"Both input and vec2 must be 1D tensors\")\nn = input.size(0)\nm = vec2.size(0)\noutput_shape = (n, m)\ndtype = input.dtype\nif input.dtype != vec2.dtype:\npass\ndevice = input.device\nif out is None:\noutput = torch.empty(output_shape, dtype=dtype, device=device)\n\nelse:\nif out.shape != output_shape:\nraise RuntimeError(f\"Expected out tensor to have shape (output_shape), but got (out.shape)\")\nif out.dtype != dtype:\nraise RuntimeError(f\"Expected out tensor to have dtype (dtype), but got (out.dtype)\")\nif out.device != device:\nraise RuntimeError(f\"Expected out tensor to be on device (device), but got (out.device)\")\noutput = out\ninput_contig = input.contiguous()\nvec2_contig = vec2.contiguous()\noutput_contig = output.contiguous()\ninput_stride = input_contig.stride(0)\nvec2_stride = vec2_contig.stride(0)\noutput_stride0 = output_contig.stride(0)\noutput_stride1 = output_contig.stride(1)\ngrid = (n,)\nkernel[grid](\ninput_contig,\nvec2_contig,\noutput_contig,\nn,\nm,\ninput_stride,\nvec2_stride,\noutput_stride0,\noutput_stride1,\n)\nreturn output\n\nB.3 nn.functional.layer_norm\n\n‚¨á\n@triton.jit\ndef kernel_mean_var(\ninput_ptr,\nmean_ptr,\nvar_ptr,\n## M,\n## N,\nepsilon,\nBLOCK_SIZE: tl.constexpr,\n):\npid = tl.program_id(0)\nif pid >= N:\nreturn\n\nsum = 0.0\nsum_sq = 0.0\nfor i in range(M):\nx = tl.load(input_ptr + pid * M + i)\nsum += x\nsum_sq += x * x\n\nmean = sum / M\nvar = sum_sq / M - mean * mean + epsilon\n\ntl.store(mean_ptr + pid, mean)\ntl.store(var_ptr + pid, var)\n@triton.jit\ndef kernel_normalize(\ninput_ptr,\noutput_ptr,\nmean_ptr,\nvar_ptr,\nweight_ptr,\nbias_ptr,\n## M,\n## N,\nepsilon,\nelementwise_affine,\nBLOCK_SIZE: tl.constexpr,\n):\npid = tl.program_id(0)\nif pid >= N:\nreturn\n\nmean = tl.load(mean_ptr + pid)\nvar = tl.load(var_ptr + pid)\n\nfor i in range(M):\nx = tl.load(input_ptr + pid * M + i)\nx_float = x.to(tl.float32)\nx_float = x_float - mean\nx_float = x_float / tl.sqrt(var) # Corrected line: removed epsilon here\n\nif elementwise_affine != 0:\nweight = tl.load(weight_ptr + i).to(tl.float32)\nbias = tl.load(bias_ptr + i).to(tl.float32)\nx_float = x_float * weight + bias\n\nx = x_float.to(x.dtype)\ntl.store(output_ptr + pid * M + i, x)\n\ndef wrapper(input, normalized_shape, weight=None, bias=None, eps=1e-5, elementwise_affine=True):\nif not isinstance(input, torch.Tensor):\nraise TypeError(\"Input must be a torch.Tensor\")\nif not input.is_contiguous():\ninput = input.contiguous()\n\nif isinstance(normalized_shape, int):\nnormalized_shape = (normalized_shape,)\nelse:\nnormalized_shape = tuple(normalized_shape)\n## D = len(normalized_shape)\ninput_shape = input.shape\nif D > len(input_shape):\nraise ValueError(\"normalized_shape cannot be larger than input shape\")\n\n## M = 1\nfor dim in normalized_shape:\n## M *= dim\n## N = 1\n\nfor dim in input_shape[:-D]:\n## N *= dim\n\nif N == 0 or M == 0:\nreturn torch.empty_like(input)\n\ninput_float = input.to(torch.float32)\ninput_float = input_float.view(N, M)\nmean = torch.empty(N, dtype=torch.float32, device=input.device)\nvar = torch.empty(N, dtype=torch.float32, device=input.device)\n\n## BLOCK_SIZE = 1\ngrid = (N,)\nkernel_mean_var[grid](\ninput_float,\nmean,\nvar,\n## M,\n## N,\neps,\n## BLOCK_SIZE=BLOCK_SIZE,\n)\n\noutput = torch.empty_like(input)\noutput = output.view(N, M)\n\nif elementwise_affine:\nif weight is None:\nweight = torch.ones(normalized_shape, dtype=input.dtype, device=input.device)\nelse:\nweight = weight.contiguous().view(-1)\nif bias is None:\nbias = torch.zeros(normalized_shape, dtype=input.dtype, device=input.device)\nelse:\nbias = bias.contiguous().view(-1)\nelse:\nweight = torch.ones(M, dtype=input.dtype, device=input.device)\nbias = torch.zeros(M, dtype=input.dtype, device=input.device)\n\ninput_flattened = input.view(N, M)\nkernel_normalize[grid](\ninput_flattened,\noutput,\nmean,\nvar,\nweight,\nbias,\n## M,\n## N,\neps,\n1 if elementwise_affine else 0,\n## BLOCK_SIZE=BLOCK_SIZE,\n)\n\noutput = output.view(input_shape)\nreturn output\n\n## C State Transition Prompts\n\nAs TritorX executes its feedback loop, we have different prompts for different scenarios. The initial prompt provides rules and background information, most critically PyTorch docstring(s) explaining both the operator in question and other referenced PyTorch docs or operators, alongside three fully working MTIA Triton reference kernels for exp, argmax, and diag:\n\n‚¨á\ninit_prompt = f\"\"\"\nWrite me a Triton implementation of the {op_name} ATen operator from PyTorch using MTIA‚Äôs version of Triton. This implementation should support the following input dtypes: {str(dtypes)}.\n\nTo make it easy to use for me please make sure to call your jitted kernel \"kernel\" and the wrapper code that translates the original signature of {op_name} to the kernel‚Äôs signature \"wrapper\".\n\nMultiple kernels are fine if needed, but try to generalize as much as possible and limit the number of kernels. Thoroughly analyze the problem and decide if one kernel is enough or if multiple are needed. If you get stuck trying to put all logic in one kernel, feel free to create multiple. Note that if you have multiple kernels, their names MUST all start with \"kernel\".\n\nDo NOT use or fallback to the base PyTorch or Triton implementations of these kernels - this is considered cheating and is NOT allowed.\n\nDo NOT include any comments or import statements (the required ones will be added while running the kernel - do not worry about import errors).\n\nAdditionally, for MTIA, we add 32-bit padding to input tensors. Take this into account while you are iterating and debugging your kernel.\n\nThink carefully about a correct and efficient implementation and finally output your code in a Python codeblock. Please only output a single module‚Äôs code containing the kernel and wrapper functions.\n\n## I‚Äôll paste the docstr of ATen‚Äôs {op_name} for reference, which defines the spec:\n{docstring}\n{supplemental_docstrings}\n\n\\n\\nFor your reference, I am including a few different types of fully working MTIA Triton implementations of ATen operators.\n\nThese will be listed in triplets, where each example will have \"Operator\", \"Kernel(s)\", and \"Wrapper\". Analyze these to better understand MTIA Triton:\\n\n\n<reference kernels>\n## Please think carefully and output a full implementation of {op_name} in MTIA Triton now!\"\"\"\n\nIf we are starting a new LLM session but have a partially working kernel already, we instead prompt it to debug an existing kernel, and provide the current implementation:\n\n‚¨á\ninit_prompt_with_existing_kernel = f\"\"\"\nDebug a Triton implementation of the {op_name} ATen operator from PyTorch, written in MTIA‚Äôs version of Triton. This implementation should support the following input dtypes: {str(dtypes)}.\n...\n...\nLastly, here is the work-in-progress implementation of this operator. It has a few issues that I need your help debugging, so be sure to analyze it thoroughly:\n<current partial implementation>\n\"\"\"\n\nWe describe the details of linting more in Appendix E, but the prompt itself is fairly straightforward, simply showing the LLM the lint errors.\n\nOur feedback prompts for compilation errors and correctness errors are more nuanced, and consist of three separate prompts. To begin with, part of our approach to optimize context length includes using an LLM-based summarizer for long compilation logs that often contain the same error message multiple times:\n\n‚¨á\nsummarization_prompt = \"\"\"\n...\n...\n## To recap, return the following information ONLY:\n## 1. The EXACT error message\n## 2. The EXACT code snippet that caused the error, both the exact line and the lines before it\n## 3. The EXACT traceback of the error, if present - do NOT include duplicates\n\"\"\"\n\nThis summary is then passed into a simple prompt telling the LLM to evaluate the compilation error and fix the kernel.\n\n## Appendix\n\nWhen the generated kernel fails a test case due to a correctness error, we provide quite a bit of feedback to the LLM based on the test case to use as context while debugging. We‚Äôve found that this does not increase the risk of overfitting to a specific test case whilst increasing the success rate:\n\n‚¨á\ncorrectness_feedback_prompt = \"\"\n...\n**Summary of the CPU output tensor for the input**:\n{str(latest_result.cpu_tensor_results)}\n**Summary of the MTIA output tensor for the input**:\n{str(latest_result.mtia_tensor_results)}\nAdditionally, here is the input and output data for the first failing test case. Do NOT overfit your kernel to this - remember you need to build a generalized kernel implementation. This is here solely to help you debug.\n\n## **INPUT SIGNATURE**:\n{str(input_sig)}\n## **OUTPUT SIGNATURE**:\n{str(output_sig)}\n## **INPUT SHAPE**:\n{str(latest_result.shape)}\n## **INPUT TENSOR**:\n{str(latest_result.input_tensor)}\n## **INPUT ARGS**:\n{str(latest_result.input_args)}\n## **INPUT KWARGS**:\n{str(latest_result.input_kwargs)}\n## Generate the corrected MTIA kernel now:\n\"\"\""
    },
    {
      "id": 9,
      "title": "D Reasoning Trajectories",
      "content": "# D Reasoning Trajectories\n\nYou are an expert in generating Triton MTIA (Meta Training and Inference Accelerator) kernels. Write me a Triton implementation of the nnfunctional.logsigmoid ATen operator from PyTorch using MTIA's version of Triton. This implementation should support the following input styles: ['bfloat16', 'float16', 'float31'].\n\nI'll paste the docstr of ATen's nnfunctional.logsigmoid for reference, which defines the spec:\n\n```txt\nlogsigmoid(input)  $\\Rightarrow$  Tensor\nApplies element-wise :math:'\\text{test(LogSigmoid)(x,i)}  $=$  \\log \\left[\\text{frac(1)(1 + \\exp(-x_i))}\\right]$\nSee :class:'\\torch.nn.logSigmoid' for more details.\nFor your reference, I am including a few different types of fully working MTIA Triton implementations of ATen operators.\n```\n\nOkay, I need to implement the logsigmoid function using MTIA Triton. Let's start by understanding the problem. The logsigmoid function applies the element-wise operation LogSigmoid(x_i) = log(1 / (1 + exp(-x_i))). This is equivalent to -softplus(-x_i), where softplus is the softplus activation function.\n\nFirst, I should consider the input styles: bfloat16, float16, and float32. MTIA Triton might handle these differently, but I'll assume that the kernel can be written in a way that works for all three types, possibly using type specialization or generic operations.\n\n```python\n@triton.jit\ndef kernel(\ninput_ptr,\noutput_ptr,\nn_elements,\nBLOCK_SIZE: tl.constexpr,\n):\npid = tl.program_id(@)\nblock_start = pid + BLOCK_SIZE\noffsets = block_start + tl.arange(@, BLOCK_SIZE)\nmask = offsets &lt; n_elements\nx = tl.load(input_ptr + offsets, mask = mask)\nexp_neg_x = tl.exp(-x)\nlogsigmoid = -tl.log(p(exp_neg_x)\ntl.store(output_ptr + offsets, logsigmoid, mask = mask)\ndef wrapper(input):\noutput = torch.empty_like(input, device = input_device)\nif input.numel() == @:\nreturn output\ninput_contig = input.contiguous().view(-1)\noutput_contig = torch.empty_like(input_contig, device = input_device)\nn_elements = input.numel()\n## BLOCK_SIZE = 120\ngrid = (triton.cdiv(n_elements, BLOCK_SIZE),)\nkernel(grid)\ninput_contig,\noutput_contig,\nn_elements,\n## BLOCK_SIZE = BLOCK_SIZE,\n)\nreturn output_contig.view(input.shape)\n```\n\nYour previous MTIA kernel implementation failed to pass the linter. Please analyze the lint error and provide a corrected version. Remember to adhere to the original output guidelines. ++Lint Error++:\n\n## Found 1 linting violation(s):\n\n[module Restrictions] forbidden tl module usage: tl.log(p (line 15)\n\nDetails: Allowed tl functions: tl.abs, tl.add, tl.advence, ...\n\n## Generate the corrected MTIA kernel now\n\nOkay, the linter is complaining about the use of tl.log(p in the kernel. Let me check the allowed tl functions. The error message lists the allowed functions, and tl.log(p is not among them.\n\n¬∑¬∑\n\nYour MTIA kernel implementation failed to compile. Please analyze the compilation error and provide a corrected version. Remember to adhere to the original output guidelines.\n\n++Compilation Error++:\n\n## ValueError: Expected dtype ['fp32', 'fp64'] but got fp16\n\n## Generate the corrected MTIA kernel now:\n\n¬∑¬∑\n\n```python\ndef kernel(\ninput_ptr,\noutput_ptr,\nn_elements,\nBLOCK_SIZE: tl.constexpr,\n):\npid = tl.program_id(@)\nblock_start = pid + BLOCK_SIZE\noffsets = block_start + tl.arange(@, BLOCK_SIZE)\nmask = offsets &lt; n_elements\nx = tl.load(input_ptr + offsets, mask = mask)\nx_float32 = tl.cast(s, tl.float32)\nexp_neg_x = tl.exp(-x_float32)\nlogsigmoid_float32 = -tl.log(1 + exp_neg_x)\nlogsigmoid = tl.cast(logsigmoid_float32, x.dtype)\ntl.store(output_ptr + offsets, logsigmoid, mask = mask)\ndef wrapper(input):\n```\n\nFigure D.5 Example of TritorX writing a kernel-wrapper pair for the nnfunctional.logsigmoid operator in 3 LLM calls after receiving the linter and compiler feedback. The reasoning steps are shortened and some details are omitted due to space constraints. Initial prompt in green, LLM reasoning in blue, LLM kernel-wrapper generation in purple, and TritorX feedback in orange.\n\n![img-4.jpeg](img-4.jpeg)\nFigure D.6 Example of TritorX writing a kernel-wrapper pair for the nnfunctional channel-shuffle operator in 14 LLM calls. The LLM has to incorporate the feedback from the scatter-store compilation error (hardware restriction) and the discrepancy between the MTIA test output compared to the CPU implementation, as well as address other compilation errors and hardware crashes.\n\n## E Triton MTIA Linter\n\nThe linter is a rule-based, static analysis tool that combines Python‚Äôs AST parsing with regex matching per rule. Several of these rules are formatting restrictions to make the output compatible with our execution system, such as disallowing import statements and enforcing a specific output format.\n\nOther rules are more concerned with what modules are being used and where. We have allowlists on several key modules, such as tl (triton.language), and torch. Anything from one of these modules not found in such an allowlist is considered a linter violation:\n\n‚¨á\n1module_restrictions:\n2 modules:\n3 - module_name: \"tl\"\n4 allowed_functions:\n5 - \"tl.load\"\n6 - \"tl.store\"\n7 - \"tl.arange\"\n8 # ... 200+ allowed Triton MTIA operations\n9 - module_name: \"torch\"\n10 allowed_functions:\n11 - \"torch.empty\"\n12 - \"torch.zeros\"\n13 # ... tensor allocation/reshaping only\n\nThis highlights exactly which aspects of upstream Triton are compatible with MTIA.\n\nAdditionally, we have scope restrictions on modules, such as allowing tl only in the kernel, not in the wrapper:\n\n‚¨á\n1module_scope_restrictions:\n2 restrictions:\n3 - module: \"tl\"\n4 allowed_scope_patterns: [\"^kernel.*\"] # tl.* only inside kernel functions\n\nAs part of our anti-cheating efforts, we enforce several rules. To prevent moving tensors back to CPU or attempting to move tensors to CUDA:\n\n‚¨á\n1forbidden_tensor_methods:\n2 enabled: true\n3 description: \"Prohibit tensor methods that move data between devices (CPU/CUDA transfers)\"\n4 forbidden_methods:\n5 - \"cpu\" # tensor.cpu() - moves tensor to CPU\n6 - \"cuda\" # tensor.cuda() - moves tensor to CUDA\n7\n8forbidden_function_arguments:\n9 enabled: true\n10 description: \"Prohibit specific argument values in function calls\"\n11 restrictions:\n12 # Forbid explicit CPU/CUDA device specifications in torch.device()\n13 - function: \"torch.device\"\n14 forbidden_string_args:\n15 - \"cpu\"\n16 - \"cuda\"\n\n## And to prevent workarounds such as eval or exec:\n\n‚¨á\n1# Rule to ban dangerous built-in functions that enable dynamic code execution\n\n```txt\nforbidden-functions:\nenabled: true\ndescription: \"Prohibit built-in functions that enable dynamic code execution\"\nforbidden-functions:\n- \"eval\" # Evaluates Python expressions from strings\n- \"exec\" # Executes Python code from strings\n- \"compile\" # Compiles Python code from strings (used with exec)"
    }
  ],
  "pdf_images": {
    "pdf_path": "inbox/2512.10977v1.pdf",
    "total_pages": 21,
    "pages": [
      {
        "page_number": 1,
        "filename": "page-001.png",
        "thumb_filename": "page-001-thumb.png",
        "mobile_filename": "page-001-mobile.png",
        "mobile_thumb_filename": "page-001-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 2,
        "filename": "page-002.png",
        "thumb_filename": "page-002-thumb.png",
        "mobile_filename": "page-002-mobile.png",
        "mobile_thumb_filename": "page-002-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 3,
        "filename": "page-003.png",
        "thumb_filename": "page-003-thumb.png",
        "mobile_filename": "page-003-mobile.png",
        "mobile_thumb_filename": "page-003-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 4,
        "filename": "page-004.png",
        "thumb_filename": "page-004-thumb.png",
        "mobile_filename": "page-004-mobile.png",
        "mobile_thumb_filename": "page-004-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 5,
        "filename": "page-005.png",
        "thumb_filename": "page-005-thumb.png",
        "mobile_filename": "page-005-mobile.png",
        "mobile_thumb_filename": "page-005-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 6,
        "filename": "page-006.png",
        "thumb_filename": "page-006-thumb.png",
        "mobile_filename": "page-006-mobile.png",
        "mobile_thumb_filename": "page-006-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 7,
        "filename": "page-007.png",
        "thumb_filename": "page-007-thumb.png",
        "mobile_filename": "page-007-mobile.png",
        "mobile_thumb_filename": "page-007-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 8,
        "filename": "page-008.png",
        "thumb_filename": "page-008-thumb.png",
        "mobile_filename": "page-008-mobile.png",
        "mobile_thumb_filename": "page-008-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 9,
        "filename": "page-009.png",
        "thumb_filename": "page-009-thumb.png",
        "mobile_filename": "page-009-mobile.png",
        "mobile_thumb_filename": "page-009-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 10,
        "filename": "page-010.png",
        "thumb_filename": "page-010-thumb.png",
        "mobile_filename": "page-010-mobile.png",
        "mobile_thumb_filename": "page-010-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 11,
        "filename": "page-011.png",
        "thumb_filename": "page-011-thumb.png",
        "mobile_filename": "page-011-mobile.png",
        "mobile_thumb_filename": "page-011-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 12,
        "filename": "page-012.png",
        "thumb_filename": "page-012-thumb.png",
        "mobile_filename": "page-012-mobile.png",
        "mobile_thumb_filename": "page-012-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 13,
        "filename": "page-013.png",
        "thumb_filename": "page-013-thumb.png",
        "mobile_filename": "page-013-mobile.png",
        "mobile_thumb_filename": "page-013-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 14,
        "filename": "page-014.png",
        "thumb_filename": "page-014-thumb.png",
        "mobile_filename": "page-014-mobile.png",
        "mobile_thumb_filename": "page-014-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 15,
        "filename": "page-015.png",
        "thumb_filename": "page-015-thumb.png",
        "mobile_filename": "page-015-mobile.png",
        "mobile_thumb_filename": "page-015-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 16,
        "filename": "page-016.png",
        "thumb_filename": "page-016-thumb.png",
        "mobile_filename": "page-016-mobile.png",
        "mobile_thumb_filename": "page-016-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 17,
        "filename": "page-017.png",
        "thumb_filename": "page-017-thumb.png",
        "mobile_filename": "page-017-mobile.png",
        "mobile_thumb_filename": "page-017-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 18,
        "filename": "page-018.png",
        "thumb_filename": "page-018-thumb.png",
        "mobile_filename": "page-018-mobile.png",
        "mobile_thumb_filename": "page-018-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 19,
        "filename": "page-019.png",
        "thumb_filename": "page-019-thumb.png",
        "mobile_filename": "page-019-mobile.png",
        "mobile_thumb_filename": "page-019-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 20,
        "filename": "page-020.png",
        "thumb_filename": "page-020-thumb.png",
        "mobile_filename": "page-020-mobile.png",
        "mobile_thumb_filename": "page-020-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 21,
        "filename": "page-021.png",
        "thumb_filename": "page-021-thumb.png",
        "mobile_filename": "page-021-mobile.png",
        "mobile_thumb_filename": "page-021-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      }
    ]
  },
  "metadata": {
    "model": "gpt-4o",
    "pdf_path": "inbox/2512.10977v1.pdf"
  },
  "attribution": {
    "type": "arxiv",
    "url": "https://arxiv.org/abs/2512.10977",
    "id": "2512.10977"
  }
}