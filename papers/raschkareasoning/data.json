{
  "title": "The State of Reinforcement Learning for LLM Reasoning",
  "slug": "raschkareasoning",
  "questions": {
    "1": {
      "question": "What were they trying to do?",
      "answer": "The main goal of the paper \"The State of Reinforcement Learning for LLM Reasoning\" is to explore and analyze the use of reinforcement learning (RL) methods to improve reasoning capabilities in large language models (LLMs). The paper highlights the limitations of traditional scaling approaches and emphasizes the potential of RL, particularly reinforcement learning with verifiable rewards (RLVR), to enhance reasoning tasks by providing structured intermediate steps and improving model accuracy on complex tasks. The big picture objective is to advance the development of reasoning models that can perform better on challenging tasks through strategic investment in RL techniques.",
      "timestamp": "2025-08-18 15:39:56"
    },
    "2": {
      "question": "Why does it matter?",
      "answer": "The paper matters because it highlights the limitations of scaling model size and data alone in improving large language models (LLMs) and emphasizes the potential of reinforcement learning (RL) methods tailored for reasoning tasks to enhance model accuracy and problem-solving capabilities. This is important for researchers and developers in AI, as it suggests a shift towards reasoning-focused post-training as a standard practice, which could lead to more efficient and capable AI systems. Ultimately, this could impact various fields that rely on AI for complex problem-solving, such as medicine, economics, and education, by enabling more sophisticated and reliable AI applications.",
      "timestamp": "2025-08-18 15:40:05"
    },
    "3": {
      "question": "What did they try?",
      "answer": "The paper discusses the use of reinforcement learning with verifiable rewards (RLVR) and Group Relative Policy Optimization (GRPO) to enhance reasoning capabilities in language models. The DeepSeek-R1 team introduced GRPO as a variant of Proximal Policy Optimization (PPO) that improves computational efficiency by eliminating the critic model and using relative quality of multiple sampled answers from the policy model to compute advantages. Additionally, RLVR replaces the need for a reward model by using deterministic tools like calculators to provide direct binary feedback on the correctness of answers, thus improving efficiency and accuracy in reasoning tasks.",
      "timestamp": "2025-08-18 15:40:11"
    },
    "4": {
      "question": "Did it work?",
      "answer": "Yes, the idea of using reinforcement learning tailored for reasoning tasks, as demonstrated by OpenAI's o3 reasoning model, showed significant improvements. The o3 model, which used 10 times more training compute than its predecessor o1, exhibited enhanced accuracy and problem-solving capabilities on challenging tasks. Additionally, the DeepSeek-R1 model, trained using reinforcement learning with verifiable rewards (RLVR) and Group Relative Policy Optimization (GRPO), demonstrated that reasoning abilities could be effectively improved without relying on human-labeled reward models.",
      "timestamp": "2025-08-18 15:40:17"
    },
    "5": {
      "question": "What did they compare it to?",
      "answer": "The paper compares the new reinforcement learning methods for reasoning models, specifically the o3 reasoning model by OpenAI, to previous models like GPT-4.5 and Llama 4, which were not explicitly trained for reasoning. It highlights that the o3 model uses reinforcement learning tailored for reasoning tasks, which is a significant improvement over the conventional models that rely on scaling model size and data alone. The baselines include models like DeepSeek-R1, which uses GRPO and RLVR, showing that these methods can enhance reasoning capabilities beyond traditional RLHF approaches.",
      "timestamp": "2025-08-18 15:40:22"
    },
    "6": {
      "question": "What was it tested on?",
      "answer": "The paper discusses the evaluation of reasoning models, particularly focusing on the DeepSeek-R1 models, which were tested using reinforcement learning with verifiable rewards (RLVR) and Group Relative Policy Optimization (GRPO). The evaluation involved training models on tasks like mathematical reasoning and logic puzzles, using tools like calculators and compilers for binary feedback on correctness. The setup demonstrated that models could improve reasoning capabilities without relying on human-labeled reward models, and it highlighted the effectiveness of rule-based rewards in enhancing model performance on reasoning tasks.",
      "timestamp": "2025-08-18 15:40:26"
    },
    "7": {
      "question": "What's cool about it?",
      "answer": "What's cool and novel about the paper is its exploration of reinforcement learning with verifiable rewards (RLVR) as a method to enhance reasoning capabilities in language models. This approach bypasses the need for human-labeled reward models by using deterministic tools like calculators or compilers to provide direct binary feedback, making the training process more efficient and less prone to noise. Additionally, the introduction of Group Relative Policy Optimization (GRPO) as an alternative to traditional PPO is clever, as it simplifies the training process by eliminating the need for a critic model, thus optimizing memory usage and computational efficiency. These innovations highlight a shift towards more efficient and scalable methods for improving reasoning in LLMs.",
      "timestamp": "2025-08-18 15:40:31"
    },
    "8": {
      "question": "What's sketchy about it?",
      "answer": "The paper raises several concerns and limitations. Firstly, it highlights the potential overstatement of reinforcement learning (RL) benefits, as some reported improvements might be noise rather than statistically significant gains. Additionally, the paper notes biases in GRPO, such as response-length and difficulty-level biases, which can lead to inefficient training and unnecessary long answers. Furthermore, the reliance on verifiable rewards and the absence of a comprehensive analysis for GRPO methods suggest areas where further research and evaluation are needed to fully understand the efficacy and generalization of these approaches.",
      "timestamp": "2025-08-18 15:40:35"
    },
    "9": {
      "question": "Can anyone use this?",
      "answer": "The methods discussed in the paper, particularly reinforcement learning with verifiable rewards (RLVR) and the use of algorithms like GRPO, are complex and likely require significant expertise in machine learning and access to computational resources. While these methods improve reasoning capabilities in language models, they are not easily accessible to the general public due to their complexity and the computational expense involved. They are more practical for research institutions or companies with the necessary resources and expertise.",
      "timestamp": "2025-08-18 15:40:41"
    },
    "10": {
      "question": "What's still left to figure out?",
      "answer": "The paper highlights several areas that remain unsolved or unclear in the field of reinforcement learning for LLM reasoning. It points out the need for better understanding of how reasoning capabilities emerge, whether from RL methods or pre-training on chain-of-thought data. Additionally, the paper suggests further exploration into the integration of external tools and retrieval-augmented generation to enhance reasoning models. It also raises questions about the statistical significance of RL improvements in distilled models and calls for more standardized evaluation methods to accurately assess RL's impact on reasoning capabilities.",
      "timestamp": "2025-08-18 15:40:45"
    }
  },
  "markdown_pages": [
    {
      "id": 1,
      "title": "The State of Reinforcement Learning for LLM Reasoning",
      "content": "# The State of Reinforcement Learning for LLM Reasoning\n\n## Understanding GRPO and New Insights from Reasoning Model Papers\n\n## SEBASTIAN RASCHKA, PHD\n## APR 19, 2025\n\nA lot has happened this month, especially with the releases of new flagship models like GPT-4.5 and Llama 4. But you might have noticed that reactions to these releases were relatively muted. Why? One reason could be that GPT-4.5 and Llama 4 remain conventional models, which means they were trained without explicit reinforcement learning for reasoning.\n\nMeanwhile, competitors such as xAI and Anthropic have added more reasoning capabilities and features into their models. For instance, both the xAI Grok and Anthropic Claude interfaces now include a \"thinking\" (or \"extended thinking\") button for certain models that explicitly toggles reasoning capabilities.\n\nIn any case, the muted response to GPT-4.5 and Llama 4 (non-reasoning) models suggests we are approaching the limits of what scaling model size and data alone can achieve.\n\nHowever, OpenAI's recent release of the o3 reasoning model demonstrates there is still considerable room for improvement when investing compute strategically, specifically via reinforcement learning methods tailored for reasoning tasks. (According to OpenAI staff during the recent livestream, o3 used 10Ã— more training compute compared to o1.)\n\n![img-0.jpeg](img-0.jpeg)\n\nSource: OpenAI livestream (https://openai.com/live/) on April 16, 2025\n\nWhile reasoning alone isn't a silver bullet, it reliably improves model accuracy and problem-solving capabilities on challenging tasks (so far). And I expect reasoningfocused post-training to become standard practice in future LLM pipelines.\n\nSo, in this article, let's explore the latest developments in reasoning via reinforcement learning."
    },
    {
      "id": 2,
      "title": "Discover more from Ahead of AI",
      "content": "# Discover more from Ahead of AI\n\nAhead of AI specializes in Machine Learning \\& AI research and is read by tens of thousands of research and practitioners who want to stay ahead in the eve\n\nEnter your email...\n![img-1.jpeg](img-1.jpeg)\n\nThis article focuses on reinforcement learning training methods used to develop and improve reasoning models\n\nBecause it is a relatively long article, I am providing a Table of Contents overview below. To navigate the table of contents, please use the slider on the left-hand side in the web view.\n\n- Understanding reasoning models\n- RLHF basics: where it all started\n- A brief introduction to PPO: RL's workhorse algorithm\n- RL algorithms: from PPO to GRPO\n- RL reward modeling: from RLHF to RLVR\n- How the DeepSeek-R1 reasoning models were trained\n- Lessons from recent RL papers on training reasoning models\n- Noteworthy research papers on training reasoning models\n\nfrom recent reasoning research papers."
    },
    {
      "id": 3,
      "title": "Understanding reasoning models",
      "content": "# Understanding reasoning models\n\nThe big elephant in the room is, of course, the definition of reasoning. In short, reasoning is about inference and training techniques that make LLMs better at handling complex tasks.\n\nTo provide a bit more detail on how this is achieved (so far), I'd like to define reasoning as follows:\n\nReasoning, in the context of LLMs, refers to the model's ability to produce intermediate steps before providing a final answer. This is a process that is often described as chain-of-thought (CoT) reasoning. In CoT reasoning, the LLM explicitly generates a structured sequence of statements or computations that illustrate how it arrives at its conclusion.\n\nAnd below is a figure along with the definition.\n\nA simplified illustration of how an LLM might tackle a multi-step reasoning task. Rather than just recalling a fact, the model needs to combine several intermediate reasoning steps to arrive at the correct conclusion. The intermediate reasoning steps may or may not be shown to the user, depending on the implementation.\n\nIf you are new to reasoning models and would like a more comprehensive introduction, I recommend my previous articles:"
    },
    {
      "id": 4,
      "title": "First Look at Reasoning From Scratch: Chapter 1",
      "content": "# First Look at Reasoning From Scratch: Chapter 1\n\n![img-2.jpeg](img-2.jpeg)\n\n## Understanding Reasoning LLMs\n\n## SEBASTIAN RASCHKA, PHD $\\cdot$ FEB 5\n## Read full story $\\rightarrow$\n\nNow, as hinted at the beginning of this section, the reasoning abilities of LLMs can be improved in two ways, as nicely illustrated in a figure from an OpenAI blog post:\n\nAccuracy improvements can be achieved through increased training or test-time compute, where test-time compute is synonymous with inference-time compute and inference-time scaling. Source: Annotated figure from https://openai.com/index/learning-to-reason-with-llms/\n\n## In my previous article:"
    },
    {
      "id": 5,
      "title": "The State of LLM Reasoning Model Inference",
      "content": "# The State of LLM Reasoning Model Inference\n\n## SEBASTIAN RASCHKA, PHD $\\cdot$ MAR 8\n## Read full story $\\rightarrow$\n\nI solely focused on the test-time compute methods. In this article, I finally want to take a closer look at the training methods.\n\n## RLHF basics: where it all started\n\nThe reinforcement learning (RL) training methods used to build and improve reasoning models are more or less related to the reinforcement learning with human feedback (RLHF) methodology that is used to develop and align conventional LLMs. So, I want to start with a small recap of how RLHF works before discussing reasoning-specific modification based on RL-based training.\n\n## 2. Supervised fine-tuning\n## 3. Alignment (typically via RLHF)\n\nThe \"original\" LLM alignment method is RLHF, which is part of the standard repertoire when developing LLMs following the InstructGPT paper, which described the recipe that was used to develop the first ChatGPT model.\n\nThe original goal of RLHF is to align LLMs with human preferences. For instance, suppose you use an LLM multiple times where the LLM generates multiple answers for a given prompt. RLHF guides the LLM towards generating more of the style of answer that you prefer. (Often, RLHF is also used to safety-tune LLMs: to avoid sharing sensitive information, using swear words, and so on.)\n\nIf you are new to RLHF, here is an excerpt from a talk I gave a few years ago that explains RLHF in less than 5 minutes:\n\n## Reinforcement Learning with Human Feedback (RLHF) in 4 minutes\n\nAlternatively, the paragraphs below describe RLHF in text form.\n\nThen, RLHF further aligns the LLM using an algorithm called proximal policy optimization (PPO). (Note that there are other algorithms that can be used instead of PPO; I was specifically saying PPO because that's what was originally used in RLHF and is still the most popular one today.)\n\n## For simplicity, we will look at the RLHF pipeline in three separate steps:\n\n- RLHF Step 1 (prerequisite): Supervised fine-tuning (SFT) of the pre-trained model\n- RLHF Step 2: Creating a reward model\n- RLHF Step 3: Fine-tuning via proximal policy optimization (PPO)\n\nRLHF Step 1, shown below, is a supervised fine-tuning step to create the base model for further RLHF fine-tuning.\n\nIn RLHF step 1, we create or sample prompts (from a database, for example) and ask humans to write good-quality responses. We then use this dataset to finetune the pre-trained base model in a supervised fashion. As mentioned before, this is not technically part of RL training but merely a prerequisite.\n\nAs depicted in the figure above, for each prompt, we generate four responses from the fine-tuned LLM created in the prior step. Human annotators then rank these responses based on their preferences. Although this ranking process is time-consuming, it might be somewhat less labor-intensive than creating the dataset for supervised fine-tuning. This is because ranking responses is likely simpler than writing them.\n\nUpon compiling a dataset with these rankings, we can design a reward model that outputs a reward score for the optimization subsequent stage in RLHF Step 3. The idea here is that the reward model replaces and automates the laborintensive human ranking to make the training feasible on large datasets.\n\nThis reward model (RM) generally originates from the LLM created in the prior supervised fine-tuning (SFT) step. To turn the model from RLHF Step 1 into a reward model, its output layer (the next-token classification layer) is substituted with a regression layer, which features a single output node.\n\nThe third step in the RLHF pipeline is to use the reward model (RM) to fine-tune the previous model from supervised fine-tuning (SFT), which is illustrated in the\n\nIn RLHF Step 3, the final stage, we are now updating the SFT model using proximal policy optimization (PPO) based on the reward scores from the reward model we created in RLHF Step 2.\n\nAhead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\n\nType your email..."
    },
    {
      "id": 6,
      "title": "A brief introduction to PPO: RL's workhorse algorithm",
      "content": "# A brief introduction to PPO: RL's workhorse algorithm\n\nAs mentioned earlier, the original RLHF method uses a reinforcement learning algorithm called proximal policy optimization (PPO).\n\nPPO was developed to improve the stability and efficiency of training a policy. (In reinforcement learning, \"policy\" just means the model we want to train; in this case, policy = LLM.)\n\ntraining.\n\nOn top of that, PPO also includes a KL divergence penalty in the loss. This term compares the current policy (the model being trained) to the original SFT model. This encourages the updates to stay reasonably close. The idea is to preferencetune the model, not to completely re-train, after all.\n\nThis is where the \"proximal\" in proximal policy optimization comes from: the algorithm tries to keep the updates close to the existing model while still allowing for improvement. And to encourage a bit of exploration, PPO also adds an entropy bonus, which this encourages the model to vary the outputs during training.\n\nIn the following paragraphs, I want to introduce some more terminology to illustrate PPO on a relatively high level. Still, there's a lot of jargon involved, so I tried to summarize the key terminology in the figure below before we continue.\n\nIllustration of the key terms in RLHF. For instance, several models are involved in PPO, where PPO is an algorithm used in RLHF (and RLHF is one of the most popular LLM alignment methods).\n\nBelow, I aim to illustrate the key steps in PPO via pseudo-code.\nIn addition, to make it more intuitive, I will also use an analogy: Imagine you are a chef running a small food delivery service. And you are constantly trying out new recipe variations to improve customer satisfaction. Your overall goal is to tweak your recipe (policy) based on customer feedback (reward).\n\n## 1. Compute the ratio of the next-token probabilities from the new vs the old policy:\n```\nratio = new_policy_prob / old_policy_prob\n```\n\nyet. We are using the current version of the policy (i.e., the model we are in the middle of training). However, it's a convention to call it \"new\". So, even though you're still experimenting, we call your current draft the \"new policy\" as per convention."
    },
    {
      "id": 7,
      "title": "2. Multiply that ratio by how good the action was (called the advantage):",
      "content": "# 2. Multiply that ratio by how good the action was (called the advantage):\n\n```\nraw_score = ratio * advantage\n```\n\n## Here, for simplicity, we may assume the advantage is computed based on the reward signal:\nadvantage = actual_reward - expected_reward\n\n## In the chef analogy, we can think of the advantage as how well the new dish performed:\nadvantage = customer_rating - expected_rating\n\nFor example, if a customer rates the new dish with a 9/10, and the customers normally give us a $7 / 10$, that's a +2 advantage.\n\nNote that this is a simplification. In reality, this involves generalized advantage estimation (GAE), which I am omitting here so as not to bloat the article further. However, one important detail to mention is that the expected reward is computed by a so-called \"critic\" (sometimes also called \"value model\"), and a reward model computes the actual reward. I.e., the advantage computation involves 2 other models, typically the same size as the original model we are finetuning.\n\nmodel is the actual customer then who gives the feedback (i.e., the actual reward)."
    },
    {
      "id": 8,
      "title": "3. Compute a clipped score:",
      "content": "# 3. Compute a clipped score:\n\nIf the new policy changes too much (e.g., ratio > 1.2 or $<0.8$ ), we clip the ratio, as follows:\n\n```\nclipped_ratio = clamp(ratio, 0.8, 1.2)\nclipped_score = clipped_ratio * advantage\n```\n\nIn the analogy, imagine that the new recipe got an exceptionally great (or bad) review. We might be tempted to overhaul the entire menu now. But that's risky. So, instead, we clip how much our recipe can change for now. (For instance, maybe we made the dish much spicier, and that one customer happened to love spicy food, but that doesn't mean everyone else will.)\n\n## 4. Then we use the smaller of the raw score and clipped score:\n\n```\nif advantage >= 0:\nfinal_score = min(raw_score, clipped_score)\nelse:\nfinal_score = max(raw_score, clipped_score)\n```\n\nAgain, this is related to being a bit cautious. For instance, if the advantage is positive (the new behavior is better), we cap the reward. That's because we don't want to over-trust a good result that might be a coincidence or luck.\n\nIf the advantage is negative (the new behavior is worse), we limit the penalty. The idea here is similar. Namely, we don't want to overreact to one bad result unless we are really sure.\n\nIn the analogy, this ensures that if a recipe is doing better than expected, we don't over-reward it unless we are confident. And if it's underperforming, we don't over-penalize it unless it's consistently bad."
    },
    {
      "id": 9,
      "title": "5. Calculating the loss:",
      "content": "# 5. Calculating the loss:\n\nThis final score is what we maximize during training (using gradient descent after flipping the sign of the score to minimize). In addition, we also add a KL penalty term, where $\\beta$ is a hyperparameter for the penalty strength:\nloss = -final_score $+\\beta$ * KL(new_policy || reference_policy)\n\nIn the analogy, we add the penalty to ensure new recipes are not too different from our original style. This prevents you from \"reinventing the kitchen\" every week. For example, we don't want to turn an Italian restaurant into a BBQ place all of a sudden.\n\nThis was a lot of information, so I summarized it with a concrete, numeric example in an LLM context via the figure below. But please feel free to skip it if it's too complicated; you should be able to follow the rest of the article just fine.\n\n## # AHEAD OF AI\n\n##\n\nThat being said, the main takeaways that will be relevant in the next section are that there are multiple models involved in PPO:\n\n## 1. The policy, which is the LLM that has been trained with SFT and that we want to further align).\n2. The reward model, which is a model that has been trained to predict the reward (see RLHF step 2).\n## 3. The critic, which is a trainable model that estimates the reward.\n4. A reference model (original policy) that we use to make sure that the policy doesn't deviate too much.\n\nBy the way, you might wonder why we need both a reward model and a critic model. The reward model is usually trained before training the policy with PPO. It's to automate the preference labeling by human judges, and it gives the score for the complete responses generated by the policy LLM.\n\nThe critic, in contrast, judges partial responses. We use it to create the final response. While the reward model typically remains frozen, the critic model is updated during training to estimate the reward created by the reward model better.\n\nMore details about PPO are out of the scope of this article, but interested readers can find the mathematical details in these four papers that predate the InstructGPT paper:\n(1) Asynchronous Methods for Deep Reinforcement Learning (2016) by Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, and Kavukcuoglu introduces policy gradient methods as an alternative to Q-learning in deep learning-based RL.\n(2) Proximal Policy Optimization Algorithms (2017) by Schulman, Wolski, Dhariwal, Radford, and Klimov presents a modified proximal policy-based reinforcement\n\n(3) Fine-Tuning Language Models from Human Preferences (2020) by Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, Irving illustrates the concept of PPO and reward learning to pretrained language models including KL regularization to prevent the policy from diverging too far from natural language.\n(4) Learning to Summarize from Human Feedback (2022) by Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, Christiano introduces the popular RLHF three-step procedure that was later also used in the InstructGPT paper."
    },
    {
      "id": 10,
      "title": "RL algorithms: from PPO to GRPO",
      "content": "# RL algorithms: from PPO to GRPO\n\nAs mentioned before, PPO was the original algorithm used in RLHF. From a technical standpoint, it works perfectly fine in the RL pipeline that's being used to develop reasoning models. However, what DeepSeek-R1 used for their RL pipeline is an algorithm called Group Relative Policy Optimization (GRPO), which was introduced in one of their earlier papers:\n\n- DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (2024)\n\n## The DeepSeek team introduced GRPO as\na variant of Proximal Policy Optimization (PPO) that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.\n\nSo, the key motivation here is to improve computational efficiency.\nThe efficiency improvements are achieved by dropping the \"critic\" (value model), i.e., the LLM that computes the value function (i.e., the expected future reward).\n\nInstead of relying on this additional model to compute the estimated reward to compute the advantages, GRPO takes a simpler approach: it samples multiple answers from the policy model itself and uses their relative quality to compute the advantages.\n\nAnnotated figure from DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (https://arxiv.org/abs/2402.03300) to illustrate the differences between PPO and GRPO.\n\nAhead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\n\nType your email...\n## Subscribe"
    },
    {
      "id": 11,
      "title": "RL reward modeling: from RLHF to RLVR",
      "content": "# RL reward modeling: from RLHF to RLVR\n\nSo far, we looked at RLHF as a procedure, and we have introduced two reinforcement learning algorithms commonly used for it: PPO and GRPO.\n\nBut if RLHF is already a core part of the LLM alignment toolkit, what does any of this have to do with reasoning?\n\nThe difference is that instead of relying on human preferences and training a reward model, the DeepSeek-R1 team used verifiable rewards. This approach is called reinforcement learning with verifiable rewards (RLVR).\n\nAgain, it's worth emphasizing: In contrast to standard RLHF, RLVR bypasses the need for a reward model.\n\nSo, rather than learning what counts as a \"good\" answer from human-labeled examples, the model gets direct binary feedback (correct or wrong) from a deterministic tool, such as symbolic verifiers or rule-based tools. Think calculators for math problems or compilers for code generation.\n\nExample of reinforcement learning with verifiable rewards (RLVR). The model is prompted to solve a math problem and produces an answer. Instead of using a learned reward model, a symbolic verifier (e.g., a calculator) checks the output and provides binary feedback based on correctness.\n\nOne motivation here is to avoid noisy or expensive human or learned rewards by using automatic correctness checks as supervision signals during RL. The other motivation is that by using \"cheap\" tools like calculators, we can replace the expensive reward model training and the reward model itself. Since the reward model is usually the whole pre-trained model (but with a regression head), RLVR is much more efficient.\n\nComparison of reinforcement learning setups in LLM training. Traditional RLHF with PPO uses both a reward model (trained on human preferences) and a critic (value model) to guide learning. GRPO eliminates the critic model. RLVR with GRPO goes a step further by also removing the reward model, relying instead on verifiable rewards from symbolic tools like calculators or compilers.\n\nIn the next section, I want to briefly go over the DeepSeek-R1 pipeline and discuss the different verifiable rewards that the DeepSeek team used."
    },
    {
      "id": 12,
      "title": "How the DeepSeek-R1 reasoning models were trained",
      "content": "# How the DeepSeek-R1 reasoning models were trained\n\nNow that we have clarified what RLHF and RLVR are, as well as PPO and GRPO, let's briefly recap the main insights from the DeepSeek-R1 paper in the context of RL and reasoning.\n\n## First, there were three types of models:\n\n## 1. DeepSeek-R1-Zero trained with pure RL\n\nI created a DeepSeek-R1 pipeline diagram to illustrate how these models relate to each other, as shown below."
    },
    {
      "id": 13,
      "title": "Training pipeline for the DeepSeek-R1 family",
      "content": "# Training pipeline for the DeepSeek-R1 family\n\nDeepSeek-R1-Zero was trained using the verifiable rewards (RLVR) with GRPO, and this turned out to be sufficient for the model to exhibit reasoning abilities via intermediate-step generation. This showed that it's possible to skip the SFT stage. The model improves its reasoning abilities through exploration instead of learning from examples.\n\nDeepSeek-R1 is the flagship model, the one with the best performance. The difference compared to DeepSeek-R1-Zero is that they alternated instruction fine-\n\nmodels; they were generated by instruction fine-tuning Llama 3 and Qwen 2.5 models using instruction data from the DeepSeek-R1 model. This approach didn't use any RL for the reasoning part (however, RLHF was used to create the Llama 3 and Qwen 2.5 base models).\n\nFor more details on explaining the DeepSeek-R1 pipeline, please see my previous article \"Understanding Reasoning LLMs\":"
    },
    {
      "id": 14,
      "title": "Understanding Reasoning LLMs",
      "content": "# Understanding Reasoning LLMs\n\nThe main takeaway here is that the DeepSeek team didn't use an LLM-based reward model to train DeepSeek-R1-Zero. Instead, they used rule-based rewards for the reasoning training of DeepSeek-R1-Zero and DeepSeek-R1:\n\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process [...]\n\nTo train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:\n(1) Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.\n(2) Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between '<think>' and '</think>' tags.\n\nI realize that the introduction (i.e., everything up to this point) turned out to be much longer than I expected. Nonetheless, I think that this lengthy introduction is perhaps necessary to put the following lessons into context.\n\nAfter going through a large number of recent papers on reasoning models last month, I have put together a summary of the most interesting ideas and insights in this section. (References like \"[1]\" point to the corresponding papers listed at the end of the article.)"
    },
    {
      "id": 15,
      "title": "1. Reinforcement learning further improves distilled models",
      "content": "# 1. Reinforcement learning further improves distilled models\n\nThe original DeepSeek-R1 paper demonstrated clearly that supervised fine-tuning (SFT) followed by reinforcement learning (RL) outperforms RL alone.\n\nGiven this observation, it's intuitive that additional RL should further improve distilled models (as distilled models essentially represent models trained via SFT using reasoning examples generated by a larger model.)\n\n## Indeed, the DeepSeek team observed this phenomenon explicitly:\nAdditionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.\n\n## Several teams independently verified these observations:\n\n- [8] Using the 1.5B DeepSeek-R1-Distill-Qwen model, researchers demonstrated substantial performance improvements from RL fine-tuning with just 7,000 examples and a modest $\\$ 42$ compute budget. Impressively, this small model surpassed OpenAI's o1-preview on the AIME24 math benchmark.\n- [15] However, another team cautioned that these gains might not always be statistically significant. This suggests that, although RL can improve smaller\n\nAnnotated figure from A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility, https://arxiv.org/abs/2504.07086"
    },
    {
      "id": 16,
      "title": "2. The problem of long incorrect answers",
      "content": "# 2. The problem of long incorrect answers\n\nI previously mentioned that RL with verifiable rewards (RLVR) does not strictly require the GRPO algorithm; DeepSeek's GRPO simply happens to be efficient and to perform well.\n\nHowever, [12] showed that vanilla PPO paired with a basic binary correctness reward was sufficient to scale models in reasoning capability and response length.\n\nMore interestingly, both PPO and GRPO have a length bias. And several papers explored methods to tackle excessively long incorrect answers:\n\n- As a follow-up to the statement above, [7] [10] specifically identified length and difficulty-level biases in GRPO. The modified variant \"Dr. GRPO\" simplifies advantage calculations by removing length and standard deviation normalization, providing clearer training signals.\n- [1] Explicitly penalized lengthy incorrect answers in GRPO while rewarding concise, correct ones.\n- [3] [6] Didn't directly control response length in GRPO but found token-level rewards beneficial, allowing models to better focus on critical reasoning steps.\n- [5] Introduced explicit penalties in GRPO for responses exceeding specific lengths, enabling precise length control during inference."
    },
    {
      "id": 17,
      "title": "3. Emergent abilities from RL",
      "content": "# 3. Emergent abilities from RL\n\nemerged naturally during training without explicit instruction.\n[1] Showed that extending context lengths (up to 128k tokens) further improves the model's self-reflection and self-correction capabilities."
    },
    {
      "id": 18,
      "title": "4. Generalization beyond specific domains",
      "content": "# 4. Generalization beyond specific domains\n\nMost research efforts so far has focused on reasoning tasks in math or coding contexts. However, [4] demonstrated successful generalization by training models on logic puzzles. And models trained on logic puzzles also achieved strong performance in mathematical reasoning tasks. This is evidence for RL's ability to induce general reasoning behaviors independent of specific domain knowledge.\n\n## 5. Extensions to broader domains\n\nAs a follow-up to the section above, another interesting insight [11] is that reasoning capabilities can naturally extend beyond structured domains like math, code, and logic.\n\nModels successfully applied reasoning to areas including medicine, chemistry, psychology, economics, and education, leveraging generative soft-scoring methods to effectively handle free-form answers.\n\n## Notable next steps for reasoning models include:\n\n- Integrating existing reasoning models (e.g., o1, DeepSeek-R1) with capabilities such as external tool use and retrieval-augmented generation (RAG); the just-released o3 model from Open AI paves the way here\n- Speaking of tool-use and search, [9] showed that giving reasoning models the ability to search induces behaviors such as self-correction and robust generalization across benchmarks, despite minimal training datasets.\n\nBased on the hoops DeepSeek-R1 team went through in terms of maintaining the performance on knowledge-based tasks, I believe adding search abilities to\n\nThe fundamental claim behind DeepSeek-R1 (and R1-Zero) is that RLVR explicitly induces reasoning capabilities. However, recent findings [10] suggest that reasoning behaviors, including the \"Aha moment,\" might already be present in base models due to pre-training on extensive chain-of-thought data.\n\nMy recent comparisons between DeepSeek V3 base and R1 reinforce this observation, as the updated base model also demonstrates reasoning-like behaviors. For instance, the comparison between the original V3 and R1 models clearly shows the difference between a non-reasoning and a reasoning model:\n\n## However, this is no longer true when comparing the updated V3 base model to R1:\n\nAdditionally, [13] identified that self-reflection and self-correction behaviors emerge progressively throughout pre-training across various domains and model sizes. This further complicates the attribution of reasoning capabilities solely to RL methods.\n\nPerhaps the conclusion is that RL definitely turns simple base models into reasoning models. However, it's not the only way to induce or improve reasoning abilities. As the DeepSeek-R1 team showed, distillation also improves reasoning. And since distillation, in this paper, meant instruction fine-tuning on chain-ofthought data, it's likely that pre-training on data that includes chain-of-thought data induces these abilities as well. (As I explained in my book through hands-on code, pre-training and instruction fine-tuning are based on the same next-token prediction task and loss functions, after all.)"
    },
    {
      "id": 19,
      "title": "Noteworthy research papers on training reasoning models",
      "content": "# Noteworthy research papers on training reasoning models\n\nAfter reading through a large number of reasoning papers last month, I tried to summarize the most interesting takeaways in the previous section. However, for\n\nPlease note that this list is also not comprehensive (I capped it at 15), as this article is already more than too long!"
    },
    {
      "id": 20,
      "title": "[1] Scaling Reinforcement Learning (And Context Length)",
      "content": "# [1] Scaling Reinforcement Learning (And Context Length)\n\n## 22 Jan, Kimi k1.5: Scaling Reinforcement Learning with LLMs, https://arxiv.org/abs/2501.12599\n\nIt's interesting that this paper came out the same day as the DeepSeek-R1 paper! Here, the authors showcase a multi-modal LLM trained with RL. Similar to DeepSeek-R1, they didn't use process reward models (PRMs) but employed verifiable rewards. A PRM is a type of reward model used in RL (especially in LLM training) that evaluates not just the final answer but also the reasoning steps that led to it.\n\nAnother key idea here is that scaling the context length (up to 128k tokens) helps the model plan, reflect, and self-correct during reasoning. So, in addition to the correctness reward that is similar to DeepSeek-R1 they also have a length reward. Specifically, they promote shorter correct responses, and incorrect long answers get penalized more.\n\nAnd they propose a method called long2short to distill these long-chain-ofthought skills into more efficient short-CoT models. (It does this by distilling shorter correct responses from the long-CoT model using methods like model merging, shortest rejection sampling, DPO, and a 2nd round of RL with stronger length penalties.)"
    },
    {
      "id": 21,
      "title": "[2] Competitive Programming with Large Reasoning Models",
      "content": "# [2] Competitive Programming with Large Reasoning Models\n\n## 3 Feb, Competitive Programming with Large Reasoning Models, https://arxiv.org/abs/2502.06807\n\nThis paper from OpenAI evaluates their o-models (like o1, o1-ioi, and o3) on competitive programming tasks. While it doesn't go into the technical details of how RL was applied, it still offers some interesting takeaways.\n\nFirst, the models were trained using outcome-based RL, rather than processbased reward models. This is similar to approaches like DeepSeek-R1 and Kimi.\n\nOne of the interesting findings is that o3 can learn its own test-time (i.e., inference-time scaling) strategies. For example, it often writes a simple bruteforce version of a problem (something that trades efficiency for correctness) and then uses it to verify the outputs of its more optimized solution. This kind of strategy wasn't hand-coded; the model figured it out on its own.\n\n(earlier) models like o1-ioi relied on handcrafted test-time strategies like clustering thousands of samples and reranking them, which required a lot of manual design and tuning.\n\nAnnotated figure from Competitive Programming with Large Reasoning Models, https://arxiv.org/abs/2502.06807"
    },
    {
      "id": 22,
      "title": "[3] Exploring the Limit of Outcome Reward",
      "content": "# [3] Exploring the Limit of Outcome Reward\n\n10 Feb, Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning, https://arxiv.org/abs/2502.06781\n\nThis paper explores how far RL with just binary \"correct\" or \"wrong\" feedback (like in DeepSeek-R1) can go for solving math problems. To do this, they start by using Best-of-N sampling to collect positive examples and apply behavior cloning on them, which they show is theoretically enough to optimize the policy.\n\nTo deal with the challenge of sparse rewards (especially when long chains of thought include partially correct steps) they add a token-level reward model that"
    },
    {
      "id": 23,
      "title": "[4] LLM Reasoning with Rule-Based Reinforcement (On Logic Data)",
      "content": "# [4] LLM Reasoning with Rule-Based Reinforcement (On Logic Data)\n\n20 Feb, Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning, https://arxiv.org/abs/2502.14768\n\nDeepSeek-R1 focused on math and code tasks. This paper trains a 7B model using logic puzzles as the main training data.\n\n## The researchers adopt a similar rule-based RL setup as DeepSeek-R1 but make several adjustments:\n\n1. They introduce a strict format reward that penalizes shortcuts and ensures the model separates its reasoning from its final answer using <think> and <answer> tags.\n\nEven with only 5K synthetic logic problems, the model develops good reasoning skills that generalize well to harder math benchmarks like AIME and AMC.\n\nThis is particularly interesting because it shows that logic-based RL training can teach models to reason in ways that transfer beyond the original domain.\n\nAnnotated figure from Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning, https://arxiv.org/abs/2502.14768"
    },
    {
      "id": 24,
      "title": "[5] Controlling How Long A Reasoning Model Thinks",
      "content": "# [5] Controlling How Long A Reasoning Model Thinks\n\n## 6 Mar, L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning, https://arxiv.org/abs/2503.04697\n\nOne hallmark of reasoning models is that they tend to generate longer outputs because of chain-of-thought reasoning. But by default, there is no explicit way to control how long the responses are.\n\nThis paper introduces Length Controlled Policy Optimization (LCPO), a simple reinforcement learning method that helps models to adhere to user-specified\n\n## Control\" implemented as\nreward = reward_correctness - $\\alpha$ * |target_length - actual_length|\nwhere the target length is provided as part of the user prompt. This LCPO method above encourages the model to adhere to the provided target length exactly.\n\nIn addition, they also introduce an LCPO-Max variant, which, instead of encouraging the model to match the target length exactly, encourages the model to stay below a maximum token length:\n\n```\nreward = reward_correctness * clip(Î± * (target_length - actual_length) +\n6, 0, 1)\n```\n\nThe authors train a 1.5B model called L1 using LCPO, which can adjust its output length based on the prompt. This lets users trade-off between accuracy and compute, depending on the task. Interestingly, the paper also finds that these long-chain models actually become surprisingly good at short reasoning too, even outperforming much larger models like GPT-40 at the same token lengths."
    },
    {
      "id": 25,
      "title": "[6] Incentivizing the Search Capability in LLMs",
      "content": "# [6] Incentivizing the Search Capability in LLMs\n\n10 Mar, R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning, https://arxiv.org/abs/2503.05592\n\nReasoning models like DeepSeek-R1 that have been trained with RL rely on their internal knowledge. The authors here focus on improving these models on knowledge-based tasks that require more time-sensitive or recent information by adding access to external search systems.\n\nSo, this paper improves these models by teaching them to use external search systems during the reasoning process. Instead of relying on test-time strategies or supervised training, the authors use a two-stage reinforcement learning method that helps the model learn how and when to search on its own. The model first learns the search format, and then learns how to use search results to find correct answers."
    },
    {
      "id": 26,
      "title": "[7] Open-Source LLM Reinforcement Learning at Scale",
      "content": "# [7] Open-Source LLM Reinforcement Learning at Scale\n\n18 Mar, DAPO: An Open-Source LLM Reinforcement Learning System at Scale, https://arxiv.org/abs/2503.14476\n\nWhile this paper is mainly about developing a DeepSeek-R1-like training pipeline and open-sourcing it, it also proposes interesting improvements to the GRPO algorithm that was used in DeepSeek-R1 training.\n\n1. Clip-higher: Increases the upper bound of the PPO clipping range to encourage exploration and prevent entropy collapse during training.\n2. Dynamic sampling: Improves training efficiency by filtering out prompts where all sampled responses are either always correct or always wrong.\n3. Token-level policy gradient loss: moves from sample-level to token-level loss calculation so that longer responses can have more influence on the gradient update.*\n\n* Standard GRPO uses a sample-level loss calculation. This involves first averaging the loss over the tokens for each sample and then averaging the loss over the samples. Since the samples have equal weight, the tokens in samples with longer responses may disproportionally contribute less to the overall loss. At the same time, researchers observed that longer responses often contain gibberish before the final answer, and this gibberish wouldn't be sufficiently penalized in the original GRPO sample-level loss calculation.\n\nreasoning models further with RL.\nSo, using the 1.5B DeepSeek-R1-Distill-Qwen model, they find that with only 7000 training examples and a $\\$ 42$ compute budget, RL fine-tuning can lead to strong improvements. In this case, the improvements are enough to outperform OpenAI's o1-preview on the AIME24 math benchmark, for example.\n\n## Furthermore, there were 3 interesting learnings in that paper:\n\n1. Small LLMs can achieve fast reasoning improvements within the first 50-100 training steps using a compact, high-quality dataset. But the performance quickly drops if training continues too long, mainly due to length limits and output instability.\n2. Mixing easier and harder problems helps the model produce shorter, more stable responses early in training. However, performance still degrades over time.\n3. Using a cosine-shaped reward function helps control output length more effectively and improves training consistency. But this slightly reduces peak performance compared to standard accuracy-based rewards.\n\nThe ReSearch framework proposed in this paper extends the RL method from the DeepSeek-R1 paper to include search results as part of the reasoning process. The model learns when and how to search based on its ongoing reasoning chain, and it then uses the retrieved information for the next steps of reasoning.\n\nThis is all done without supervised data on reasoning steps. The researchers also show that this approach can lead to useful behaviors like self-correction and reflection, and that it generalizes well across multiple benchmarks despite being trained on just one dataset."
    },
    {
      "id": 27,
      "title": "Annotated figure from ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning, https://arxiv.org/abs/2503.19470",
      "content": "# Annotated figure from ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning, https://arxiv.org/abs/2503.19470\n\n## PS: How does this method differ from the R1-Searcher discussed earlier?\nR1-Searcher uses a two-stage, outcome-based reinforcement learning approach. In the first stage, it teaches the model how to invoke external retrieval; in the second, it learns to use the retrieved information to answer questions.\n\nReSearch, in contrast, integrates search directly into the reasoning process. It trains the model end-to-end using reinforcement learning, without any"
    },
    {
      "id": 28,
      "title": "[10] Understanding R1-Zero-Like Training",
      "content": "# [10] Understanding R1-Zero-Like Training\n\n26 Mar, Understanding R1-Zero-Like Training: A Critical Perspective, https://arxiv.org/abs/2503.20783\n\nThis paper investigates why DeepSeek-R1-Zero's pure RL approach works to improve reasoning.\n\nThe authors find that some base models like Qwen2.5 already show strong reasoning and even the \"Aha moment\" without any RL. So the \"Aha moment\" might not be induced by RL, but instead inherited from pre-training. This challenges the idea that RL alone is what creates deep reasoning behaviors.\n\n## The paper also identifies two biases in GRPO:\n\n1. Response-length bias: GRPO divides the advantage by the length of the response. This makes long incorrect answers get smaller penalties, so the model learns to generate longer bad answers.\n2. Difficulty-level bias: GRPO also normalizes by the standard deviation of rewards for each question. Easy or hard questions (with low reward variance) get overweighted.\n\nTo fix this, the authors introduce Dr. GRPO, which is a modification of standard GRPO. Here, they get rid of the response length normalization in the advantage computation. Also, they get rid of the question-level standard deviation. This will result in more efficient training and fewer unnecessary long answers. Especially if the model is wrong, generating a long answer is no longer encouraged.\n\n## [11] Expanding RL with Verifiable Rewards Across Diverse Domains\n\n31 Mar, Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains, https://arxiv.org/abs/2503.23829\n\npsychology, economics, and education, where answers are usually free-form and harder to verify (beyond a simple correct/incorrect).\n\nThe authors find that using expert-written reference answers makes evaluation more feasible than expected, even in these broader domains. To provide reward signals, they introduce a generative, soft-scoring method without needing heavy domain-specific annotation.\n\nAnnotated figure from Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains, https://arxiv.org/abs/2503.23829"
    },
    {
      "id": 29,
      "title": "[12] Scaling Up Reinforcement Learning (With a Simple Setup)",
      "content": "# [12] Scaling Up Reinforcement Learning (With a Simple Setup)\n\n31 Mar, Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model, https://arxiv.org/abs/2503.24290\n\nincluded in RLHF pipelines.\nInterestingly, they find that this simple setup (vanilla PPO and a basic binary reward function based on answer correctness) is sufficient to train models that scale up in both reasoning performance and response length.\n\nUsing the same Qwen-32B base as DeepSeek-R1-Zero, their model outperforms it on multiple reasoning benchmarks while requiring only $1 / 10$ the training steps.\n\nBased on the interesting insights from the DeepSeek-R1 paper, namely applying pure RL to a base model, we think that reasoning abilities in LLMs emerge from RL. This paper provides a bit of a plot twist, saying that self-correction already appears earlier during pre-training.\n\nConcretely, by introducing deliberately flawed chains-of-thought into tasks, the authors measure whether models can identify and correct these errors. They find that both explicit and implicit forms of reflection emerge steadily throughout pretraining. This happens across many domains and model sizes. Even relatively early checkpoints show signs of self-correction, and the ability becomes stronger as pre-training compute increases.\n\nAs we all know by now, reasoning models often generate longer responses, which raises compute costs. Now, this new paper shows that this behavior comes from the RL training process, not from an actual need for long answers for better accuracy. The RL loss tends to favor longer responses when the model gets negative rewards, which I think explains the \"aha\" moments and longer chains of thought that arise from pure RL training.\nI.e., if the model gets a negative reward (i.e., the answer is wrong), the math behind PPO causes the average per-token loss becomes smaller when the response is longer. So, the model is indirectly encouraged to make its responses longer. This is true even if those extra tokens don't actually help solve the problem.\n\nWhat does the response length have to do with the loss? When the reward is negative, longer responses can dilute the penalty per individual token, which results in lower (i.e., better) loss values (even though the model is still getting the answer wrong).\n\nSo the model \"learns\" that longer responses reduce the punishment, even though they are not helping correctness.\n\n## However, it's important to emphasize that this analysis was done for PPO:\nOf note, our current analysis is not applicable to GRPO, and a precise analysis of such methods is left for future work.\n\nIn addition, the researchers show that a second round of RL (using just a few problems that are sometimes solvable) can shorten responses while preserving or even improving accuracy. This has big implications for deployment efficiency."
    },
    {
      "id": 30,
      "title": "[15] A Sober Look at Progress in Language Model Reasoning",
      "content": "# [15] A Sober Look at Progress in Language Model Reasoning\n\n## 9 Apr, A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility, https://arxiv.org/abs/2504.07086\n\nThis paper takes a closer look at recent claims that RL can improve distilled language models, like those based on DeepSeek-R1.\n\nFor instance, I previously discussed the \"20 Mar, Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't\" paper that found RL is effective for distilled models.\n\n## And also the DeepSeek-R1 paper mentioned\nAdditionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.\n\nSo, while earlier papers reported large performance boosts from RL, this work finds that many of those improvements might just be noise. The authors show\n\nWhen RL models are evaluated under more controlled and standardized setups, the gains turn out to be much smaller than originally reported, and often not statistically significant. However, some models trained with RL do show modest improvements, but these are usually weaker than what supervised fine-tuning achieves, and they often don't generalize well to new benchmarks.\n\nSo, while RL might help in some cases to improve smaller distilled models, this paper argues that its benefits have been overstated and better evaluation standards are needed to understand what's actually working.\n\nAnnotated figure from A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility, https://arxiv.org/abs/2504.07086\n\nThis magazine is a personal passion project. To support me as an independent researcher, please consider purchasing a copy of my book, Build a Large Language"
    },
    {
      "id": 31,
      "title": "Build a Large Language Model (From Scratch) now available on Amazon",
      "content": "# Build a Large Language Model (From Scratch) now available on Amazon\n\nIf you read the book and have a few minutes to spare, I'd really appreciate a brief review. It helps us authors a lot!\n\n## Your support means a great deal! Thank you!\n\n## Subscribe to Ahead of AI\n\n## By Sebastian Raschka $\\cdot$ Hundreds of paid subscribers\nAhead of AI specializes in Machine Learning \\& AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\n\n## Type your email...\n\n## Subscribe\n\nBy subscribing, I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy."
    },
    {
      "id": 32,
      "title": "Comments Restacks",
      "content": "# Comments Restacks\n\nWrite a comment...\n\n## Xavier Apr 20\n\n- Liked by Sebastian Raschka, PhD\n\nGreat stuff, as always. The inline cards linking to previous articles are very handy. $\\Delta$ A\n$\\nabla$ LIKE (3) $\\bigcirc$ REPLY\n## (1) SHARE\n\n1 reply by Sebastian Raschka, PhD\n\n## Zia Khan Apr 19\n\n- Liked by Sebastian Raschka, PhD\n\nWow. Great article! You saved me hours of reading. Thank you!\n$\\nabla$ LIKE (3) $\\bigcirc$ REPLY\n## (1) SHARE\n\n29 more comments...\n\n## Top Latest Discussions\n\n## Understanding Reasoning LLMs\n\n## Methods and Strategies for Building and Refining Reasoning Models\n## FEB 5 $\\cdot$ SEBASTIAN RASCHKA, PHD\n$\\nabla 1,058$\n$\\bigcirc 40$\n$\\square$\n\nThis article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4...\n## (4) JAN 14, 2024\n$\\nabla 374$\n## D 41\n$\\varnothing$\n(1)\n\n## See all >"
    },
    {
      "id": 33,
      "title": "Ready for more?",
      "content": "# Ready for more?\n\nType your email...\n## Subscribe\n(c) 2025 Raschka AI Research (RAIR) Lab LLC $\\cdot$ Privacy $\\cdot$ Terms $\\cdot$ Collection notice\n\n## Start writing Get the app\n\n## Substack is the home for great culture"
    }
  ],
  "pdf_images": {
    "pdf_path": "inbox/raschka_reasoning.pdf",
    "total_pages": 50,
    "pages": [
      {
        "page_number": 1,
        "filename": "page-001.png",
        "thumb_filename": "page-001-thumb.png",
        "mobile_filename": "page-001-mobile.png",
        "mobile_thumb_filename": "page-001-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 2,
        "filename": "page-002.png",
        "thumb_filename": "page-002-thumb.png",
        "mobile_filename": "page-002-mobile.png",
        "mobile_thumb_filename": "page-002-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 3,
        "filename": "page-003.png",
        "thumb_filename": "page-003-thumb.png",
        "mobile_filename": "page-003-mobile.png",
        "mobile_thumb_filename": "page-003-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 4,
        "filename": "page-004.png",
        "thumb_filename": "page-004-thumb.png",
        "mobile_filename": "page-004-mobile.png",
        "mobile_thumb_filename": "page-004-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 5,
        "filename": "page-005.png",
        "thumb_filename": "page-005-thumb.png",
        "mobile_filename": "page-005-mobile.png",
        "mobile_thumb_filename": "page-005-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 6,
        "filename": "page-006.png",
        "thumb_filename": "page-006-thumb.png",
        "mobile_filename": "page-006-mobile.png",
        "mobile_thumb_filename": "page-006-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 7,
        "filename": "page-007.png",
        "thumb_filename": "page-007-thumb.png",
        "mobile_filename": "page-007-mobile.png",
        "mobile_thumb_filename": "page-007-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 8,
        "filename": "page-008.png",
        "thumb_filename": "page-008-thumb.png",
        "mobile_filename": "page-008-mobile.png",
        "mobile_thumb_filename": "page-008-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 9,
        "filename": "page-009.png",
        "thumb_filename": "page-009-thumb.png",
        "mobile_filename": "page-009-mobile.png",
        "mobile_thumb_filename": "page-009-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 10,
        "filename": "page-010.png",
        "thumb_filename": "page-010-thumb.png",
        "mobile_filename": "page-010-mobile.png",
        "mobile_thumb_filename": "page-010-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 11,
        "filename": "page-011.png",
        "thumb_filename": "page-011-thumb.png",
        "mobile_filename": "page-011-mobile.png",
        "mobile_thumb_filename": "page-011-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 12,
        "filename": "page-012.png",
        "thumb_filename": "page-012-thumb.png",
        "mobile_filename": "page-012-mobile.png",
        "mobile_thumb_filename": "page-012-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 13,
        "filename": "page-013.png",
        "thumb_filename": "page-013-thumb.png",
        "mobile_filename": "page-013-mobile.png",
        "mobile_thumb_filename": "page-013-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 14,
        "filename": "page-014.png",
        "thumb_filename": "page-014-thumb.png",
        "mobile_filename": "page-014-mobile.png",
        "mobile_thumb_filename": "page-014-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 15,
        "filename": "page-015.png",
        "thumb_filename": "page-015-thumb.png",
        "mobile_filename": "page-015-mobile.png",
        "mobile_thumb_filename": "page-015-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 16,
        "filename": "page-016.png",
        "thumb_filename": "page-016-thumb.png",
        "mobile_filename": "page-016-mobile.png",
        "mobile_thumb_filename": "page-016-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 17,
        "filename": "page-017.png",
        "thumb_filename": "page-017-thumb.png",
        "mobile_filename": "page-017-mobile.png",
        "mobile_thumb_filename": "page-017-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 18,
        "filename": "page-018.png",
        "thumb_filename": "page-018-thumb.png",
        "mobile_filename": "page-018-mobile.png",
        "mobile_thumb_filename": "page-018-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 19,
        "filename": "page-019.png",
        "thumb_filename": "page-019-thumb.png",
        "mobile_filename": "page-019-mobile.png",
        "mobile_thumb_filename": "page-019-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 20,
        "filename": "page-020.png",
        "thumb_filename": "page-020-thumb.png",
        "mobile_filename": "page-020-mobile.png",
        "mobile_thumb_filename": "page-020-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 21,
        "filename": "page-021.png",
        "thumb_filename": "page-021-thumb.png",
        "mobile_filename": "page-021-mobile.png",
        "mobile_thumb_filename": "page-021-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 22,
        "filename": "page-022.png",
        "thumb_filename": "page-022-thumb.png",
        "mobile_filename": "page-022-mobile.png",
        "mobile_thumb_filename": "page-022-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 23,
        "filename": "page-023.png",
        "thumb_filename": "page-023-thumb.png",
        "mobile_filename": "page-023-mobile.png",
        "mobile_thumb_filename": "page-023-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 24,
        "filename": "page-024.png",
        "thumb_filename": "page-024-thumb.png",
        "mobile_filename": "page-024-mobile.png",
        "mobile_thumb_filename": "page-024-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 25,
        "filename": "page-025.png",
        "thumb_filename": "page-025-thumb.png",
        "mobile_filename": "page-025-mobile.png",
        "mobile_thumb_filename": "page-025-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 26,
        "filename": "page-026.png",
        "thumb_filename": "page-026-thumb.png",
        "mobile_filename": "page-026-mobile.png",
        "mobile_thumb_filename": "page-026-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 27,
        "filename": "page-027.png",
        "thumb_filename": "page-027-thumb.png",
        "mobile_filename": "page-027-mobile.png",
        "mobile_thumb_filename": "page-027-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 28,
        "filename": "page-028.png",
        "thumb_filename": "page-028-thumb.png",
        "mobile_filename": "page-028-mobile.png",
        "mobile_thumb_filename": "page-028-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 29,
        "filename": "page-029.png",
        "thumb_filename": "page-029-thumb.png",
        "mobile_filename": "page-029-mobile.png",
        "mobile_thumb_filename": "page-029-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 30,
        "filename": "page-030.png",
        "thumb_filename": "page-030-thumb.png",
        "mobile_filename": "page-030-mobile.png",
        "mobile_thumb_filename": "page-030-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 31,
        "filename": "page-031.png",
        "thumb_filename": "page-031-thumb.png",
        "mobile_filename": "page-031-mobile.png",
        "mobile_thumb_filename": "page-031-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 32,
        "filename": "page-032.png",
        "thumb_filename": "page-032-thumb.png",
        "mobile_filename": "page-032-mobile.png",
        "mobile_thumb_filename": "page-032-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 33,
        "filename": "page-033.png",
        "thumb_filename": "page-033-thumb.png",
        "mobile_filename": "page-033-mobile.png",
        "mobile_thumb_filename": "page-033-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 34,
        "filename": "page-034.png",
        "thumb_filename": "page-034-thumb.png",
        "mobile_filename": "page-034-mobile.png",
        "mobile_thumb_filename": "page-034-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 35,
        "filename": "page-035.png",
        "thumb_filename": "page-035-thumb.png",
        "mobile_filename": "page-035-mobile.png",
        "mobile_thumb_filename": "page-035-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 36,
        "filename": "page-036.png",
        "thumb_filename": "page-036-thumb.png",
        "mobile_filename": "page-036-mobile.png",
        "mobile_thumb_filename": "page-036-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 37,
        "filename": "page-037.png",
        "thumb_filename": "page-037-thumb.png",
        "mobile_filename": "page-037-mobile.png",
        "mobile_thumb_filename": "page-037-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 38,
        "filename": "page-038.png",
        "thumb_filename": "page-038-thumb.png",
        "mobile_filename": "page-038-mobile.png",
        "mobile_thumb_filename": "page-038-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 39,
        "filename": "page-039.png",
        "thumb_filename": "page-039-thumb.png",
        "mobile_filename": "page-039-mobile.png",
        "mobile_thumb_filename": "page-039-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 40,
        "filename": "page-040.png",
        "thumb_filename": "page-040-thumb.png",
        "mobile_filename": "page-040-mobile.png",
        "mobile_thumb_filename": "page-040-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 41,
        "filename": "page-041.png",
        "thumb_filename": "page-041-thumb.png",
        "mobile_filename": "page-041-mobile.png",
        "mobile_thumb_filename": "page-041-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 42,
        "filename": "page-042.png",
        "thumb_filename": "page-042-thumb.png",
        "mobile_filename": "page-042-mobile.png",
        "mobile_thumb_filename": "page-042-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 43,
        "filename": "page-043.png",
        "thumb_filename": "page-043-thumb.png",
        "mobile_filename": "page-043-mobile.png",
        "mobile_thumb_filename": "page-043-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 44,
        "filename": "page-044.png",
        "thumb_filename": "page-044-thumb.png",
        "mobile_filename": "page-044-mobile.png",
        "mobile_thumb_filename": "page-044-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 45,
        "filename": "page-045.png",
        "thumb_filename": "page-045-thumb.png",
        "mobile_filename": "page-045-mobile.png",
        "mobile_thumb_filename": "page-045-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 46,
        "filename": "page-046.png",
        "thumb_filename": "page-046-thumb.png",
        "mobile_filename": "page-046-mobile.png",
        "mobile_thumb_filename": "page-046-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 47,
        "filename": "page-047.png",
        "thumb_filename": "page-047-thumb.png",
        "mobile_filename": "page-047-mobile.png",
        "mobile_thumb_filename": "page-047-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 48,
        "filename": "page-048.png",
        "thumb_filename": "page-048-thumb.png",
        "mobile_filename": "page-048-mobile.png",
        "mobile_thumb_filename": "page-048-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 49,
        "filename": "page-049.png",
        "thumb_filename": "page-049-thumb.png",
        "mobile_filename": "page-049-mobile.png",
        "mobile_thumb_filename": "page-049-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      },
      {
        "page_number": 50,
        "filename": "page-050.png",
        "thumb_filename": "page-050-thumb.png",
        "mobile_filename": "page-050-mobile.png",
        "mobile_thumb_filename": "page-050-mobile-thumb.png",
        "width": 1275,
        "height": 1650,
        "mobile_width": 2550,
        "mobile_height": 3300
      }
    ]
  },
  "metadata": {
    "model": "gpt-4o",
    "pdf_path": "inbox/raschka_reasoning.pdf"
  },
  "attribution": null
}